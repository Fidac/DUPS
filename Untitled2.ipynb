{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flask_restful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, XLNetConfig, XLNetModel, XLNetTokenizer\n",
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class BERTComponent:\n",
    "    tokenizer = None\n",
    "    bert_model = None\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.bert_vector_size = 3072\n",
    "        self.sent_vector_size = 768\n",
    "        self.model = model\n",
    "        print(\"Tokenizer: \", BERTComponent.tokenizer)\n",
    "        self.tokenizer = BERTComponent.tokenizer if BERTComponent.tokenizer else BertTokenizer.from_pretrained(model)\n",
    "        BERTComponent.tokenizer = self.tokenizer\n",
    "        self.bert_model = BERTComponent.bert_model if BERTComponent.bert_model else BertModel.from_pretrained(model)\n",
    "        BERTComponent.bert_model = self.bert_model\n",
    "        self.bert_model.eval()\n",
    "\n",
    "\n",
    "    def get_bert_spans(self, words, bert_tokens):\n",
    "        if self.model == 'bert-base-uncased':\n",
    "            words = [self._flat_word(word) for word in words]\n",
    "\n",
    "        i = 0\n",
    "        j = 1\n",
    "        idx = 0\n",
    "\n",
    "        bert_words_indexes = []\n",
    "        bert_words = []\n",
    "        while i < len(words):\n",
    "            word = words[i]\n",
    "\n",
    "            bert_word = bert_tokens[j]\n",
    "            bert_word = bert_word[2:] if bert_word.startswith(\"##\") else bert_word\n",
    "            bert_word = bert_word[idx:]\n",
    "\n",
    "            #Spacing control\n",
    "            if word in [\" \", \"  \", \"   \"]:\n",
    "                bert_words.append([word])\n",
    "                bert_words_indexes.append([-1])\n",
    "\n",
    "            #When the current word is [UNK] for bert\n",
    "            elif bert_word == \"[UNK]\":\n",
    "                bert_words.append([\"[UNK]\"])\n",
    "                bert_words_indexes.append([j])\n",
    "                j += 1\n",
    "                idx = 0\n",
    "\n",
    "            #When the current word is contained in bert token. Very weird\n",
    "            elif len(word) < len(bert_word) and bert_word.find(word) >= 0:\n",
    "                bert_words.append([bert_word])\n",
    "                bert_words_indexes.append([j])\n",
    "\n",
    "                idx = bert_word.find(word) + len(word)\n",
    "                if idx == len(bert_word):\n",
    "                    j += 1\n",
    "                    idx = 0\n",
    "\n",
    "            #Otherwise\n",
    "            else:\n",
    "                k = 0\n",
    "                span = []\n",
    "                span_indexes = []\n",
    "\n",
    "                while k < len(word):\n",
    "                    if word.find(bert_word, k) == k:\n",
    "                        span.append(bert_word)\n",
    "                        span_indexes.append(j)\n",
    "                        k += len(bert_word)\n",
    "                        j += 1\n",
    "                        idx = 0\n",
    "                        bert_word = bert_tokens[j]\n",
    "                        bert_word = bert_word[2:] if bert_word.startswith(\"##\") else bert_word\n",
    "                    else:\n",
    "                        print(\"Error\")\n",
    "                        return bert_words, bert_words_indexes\n",
    "\n",
    "                bert_words.append(span)\n",
    "                bert_words_indexes.append(span_indexes)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        assert len(bert_words_indexes) == len(words)\n",
    "\n",
    "        return bert_words, bert_words_indexes\n",
    "\n",
    "    def _flat_word(self, word):\n",
    "        word = word.lower()\n",
    "        word = word.replace(\"ñ\", \"n\")\n",
    "        word = word.replace(\"á\", \"a\")\n",
    "        word = word.replace(\"é\", \"e\")\n",
    "        word = word.replace(\"í\", \"i\")\n",
    "        word = word.replace(\"ó\", \"o\")\n",
    "        word = word.replace(\"ú\", \"u\")\n",
    "        word = word.replace(\"ä\", \"a\")\n",
    "        word = word.replace(\"ü\", \"u\")\n",
    "        word = word.replace(\"ö\", \"o\")\n",
    "        word = word.replace(\"ū\", \"u\")\n",
    "        word = word.replace(\"ā\", \"a\")\n",
    "        word = word.replace(\"ī\", \"i\")\n",
    "        word = word.replace(\"ș\", \"s\")\n",
    "        word = word.replace(\"ã\", \"a\")\n",
    "        word = word.replace(\"ô\", \"o\")\n",
    "\n",
    "        return word\n",
    "\n",
    "    def _sum_merge(self, vectors):\n",
    "        return torch.sum(torch.stack(vectors), dim=0)\n",
    "\n",
    "    def _mean_merge(self, vectors):\n",
    "        return torch.mean(torch.stack(vectors), dim=0)\n",
    "\n",
    "    def _last_merge(self, vectors):\n",
    "        return vectors[-1]\n",
    "\n",
    "    def _get_merge_tensors(self, token_vec_sums, words_indexes):\n",
    "        pad_tensor = torch.zeros(self.bert_vector_size)\n",
    "        real_vec = []\n",
    "        for word_indexes in words_indexes:\n",
    "            vectors = [(token_vec_sums[idx] if idx != -1 else pad_tensor) for idx in word_indexes]\n",
    "            real_vec.append(self._mean_merge(vectors))\n",
    "\n",
    "        return real_vec\n",
    "\n",
    "    def get_bert_embeddings(self, sentence, spans):\n",
    "        tokenized_sentence = self.tokenizer.tokenize(sentence)\n",
    "        tokenized_sentence = ['[CLS]'] + tokenized_sentence + ['[SEP]']\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "        segments_ids = [1] * len(tokenized_sentence)\n",
    "\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoded_layers = self.bert_model(tokens_tensor, segments_tensors, output_hidden_states=True)\n",
    "\n",
    "        #print(\"This is enconded layers: \", len(encoded_layers.hidden_states))\n",
    "        \n",
    "        token_embeddings = torch.stack(encoded_layers.hidden_states, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "        token_vec_sums = []\n",
    "        for token in token_embeddings:\n",
    "            cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=-1)\n",
    "            token_vec_sums.append(cat_vec)\n",
    "\n",
    "        words = [sentence[beg:end] for (beg, end) in spans]\n",
    "        bert_words, bert_words_indexes = self.get_bert_spans(words, tokenized_sentence)\n",
    "\n",
    "        bert_embeddings = self._get_merge_tensors(token_vec_sums, bert_words_indexes)\n",
    "        sentence_embedding = torch.mean(torch.stack(token_vec_sums), dim=0)\n",
    "        \n",
    "        return bert_embeddings, sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer as twt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "class DocumentRanker:\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        \n",
    "#         self.bert = BERTComponent('bert-large-cased')\n",
    "        #self.__model = SentenceTransformer('bert-base-cased')\n",
    "        self.__model = model\n",
    "    \n",
    "    def setDocuments(self, documents):\n",
    "        self.documents = documents\n",
    "    \n",
    "    def __get_info_rep(self, document):\n",
    "        pass\n",
    "    \n",
    "    def __get_embedding(self, text):\n",
    "#         spans = twt().span_tokenize(text)\n",
    "#         text_word_embeddings, text_embedding = self.bert.get_bert_embeddings(text, spans)\n",
    "#         return text_embedding\n",
    "        return self.__model.encode(text)\n",
    "    \n",
    "    def get_clustered_documents(self):\n",
    "        \n",
    "        clusters = {}\n",
    "        centroids = {}\n",
    "        clusterNumber = 1\n",
    "        clusters[clusterNumber] = [(self.documents[0]['id'],self.documents[0]['embedding'])]\n",
    "        centroids[clusterNumber] = self.documents[0]['embedding']\n",
    "        \n",
    "        for i in range(1, len(self.documents)):\n",
    "            document = self.documents[i]\n",
    "            inserted = False\n",
    "            for j in range(1, clusterNumber + 1):\n",
    "                centroid = centroids[clusterNumber]\n",
    "                score = cosine_similarity([centroid], [document['embedding']])[0][0]\n",
    "                #print(\"SCORE: \", score)\n",
    "                if score > 0.9:\n",
    "                    inserted = True\n",
    "                    clusters[j].append((document['id'], document['embedding']))\n",
    "                    centroids[j] = document['embedding']\n",
    "                    break\n",
    "            \n",
    "            if(not inserted):\n",
    "                clusterNumber += 1\n",
    "                clusters[clusterNumber] = [(document['id'],document['embedding'])]\n",
    "                centroids[clusterNumber] = document['embedding']\n",
    "        \n",
    "        return list(clusters.items())\n",
    "    \n",
    "    def get_related_documents(self, query):\n",
    "        index = {}\n",
    "        embeddings = {}\n",
    "        last = 0\n",
    "        related_documents = []\n",
    "        \n",
    "        q_sent_embedding = self.__get_embedding(query)\n",
    "        \n",
    "        for document in self.documents:\n",
    "            abstract = document['title'] + \" \" + document['abstractText'] + \". \"\n",
    "#             for keyword in document['authorKeywords']:\n",
    "#                 abstract += keyword['keyword']\n",
    "            if(document['embedding'] is not None and len(document['embedding']) > 0):\n",
    "                #print(\"USING SAVED EMBEEDING\")\n",
    "                #abstract_embedding = torch.FloatTensor(document['embedding'])\n",
    "                abstract_embedding = np.asarray(document['embedding'], dtype=np.float32)\n",
    "            else:\n",
    "                #print(\"Computing new Embedding\")\n",
    "                abstract_embedding = self.__get_embedding(abstract)\n",
    "                #print(\"Embedding Shape: \", abstract_embedding.shape)\n",
    "                document['embedding'] = abstract_embedding\n",
    "            #print(\"This is the embedding type: \", type(abstract_embedding))\n",
    "            #print(\"This is the embedding: \", abstract_embedding)\n",
    "            #index[last] = torch.dot(q_sent_embedding, abstract_embedding)\n",
    "            index[document['id']] = (cosine_similarity([q_sent_embedding], [abstract_embedding])[0][0], abstract_embedding.tolist())\n",
    "#             embeddings[document['id']] = abstract_embedding.toList()\n",
    "#             last += 1\n",
    "        \n",
    "        doc_scores = list(index.items())\n",
    "        doc_scores = [(x[0], x[1][0].tolist(), x[1][1]) for x in doc_scores]\n",
    "        doc_scores= sorted(doc_scores, key = lambda x: x[1], reverse=True)\n",
    "        return doc_scores\n",
    "        #print(\"Scores: \", scores)\n",
    "#         probs = F.softmax(scores, dim=0)\n",
    "#         probs = [t.tolist() for t in probs]\n",
    "#         probs.sort(reverse=True)\n",
    "        #print(\"Probs: \", probs)\n",
    "        print(doc_scores)\n",
    "        \n",
    "#         if number_of_documents > len(doc_scores):\n",
    "#             return doc_scores\n",
    "#         else:\n",
    "#             return doc_scores[:number_of_documents]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name C:\\Users\\ernes/.cache\\torch\\sentence_transformers\\bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\ernes/.cache\\torch\\sentence_transformers\\bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "from flask_restful import Resource, Api, reqparse\n",
    "from flask import request\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "\n",
    "\n",
    "model = SentenceTransformer('bert-base-cased')\n",
    "app = Flask(\"NlpApp\")\n",
    "api = Api(app)\n",
    "\n",
    "# class ResponseDTO:\n",
    "    \n",
    "\n",
    "\n",
    "class NLP(Resource):\n",
    "    # def get(self):\n",
    "    #     data = pd.read_csv('users.csv')  # read CSV\n",
    "    #     data = data.to_dict()  # convert dataframe to dictionary\n",
    "    #     return {'data': data}, 200  # return data and 200 OK code\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ranker = DocumentRanker(model)\n",
    "\n",
    "    def post(self):        \n",
    "        #args = parser.parse_args()  # parse arguments to dictionary\n",
    "        args = request.get_json()\n",
    "        #args = request.args.get('query')\n",
    "        print(args)\n",
    "        calculateClusters = args['calculateClusters']\n",
    "        query=args['query']\n",
    "        print(query)\n",
    "#         args['articles'] = args['articles'].replace(\"\\'\", \"\\\"\")\n",
    "        print(args['articles'])\n",
    "        documents = args['articles']\n",
    "\n",
    "        self.ranker.setDocuments(documents)\n",
    "        results = self.ranker.get_related_documents(query)\n",
    "\n",
    "        if(calculateClusters):\n",
    "            clusterResults = self.ranker.get_clustered_documents()\n",
    "\n",
    "        response = []\n",
    "\n",
    "        if(calculateClusters):\n",
    "            for result in results:\n",
    "                clusterNumber = 0\n",
    "                for clusterResult in clusterResults:\n",
    "                    #print(\"CCLCLCLC:\", clusterResult)\n",
    "                    #print(\"AAAAAA: \",clusterResult[1], result[0])\n",
    "                    #print(\"BBBBBB:\", [x[1] for x in clusterResult])\n",
    "                    if(result[0] in [x[0] for x in clusterResult[1]]):\n",
    "                        clusterNumber = clusterResult[0]\n",
    "                        break\n",
    "\n",
    "                response.append({\"id\": result[0], \"weight\": result[1], \"embedding\": result[2], \"clusterId\": clusterNumber})\n",
    "\n",
    "        else:\n",
    "            for result in results:\n",
    "                response.append({\"id\": result[0], \"weight\": result[1], \"embedding\": result[2]})\n",
    "\n",
    "        # result = pd.DataFrame({\n",
    "        #     'userId': args['userId'],\n",
    "        #     'name': args['name'],\n",
    "        #     'city': args['city'],\n",
    "        #     'locations': [[]]\n",
    "        # })\n",
    "\n",
    "        #print(\"RESPONSE: \", response)\n",
    "\n",
    "        return response, 200  # return data with 200 OK\n",
    "\n",
    "    \n",
    "# class NLPCluster(Resource):\n",
    "#     # def get(self):\n",
    "#     #     data = pd.read_csv('users.csv')  # read CSV\n",
    "#     #     data = data.to_dict()  # convert dataframe to dictionary\n",
    "#     #     return {'data': data}, 200  # return data and 200 OK code\n",
    "    \n",
    "#     def post(self):        \n",
    "#         #args = parser.parse_args()  # parse arguments to dictionary\n",
    "#         args = request.get_json()\n",
    "#         #args = request.args.get('query')\n",
    "#         print(args)\n",
    "# #         query=args['query']\n",
    "#         print(query)\n",
    "# #         args['articles'] = args['articles'].replace(\"\\'\", \"\\\"\")\n",
    "#         print(args['articles'])\n",
    "#         documents = args['articles']\n",
    "        \n",
    "#         ranker = DocumentRanker(documents)\n",
    "#         results = ranker.get_clustered_documents()\n",
    "        \n",
    "#         response = []\n",
    "        \n",
    "#         for result in results:\n",
    "#             response.append({\"id\": result[1][0], \"clusterNumber\": result[0]})\n",
    "        \n",
    "#         # result = pd.DataFrame({\n",
    "#         #     'userId': args['userId'],\n",
    "#         #     'name': args['name'],\n",
    "#         #     'city': args['city'],\n",
    "#         #     'locations': [[]]\n",
    "#         # })\n",
    "        \n",
    "#         return response, 200  # return data with 200 OK\n",
    "\n",
    "\n",
    "api.add_resource(NLP, '/nlp')  # add endpoints\n",
    "# api.add_resource(NLPCluster, '/nlpCluster')  # add endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"NlpApp\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '(\"Abstract\":mapping study)', 'articles': [{'id': 14692, 'abstractText': 'The field of human information behavior runs the gamut of processes from the realization of a need or gap in understanding, to the search for information from one or more sources to fill that gap, to the use of that information to complete a task at hand or to satisfy a curiosity, as well as other behaviors such as avoiding information or finding information serendipitously. Designers of mechanisms, tools, and computer-based systems to facilitate this seeking and search process often lack a full knowledge of the context surrounding the search. This context may vary depending on the job or role of the person; individual characteristics such as personality, domain knowledge, age, gender, perception of self, etc.; the task at hand; the source and the channel and their degree of accessibility and usability; and the relationship that the seeker shares with the source. Yet researchers have yet to agree on what context really means. While there have been various research studies incorporating context, and biennial conferences on context in information behavior, there lacks a clear definition of what context is, what its boundaries are, and what elements and variables comprise context. In this book, we look at the many definitions of and the theoretical and empirical studies on context, and I attempt to map the conceptual space of context in information behavior. I propose theoretical frameworks to map the boundaries, elements, and variables of context. I then discuss how to incorporate these frameworks and variables in the design of research studies on context. We then arrive at a unified definition of context. This book should provide designers of search systems a better understanding of context as they seek to meet the needs and demands of information seekers. It will be an important resource for researchers in Library and Information Science, especially doctoral students looking for one resource that covers an exhaustive range of the most current literature related to context, the best selection of classics, and a synthesis of these into theoretical frameworks and a unified definition. The book should help to move forward research in the field by clarifying the elements, variables, and views that are pertinent. In particular, the list of elements to be considered, and the variables associated with each element will be extremely useful to researchers wanting to include the influences of context in their studies.', 'title': 'Exploring Context in Information Behavior: Seeker, Situation, Surroundings, and Shared Identities', 'embedding': []}, {'id': 14693, 'abstractText': 'A representation is a thing that can be interpreted as providing information about something: a map, or a graph, for example. This book is about the expanding world of computational representations, representations that use the power of computation to provide information in new forms, and in new ways. Unlike printed maps or graphs, computational representations can be dynamic, and even interactive, so that what is represented, and how, can be shaped by user actions. Exploring these new possibilities can be guided by an emerging theory of representation, that clarifies what characteristics representations must have to express the meaning being represented, and to enable users to discern that meaning easily and accurately. The theory also shows the way to inclusive design, for example using sounds to represent information commonly presented visually, so that people who cannot see can understand what is being presented. Because representations must be shaped by the abilities of their users, and by the nature of the meanings they convey, creating them requires perspectives from multiple disciplines, including psychology, as well as computer science, and the sciences appropriate to the content being expressed. The book presents a series of explorations of this large and complicated space, as invitations to further study, and to innovation.', 'title': 'Representation, Inclusion, and Innovation: Multidisciplinary Explorations', 'embedding': []}, {'id': 14694, 'abstractText': 'In this study, phenomena observed in scale-free coupled circle map are investigated. The circle map is a one-dimensional discrete-time dynamical system which exhibits various kinds of behavior as the parameters change. As the high-dimensional coupled circle map, the coupled map lattice and the globally coupled map have been studied. However, the scale-free coupled circle map has not been well investigated so far. We study the model in which one of the circle maps corresponding to a hub node of the scale-free network has the parameter which leads the single circle map to generate high-order periodic points, and the parameter values of the other maps are set to converge to a fixed point. Changing the coupling strength between each map, we investigated the synchronization in the scale-free coupled circle map by calculating the value of the order parameter. The result of this study elucidated that when the coupling strength between each map was negative, all circle maps in the network behaved chaotic whatever the parameter value of the hub node was set to. That means the phenomena was generated because of the scale-free network structure itself but not the state of the hub node. On the other hand, the coupling strength was positive, the behavior observed in the network was based on the state of the hub node. In addition, the result showed the periodic point observed in the hub node could change to the fixed point after the scale-free network generated. The result suggests that the phenomena such as chaos and periodic oscillation could change to be converged into stable fixed point by forming the scale-free network and appropriately controlling the parameters.', 'title': 'An investigation of phenomena observed in scale-free coupled circle map', 'embedding': []}, {'id': 14695, 'abstractText': \"The increasing number of studies in the knowledge map shows attention from researchers in academic and professional areas. However, the knowledge map implementation has not effectively implemented in an organization whose business in the digital business industry, especially startup organization. The main reason is the lack of stakeholder's understanding of the knowledge map concept. Thus, this study gives a comprehensive understanding of knowledge map implementation in the digital business industry within the last five years period. The study will answer what problems knowledge map tackled, tools, and techniques used currently, the obstacles and benefits of using a knowledge map. The review was conducted through the structured systematic literature review procedure. It started with a review protocol declaration and ended with an analysis of the prior researches obtained from five credible sources. Only 25 of 775 studies remain after several filtering stages. It is found that a knowledge map is mostly used for decision-making purposes. Most studies adopted a visual knowledge map and concept map, even though it is difficult to align the knowledge depth. In the end, this study's result will help stakeholders to reflect on their existing knowledge relationship structure. This study also offers directions for future research and professional practices in digital business industry firms to perfect their existing organizational intellectual capital through a knowledge map.\", 'title': 'An Overview of Knowledge Mapping in Digital Business Industry: A Systematic Literature Review', 'embedding': []}, {'id': 14696, 'abstractText': \"MAP algorithm has outperformed in medical image reconstruction with noise suppression and edge preservation. However MAP-OSL algorithm could hardly apply a strong prior by using a large regularization parameter. In this study we proposed a MAP-Newton reconstruction framework to enable a strong prior to be applied in MAP optimization. EM iterative framework was used in MAP-Newton. In the step of maximizing expectation of the complete data log-likelihood function, MAP-Newton solves the non-linear equation accurately with Newton iterative algorithm, which is potentially better than approximate linearization method used in MAP-OSL. MAP-Newton reconstruction algorithm was implemented with both Bowsher's prior and joint total variation (JTV) prior based on anatomical image. <sup>18</sup>F PET and CT data of an image-quality phantom were acquired on the small animal PET/SPECT/ CT Iniview 3000 system for performance evaluation. The priors with three different strength (regularization parameter was from 0.01 to 1.0) were applied in the list mode reconstruction studies with three noise levels (100 M, 12 M and 2 M LORs separately). The normalized standard derivation (NSTD) of region of interest was calculated. The results with Bowsher's prior for all noise level cases showed no significant difference between MAP-Newton and MAP-OSL when the regularization parameter was smaller than 1.0. When the parameter was set to be 1.0, the proposed MAP-Newton can greatly reduce NSTD with reasonable image quality although no reasonable images could be obtained with MAP-OSL. The results with JTV prior showed the same trend, except that increased regularization parameter did not reduce NSTD. In conclusion, with a strong prior, MAP-Newton can result in better image quality than MAP-OSL in terms of noise suppression, while no significant difference when applying a light prior. It indicated that the MAP-Newton reconstruction with a strong prior could be applied, when we have enough confidence on the prior.\", 'title': 'Regularized MLEM reconstruction with a strong anatomical prior using newton iterative algorithm', 'embedding': []}, {'id': 14697, 'abstractText': 'One of the topics that have successful applications in engineering technologies and computer science is chaos theory. The remarkable area among these successful applications has been especially the subject of chaos-based cryptology. Many practical applications have been proposed in a wide spectrum from image encryption algorithms to random number generators, from block encryption algorithms to hash functions based on chaotic systems. Logistics map is one of the chaotic systems that has been the focus of attention of researchers in these applications. Since, Logistic map can be shown as the most widely used chaotic system in chaos-based cryptology studies due to its simple mathematical structure and its characterization as a strong entropy source. However, in some studies, researchers stated that the behavior displayed in relation to the dynamics of the Logistic map may pose a problem for cryptology applications. For this reason, alternative studies have been carried out using different chaotic systems. In this study, it has been investigated which one is more suitable for cryptographic applications for five different derivatives of the Logistic map. In the study, a substitution box generator program has been implemented using the Logistic map and its five different derivatives. The generated outputs have been tested for five basic substitution box design criteria. Analysis results showed that the proposals for maps derived from Logistic map have a more robust structure than many studies in the literature.', 'title': 'Eligibility Analysis of Different Chaotic Systems Derived from Logistic Map for Design of Cryptographic Components', 'embedding': []}, {'id': 14698, 'abstractText': 'Modern geophysical data acquisition technology makes it possible to measure multiple geophysical properties with high spatial density over large areas with great efficiency. Instead of presenting these co-located multigeophysical data sets in separate maps, we take advantage of cluster analysis and its pattern exploration power to generate a cluster map with objectively integrated information. Each cluster in the resulting cluster map is characterized by multigeophysical properties and can be associated with certain geological attributes or rock types based on existing geological maps, field data and rock sample analysis. Such a cluster map is usually high in resolution and proven to be more helpful than single-attribute maps in terms of assisting geological mapping and interpretation. In this paper, we present the workflow and technical details of applying cluster analysis to multigeophysical data of a study area in the Tr⊘ndelag region in Mid-Norway. We address the importance of carefully designed pre-processing procedures regarding the input data sets to ensure an unbiased data integration using cluster analysis. Random forest as a supervised machine learning method for classification/regression is strategically employed post-clustering for quality evaluation of the results. The multigeophysical data used for this study include airborne magnetic, frequency electromagnetic and radiometric measurements, together with ground gravity measurements. Due to the nature of these input data, the resulting cluster map carries multidepth information. When associated with available geological information, the cluster map can help interpret not only bedrock outcrops but also rocks underneath the sediment cover.', 'title': 'Multigeophysical data integration using cluster analysis: assisting geological mapping in Tr⊘ndelag, Mid-Norway', 'embedding': []}, {'id': 14699, 'abstractText': 'We apply random matrix and free probability techniques to the study of linear maps of interest in quantum information theory. Random quantum channels have already been widely investigated with spectacular success. Here, we are interested in more general maps, asking only for <tex>$k$</tex>-positivity instead of the complete positivity required of quantum channels. Unlike the theory of completely positive maps, the theory of <tex>$k$</tex>-positive maps is far from being completely understood, and our techniques give many new parametrized families of such maps. We also establish a conceptual link with free probability theory and show that our constructions can be obtained to some extent without random techniques in the setup of free products of von Neumann algebras. Finally, we study the properties of our examples and show that for some parameters, they are indecomposable. In particular, they can be used to detect the presence of entanglement missed by the partial transposition test, that is, positive partial transposition entanglement. As an application, we considerably refine our understanding of positive partial transposition states in the case where one of the spaces is large, whereas the other one remains small.', 'title': 'Random and Free Positive Maps with Applications to Entanglement Detection', 'embedding': []}, {'id': 14700, 'abstractText': 'Objective: The ability to monitor catheter contact force (CF) plays a major role in assessing radiofrequency ablation, impacting lesion size and arrhythmia recurrence, and dictating ablation duration and/or overall patient safety. Our study sought to determine the relative CFs required to elicit reproducible monophasic action potential (MAP) recordings. Methods: The study utilized four swine in which: first, median sternotomies were performed and MAPs were collected from seven ventricular locations on the epicardial surface of each heart; and second, a subset of endocardial signals was recorded from a reanimated heart. In these studies, the initial elicitation and then loss of stable MAP waveforms were recorded, as were their associated catheter CFs (n = 371). Results: Mean CF at the onset of stable MAP recordings was 14.2 ± 2.9 g for epicardial and 16.6 ± 2.5 g for endocardial locations. Across epicardial locations, no significant differences in CF were required to elicit MAPs. Additionally, endocardial and epicardial CFs for MAPs did not significantly differ for respective locations, i.e., right ventricular septum endocardial versus epicardial. In our study, the catheter CFs required to elicit MAPs were within optimal ranges previously reported for eliciting clinically viable radiofrequency ablations. Conclusion: We believe that MAP recordings could complement CF measurements with electrical data, providing additional clinical feedback for physicians performing cardiac ablation. Significance: If applied clinically, MAP recordings could potentially improve ablation outcomes in patients with cardiac arrhythmias.', 'title': 'Contact Forces Required to Record Monophasic Action Potentials: A Complement to Catheter Contact Force Measurement', 'embedding': []}, {'id': 14701, 'abstractText': 'In order to monitoring the soil heavy metal pollution, and evaluate the effect of mine ecological environment restoration. This paper use Manganese Mine in Xiangtan as study area, and the image is divided into study area, whole map and control area, statistical and analysis the three regions farmland Vegetation index, get some parameters like Mean Value Difference ratio, STD, STD/MEAM. Based on those data, growth status and heavy metal stress condition of farmland vegetation in Manganese Mining from 1996 to 2016 were analyzed, and the ecological restoration effect in Manganese Mining was analyzed and evaluated by using different years data. The study shows; (1)The Mean of NDVI in the whore map, the study area and the contrast area Mean of NDVI decreased from 0.7906, 0.7978 and 0.7900 to 0.6322, 0.6443 and 0.6181, the difference ratio between the study area and the whole map increased from 0.902% to 1.878%, the study area and the contrast area increased from 0.902% to 1.878%, the STD/MEAN between study area, the whole map and contrast area were increased from 6.543%, 6.906% and 6.582%to 13.503%, 15.755% and 15.839%; (2)The growth of three areas was decreased year by year; the growth of vegetation in study area is better than contrast area and whole area, and the gap was increased year by year, but the increase trend slowed down, and the vegetation growth status in study area is more stable more balance than the contrast area and whole area; (3) Through above study, the effect of ecological restoration in the study area is better than the effect of natural restoration in other area, the ecological restoration project of the Manganese Mine got a good result. The study shows analyst Vegetation index background can monitor the mine ecological restoration effect quickly, and provide reference for mine ecological restoration project.', 'title': 'The study on the method of monitoring the restoration effect of heavy metals in mine based on the background value of vegetation', 'embedding': []}, {'id': 14702, 'abstractText': 'The purpose of this study is to build a learning-based attenuation map estimation scheme for brain attenuation correction of the positron emission tomography/magnetic resonance (PET/MR) imaging using an artificial neural network (ANN). The attenuation map estimation is cast as a regression problem that models a nonlinear mapping between the MR image patches and the corresponding patches of the attenuation map. An ANN is used to solve the regression problem through learning from examples. Using the BrainWeb phantoms, we simulated brain PET data with corresponding MR image and attenuation map pairs of 20 subjects. The ANN was trained with the image patches from the MR image and attenuation map of 1 subject. The trained ANN was then applied to estimate the attenuation maps from the MR images for the other 19 subjects. The estimated attenuation maps were compared with their simulated counterparts using the mean absolute error (MAE) on regions of soft tissue and bone. The impact of estimation accuracy on the reconstructed PET images was evaluated by calculating the relative error with respect to the PET images reconstructed using the AC with the simulated attenuation maps. The estimated attenuation maps obtain the MAE of 0.0021 for soft tissue and 0.0096 for bone, revealing strong agreement with the corresponding simulated ones. The images reconstructed with the estimated attenuation maps have average relative errors of -0.24%± 0.8% on gray matter and of -0.28%± 0.28% on white matter. We demonstrate that the ANN model trained using the MR image and attenuation map of one subject applies well to other subjects and the estimated attenuation map has potential to produce accurate AC for brain PET/MR imaging.', 'title': 'Learning-Based Attenuation Correction for Brain PET/MRI Using Artificial Neural Networks', 'embedding': []}, {'id': 14703, 'abstractText': \"Accurate vehicle localization with map is an important task in urban environment. Currently, vehicle positioning with various types of maps has studied. In this paper, we propose a method which matches 2D-NDT map with road marking image for fast and accurate vehicle positioning. This method extracts features from occupancy grid map by Maximally Stable Extremal Region (MSER) detector. The extracted features on the map is modelled to centroid and covariance and these are stored on the map. This map contains little data but also key information. Camera images also require feature extraction as well as map to match with 2D-NDT map. Images of forward looking camera are converted to top view images via Inverse Perspective Mapping(IPM). Road markings can be accurately extracted by MSER in a bird's eye view image. Vehicle Localization is performed by NDT map matching between the map and road markings in a bird's eye view road image. The evaluation of this method was conducted on a straight road in urban environment. Vehicle positioning results revealed accurate lateral, longitudinal position estimation and orientation estimation. Since NDT map matching was done with only significant lanes on the road, matching speed was fast.\", 'title': 'Vehicle Localization Using Road Marking Image Matching (ICCAS 2018)', 'embedding': []}, {'id': 14704, 'abstractText': \"Various coordinate systems of cadastral map are not very easy to be unified in Taiwan area due to temporal-spatial limitation of these maps. Therefore, the urban planning map, topographic map and other base map cannot integrate into geographic information system (GIS) for additional value-added applications. Cadastral map is directly related to the people's property, it is essential to develop “three map-in-one” project to precisely overlap three types of base maps (cadastral map, topographic map, and urban planning map) through rigorous operation procedures and innovative technology. For urban planning and development, “three map in one” project not only enhances the accuracy of three types of urban base maps, but also fulfills people's need in geospatial information. However, it is a very time and labor consuming task to integrate such huge capacity of maps. It is critical for local government to promote operation efficiency and to reduce the required human resources for this project. Recently, Unmanned Aerial Vehicle (UAV) equips with Global Position System (GPS), Inertial Measurement Unit (IMU), and high resolution camera which draws great research attention in this field. The purpose of this study is to access the feasibility of utilizing UAV for “three map-in-one” project operation by area analysis and man hour comparison. We conclude that the UAV-derived true orthophoto, can provide precise topography and building geographic position for “three map-in-one” project to reduce the required human resources and increase the operation efficiency.\", 'title': 'Assessment on the feasibility of three map in one work using unmanned aerial vehicle', 'embedding': []}, {'id': 14705, 'abstractText': 'Maximum a posteriori probability (MAP) decoding minimizes the symbol or bit error probability, however, few studies have performed an exact error performance evaluation, although the optimality does not require explanation. The MAP algorithm is much more complex than maximum likelihood decoding methods, therefore, suboptimal MAP algorithms are considered for practical systems. The Max-Log-MAP decoding algorithm is one of several near optimum algorithms that reduce decoding complexity. However, it is shown that turbo decoding with Max-Log-MAP has an error-performance degradation compared with MAP decoding. Log-MAP decoding can be realized using Max-Log-MAP decoding with a correction term, which corrects the error induced by maximum approximation. Constant Log-MAP decoding employs the constant correction term instead of the log-domain correction term. In this paper, analytical results of bit error probability of convolutional codes with constant Log-MAP decoding are shown. Furthermore, the analytical results are compared with the result of Max-Log-MAP decoding, and the improvement by the correction term which correct error induced by maximum approximation is presented, theoretically. The results show that the error performance of constant Log-MAP decoding is slightly better than Max-Log-MAP decoding.', 'title': 'On the bit error probability for constant log-MAP decoding of convolutional codes', 'embedding': []}, {'id': 14706, 'abstractText': 'Considering the success of generative adversarial networks (GANs) for image-to-image translation, researchers have attempted to translate satellite images to maps (si2map) through GAN for cartography. However, these studies involved limited scales, which hinders multi-scale map creation. By extending their method, high-resolution satellite images can be trivially translated to multi-scale maps through scale-wise si2map generators trained for certain scales. However, this strategy has two theoretical limitations. First, inconsistency between high-resolutions satellite images and object generalization on multi-scale maps (SI-M inconsistency) increasingly complicates the extraction of geographical information from satellite images for generators with decreasing scale. Second, as si2map translation is cross-domain, generators incur high computation costs to transform the pixel distribution on satellite images to that on maps. Thus, we designed a series strategy of generators for multi-scale si2map translation to address these limitations. In this strategy, high-resolution satellite images are inputted to an si2map generator to output large-scale maps, which are translated to multi-scale maps through series multi-scale map generators. The series strategy avoids SI-M inconsistency as high-resolution satellite images are only translated to large-scale maps, and transforms cross-domain translation to approximately intradomain translation when generates multi-scale maps. Our experimental results showed better quality multi-scale map generation with the series strategy, as shown by average increases of 11.69%, 53.78%, 55.42%, and 72.34% in the structural similarity index, edge structural similarity index, intersection over union (road), and intersection over union (water) for data from Mexico City and Tokyo at zoom level 17–13.', 'title': 'Generating Multi-scale Maps from Satellite Images via Series Generative Adversarial Networks', 'embedding': []}, {'id': 14707, 'abstractText': 'Empirical Research in Information Systems: 2001-2015 provides a first step in providing empirical evidence and knowledge about the practical relevance of IS research. The monograph first develops a broad yet sufficiently fine-grained framework of IS research by integrating earlier frameworks. It then identifies all empirical IS research published from 2001 to 2015 in four top IS journals (Journal of the Association for Information Systems, Journal of Management Information Systems, Information Systems Research, and MIS Quarterly), and maps onto this framework all the constructs and relationships that were examined by the 1,361 empirical papers published in this 15-year period. Next, based on this mapping and by drawing on criteria proposed by organizational and IS researchers, it provides a preliminary assessment of the relevance of empirical IS research to practice, and discusses the study’s findings and their implications.', 'title': 'Empirical Research in Information Systems: 2001-2015', 'embedding': []}, {'id': 14708, 'abstractText': 'For robots to navigate and interact more richly with the world around them, they will likely require a deeper understanding of the world in which they operate. In robotics and related research fields, the study of understanding is often referred to as semantics, which dictates what does the world ‘mean’ to a robot, and is strongly tied to the question of how to represent that meaning. With humans and robots increasingly operating in the same world, the prospects of human-robot interaction also bring semantics and ontology of natural language into the picture. Driven by need, as well as by enablers like increasing availability of training data and computational resources, semantics is a rapidly growing research area in robotics. The field has received significant attention in the research literature to date, but most reviews and surveys have focused on particular aspects of the topic: the technical research issues regarding its use in specific robotic topics like mapping or segmentation, or its relevance to one particular application domain like autonomous driving. A new treatment is therefore required, and is also timely because so much relevant research has occurred since many of the key surveys were published. This survey provides an overarching snapshot of where semantics in robotics stands today. We establish a taxonomy for semantics research in or relevant to robotics, split into four broad categories of activity in which semantics are extracted, used, or both. Within these broad categories, we survey dozens of major topics including fundamentals from the computer vision field and key robotics research areas utilizing semantics such as mapping, navigation and interaction with the world. The survey also covers key practical considerations, including enablers like increased data availability and improved computational hardware, and major application areas where semantics is or is likely to play a key role. In creating this survey, we hope to provide researchers across academia and industry with a comprehensive reference that helps facilitate future research in this exciting field.', 'title': 'Semantics for Robotic Mapping, Perception and Interaction: A Survey', 'embedding': []}, {'id': 14709, 'abstractText': 'Rayleigh wave amplitudes are the primary data set used for imaging shear attenuation in the upper mantle on a global scale. In addition to attenuation, surface-wave amplitudes are influenced by excitation at the earthquake source, focusing and scattering by elastic heterogeneity, and local structure at the receiver and the instrument response. The challenge of isolating the signal of attenuation from these other effects limits both the resolution of global attenuation models and the level of consistency between different global attenuation studies. While the source and receiver terms can be estimated using relatively simple approaches, focusing effects on amplitude are a large component of the amplitude signal and are sensitive to multiscale velocity anomalies. In this study we investigate how different theoretical treatments for focusing effects on Rayleigh wave amplitude influence the retrieved attenuation models. A new data set of fundamental-mode Rayleigh wave phase and amplitude at periods of 50 and 100 sis analysed. The amplitudes due to focusing effects are predicted using the great-circle ray approximation (GCRA), exact ray theory (ERT), and finite-frequency theory (FFT). Phase-velocity maps expanded to spherical-harmonic degree 20 and degree 40 are used for the predictions. After correction for focusing effects, the amplitude data are inverted for global attenuation maps and frequency-dependent source and receiver correction factors. The degree-12 attenuation maps, based on different corrections for focusing effects, all contain the same large-scale features, though the magnitude of the attenuation variations depends on the focusing correction. The variance reduction of the amplitudes strongly depends on the predicted focusing amplitudes, with the highest variance reduction for the ray-based approaches at 50\\ue251s and for FFT at 100\\ue251s. Although failure to account for focusing effects introduces artefacts into the attenuation models at higher spherical-harmonic degrees, the low-degree structure can be robustly retrieved. The new attenuation maps compare favourably with previous attenuation studies derived using independent amplitude data sets.', 'title': 'Effects of elastic focusing on global models of Rayleigh wave attenuation', 'embedding': []}, {'id': 14710, 'abstractText': 'Rock-magnetic and geochemical characteristics of three Vertisol profiles with different degree of textural differentiation have been studied. Thermomagnetic analyses, thermal demagnetization of laboratory remanences and acquisition of isothermal remanence curves are applied for identification of iron oxide mineralogy. The main magnetic minerals in Vertisols are ferrihydrite, single-domain magnetite, maghemite and hematite. Variations in magnetic susceptibility, anhysteretic remanent magnetization, isothermal remanent magnetization, as well as different ratios (Xarm/X, ARM/SIRM, S-ratio) along depth are studied. Concentration of magnetic minerals in Vertisols is low, influenced by the intense reductomorphic processes. The lowest magnetic susceptibility is found in the most texturally differentiated soil. However, rock-magnetic data suggest the presence of small, but well defined fraction of single domain-like magnetite with relatively wide grain-size distribution found in those parts of the profiles, which are subjected to most intense and frequent seasonal changes in oxidation-reduction conditions. It is suggested that this fraction is formed as a result of transformations of ferrihydrite under repeated cycles of anaerobic/aerobic conditions. Based on geochemical data, CALMAG weathering index was calculated for the three Vertisols. Using the established relation between CALMAG and mean annual precipitation (MAP), palaeo-MAP was evaluated for the studied profiles. The obtained MAP estimations fall in the range 1000–1200\\ue251mm and are much higher compared to contemporary precipitation in the area (MAP in the interval 540–770\\ue251mm). This finding confirms the relict character of Vertisols on Bulgarian territory and gives more information about the palaeoclimate during the initial stages of Vertisol formation.', 'title': 'Rock-magnetic and geochemical characteristics of relict Vertisols—signs of past climate and recent pedogenic development', 'embedding': []}, {'id': 14711, 'abstractText': 'Land Use and Land Cover (LULC) are the basic units of human activities and also serve as the significant factors to assess the climate change studies and environmental protection. Therefore, it is of significance to accurately and timely obtain the LULC maps on the earth’s surface, in particular for those regions where dramatic LULC changes are undergoing. Since 2017 April, a new state-level new area, Xiong’an New Area, was established in China, which would inevitably lead to some LULC changes in this region. In order to better understand what kinds of LULC change in this region with more details and further evaluate the potential impacts that the LULC changes will bring, this study makes full use of the 10-m multi-temporal Sentinel-2 images on the cloud-computing Google Earth Engine (GEE) and the powerful classification capability of Random Forest (RF) models, to generate the continuous LULC maps in Xiong’an New Area from 2017 to 2020. The derived LULC map for each year in Xiong’an New Area achieves high accuracy, with OA and Kappa values less than 0.9. Based on the obtained LULC maps, this study analyzed the spatial and temporal changes of LULC types in the last four years. It revealed that the estimated dry farmland has decreased from 2017 to 2018 and then kept almost stable, and the majority of it has converted to non-cropland, especially to the tree seedlings and landscape dual-use regions. The estimated impervious areas that were mainly composed of buildings and transport infrastructures first increased due to the newly constructed Xiong’an Station with some high-speed roads and then decreased from 2019 to 2020, mainly due to the relocation of some small villages. Accordingly, some building groups and aggregations were going to show in this region, and are expected to further developed in the future. Other LULC types such as lakes and forests have not changed dramatically, indicating that the environmental protection in Xiong’an New Area was, to date, satisfactory. The obtained 10-m and 4-year LULC maps in this study can provide some valuable information on the monitoring and understanding of what kinds of changes were and will be undergoing in Xiong’an New Area, and can also be used for evaluating the potential impacts and challenges such as environmental protection. Additionally, the utilization of GEE and the multi-temporal 10-m Sentinel-2 images can help to achieve LULC mapping in this region in an accurate and timely manner, which can be used in future studies.', 'title': 'The rapid Land Use and Land Cover change analysis using the Sentinel-2 images in Google Earth Engine: A Case Study of Xiong’an New Area from 2017 to 2020', 'embedding': []}, {'id': 14712, 'abstractText': \"In recent years, many support systems intended for elderly and disabled people have been developed. Among them, the study on the interface utilizing human gaze has been attracting attention. Human vision contains a lot of important information. Some studies that estimate a next action of operator intended to perform by using the visual information have been reported. Also we performed the research that recognizes the human instruction based on the operator's gaze point and controls the locomotion of wheelchair by paying attention to the motion and the characteristics of human eyes. As an example, the visual saliency map model is one of the means using to estimate the intention of the human gaze. This model enables to estimate the place directed human attention from the image. Also, this model has an advantage that uses only the image information. As the research dealing with the visual saliency map model, the study suppressing the changing value of saliency map by the movement of operator's head and the study controlling the weight of saliency map used a human knowledge as a value have been reported. However, there are few applications used the visual saliency map model by the movement of human eyes in real time because it is difficult to adjust it in the conventional studies. In this research, we focus on the combination of visual saliency map and gaze information, and extract a behavioral intention estimation map from the landscape and the movement of human eye. We propose a gaze instruction system using panoramic expansion image for controlling the locomotion of omni-directional wheelchair. In this system, the human intention by various input information is estimated by using fuzzy reasoning. To evaluate the effectiveness of this method, we carried out the comparison experiments for gaze instruction system by using only movement of human eyes.\", 'title': 'Gaze Instruction System Used Panoramic Expansion Image of Omnidirectional Camera', 'embedding': []}, {'id': 14713, 'abstractText': 'Synthetic aperture radar (SAR) can be used to obtain remote sensing images of different growth stages of crops under all weather conditions. Such time-series SAR images can provide an abundance of temporal and spatial features for use in large-scale crop mapping and analysis. In this study, we propose a temporal feature-based segmentation (TFBS) model for accurate crop mapping using time-series SAR images. This model first extracts deep-seated temporal features and then learns the spatial context of the extracted temporal features for crop mapping. The results indicate that the TFBS model significantly outperforms traditional long short-term memory (LSTM), U-network, and convolutional LSTM models in crop mapping based on time-series SAR images. TFBS demonstrates better generalizability than other models in the study area, which makes it more transferable, and the results show that data augmentation can significantly improve this generalizability. The visualization of the temporal features extracted by the TFBS shows that there is a high degree of intraclass homogeneity among rice fields and interclass heterogeneity between rice fields and other features. TFBS also achieved the highest accuracy of the four deep learning models for multicrop classification in the study area. This study presents a feasible way of producing high-accuracy large-scale crop maps based on the proposed model.', 'title': 'Semantic Segmentation Based on Temporal Features: Learning of Temporal-Spatial Information From Time-Series SAR Images for Paddy Rice Mapping', 'embedding': []}, {'id': 14714, 'abstractText': 'Map retrieval, the problem of similarity search among a large collection of 2D pointset maps previously built by mobile robots, is crucial for autonomous navigation in both indoor and outdoor environments. Unlike previous Bag-of-words scene model which lacks spatial layout information, we exploit a holistic local map descriptor which is view-independent and highly discriminative. The current study is inspired by our recent papers on “local map descriptor”, in which the viewpoint or the origin of local map coordinate is planned by scene parsing and the spatial layout of a local map is represented with respect to the planned viewpoint. The main contribution of this study is to build on the framework of local map descriptor and experimentally evaluate the effectiveness of the proposed method. The key observation is that accuracy of the spatial layout should be improved by introducing an accurate local mapping technique. To this end, we implemented the state-of-the-art particle filter-based mapping algorithm of Grid Mapping. The next contribution is that we conducted map retrieval experiments using a real mobile robot in feature-less office-like environments. We experimentally demonstrate the effectiveness of the proposed approach.', 'title': 'Combining Grid Mapping with local map descriptor for fast succinct map retrieval', 'embedding': []}, {'id': 14715, 'abstractText': \"The mapping of vegetation and Land Cover (LC) is important for research and for public policy planning but, in Brazil, although diverse maps exist there are few studies comparing them. The semiarid region of the Caatinga, in northeastern Brazil is an area long neglected by scientific research and its vegetation is diverse and relatively rich despite years of human occupation and very little preservation effort. In this study we make a comparison between the main maps made for the Caatinga from four different sources: IBGE (Brazilian Institute of Geography and Statistics), TCN (Third National Communication), ProBio (Project for Conservation and Sustainable Use of Biological Biodiversity) and MapBiomas. We also test these maps against well-known Land Cover maps from ESA and NASA: ESA's GlobCover and Climate Change Initiative (CCI) Land Cover, and NASA's MODIS MCD12Q1. This was done on a sample area where many of the Caatinga's vegetation physiognomies can be found, using well-established Difference metrics and the new SPAtial EFficiency (SPAEF) algorithm as they present complementary viewpoints to test the correspondence of mapped classes as well as that of their spatial patterns. Our results show considerable disagreement between the maps tested and their class semantics, with IBGE's and ProBio's being the most similar among all national maps and MapBiomas' the most closely related to global LC maps. The nature of the observed disagreement between these maps shows they diverge not only in the application of their classification systems, but also in their mapped spatial pattern, signaling the need for a better classification system and a better map of vegetation and land cover for the region.\", 'title': 'Classification System Drives Disagreement Among Brazilian Vegetation Maps at a Sample Area of the Semiarid Caatinga', 'embedding': []}, {'id': 14716, 'abstractText': \"The computational capabilities of today's smartphones make it possible to take advantage of mobile three-dimensional (3-D) maps to support navigation in the physical world. In particular, 3-D maps might be useful to facilitate indoor wayfinding in large and complex buildings, where the typical orientation cues (e.g., street names) and location tracking technologies that can be used outdoors are unavailable. The use of mobile 3-D maps for indoor wayfinding is still largely unexplored and research on how to best design such tools has been scarce to date. One overlooked but important design decision for 3-D maps concerns the perspective from which the map content should be displayed, with first-person and third-person perspectives being the two major options. This paper presents a user study involving wayfinding tasks in a large and complex building, comparing a mobile 3-D map with first-person perspective, a mobile 3-D map with third-person perspective, and a traditional mobile 2-D map. The first-person perspective shows the mobile 3-D map of the building from a floor-level egocentric point of view, whereas the third-person perspective shows the surroundings of the user from a fixed distance behind and above her position. Results of the study reveal that the mobile 3-D map with third-person perspective leads to shorter orientation time before walking, better clarity ratings, lower workload, mental demand and effort scores, and higher preference score compared to the mobile 3-D map with first-person perspective. Moreover, it leads to shorter orientation time before walking, better pleasantness ratings, lower mental demand scores, and higher preference score compared to the mobile 2-D map.\", 'title': 'Mobile Three-Dimensional Maps for Wayfinding in Large and Complex Buildings: Empirical Comparison of First-Person Versus Third-Person Perspective', 'embedding': []}, {'id': 14717, 'abstractText': 'This chapter provides a summary of the current research on urban impervious surface estimation and mapping. It focuses on the examination of sub‐pixel estimation techniques, including linear spectral mixture analysis (LSMA), artificial neural networks, and fuzzy classifiers. The chapter presents a case study to demonstrate the capability of two conventional methods (LSMA and multilayer perceptron) for impervious surface estimation using Hyperion imagery. Satellite remote sensing provides a cost‐effective and time‐efficient way for impervious surface mapping. Medium spatial resolution imagery has been utilized for large‐area mapping, and high spatial resolution imagery, air photos, and Light Detection and Ranging data for extracting urban features. Numerous methods have been developed and applied in previous studies based on per‐pixel, sub‐pixel, and object‐based algorithms. However, fewer studies have examined the spectral diversity of impervious surfaces. Hyperspectral imagery with rich spectral information is suitable for spectral analysis and should be extensively employed in future studies.', 'title': 'Estimation and Mapping of Impervious Surfaces', 'embedding': []}, {'id': 14718, 'abstractText': 'Simultaneous Localization and Mapping (SLAM) has seen a tremendous interest amongst research community in recent years due to its ability to make the robot truly independent in navigation. The capability of an autonomous robot to locate itself within the environment and construct a map at the same time, it is known as Simultaneous Localization and Mapping (SLAM). Visual Simultaneous Localization and Mapping (VSLAM) is when autonomous robot employs a vision sensor such a camera to explore the environment. Various researchers have embarked on the study of Visual Simultaneous Localization and Mapping (VSLAM) with excellent results, however the challenge of environmental noise such as light intensity still persist. In this study we propose a framework for Visual Simultaneous Localization and Mapping (VSLAM) that will address the challenge of light intensity in an environment in order to improve the performance of Visual Simultaneous Localization and Mapping (VSLAM) system. In executing of Visual Simultaneous Localization and Mapping (VSLAM) system, we have introduced a filtering algorithm to reduce or limit the effects of noise on images taken from the environment. The outcome of our study is a framework that will enable an autonomous robot to successfully navigate, localize itself and map the environment.', 'title': 'Framework for Visual Simultaneous Localization and Mapping in a Noisy Static Environment', 'embedding': []}, {'id': 14719, 'abstractText': 'Recently, Unmanned Aerial Vehicle (UAV), equipped with different high accurate remote sensing sensors acquiring various physical or geometric spatial information has many mature applications in different fields, such as environmental patrol, pollution monitoring, disaster prevention and relief, and mapping. High resolution imagery acquisition and spatial geometric information extraction is still the current major application for UAV. The visual representation of traditional cadastral maps is still limited in 2D drawings. It seems insufficient to provide intuitive understanding of the variety of problems in high density residential areas. Even though the 2D cadastral maps can be overlapped on traditional semi orthophoto, it still represents in two dimensional map. It is very hard to represent land legal boundary and physical information on imagery preciously. Because 2D cadastral maps and 3D image-based model are produced separately, it is also a very challenge issue for the land boundary line to follow the variation of terrain surface. This study propose a 3D cadastral map production and update technology using UAV-derived imagery. This study overlaps the cadastral map with true orthophoto using 3D visualization technique which change the way for visual inspection of traditional cadastral map. In this study, digitized land corner coordinates from UAV-derived true orthophoto are verified by field survey data. The accuracy of these land corner points is proved to fulfill the requirement of cadastral map accuracy.', 'title': 'Research on the production of 3D image cadastral map', 'embedding': []}, {'id': 14720, 'abstractText': 'Simultaneous Localization and Mapping (SLAM) has seen a tremendous interest amongst research community in recent years due to its ability to make the robot truly independent in navigation. The capability of an autonomous robot to locate itself within the environment and construct a map at the same time, it is known as Simultaneous Localization and Mapping (SLAM). They are various sensors that are employed in a Simultaneous Localization and Mapping (SLAM) which characterized either as a laser, sonar and vision sensor. Visual Simultaneous Localization and mapping (VSLAM) is when autonomous robot embedded with a vision sensor such as monocular, stereo vision, omnidirectional or Red Green Blue Depth (RGBD) camera to localize and map the environment. Numerous researchers have embarked on the study of Visual Simultaneous Localization and Mapping (VSLAM) with incredible results, however many challenges stills exist. The purpose of this paper is to review the work done by some of the researchers in Visual Simultaneous Localization and Mapping (VSLAM). We conducted a literature survey on several studies and outlined the frameworks, challenges and limitation of these studies. Open issues, challenges and future research in Visual Simultaneous Localization and Mapping (VSLAM) are also discussed.', 'title': 'A Review on Vision Simultaneous Localization and Mapping (VSLAM)', 'embedding': []}, {'id': 14721, 'abstractText': 'Ionospheric scintillation occurs mainly at high and low latitude regions of the Earth and may impose serious degradation on GNSS (Global Navigation Satellite System) functionality. The Brazilian territory sits on one of the most affected areas of the globe, where the ionosphere behaves very unpredictably, with strong scintillation frequently occurring in the local postsunset hours. The correlation between scintillation occurrence and sharp variations in the ionospheric total electron content (TEC) in Brazil is demonstrated in Spogli et al. (2013). The compounded effect of these associated ionospheric disturbances on long baseline GNSS kinematic positioning is studied in this paper, in particular when ionospheric maps are used to aid the positioning solution. The experiments have been conducted using data from GNSS reference stations in Brazil. The use of a regional TEC map generated under the CALIBRA (Countering GNSS high-Accuracy applications Limitations due to Ionospheric disturbances in BRAzil) project, referred to as CALIBRA TEC map (CTM), was compared to the use of the Global Ionosphere Map (GIM), provided by the International GNSS Service (IGS). Results show that the use of the CTM greatly improves the kinematic positioning solution as compared with that using the GIM, especially under disturbed ionospheric conditions. Additionally, different hypotheses were tested regarding the precision of the TEC values obtained from ionospheric maps, and its effect on the long baseline kinematic solution evaluated. Finally, this study compares two interpolation methods for ionospheric maps, namely, the Inverse Distance Weight and the Natural Neighbor.', 'title': 'Performance of ionospheric maps in support of long baseline GNSS kinematic positioning at low latitudes', 'embedding': []}, {'id': 14722, 'abstractText': 'We study the effect of baryonic processes on weak lensing (WL) observables with a suite of mock WL maps, the κTNG, based on the cosmological hydrodynamic simulations IllustrisTNG. We quantify the baryonic effects on the WL angular power spectrum, one-point probability distribution function (PDF), and number counts of peaks and minima. We also show the redshift evolution of the effects, which is a key to distinguish the effect of baryons from fundamental physics such as dark energy, dark matter, and massive neutrinos. We find that baryonic processes reduce the small-scale power, suppress the tails of the PDF, peak and minimum counts, and change the total number of peaks and minima. We compare our results to existing semi-analytical models and hydrodynamic simulations, and discuss the source of discrepancies. The κTNG suite includes 10\\u2009000 realizations of <tex>$5 \\\\times 5 \\\\, \\\\mathrm{deg}^2$</tex> maps for 40 source redshifts up to z<inf>s</inf> = 2.6, well covering the range of interest for existing and upcoming WL surveys. We also produce the κTNG-Dark suite of maps, generated based on the corresponding dark matter-only IllustrisTNG simulations. Our mock maps are not only suitable for developing analytical models that incorporate the effect of baryons, but also particularly useful for studies that rely on mass maps, such as non-Gaussian statistics and machine learning with convolutional neural networks. The suite of mock maps is publicly available at Columbia Lensing (http://columbialensing.org).', 'title': 'κTNG: effect of baryonic processes on weak lensing with IllustrisTNG simulations', 'embedding': []}, {'id': 14723, 'abstractText': 'Lithological mapping is important parameters for interpretation, identification and mapping of minerals. Lithological mapping in the study area defines the characteristics of nature of rock types and their association and formation. In this research study Landsat 8 OLI remote sensing data used for lithological mapping of Jahajpur region of Bhilwara super group. The task of lithological mapping completed by analogical and numerical analysis of digital image processing method which involves several digital image processing techniques such as natural band combination, false color composite, principal component analysis, band ratio and minimum noise fraction. The observed result is verified and validated by field survey and published geological survey of India geological map. The observe result have shown complete correlation and similarity with established map.', 'title': 'Lithological Mapping using Digital Image Processing Techniques on Landsat 8 OLI Remote Sensing Data in Jahajpur, Bhilwara, Rajasthan', 'embedding': []}, {'id': 14724, 'abstractText': \"Precision agriculture has been proposed to improve the sustainability of agriculture and solve the environmental pollution of soil. In precision agriculture process, the management of water and fertilizer is carried out on agricultural operation units. Therefore, acquisition of accurate soil nutrient distribution information is a key step for precision agriculture application and digital soil mapping is an effective technology. Significant progress has been made in digital soil mapping over the past 20 years. However, the current digital soil mapping framework was implemented based on grids, which was not consistent with the operation units of precision agriculture. This paper proposed a geo-parcel based digital soil mapping framework on the support of artificial intelligence technology for precision agriculture application. Two key technologies were studied for the implementation of this framework. Geo-parcels automatic extraction was the basis of this method, and a modified VGG 16 network was used for geo-parcels' accurate boundary extraction from high resolution images. Different machine learning methods were attempted to construct the relationship between soil available phosphorus and environment on geo-parcels. We chose an agricultural region in Zhongning County, Ningxia Province as the study area, and the new digital soil mapping framework was applied for soil available phosphorus mapping. This research showed that geo-parcel based digital mapping method could reduce the number of prediction units more than 50% for fine soil mapping, and effectively improve the prediction and application efficiency. This study was an attempt to realize soil mapping based on agricultural operation units for precision agriculture application. The high resolution remote sensing images provide basic data for the realization of this idea and the development of AI technology provides technical support for it. In the future, we will carry out experiments in larger areas to further optimize this method and key technologies for the applications in more complex environments.\", 'title': 'Digital Mapping of Soil Available Phosphorus Supported by AI Technology for Precision Agriculture', 'embedding': []}, {'id': 14725, 'abstractText': 'A timely and reliable inventory is essential for landslide hazard assessment and risk management. In this study, we use images from PlanetScope, which provides global 3 m daily Earth observations, for rapid mapping of landslide inventory. We propose a semi-automated method that combines change detection and region-based level set evolution (RLSE) to improve the landslide mapping efficiency. In particular, our approach uses the change detection methods of independent component analysis (ICA), principal component analysis (PCA) and change vector analysis (CVA) for automated generation of landslide zero-level curves (ZLCs), and then incorporates the RLSE method to refine the landslide mapping results. To corroborate the applicability of the proposed method, we test the landslide mapping performance on the Kodagu event (India, 2018) using ICA-, PCA- and CVA-based RLSE. The results show that ICA-based RLSE can achieve better landslide mapping accuracy in terms of completeness, correctness, and the Kappa coefficient. This study demonstrates the suitability and potential of low-orbit miniature satellites such as PlanetScope for landslide mapping. To the best of our knowledge, it is the first attempt to incorporate PlanetScope images and the change detection-based RLSE method for landslide mapping.', 'title': 'Landslide Mapping from PlanetScope Images Using Improved Region-based Level Set Evolution', 'embedding': []}, {'id': 14726, 'abstractText': 'The mining of software repositories has provided significant advances in a multitude of software engineering fields, including defect prediction. Several studies show that the performance of a software engineering technology (e.g., prediction model) differs across different project repositories. Thus, it is important that the project selection is replicable. The aim of this paper is to present STRESS, a semi-automated and fully replicable approach that allows researchers to select projects by configuring the desired level of diversity, fit, and quality. STRESS records the rationale behind the researcher decisions and allows different users to re-run or modify such decisions. STRESS is open-source and it can be used used locally or even online (www.falessi.com/STRESS/). We perform a systematic mapping study that considers studies that analyzed projects managed with JIRA and Git to asses the project selection replicability of past studies. We validate the feasible application of STRESS in realistic research scenarios by applying STRESS to select projects among the 211 Apache Software Foundation projects. Our systematic mapping study results show that none of the 68 analyzed studies is completely replicable. Regarding STRESS, it successfully supported the project selection among all 211 ASF projects. It also supported the measurement of 100 projects characteristics, including the 32 criteria of the studies analyzed in our mapping study. The mapping study and STRESS are, to our best knowledge, the first attempt to investigate and support the replicability of project selection. We plan to extend them to other technologies such as GitHub.', 'title': 'STRESS: A Semi-Automated, Fully Replicable Approach for Project Selection', 'embedding': []}, {'id': 14727, 'abstractText': 'The concept of eServices originated in the early 2000s in the field of business and commerce. However, in recent years, eServices are being applied in many domains. Therefore, a thorough study on eServices is required to identify the areas in which eServices have been applied till date and to what extent. The main objective of this research is to perform a mapping study to provide an extensive review, gather trends, and identify the state of the art in the research on eServices to answer the research questions designed to conduct this research. A mapping study has been conducted employing an automatic search in digital repositories by developing a mapping protocol. Mapping studies are useful for categorizing and classifying the existing information concerning a particular research question in an unbiased manner. The search procedure identified 806 studies of which 318 were selected for full analysis during the years 2000 and 2016 in the field of computer science. No study was published before this time period. Research on eServices were recorded and classified into tabulated spread sheets, and finally analyzed. According to the study, the range of eService service and application domains is quite wide. Most studies conducted have focused on eService composition and eService Adoption. However, the most common application domains identified were eGovernment, eBusiness, eHealth, and eLearning. The study findings show that the research on eService composition, design, provision, and adoption is increasing with the passage of time. The literature not only discusses various domains of eServices but also provides the in-depth classification, review and trend of eService studies over time.', 'title': 'eServices Classification, Trends, and Analysis: A Systematic Mapping Study', 'embedding': []}, {'id': 14728, 'abstractText': 'This paper addresses on an autonomous exploration algorithm of unknown environment for mapping. An autonomous indoor carrying robot requires a kind of map as knowledge of the work space in order to efficiently execute the navigation task. SLAM (Simultaneously Localization and Mapping) is widely used as an generating a map. However, the map is based on the premise that humans operate robots equipped with sensors to generate the map. This is a burdensome task for the operator. Thus, an exploration method in an unknown environment for autonomously generating a map has been studied for decades. The main method is frontier-based exploration [1] which performs autonomous exploration using local map information. The algorithm is useful approach for the mapping unknown environments, on the other hand, the method has the following problem for applying to mapping method in a wide environment like a factory; since the map information which is being gradually extended is used, the calculation cost increases in proportion to the time [2]. From this problem, when the calculation time of the map information becomes long, the exploring efficiency will be deteriorated. Thus, it takes the time to generate the map. For more efficient exploration, an autonomous exploration algorithm using only infrared sensor and odometry information of a robot is proposed as a sensor-based exploration approach without using map information. The proposed method requires only a depth sensor and camera for which the Kinect is used as the equipment and one mobile robot. Turtlebot2 with Kinect is used as a wheeled mobile robot, and the effectiveness of the proposed method will be confirmed by comparing with frontier-based exploration and proposed method by indoor experiments.', 'title': 'A sensor-based exploration algorithm for autonomous map generation on mobile robot using kinect', 'embedding': []}, {'id': 14729, 'abstractText': 'Pathfinding is a fundamental problem for many areas, e.g., robotics, automation, computer-aided design, and computer graphics. Although outdoor pathfinding is fledged, indoor pathfinding remains a challenge due to the lack of indoor maps. Currently, some efforts have utilized building information modeling (BIM) to generate either the grid-based map or the topological map. However, either the grid-based map or the topological map is not sufficient to provide accurate and efficient pathfinding service. This article proposes a novel grid-topological map and develops an accurate and efficient indoor pathfinding scheme based on BIM. The grid-topological map is modeled jointly adopting the advantages of both the grid-based map and the topological map. First, the grid-based map is generated using the BIM data by extracting and mapping geometric and semantic data into planar grids. Second, a grid thinning algorithm is proposed to produce the topological map directly using the grid-based map. Third, a grid-topological map is presented by combining both the grid-based map and topological map. On top of the grid-topological map, an accurate and efficient pathfinding algorithm is developed. Empirical studies proved the effectiveness of the grid-topological map, as well as the accuracy and efficiency of the proposed indoor pathfinding algorithm.', 'title': 'Accurate and Efficient Indoor Pathfinding Based on Building Information Modeling Data', 'embedding': []}, {'id': 14730, 'abstractText': 'The rapid development of industrialized agriculture has leads to the problems of soil pollution and water pollution. In order to solve these problems, precision agriculture (PA) has been applied to achieve precise management of agricultural water and fertilizer. In PA process, fine mapping of soil nutrient is an effective technology to acquire accurate water and fertilizer distribution information and make agricultural decision. A significant progress has been made in digital soil mapping (DSM) of soil nutrient content over the past 20 years. However, the accuracy of grid-based DSM cannot meet the practical application needs of PA. This paper proposed a fine DSM method of soil nutrient content using high resolution remote sensing images and multi-scale auxiliary data for PA application. Three key technologies were studied for the implementation of this method. The automatic extraction of fine mapping units was the basis of this method. We designed different automatic extraction methods based on high resolution remote sensing images for agricultural production units in plains and mountainous areas. The auxiliary variables in different scales were chosen and converted to construct fine-scale soil nutrient-environment relationship model. Finally, machine learning methods were used to map the spatial distribution of soil nutrients. We chose Zhongning County, Ningxia Province as the study area, which includes typical plain and mountainous agriculture. The proposed method and technologies were applied for typical soil nutrients mapping. A common grid-based spatial interpolation method was implemented with the same soil sample dataset to evaluate the effect of the proposed method. The result showed that this method could reduce the number of prediction units and effectively improve the prediction efficiency in both plain and mountainous areas for fine soil mapping and precision agriculture application. This study was an attempt to realize fine soil mapping based on PA application unit in different environments. The high-resolution remote sensing images provide basic data for the realization of this idea, and the conversion technology of multi-scale data provides better support for the spatial inference of fine soil attribute information. In the future, we will carry out experiments in larger areas to further improve the efficiency of application, and plan to expand this study to consider three-dimensional soil property prediction.', 'title': 'Fine mapping of key soil nutrient content using high resolution remote sensing image to support precision agriculture in Northwest China', 'embedding': []}, {'id': 14731, 'abstractText': 'Due to change in global mean temperature, most Himalayan glaciers have shown retreat, resulting in an increase in the number and size of glacial lakes, which may give rise to glacial lake outburst flood event. These glacial lakes typically grow from small supraglacial lakes (SGLs). Therefore, it is important to map and monitor the SGLs on a regular basis. Most of the studies have utilized medium to coarse resolution images for the extraction of glacial lakes. With the availability of high spatial resolution data, it has also become possible to map small glacial lakes. The literature suggests some studies on the use of high resolution data to map glacial lakes. However, the extraction is majorly based on simple to apply pixel-based classification methods, which results into high misclassification thereby increasing the task of manual post processing. In this study, we used a novel approach, known as object-based image analysis (OBIA), for the mapping of SGLs. A new index has also been proposed for classification of SGLs. As a case study, the SGLs of Gangotri glacier (Uttarakhand Himalayas) have been mapped from the high spatial resolution data of LISS-IV using the proposed OBIA approach. The results have been compared with those obtained from object-based normalized difference water index (NDWI) and pixel-based mapping methods. The validation of the SGLs boundaries has been carried out with respect to the manually digitized database of SGLs. A significant increase in accuracy of the mapping has been observed over the benchmarked traditional methods.', 'title': 'Extraction of Glacial Lakes in Gangotri Glacier Using Object-Based Image Analysis', 'embedding': []}, {'id': 14732, 'abstractText': 'Mangrove forests constitute an important coastal ecosystem, which provides valuable ecosystem services such as coastal erosion protection, water filtration and shelters for a wide variety of plants and animals. Remote sensing satellite imagery provides valuable information for mangrove mapping and monitoring. In this study, we use high resolution images from SPOT-5, Landsat-7 and Landsat-8 satellites to perform change detection on an area of interest that covers the mangrove forests at the Ramsar sites of Pulau Kukup, Tanjung Piai and Sungai Pulai in the southwestern part of Johor State of Malaysia. Land use/land cover maps and the mangrove change map are generated to identify the mangrove distribution and temporal variation between 2000 and 2016. A hierarchical object-based classification method was used to produce the land use/land cover map in 2000, 2005, 2013 and 2016. The WorldView2 data were also used for validation of the classification results. This study presented an approach for mangrove mapping and change detection analysis. The map provides an up to date information for the study area and can be used for future comparative study.', 'title': 'Mangrove mapping and change detection using satellite imagery', 'embedding': []}, {'id': 14733, 'abstractText': \"This Research Full Paper presents a study using brain monitoring technology to measure and compare engineering students' performance on complex tasks that require systems thinking and connecting knowledge from different domains. The study used an electroencephalograph (EEG) and self-report data to investigate students' cognitive load and performance when completing concept mapping and listing tasks related to complex issues like food security and water availability. The study was designed to test two hypotheses: first, concept maps allow individuals to organize their thoughts using a networked or systems thinking framework, and thus will result in a more complete and holistic response than listing tasks; and second, creating a concept map is a more complex cognitive process and thus students will experience greater cognitive load during concept mapping tasks than listing tasks. Twenty-seven students at a mid-size public university participated in the study, which is an adequate size for EEG data analysis. For each participant, over forty pieces of data were recorded, including: demographic data, responses to the Revised Systems Thinking Scale, order effects, EEG performance variables, NASA-TLX scores, listing task metrics, and concept map scores. The paper presents and discusses quantitative results related to three questions: (1) do students perform better on listing or concept mapping tasks? (2) do students exert more mental effort (cognitive load) for listing or concept mapping? (3) how did performance compare across different direct and self-report measures? The ultimate goal of this research is to create learning approaches that enhance students' cognitive resources to meet and exceed the requirements of working within the sustainable design paradigm. More broadly, we expect that using neuroeducation measures to triangulate results with other qualitative and quantitative assessments could provide powerful evidence for the effectiveness of different learning interventions aimed at improving applications of engineering knowledge.\", 'title': 'Measuring connections: Engineering students’ cognitive activities and performance on complex tasks', 'embedding': []}, {'id': 14734, 'abstractText': 'More than 3800 landslide locations have been reported in the area near the Three Gorges Reservoir (TGR) along the Yangtze River in China, which poses a serious threat to the socioeconomic stability of the region. An efficient and accurate method of generating landslide susceptibility maps is very important to mitigate the loss of lives and properties caused by these landslides. In this paper, a deep belief network (DBN) with datasets developed via a geographic information system (GIS) and remotely sensed data was used to create a landslide spatial susceptibility map for the TGR region on the Yangtze River in Zigui County. The landslide inventory map was initially constructed using field surveys, aerial photographs, and a literature search of historical landslide records. The twelve causative factors were evaluated in different ways: elevation, topographic slope, topographic aspect, curvature, distance from drainage, distance from road, schedule performance index and topographic wetness index were derived from a digital topographical map at 1:10,000 scale; engineering petrofabric, slope structure and distance from faults were obtained from a geological map at 1: 50,000 scale; normalized difference vegetation index were generated from CBERS (China-Brazil Earth Resources Satellite) data. All the causative factors were resampled to a grid cell size of 10 * 10 m by using the grid analysis function of ArcGIS software. Initially, 30% of the landslides pixels and equal number of non-landslides pixels were randomly selected as training data. Then these twelve factors were used as the input to DBN. By integrating the twelve factor maps in the GIS via pixel-based computing, the landslide spatial susceptibility map was obtained. Then the study area was reclassified into four categories of landslide susceptibility: high, moderate, low, and very low by using natural breaks classification methods. Approximately 13.6% of the study area was identified as severe susceptibility, and moderate, low, very low susceptibility zones covered 1.3%, 1.4%, and 83.7% of the study area, respectively. To show the performance of the DBN, the results were then compared with a logistic regression model (LRM) by using the receiver operating characteristics (ROC). The area under the ROC curve was 0.949 for the DBN and 0.859 for the LRM. The results showed that, the DBN proposed in this study outperforms the LSM. Based on the efficiency and accuracy of DBN, the proposed approach can be employed for rapid response to natural hazards in the Three Gorges area.', 'title': 'Landslide spatial susceptibility mapping by using deep belief network', 'embedding': []}, {'id': 14735, 'abstractText': \"Deep Learning (DL), a subset of Artificial Intelligence (AI), is growing rapidly with possible applications in different domains such as speech recognition, computer vision etc. Deep Neural Network (DNN), the backbone of DL algorithms is a directed graph containing multiple layers with different number of neurons residing in each layer. The use of these networks has been increased in the last few years due to availability of large data sets and huge computation power. As the size of DNN is growing over the years, researchers have developed specialized hardware accelerators to reduce the inference compute time. An example of such domain specific architecture designed for Neural Network acceleration is Tensor Processing Unit (TPU) which outperforms GPU in the inference stage of DNN execution. The heart of this inference engine is a Matrix Multiplication unit which is based on systolic array architecture. The TPU's systolic array is a grid-like structure made of individual processing elements that can be extended along rows and columns. Due to external environmental factors or internal scaling of semiconductor, these systems are often prone to faults which leads to improper calculations and thereby resulting in inaccurate decisions by the DNN. Although a lot of work has been done in the past on the computing array implementation and it's reliability concerns, their fault tolerance behavior for DNN application is not very well understood. It is not even clear what would be the impact of various different faults on the accuracy. We in this work, first study possible mapping strategies to implement a convolution and dense layer weights on TPU systolic array. Next we consider various faults scenarios that may occur in the array. We divide these fault scenarios into low, high row and column faults (Fig. 1(a) pictorially represents column faults) modes with respect to the multiplication unit. Next, we study the impact of these fault models on the overall accuracy of the DNN performance on a faculty TPU unit. The goal is to study the resiliency and overcome the limitations of earlier work. The previous work was very effective in masking the random faults which used pruning of weights (removing weights or connections in the DNN) plus retraining to mask the faults on the array. However, it failed in the case of column faults which is clearly shown in Fig. 1(b). We also propose techniques to mitigate or bypass the row and column faults. Our mapping strategy follows physical_x(i) = i%N and physical_y(j) = j%N where (i,j) represents the index of dense (FC) weight matrix and (physical x(i), physical y(j)) indicates the actual physical location on the array of size N. The convolution filters are linearized with respect to every channel so as to convert them into proper weight matrix and mapped according to the previous mentioned policy. It was shown that DNNs can up to certain faults in the array while retaining the original accuracy (low row faults). The accuracy of the network decreases even with one column faults if it (column) is in the use. As per the results, it is proved that for the same number of row and column faults, the latter has most impact on the network accuracy because pruning input neuron has very little effect than pruning an output neuron. We experimented with three different networks and found the influence of these different faults to be the same. These faults can be mitigated using techniques like Matrix Transpose and Array Reduction which does not require retraining of weights. For low row faults, the original mapping policy can be retained such that weights can be mapped at their exact locations which does not affect the accuracy. Low column faults can be converted into low row faults by transposing the matrix. In the case of high row (column) faults, the entire row (column) has to be avoided to completely bypass the faulty locations. Static mapping of weights along with retraining the network on the array can be effective in the case of random faults. Adapting to change in the case of structured faults can reduce the burden of retraining which happens outside the TPU.\", 'title': 'Impact of Structural Faults on Neural Network Performance', 'embedding': []}, {'id': 14736, 'abstractText': 'Context: A tertiary study can be performed to identify related reviews on a topic of interest. However, the elaboration of an appropriate and effective search string to detect secondary studies is challenging for Software Engineering (SE) researchers. Objective: The main goal of this study is to propose a suitable search string to detect secondary studies in SE, addressing issues such as the quantity of applied terms, relevance, recall and precision. Method: We analyzed seven tertiary studies under two perspectives: (1) structure – strings’ terms to detect secondary studies; and (2) field: where searching – titles alone or abstracts alone or titles and abstracts together, among others. We validate our string by performing a twostep validation process. Firstly, we evaluated the capability to retrieve secondary studies over a set of 1537 secondary studies included in 24 tertiary studies in SE. Secondly, we evaluated the general capacity of retrieving secondary studies over an automated search using the Scopus digital library. Results: Our string was capable to retrieve an optimum value of over 90% of the included secondary studies (recall) with a high general precision of almost 60%. Conclusion: The suitable search string for finding secondary studies in SE contains the terms “systematic review”, “literature review”, “systematic mapping”, “mapping study” and “systematic map”.', 'title': 'Establishing a Search String to Detect Secondary Studies in Software Engineering', 'embedding': []}, {'id': 14737, 'abstractText': 'Maps of the large-scale structure of the Universe at redshifts 2–4 can be made with the Lyman α forest which are complementary to low-redshift galaxy surveys. We apply the Wiener interpolation method of Caucci et\\xa0al. to construct three-dimensional maps from sets of Lyman α forest spectra taken from cosmological hydrodynamic simulations. We mimic some current and future quasar redshift surveys [Baryon Oscillation Spectroscopic Survey (BOSS), extended BOSS (eBOSS) and Mid-Scale Dark Energy Spectroscopic Instrument (MS-DESI)] by choosing similar sightline densities. We use these appropriate subsets of the Lyman α absorption sightlines to reconstruct the full three-dimensional Lyman α flux field and perform comparisons between the true and the reconstructed fields. We study global statistical properties of the intergalactic medium (IGM) maps with autocorrelation and cross-correlation analysis, slice plots, local peaks and point-by-point scatter. We find that both the density field and the statistical properties of the IGM are recovered well enough that the resulting IGM maps can be meaningfully considered to represent large-scale maps of the Universe in agreement with Caucci et\\xa0al., on larger scales and for sparser sightlines than had been tested previously. Quantitatively, for sightline parameters comparable to current and near future surveys the correlation coefficient between true and reconstructed fields is r &gt; 0.9 on scales &gt;30\\u2009h<sup>−1</sup>\\u2009Mpc. The properties of the maps are relatively insensitive to the precise form of the covariance matrix used. The final BOSS quasar Lyman α forest sample will allow maps to be made with a resolution of ∼30\\u2009h<sup>−1</sup>\\u2009Mpc over a volume of ∼15\\u2009h<sup>−3</sup>\\u2009Gpc<sup>3</sup> between redshifts 1.9 and 2.3.', 'title': 'Large-scale 3D mapping of the intergalactic medium using the Lyman α forest', 'embedding': []}, {'id': 14738, 'abstractText': 'Aiming at the problem that the navigation map operation time occupies more resources in the embedded system. In this paper, by studying the digital map of MIF format, the corresponding map is drawn and the map is optimized. Using the GPS module to locate and use the Dijiestra algorithm to achieve the shortest path of the navigation module design, the use of the map database to remove the corresponding area of the data mapping map information, in a translation to a certain distance when the scene clear the data, Draw a map. So there will be no data left in the view. Since the Dijiestra algorithm is based on the graph, it is necessary to link the roads in the area to create an undirected graph to find the shortest path to the end point. If the point of the line as an undirected node, the composition of the undirected map will be very large. And the use of the line as an undirected node, you can reduce the number of nodes without the map, to improve the efficiency of Dijie Stella algorithm traversal. Based on the open system platform and data standard, this paper designs and develops a secure, stable and low cost embedded GPS navigation and navigation system. The experimental results are optimized.', 'title': 'QT-based car navigation map realization and optimization', 'embedding': []}, {'id': 14739, 'abstractText': 'With the dawn of 5G network, a new set of requirements for site spectrum monitoring, cellular planning are emerging, all of which are relying on fine-grained signal map. Although with significant importance, the traditional signal map construction could be time-consuming and labor-intensive. The state-of-the arts usually employ crowdsourcing scheme and matrix completion algorithm to solve the dilemma. However, the crowdsourcing scheme usually suffers from uneven distributed and inadequate participants. To this end, in this paper, we study how to effectively reconstruct and update the signal map in the case of partially measured signal maps with smaller cost and propose a GAN-based active signal map reconstruction method (GSMAC). Our method is mainly innovative in two parts: GSMC, GAN-based signal map construction, and ACS, an active crowdsourcing scheme. Specifically, GSMC can effectively update the signal map with only a small number of observations while updating the signal map online. ACS consists of a reinforce learning-based active query mechanism which quantitatively evaluates the most valuable measurement site for reconstruction. The simulation results and real implemented data-driven experiments demonstrate the advantages and effectiveness of our approach in both accuracy and cost.', 'title': 'GSMAC: GAN-based Signal Map Construction with Active Crowdsourcing', 'embedding': []}, {'id': 14740, 'abstractText': 'With the use of deep learning, object detection has achieved great breakthroughs. However, existing object detection methods still can not cope with challenging environments, such as dense objects, small objects, and object scale variations. To address these issues, this paper proposes a novel keypoint-based detection framework, called CrossNet, which significantly improves detection performance with minimal costs. In our approach, an object is modeled as a cross that consists of a center keypoint and a specific size, which eliminates the need of hand-craft anchor design. The proposed CrossNet outputs three types of maps: the center map, size map, and offset map, where both center map and offset map are to predict the center keypoints of objects and the size map is to estimate the sizes (width and height) of objects. Specifically, we first design a cascaded center prediction method that introduces a coarse-to-fine idea to improve center prediction. Furthermore, since center prediction considered as a classification task is easier than size regression relatively, we design a center-attention size regression module that uses the detection results of centers to assist the size prediction. In addition, a slightly modified hourglass network is designed to enhance the quality of feature maps for center and size prediction. Extensive experiments are conducted to demonstrate the effectiveness of CrossNet on the challenging PASCAL VOC, COCO, KITTI, and WiderFace datasets. Empirical studies show that CrossNet achieves competitive results with top-ranked one-stage and two-stage detectors while being time-efficient.', 'title': 'CrossNet: Detecting Objects as Crosses', 'embedding': []}, {'id': 14741, 'abstractText': 'Ontology Matching is a process to find correspondences between semantically related entities of two ontologies. Most matching systems do evaluation by comparing the correspondences with reference alignment. Since 2010 another method has been used to measure a logic-based of correspondence or mapping, called incoherent mapping measurement. The more incoherent of the mapping the lower quality of mapping. Incoherent mapping repair will restore the incoherent to coherent condition in mapping, by removing unwanted mapping. The process of removing unwanted mapping to restore the coherent condition is called diagnosis process. Since mappings are very important sources to support data integration and exchange, then diagnosis should be done as minimal as possible. We propose two focuses minimal using global new technique to repair the incoherent mapping. This approach should (1) ensure minimal impact on the input alignment by minimizing the number of mapping removed; and (2) minimize the average of confidence values of the mapping removed. The next study about minimal diagnosis is finding the right method to implement the two focuses minimal with global new techniques in the real world.', 'title': 'Diagnosis process with two focuses minimal in incoherent mapping repair', 'embedding': []}, {'id': 14742, 'abstractText': 'Super-resolution Reconstruction (SRR) is technique to increase the spatial resolution of images. It is especially useful for hyperspectral images (HSI), which have good spectral resolution but low spatial resolution. In this study, we propose an improvement to our previous work and present a novel MAP-MRF (maximum a posteriori-Markov random Fields) based approach for the SRR of HSI. The key point of our approach is to find the abundance maps of an HSI and perform SRR on the abundance maps using MRF based energy minimization, without needing any other additional source of information. In order to do so, first, PCA is used to determine the endmembers. Second, SISAL and fully constraint least squares (FCLS) are used to estimate the abundance maps. Third, in order to find the high resolution abundance maps, the ill-posed inverse SRR problem for abundances is regularized with a MAP-MRF based approach. The MAP-MRF formulation is restricted with the constraints which are specific to the abundances. Using the non-linear programming (NLP) techniques, the convex MAP formulation is minimized and High Resolution (HR) abundance maps are obtained. Then, these maps are used to construct the HR HSI. This improved SRR method is verified on real data sets, and quantitative performance comparison is achieved using PSNR, SSIM and PSNR metrics. Our results indicate that this improved method gives very close results to the original high resolution images, keeps the spectral consistency, and performs better than the compared algorithms.', 'title': 'Super-resolution Reconstruction of hyperspectral images via an improved MAP-based approach', 'embedding': []}, {'id': 14743, 'abstractText': \"At the Fukushima Daiichi Nuclear Power Station (NPS), robots are used to propel the decommissioning work. Creating a 3D map of the internal environment of the decommissioning work is a necessary technology for improving the working efficiency of a decommissioning robot. The purpose of this research is to realize a 3D map creating a system using a camera by operating the humanoid general-purpose robot Pepper using the Robot Operating System (ROS) and implementing and executing Visual SLAM. The system aims to be applied to a decommissioning robot in the future. In this study, we implemented Visual SLAM methods R-tab Map, which is a method for constructing 3D maps in real-time, and Large-Scale Direct SLAM, which is method that generates a map using luminance with a large gradient between frames without using features for map generation. We also compared and evaluated the effectiveness of the generated maps. Besides, the robot was manually operated using nao_teleop, which can operate Pepper from the ROS library with the PS3 controller, the visualization software rviz, and the point cloud visualization library PointCloudViewer. Using two Visual SLAM's methods, we implemented the experiment. During the experiment, a chair was placed in front of Pepper as an obstacle. In this environment, Pepper was rotated 360 [deg] by manual operation. However, nao_teleop and Visual SLAM methods caused a conflict between the manual operation of Pepper and the 3D map generation process, and both processes stopped. Therefore, the 3D map wasn't generated. To resolve the conflict with operation, Pepper was moved by human and the experiment was again. As a result, the camera image obtained from Pepper was distorted, the map could not be optimized. Therefore, the part of 3D map was only generated. For this reason, to realize a 3D map generation system, it was important to properly calibrate the camera that could acquire a flat image without distortion.\", 'title': '3D Map Generation for Decommissioning Work', 'embedding': []}, {'id': 14744, 'abstractText': \"Recently, autonomous vehicle of self-localization based on the high definition (HD) map become more popular due to the availability of HD map and the dropping prices of LIDAR. Many types of studies have improved the local and global accuracy of HD map for accurate localization. However, the global accuracy of a map does not guarantee the accurate self-localization within the map. In this paper, by investigating the scene errors in the map and comparing their characteristics, we introduced four factors that affect the self-localization ability of the map. These factors are highly related to the environment, namely feature sufficiency, layout, local similarity, and representation quality of the map. Then, in order to overcome the limitations brought by the map itself, we proposed a multi-sensor information fusion self-localization system. The system uses GNSS, LIDAR, IMU, and local map to fuse multi-source information. Global mapping is not required for autonomous vehicle's accurate self-localization. To achieve a high precision of global positioning, the system uses Kalman filter to integrate GNSS positioning, slam and inertial navigation solution to improve the robustness of the system, and the method of local map matching is used to eliminate the accumulated error and modify the positioning system. By conducting the experiments in a typical testing environment, we have evaluated the performance of this system by comparing the ground truth with the self-localization error.\", 'title': 'Robust self-localization system based on multi-sensor information fusion in city environments', 'embedding': []}, {'id': 14745, 'abstractText': 'Mapping human hand motion to robotic hands has great significance in a wide range of applications, such as teleoperation and imitation learning. The ultimate goal is to develop a device-independent control solution based on human hand synergies. Over the past twenty years, a considerable number of mapping methods have been proposed, but most of the mapping methods use intrusive devices, such as the CyberGlove data gloves, to capture human hand motion. Until recently, a very small number of mapping methods have been proposed based on vision-based human hand pose estimation. Traditionally, mapping methods and vision-based human hand pose estimation have been studied independently. To the best of our knowledge, no review has been conducted to summarize the achievements on haptic mapping methods or explore the feasibility of applying off-the-shelf human hand pose estimation algorithms to teleoperation. To address this literature gap, we present the first survey on mapping human hand motion to robotic hands from a kinematic and algorithmic perspective. We discuss the realistic challenges, intuitively summarize recent mapping methods, analyze the theoretical solutions, and provide a teleoperation-oriented human hand pose estimation overview. As a preliminary exploration, a vision-based human hand pose estimation algorithm is introduced for robotic hand teleoperation.', 'title': 'Survey on Mapping Human Hand Motion to Robotic Hands for Teleoperation', 'embedding': []}, {'id': 14746, 'abstractText': 'Before the digital measurement technology has been applied, various maps measurement results were use graphical method. Among them, the number of cadastral maps and topographic maps is mostly. Cadastral maps related to peoples rights, topographic map is used by engineering design. Both must be kept as the basis for data traceability. At present, most of these maps are digitally kept by scanning. However, there are different sizes of maps that need different sizes scanners. Image distortion may occur during scanning, causing misjudgment of subsequent map data.In recent years, processing paper data into true orthophoto through multi-view image processing technology, the result of obtaining consistency with paper datas and maps has been verified and feasible. However, the verification method is too simple. It is quite inconvenient for the map datas that need to be processed in large quantities. This study will design a simple and convenient photography platform, through Smooth moving slide rail system and height-adjustable shooting base, can quickly capture fixed-position results for image processing. Expand the number of experiments and the type of maps in the verification process. Research results show that this new photography platform can effectively speed up processing and expand the multi-size maps. There is considerable help in the subsequent preservation of cadastral data.', 'title': 'Innovative Design on Photography Platform for the Digitized Data of Original Cadastral Maps', 'embedding': []}, {'id': 14747, 'abstractText': \"The Simultaneous Localization and Mapping (SLAM) is the process of building a map of an environment with an unknown topography by a mobile robot. The purpose of this paper is to build a mapping of an unknown environment by the mobile robot which we designed through the help of sensor fusion algorithms we have established. The mobile robot performs its mapping process by using the combination of ultrasonic, optical encoder and IMU sensors. Determining the position of the obstacles and its own location, for the mobile robot, is the core of this study. Inertial and rotational sensors are utilized to calculate the distance and position of the mobile robot. Due to low cost, the ultrasonic sensor is used instead of a Lidar laser, and the real-like results were provided. In this study, the robot's direction and movement is performed by an algorithm developed on the Raspberry Pi processor. This algorithm controls the movement of the wheels with the information received from the optical encoder and protractor. The data received from the gyroscope and the accelerometer is very affected from many external factors such as vibrational motion and the noise, eventhough, we used moving average filter and complementary filter to reduce the effect of the noise and measurement error problems. However, they still produce faulty results when calculating distance values. Therefore, the distance computation is carried out by using optical encoder instead of the accelerometer. The algorithm of the distance computation is written in Python programming language. In this study, it is established that the comparative usage of several detectors provide more accurate results. At the same time, the system is quite efficiently developed by using open structure software (Raspberry Pi, Linux etc.) and writing authentic libraries. The robot's coordinate information are combined under simulation medium by using Pygame library and by computing the coordinates of its location and the coordinates of the objects' locations it detects during its navigation. The mobile robot executes its mapping process according to these data derived. Also, the effects of margin of error in the information obtained during the comparable usage of detectors are studied within the scope of this study.\", 'title': 'Effective sensor fusion of a mobile robot for SLAM implementation', 'embedding': []}, {'id': 14748, 'abstractText': 'Minkowski functionals (MF) are excellent tools to investigate the statistical properties of the cosmic background radiation (CMB) maps. Between their notorious advantages is the possibility to use them efficiently in patches of the CMB sphere, which allow studies in masked skies, inclusive analyses of small sky regions. Then, possible deviations from Gaussianity are investigated by comparison with MF obtained from a set of Gaussian isotropic simulated CMB maps to which are applied the same cut-sky masks. These analyses are sensitive enough to detect contaminations of small intensity like primary and secondary CMB anisotropies. Our methodology uses the MF, widely employed to study non-Gaussianities in CMB data, and asserts Gaussian deviations only when all of them points out an exceptional χ<sup>2</sup> value, at more than 2.2σ confidence level, in a given sky patch. Following this rigorous procedure, we find 13 regions in the foreground-cleaned Planck maps that evince such high levels of non-Gaussian deviations. According to our results, these non-Gaussian contributions show signatures that can be associated to the presence of hot or cold spots in such regions. Moreover, some of these non-Gaussian deviations signals suggest the presence of foreground residuals in those regions located near the Galactic plane. Additionally, we confirm that most of the regions revealed in our analyses, but not all, have been recently reported in studies done by the Planck collaboration. Furthermore, we also investigate whether these non-Gaussian deviations can be possibly sourced by systematics, like inhomogeneous noise and beam effect in the released Planck data, or perhaps due to residual Galactic foregrounds.', 'title': 'Local analyses of Planck maps with Minkowski functionals', 'embedding': []}, {'id': 14749, 'abstractText': \"This Research to Practice full paper reviews two student-made systematic mapping studies and has two goals. The first goal is to report the students' and the supervisor's experiences with this research method, and thereby contribute to practice and knowledge on student research. A mapping study was experienced to fit a situation where the student initially had no personal research interests. Moreover, the method was argued to be appropriate for technology-related disciplines where student identities readily match with the pedantry required. Such student reflections suggest that rewarding research experiences can arise from an initially open starting point, and by considering student background in relation to the nature of work with a particular research method. Supervisors should hence learn about their students' identities when proposing research methods-method matters. The second goal is to disseminate the results of the two mapping studies. The other study mapped intellectual property as an educational research topic, while the other focused on creative coding, similarly as an educational topic. The main results of these studies are summarized and explanation for how the results demonstrate acknowledged benefits of professional mapping studies is provided.\", 'title': 'Method Matters: Reflections from Student-Made Mapping Studies', 'embedding': []}, {'id': 14750, 'abstractText': 'We study the morphology of convergence maps by perturbatively reconstructing their Minkowski functionals (MFs). We present a systematic study using a set of three generalized skew spectra as a function of source redshift and smoothing angular scale. These spectra denote the leading-order corrections to the Gaussian MFs in the quasi-linear regime. They can also be used as independent statistics to probe the bispectrum. Using an approach based on pseudo-S<inf>ℓ</inf>s, we show how these spectra will allow the reconstruction of MFs in the presence of an arbitrary mask and inhomogeneous noise in an unbiased way. Our theoretical predictions are based on a recently introduced fitting function to the bispectrum. We compare our results against state-of-the-art numerical simulations and find an excellent agreement. The reconstruction can be carried out in a controlled manner as a function of angular harmonics ℓ and source redshift z<inf>s</inf>, which allows for a greater handle on any possible sources of non-Gaussianity. Our method has the advantage of estimating the topology of convergence maps directly using shear data. We also study weak lensing convergence maps inferred from cosmic microwave background observations, and we find that, though less significant at low redshift, the post-Born corrections play an important role in any modelling of the non-Gaussianity of convergence maps at higher redshift. We also study the cross-correlations of estimates from different tomographic bins.', 'title': 'Morphology of weak lensing convergence maps', 'embedding': []}, {'id': 14751, 'abstractText': 'Automotive players have recently shown an increasing interest in high-precision mapping, with the aim of enhancing vehicles safety and autonomy. Nevertheless, the acquisition, processing, and updates of accurate maps remains an economic challenge. Collaborative mapping through vehicles crowdsourcing represents a promising solution to tackle this problem. However, the potential scalability and accuracy provided by such an approach have yet to be studied and assessed. In this paper, we study the use of graph optimization in the scope of collaborative mapping. We build a map of geo-localized landmarks by crowdsourcing observations from multiple vehicles, and applying several successive map updates. We present different strategies to adapt graph optimization to the crowdsourced approach, and compare their performances in terms of map quality and scalability on simulation data. We show the critical requirement, in a long-term context, to ensure consistency of the map updates, and we propose a scalable solution which is able to build an accurate map of geolocalized landmarks.', 'title': 'Graph Optimization Methods for Large-Scale Crowdsourced Mapping', 'embedding': []}, {'id': 14752, 'abstractText': 'Virtual network mapping (VNM) is to build a network on demand by deploying virtual machines in a substrate network, subject to constraints on capacity, bandwidth and latency. It is critical to data centers for coping with dynamic cloud workloads. This paper shows that VNM can be approached by graph pattern matching, a well-studied database topic. (i) We propose to model a virtual network request as a graph pattern carrying various constraints, and treat a substrate network as a graph in which nodes and edges bear attributes specifying their capacity. (ii) We show that a variety of mapping requirements can be expressed in this model, such as virtual machine placement, network embedding and priority mapping. (iii) In this model, we formulate VNM and its optimization problem with a mapping cost function. We establish complexity bounds of these problems for various mapping constraints, ranging from polynomial time to NP-complete. For intractable problems, we show that their optimization problems are approximation-hard, i.e. NPO-complete in general and APX-hard even for special cases. (iv) We also develop heuristic algorithms for priority mapping, an intractable problem. (v) We experimentally verify that our algorithms are efficient and are able to find high-quality mappings, using real-life and synthetic data.', 'title': 'Virtual Network Mapping in Cloud Computing: A Graph Pattern Matching Approach', 'embedding': []}, {'id': 14753, 'abstractText': 'The maximum a posteriori (MAP) estimation model-based sub-pixel mapping (SPM) method is an alternative way to solve the ill-posed SPM problem. The MAP estimation model has been proven to be an effective SPM approach and has been extensively developed over the past few years, as a result of its effective regularization capability that comes from the spatial regularization model. However, various spatial regularization models do not always truly reflect the detailed spatial distribution in a real situation, and the over-smoothing effect of the spatial regularization model always tends to efface the detailed structural information. In this article, under the scenario of time-series observation by remote sensing imagery, the joint spectral-spatial-temporal MAP-based (SST_MAP) model for SPM is proposed. In SST_MAP, a newly developed temporal regularization model is added to the MAP model, based on the prerequisite for a temporally close fine image covering the same study region. This available fine image can provide the specific spatial structures most closely conforming to the ground truth for a more precise constraint, thereby reducing the over-smoothing effect. Furthermore, the three dimensions are mutually balanced and mutually constrained, to reach an equilibrium point and achieve restoration of both smooth areas for the homogeneous land-cover classes and a detailed structure for the heterogeneous land-cover classes. Four experiments were designed to validate the proposed SST_MAP: three synthetic-image experiments and one real-image experiment. The restoration results confirm the superiority of the proposed SST_MAP model. Notably, under the background of time-series observation, SST_MAP provides an alternative way of land-cover change detection (LCCD), achieving both detailed spatial-scale and high-frequency temporal LCCD observation for the study case of urbanization analysis within the city of Wuhan in China.', 'title': 'Spectral–Spatial–Temporal MAP-Based Sub-Pixel Mapping for Land-Cover Change Detection', 'embedding': []}, {'id': 14754, 'abstractText': 'In many areas of Taiwan, there are still used graphically digitized cadastral maps. Due to poor precision and environmental changes, there are often inconsistencies in the application. There are two ways to update cadastral maps: cadastral map resurveying and cadastral rearranging. The purpose is to redraw the cadastral map and calculate the area so that the location, shape, place number, and area of each land are consistent with the contents of the register. Currently, the cadastral resurveying method is adopted. However, due to financial and manpower problems, there are still many areas where cadastral maps cannot be updated. In this study, the Miaoli county cadastral resurveying area was used as the experimental area. Using the UAS aerial survey to obtain the true orthophoto image overlay cadastral map, it was explored whether UAS aerial survey could improve the current cadastral map resurveying process. All experimental data in this study were compared with the data of the current cadastral retesting procedure, confirming that UAS aerial survey can improve the following, 1. Assist in cadastral surveys and results announcements to reduce public concerns about resurvey results. 2. Improve image efficiency and correctness by image-assisted area analysis. After comparing the image measurement area and the registered area of each piece of land, the maximum difference is 0.8276 square meters still within the regulation margin. 3. The ground measurement work can be replaced by the image point coordinate measurement. 4. The cadastral boundary line after the area analysis can be converted into a new cadastral map.', 'title': 'Improvement on the Process of Cadastral Map Resurveying Using UAS Photogrammetry', 'embedding': []}, {'id': 14755, 'abstractText': \"Measurement of precise three-dimensional surface displacements is really helpful to understand Earth's internal processes. In this point of view, many researches have been performed to retrieve three-dimensional surface displacements using three or more surface displacements maps from SAR, which represent different geometries respectively [1-6]. SAR interferometry, Multiple-aperture SAR interferometry or Offset tracking have been selectively employed to restore three-dimensional surface displacements occurring by earthquakes, volcanisms, glaciers or land subsidence [7-14]. However the limitations are still remain. SAR interferometry is difficult to measure large surface displacements. Although offset tracking is appropriate to measure large surface displacements, generally produce the surface displacements map of worse measurement precision than SAR interferometry [1]. Meanwhile, recent developments in SAR sensor and improvements in offset tracking have made it possible to complement each of these limitations [9]. In this study, we precisely measured the large surface displacements by employing the integration of SAR Interferometry and Offset tracking methods. Figure 1 show the shaded relief map for Kumamoto, Japan. Three interferometric pairs, which cover surface displacements occurring by the 2016 Kumamoto Earthquakes, were collected from ascending and descending paths of L-band ALOS PALSAR-2 datasets (asc1: 20151115_20160616; asc2: 20160211_20160602; dsc: 20160307_20160419) (Fig. 1). When the offset tracking map and integrated LOS displacements map are compared, integrated LOS map could observe surface displacements without unwrapping error, even in the decorrelated region of SAR interferometry because of the large surface displacements [15]. It enable us to extract the three-dimensional displacements without unmeasured area. Figure 2 shows three-dimensional surface-deformation field for the 2016 Kumamoto Earthquakes. The maximal surface displacements for east-west, north-south and up-down direction were -1.91, -1.57 and -2.49 m respectively. The root mean square errors of 1.76, 1.41 and 1.85 cm in the east, north and up directions, respectively, were estimated by comparing with the GPS measurements [16]. These results suggest that the integration method can be help to map the three-dimensional surface displacements even near the fault lines, which was impossible to observe by conventional SAR interferometry. Since it can be applied not only to earthquakes but also to large surface displacements of various factors, the observation method of this study will be produce more beneficial data to understand the Earth's internal processes.\", 'title': 'Precise three-dimensional mapping of the 2016 Kumamoto earthquake through the integration of SAR interferometry and offset tracking', 'embedding': []}, {'id': 14756, 'abstractText': 'In this paper, we present a method of effectively creating environment maps on an auto-transport system in logistics and industrial site management applications, e.g., an automobile assembly plant. The key objective of the study is creating a map effectively. Simultaneous Localization and Mapping (SLAM) is established as a general map-generating method. The map is, however, created with ad hoc and manual. Thus, an exploration method in an unknown environment for autonomously generating a map has been studied for decades. The main method is frontier-based exploration. This method presents a problem for an efficient mapping method in a wide environment, and for accuracy of the map depending on the local area. In the backgrounds, an autonomous exploration algorithm using only infrared sensor and odometer information from a robot is proposed as a sensor-based exploration approach without using map information. The proposed method requires only a depth sensor and camera on a robot. Next, we propose a hybrid exploration to decrease unavailable areas in frontier-based exploration. To perform our proposed method, an environment map is created by a mobile robot, and the effectiveness of the hybrid exploration method is demonstrated.', 'title': 'Hybrid Sensor-Based and Frontier-Based Exploration Algorithm for Autonomous Transport Vehicle Map Generation', 'embedding': []}, {'id': 14757, 'abstractText': 'In recent electroencephalograph (EEG)-based emotion recognition, the differential entropy (DE) features extracted from multiple electrodes are organized as a 2D feature map for convolutional neural network (CNN) in order to utilize the information hidden in the electrodes. In this study, we attempt to investigate the influence of different feature maps on the recognition performance. Six different 2D feature maps (M1-M4: baseline feature maps without sparsity and location relationship, M5-M6: pre-defined feature maps with sparsity and location relationship) are used to organize the DE features for the traditional CNN model. Evaluation study on the DEAP dataset finds that the 2D feature map configuration exhibits statistically significant effect on the classification performance of the traditional CNN model in classifying the high/low arousal and high/low valence, respectively. However, the differences are rather limited, e.g., only 1% improvement can be resulted from selecting the optimal 2D feature map among 6 feature maps. This implies that the feature map may not be a critical issue when applying the DE features to classifying the emotion states in a CNN.', 'title': 'EEG-based Emotion Recognition Under Convolutional Neural Network with Differential Entropy Feature Maps', 'embedding': []}, {'id': 14758, 'abstractText': 'We present an experimental study for the generation of large 3D maps using our CoMapping framework. This framework considers a collaborative approach to efficiently manage, share, and merge maps between vehicles. The main objective of this work is to perform a cooperative mapping for urban and rural environments denied of continuous-GPS service. The study is split in to 2 stages: Pre-Local and Local. In the first stage, each vehicle builds a Pre-Local map of its surroundings in real-time using laser-based measurements, then relocates the map in a global coordinate system using just the low cost GPS data from the first instant of the map construction. In the second stage, vehicles share their pre-local maps, align and merge them in a decentralized way in order to generate more consistent and larger maps, named Local maps. To evaluate performance of all the cooperative system in terms of map alignments, tests are conducted using 3 cars equipped with LiDARs and GPS receiver devices in urban outdoor scenarios of the Ecole Centrale Nantes campus and rural environments.', 'title': 'CoMapping: Multi-robot Sharing and Generation of 3D-Maps applied to rural and urban scenarios', 'embedding': []}, {'id': 14759, 'abstractText': \"In this study, the effect of practical courses (especially robotics) on students' success is evaluated using fuzzy cognitive maps. This study particularly focuses on how practical courses increase the learning achievements in vocational schools. For this purpose, students and academicians were chosen as stakeholders for this study. Student were selected from departments of machine, electronic and computer. In total views of 30 students were taken and evaluated using fuzzy cognitive maps before the study. Students were studied on three projects within this study. These projects are autonomous cleaning robot, chameleon robot and sumo wrestler robot. Each robot was developed by a group consisting of ten students. Three main outputs determined to be gained by students in the robot development process as sensor calibration, improvement of mechanical systems and software optimization. These outputs are also among the output concepts of the fuzzy cognitive map. Sensor calibration, improvement of the mechanical systems and optimization of the software are the critical tasks respectively for the student groups of electronics, machine and computer. Each student group shared students of their departments homogeneously. Academicians were selected as one from each department. Fuzzy cognitive maps' survey applied to 15 academicians. After the design and development processes of robots, students and academicians views re-evaluated. It was observed clearly from the comparison of the results that, learning achievement, desire to learn, desire to interdisciplinary collaboration, self-confidence, interest in robotic technologies, satisfaction and desire to give priority to practical training in other subjects are increased by %45, %63, %42, %74, %85, %88 and %76 respectively.\", 'title': 'Performance evaluation of practice courses using fuzzy cognitive maps', 'embedding': []}, {'id': 14760, 'abstractText': 'Since elderly users lack intellectual ability, which becomes less due to age factor their interactions with computer interfaces exhibit limitations essentially arising from problems inherent to their age. This is due to the minimal cognitive flow, physical impairments, and lack of knowledge about computers and technologies. All these factors are often analyzed by employing qualitative and quantitative research methods. The empathy map tool was used as the qualitative analysis method in this research study. To address this study objective, the study target group was elderly people 60 years and above, and the group consists of 15 volunteers, with seven females and eight males. Age is the only factor in this study. The test used in this study is the usability test to do seven search tasks by using Google engine and seven different search tasks with Bing engine individually by each volunteer. The observation method and note-taking are used in this study to collect data. Also, a SUS questionnaire was used to be a validation method to prove the results obtained by the empathy map instrument. The result of the empathy map tool enables to explore, understand, and empathize, from the perspective of the elderly user and their interaction with search engines; more specifically, with the Google and Bing interface. The results of the empathy mapping method show that the elderly are dissatisfied with the usability of search interfaces, which is consistent with the findings of the SUS questionnaire.', 'title': 'Empathy Map Instrument for Analyzing Human-Computer Interaction in Using Web Search UI by Elderly Users', 'embedding': []}, {'id': 14761, 'abstractText': \"The phenomenon of deforestation is a reality in Côte d'Ivoire, which dates back to the colonial period [1]. This study aims to determine the spatial and temporal evolution of dense rainforests using remote sensing data. Many previous studies have attempted to determine the area of forest cover in Côte d'Ivoire ([2], [3], [4], [5], [6] and [7]). Most of these studies were conducted from either aerial photographs or satellite images of low spatial resolutions such as NOAA / AVHRR [8]. However, through these studies, methods of estimates of the area of forest cover generally vary from one author to another, thus causing an inconsistency in the results [9]. Remote sensing allows a better assessment of the scope and scale of the problem of deforestation. Using multi-temporal data, one can better detect and analyze changes in the forest cover. By comparing images of previous years with recent scenes, it becomes possible to effectively measure the differences in the extent of deforestation and loss of rainforests in Côte d'Ivoire ([10] and [11]). In a first phase, the country's dense forest cover of the 1960s was determined from the digitization of existing vegetation map. In a second phase, the dense forest cover of the 1980s and 2000s were obtained by processing of Landsat TM and ETM+ satellite images. First the geometric correction of the satellite images was performed. Then, series of supervised classifications based on the maximum likelihood method were done to generate maps of land cover, with overall accuracies of 90%. From these land cover maps, the class of dense forests was extracted and converted into a map layer in vector format. By crossing dense forests covers of the 1980s and 2000 with those of the 1960s, the specific characterizations of those forests were derived (Figure 1, 2, 3). The results show a dense humid forest cover of about 8.4 million ha in 1960, with the presence of large blocks. The 1980s and 2000s have dense forest cover of 2.6 million ha and 1.35 million ha, respectively (Tables 1, 2, 3). These latter periods are characterized by high loss and fragmentation of rainforests in Côte d'Ivoire. Thus, since its independence, the country has lost more than 80% of its forest cover. The results of this study show that clearing associated with industrial agriculture mainly cocoa, coffee and rubber farming as well as demographic pressure and abusive exploitation of valuable wood species for export are the main causes of deforestation in Cote d'Ivoire. This study advocates the restoration of forests through the implementation of an awareness policy and reforestation one hand, and an efficient monitoring and protection of national protected areas and parks.\", 'title': \"Multitemporal monitoring of the forest cover in Côte d'Ivoire from the 1960s to the 2000s, using Landsat satellite images\", 'embedding': []}, {'id': 14762, 'abstractText': 'Radio map, serving as an efficient indicator of wireless environments, has been widely used in smart-city applications, including network monitoring/planning, anomaly signal detection, and indoor/outdoor localization. It is hard to maintain an update-to-date fine-grained radio map within a large area, since the radio map changes rapidly due to the internal and external factors. Previous studies usually relied on time-consuming site surveys at densely predefined reference points, leading to either coarse-grained or out-of-date radio maps. In this paper, we propose a fine-grained radio map reconstruction framework, called Supreme, based on crowd-sourced data in an image super-resolution manner. Specifically, Supreme explores spatial-temporal relationships in historical coarse-grained radio maps and builds a real-time fine-grained radio map using deep spatial-temporal reconstruction networks. Furthermore, a heterogeneous data fusion module is devised to make full use of external information. To evaluate the performance of Supreme, we conduct extensive experiments and ablation studies on a large-scale dataset with a total of six-month data collected from two university campuses. Besides, we investigate the transferability of Supreme in different locations and service networks, showing that the fine-tuned model can largely reduce the training time and achieve better performance. Experimental results demonstrate that our model outperforms state-of-the-art baselines and a case study on the localization is enhanced with marginal improvements on accuracy.', 'title': 'Supreme: Fine-grained Radio Map Reconstruction via Spatial-Temporal Fusion Network', 'embedding': []}, {'id': 14763, 'abstractText': 'The study of molecular gas is crucial for understanding star formation, feedback and the broader ecosystem of a galaxy as a whole. However, we have limited understanding of its physics and distribution in all but the nearest galaxies. We present a new technique for studying the composition and distribution of molecular gas in high-redshift galaxies inaccessible to existing methods. Our proposed approach is an extension of carbon monoxide intensity mapping methods, which have garnered significant experimental interest in recent years. These intensity mapping surveys target the 115\\xa0GHz <sup>12</sup>CO (1–0) line, but also contain emission from the substantially fainter 110\\xa0GHz <sup>13</sup>CO (1–0) transition. The method leverages the information contained in the <sup>13</sup>CO line by cross-correlating pairs of frequency channels in an intensity mapping survey. Since <sup>13</sup>CO is emitted from the same medium as the <sup>12</sup>CO, but saturates at a much higher column density, this cross-correlation provides valuable information about both the gas density distribution and isotopologue ratio, inaccessible from the <sup>12</sup>CO alone. Using a simple model of these molecular emission lines, we show that a future intensity mapping survey can constrain the abundance ratio of these two species and the fraction of emission from optically thick regions to order ∼30\\u2009per\\u2009cent. These measurements cannot be made by traditional CO observations, and consequently the proposed method will provide unique insight into the physics of star formation, feedback and galactic ecology at high redshifts.', 'title': 'Feeding cosmic star formation: exploring high-redshift molecular gas with CO intensity mapping', 'embedding': []}, {'id': 14764, 'abstractText': 'Showing flows of people and resources between multiple geographic locations is a challenging visualisation problem. We conducted two quantitative user studies to evaluate different visual representations for such dense many-to-many flows. In our first study we compared a bundled node-link flow map representation and OD Maps [37] with a new visualisation we call MapTrix. Like OD Maps, MapTrix overcomes the clutter associated with a traditional flow map while providing geographic embedding that is missing in standard OD matrix representations. We found that OD Maps and MapTrix had similar performance while bundled node-link flow map representations did not scale at all well. Our second study compared participant performance with OD Maps and MapTrix on larger data sets. Again performance was remarkably similar.', 'title': 'Many-to-Many Geographically-Embedded Flow Visualisation: An Evaluation', 'embedding': []}, {'id': 14765, 'abstractText': \"Non-linear learning technique called as mind mapping has recently emerged in higher education. With the help of information computer and technology, this study proposes the use of mind mapping technique in the development of e-Integral Map to assist students to think critically and link information in the teaching and learning of integration. This study aims to evaluate the students' perceptions on e-Integral Map web-based instruction by using a survey, to the selected Diploma of Chemical Engineering and Electrical Engineering respondents from UiTM Sarawak Campus. All respondents had used e-Integral Map in their study and had applied it through Integral Calculus courses. All the respondents completed the online survey form and submitted their feedback online. From the survey results, it was shown that all three aspects in web-based contents: the content, the instructional and the technical of e-Integral Map, were satisfactory. Majority of the respondents were positive on the use of e-Integral Map in assisting them to learn and solve Integral Calculus problems. In addition, the findings showed that the students' first experience on learning Integral Calculus with this technique provides the reason for such technique to be used in other topics of calculus education.\", 'title': \"Students' perceptions on teaching and learning of integral calculus through e-Integral Map\", 'embedding': []}, {'id': 14766, 'abstractText': 'We introduce <italic>Tilt Map</italic>, a novel interaction technique for intuitively transitioning between 2D and 3D map visualisations in immersive environments. Our focus is visualising data associated with areal features on maps, for example, population density by state. <italic>Tilt Map</italic> transitions from 2D choropleth maps to 3D prism maps to 2D bar charts to overcome the limitations of each. Our article includes two user studies. The first study compares subjects’ task performance interpreting population density data using 2D choropleth maps and 3D prism maps in virtual reality (VR). We observed greater task accuracy with prism maps, but faster response times with choropleth maps. The complementarity of these views inspired our hybrid <italic>Tilt Map</italic> design. Our second study compares <italic>Tilt Map</italic> to: a side-by-side arrangement of the various views; and interactive toggling between views. The results indicate benefits for <italic>Tilt Map</italic> in user preference; and accuracy (versus side-by-side) and time (versus toggle).', 'title': 'Tilt Map: Interactive Transitions Between Choropleth Map, Prism Map and Bar Chart in Immersive Environments', 'embedding': []}, {'id': 14767, 'abstractText': 'This paper studies the judgement problem of full-period maps on Z(p<sup>n</sup>) and proposes a novel congruential map with double modulus on Z(p<sup>n</sup>). The full-period properties of the sequences generated by the novel map are studied completely. We prove some theorems including full-period judgement theorem on Z(p<sup>n</sup>) and validate them by some numerical experiments. In the experiments, full-period sequences are generated by a full-period map on Z(p<sup>n</sup>). By the binarization, full-period sequences are transformed into binary sequences. Then, we test the binary sequences with the NIST SP 800-22 software package and make the poker test. The passing rates of the statistical tests are high in NIST test and the sequences pass the poker test. So the randomness and statistic characteristics of the binary sequences are good. The analysis and experiments show that these full-period maps can be applied in the pseudo-random number generation (PRNG), cryptography, spread spectrum communications and so on.', 'title': 'On the judgement of full-period sequences and a novel congruential map with double modulus on Z(p<sup>n</sup>)', 'embedding': []}, {'id': 14768, 'abstractText': \"Accurate and high-resolution maps of vegetation are critical for projects seeking to understand the terrestrial ecosystem processes and land-atmosphere interactions in Arctic ecosystems, such as U.S. Department of Energy's Next Generation Ecosystem Experiment (NGEE) Arctic. However, most existing Arctic vegetation maps are at a coarse resolution and with a varying degree of detail and accuracy. Remote sensing-based approaches for mapping vegetation, while promising, are challenging in high latitude environments due to frequent cloud cover, polar darkness, and limited availability of high-resolution remote sensing datasets (e.g., ~ 5 m). This study proposes a new remote sensing based multi-sensor data fusion approach for developing high-resolution maps of vegetation in the Seward Peninsula, Alaska. We focus detailed analysis and validation study around the Kougarok river, located in the central Seward Peninsula of Alaska. We seek to evaluate the integration of hyper-spectral, multi-spectral, radar, and terrain datasets using unsupervised and supervised classification techniques over a ~343.72 km<sup>2</sup> area for generating vegetation classifications at a variety of resolutions (5 m and 12.5 m). We fist applied a quantitative goodness-of-fit method, called Mapcurves, that shows the degree of spatial concordance between the public coarse resolution maps and k-means clustering values and relabels the k values based on the best overlap. We develop a convolutional neural network (CNN) approach for developing high resolution vegetation maps for our study region in Arctic. We compare two CNN approaches: (1) breaking up the images into small patches (e.g., 6 × 6) and predict the vegetation class for entire patch and (2) semantic segmentation and predict the vegetation class for every pixel. We also perform accuracy assessments of the developed data products and evaluate varying CNN architectures. The fusion of hyperspectral and optical datasets performed the best, with accuracy values increased from 0.64 to 0.96-0.97 when using a training map produced by unsupervised clustering and Mapcurves labeling for both CNN models.\", 'title': 'Convolutional Neural Network Approach for Mapping Arctic Vegetation Using Multi-Sensor Remote Sensing Fusion', 'embedding': []}, {'id': 14769, 'abstractText': 'Antarctica plays a significant role in global change studies. As a typical land cover in Antarctica, exposed rock is considered indispensable in many studies, and the mapping of exposed rocks is seen as a key basis of work to meet the demand for more accurate and updated datasets with the consecutive development of satellite technology. Although the normalized difference snow index has been commonly used for differentiating exposed rocks and snow, it often misidentifies clouds as rocks. The British Antarctic Survey has used Landsat 8 data to create a new rock outcrop map for Antarctica, overcoming the limitations of previous techniques and generating Antarctic Digital Database (ADD) New Rock Outcrop with higher accuracy than previously achieved. However, there are still some omission and commission errors apparent in the shaded areas, which affect the accuracy. Widespread shaded areas in Antarctica due to low solar elevation angles and extreme topography cause difficulty in accurately mapping exposed rock. In addition, major differences are present between existing products. Addressing the existing issues about extraction of exposed rock, this study used the near infrared band and shortwave infrared 2 band of Landsat 8 reflectance data to build a specific exposed rock index for the extraction of exposed rocks. A shadow detection method combined with a blue reflectance threshold is used for the shadowed rock identification. Accuracy assessment of these extraction results showed that the accuracy of the new product is higher than all existing exposed rock products. The conversion from DN values to top of atmosphere reflectance and the solar elevation correction for each pixel individually eliminate a variety of errors associated with the different acquisition times of each image. From the statistics of the reflectance related to the training samples, this paper established the threshold of exposed rock extraction so as to ensure the applicability of the same threshold for exposed rock extraction in all images. This method is applied to a total of 1100 high-quality images that were collected for covering the Antarctic continent from November 2013 to February 2014. The results show that 253 of the images contain exposed rocks, and these images were used for mapping work. The map showed that the main exposed rock areas are mainly distributed in four coastal regions: The Antarctic Peninsula, Queen Maud Land, Lambert Glacier basin, and Victoria Land regions. We also compared our results with ADD New Rock Outcrop and Bedrock Mapping Project 2 (Bedmap2) data in the four main regions. Our results were close to the ADD rock outcrops and exhibited remarkable differences with Bedmap2. We explored the possibility of analyzing and explaining these differences. Especially, as using the same data source Landsat 8 but with a different method, the comparison between our results and ADD New Rock Outcrop is discussed and concludes in shadowed rocks extraction, mixed-pixels, and omission disagreement. The results also show that shadowed rocks accounts for nearly 12% of the total exposed rocks and cannot be neglected. The method we developed can be quickly applied for extraction and mapping of large areas of exposed rocks using Landsat 8.', 'title': 'An Accurate and Automated Method for Identifying and Mapping Exposed Rock Outcrop in Antarctica Using Landsat 8 Images', 'embedding': []}, {'id': 14770, 'abstractText': \"Students attending object-oriented analysis and design (OOAD) courses typically encounter difficulties transitioning from requirements analysis to logical design and then to physical design. Concept maps have been widely used in studies of user learning. The study reported here, based on the relationship of concept maps to learning theory and semantic memory, suggests that integrating OOAD instruction with concept maps might help learners understand these transitions more clearly. An empirical experiment, involving two treatments based on concept maps and conventional instruction, was conducted to examine the relevance of concept map-based instruction for an effective understanding of phase transitions. The results indicate that compared with conventional instruction, concept map-based instruction is more efficient in improving learner understanding because of greater learner engagement and because of concept maps' properties when they are used as an information medium. This study can aid instructors in realizing the difficulties in learning phase transitions and motivate researchers to develop more effective learning instructions.\", 'title': 'Concept Maps as Instructional Tools for Improving Learning of Phase Transitions in Object-Oriented Analysis and Design', 'embedding': []}, {'id': 14771, 'abstractText': 'Many-core architectures are more promising hardware to design real-time systems than multi-core systems as they should enable an easier mastered integration of an higher number of applications, potentially of different level of criticalities. However, the worst-case behavior of the Network-on-Chip (NoC) for both inter-core and core-to-Input/Output (I/O) communications of critical applications must be established. We use the term core-to-I/O for both core communications from or to I/O interfaces. The mapping over the NoC of both critical and non-critical applications has an impact on the network contention these critical communications exhibit. So far, all existing mapping strategies have focused on inter-core communications. However, we claim that many-cores in embedded real-time systems will be integrated within backbone ethernet networks, as they mostly provide ethernet controllers as I/O interfaces. In this work, we first show that ethernet packets can be dropped due to an internal congestion in the NoC, if these core-to-I/O communications are not taken into account while mapping applications. To this end, we rely on a case study from the avionic domain. It is made of a critical Full Authority Digital Engine (FADEC) application and a non-critical Health Monitoring (HM) application of the engine, used for recognizing incipient failure conditions. Based on this analysis, we introduce our approach to map critical and non critical real-time applications over many-cores that reduces the WCTT of core-to-I/O communications. We show for two variants of our case study that our algorithm successfully find a mapping that avoids ethernet packets, whose payload are making the core-to-I/O communications, to be dropped. This demonstrates the benefits of our proposal compared to a state of the art mapping strategy that fails to do so.', 'title': 'Poster Abstract: I/O Contention Aware Mapping of Multi-Criticalities Real-Time Applications over Many-Core Architectures', 'embedding': []}, {'id': 14772, 'abstractText': 'Based on new data from permanent and temporary networks, we present fundamental mode Rayleigh wave group velocity maps at periods of 10–150 s related to the lithosphere beneath South America. We analyse waveform data from 1043 earthquakes, from 2002 to 2019, which were recorded by 282 stations. To isolate fundamental mode Rayleigh waves, a phase-matched filter is applied, and the measurements of group velocity are obtained from multiple filter analysis techniques. Thus, we obtain 17\\u2009838 paths, covering most of the South American continent, which reach their maximum at the period of 30 s and decrease for both shorter and longer periods. We calculate average dispersion curves and probability density distribution of all measured curves to check the consistency of our data set. Then, regionalized group velocity maps are obtained by iteratively combining the fast marching method and the subspace inversion method. The resolution of our models is assessed by checkerboard tests, which show that the synthetic group velocities are well recovered, despite some amplitude and smearing effects in some portions of the model, probably owing to regularization and uneven ray path coverage. Compared to previous group velocity studies for South America, our models present better resolution, mainly for shorter periods. Our maps of 10 and 20 s, for example, show an excellent correlation with the sedimentary thickness (CRUST1.0) and topography density (UNB<tex>$\\\\_$</tex>TopoDens). Regions of exposed basement and high-density are related to fast group velocities, while sedimentary basins and low-densities are observed as areas of slow group velocities. We identify small-scale fast group velocity heterogeneities that may be linked to the Rio Apa and Rio Tebicuary cratons as well as to the geochronological provinces of the Amazonian Craton. The most striking feature of our map at 40 s is a fast group velocity structure with the same NE trend of the Transbrasiliano lineament, a Neoproterozoic megashear fault that crosses a large part of the South American continent. Our long-period maps sample lithospheric depths, revealing that cratonic areas of South America, such as the Amazonian and São Francisco cratons, correlate well with fast group velocities. Another interesting feature is the presence of a strong group velocity gradient between the Paraná and Chaco-Paraná basins, which nearly coincides with the location of the Western Paraná Suture, a continental-scale gravity discontinuity. From our group velocity maps, we estimate 1-D S-wave velocity depth profiles at 10 locations in South America: Chaco-Tarija Basin, Borborema Province (BP), Amazonian Craton, Paraná Basin, Tocantins Province, Acre Basin (AcB), Altiplano-Puna Volcanic Complex, Mantiqueira Province (MP), Parnaíba Basin and São Francisco Craton. Most of our inverted S-wave velocity profiles show good agreement with the SL2013sv model at lithospheric depths, except the BP, AcB and MP profiles. Particularly for the BP, a low shear wave velocity, from about 75 to 150 km depth, is a feature that is not present in the SL2013sv model and was probably resolved in our model because of our denser ray path coverage. This decreased S-wave velocity may be due to a lithospheric thinning beneath the BP, as already pointed out by previous studies.', 'title': 'Rayleigh wave group velocity maps at periods of 10–150 s beneath South America', 'embedding': []}, {'id': 14773, 'abstractText': 'We study the problem of continuous maintenance of range sum heat maps over dynamically updating data objects. The range sum (RS) here refers to the sum of the weights of the data objects enclosed by a given range (rectangle) R. Range sum problems are useful in spatio-temporal data analytics and decision making processes. Recent studies on range sum problems focus on computing the MaxRS query, which finds a location to place a rectangle R such that its RS is maximized. In real applications, knowing only the location with the maximum RS may be insufficient, because decision making is a multi-factor process where maximizing the RS may just be one of the factors. It is also important to gain an overview of the RS distribution at different locations, so that decisions can be made based on global knowledge. We therefore propose to compute a range-sum heat map that visualizes the RS value for every location in a data space. Considering that data objects may be inserted into or removed from the data space dynamically, we further study the continuous maintenance of range-sum heat maps over dynamically updating data objects. We adapt algorithms to compute range-sum heat maps and to perform heat map updates. We build a demo system to showcase the usefulness of range sum heat maps and the effectiveness of the adapted algorithms.', 'title': 'Continuous Maintenance of Range Sum Heat Maps', 'embedding': []}, {'id': 14774, 'abstractText': 'Fuzzy cognitive maps are a soft computational technique with artificial neural network nature expressed by graphs used to model complex systems. Fuzzy cognitive maps are used in areas like health, business, energy, computer science, etc. in the literature. It is used to solve problems in many areas. Fuzzy cognitive maps have a non-dynamic method of weight determination and updating in their classical applications. This is usually done by subjecting the expert opinions to fuzzy membership functions. In some studies, the weights obtained by using expert data are improved by utilizing historical data of the system modeled. In some studies, weight determination and updating process is realized by using intuitive optimization methods without benefiting from expert opinions. Here, fuzzy cognitive maps using a different weight matrix for different input values are considered dynamic. In this study, we propose a dynamic fuzzy cognitive map structure that performs the weight update process with deep learning. Here, the cognitive map weights determined in the data set used for deep neural network training are determined by a genetic optimization-based method using past system data. The proposed new DFCM structure has been tested on two different scenarios. In addition, the performance comparison with deep artificial neural network models dealing with the same scenarios was performed. When the experimental and comparative results are examined, the performance of the proposed method is quite satisfactory.', 'title': 'A New Deep Neural Network Based Dynamic Fuzzy Cognitive Map Weight Updating Approach', 'embedding': []}, {'id': 14775, 'abstractText': 'Objective: For cardiac arrhythmia mapping and ablation procedures, the ability to record focal cardiac action potentials could aid in precisely identifying lesions, scarred tissue, and/or arrhythmic foci. Our study objective was to validate the electrophysiologic properties of a routinely employed large mammalian in vitro working heart model. Methods: Monophasic action potentials (MAPs) were recorded from 18 swine hearts during viable hemodynamic function both in situ (postmedian sternotomy) and in vitro (using Visible Heart methodologies). We placed specially designed mapping catheters in epicardial and endocardial locations. High-quality MAP signals were recorded for up to 2 h, and MATLAB was utilized to evaluate relative duration and temporal/regional changes in waveform morphology. Results: MAPs were reproducibly recorded from both epicardial and endocardial locations in situ and in vitro. No significant differences were noted in right atrial endocardial, right ventricular endocardial, right ventricular epicardial, or left atrial epicardial waveforms, when baseline recordings were compared to all other in situ and in vitro time points. Furthermore, MAP duration between right ventricular endocardial and epicardial waveforms was not significantly different, in situ or in vitro. Conclusion: The use of in vitro models like the Visible Heart is considered invaluable for the study of cardiac arrhythmias, the development of novel therapies, and/or preclinical testing of future cardiac mapping catheters and systems. Significance: Preclinical studies assessing in situ and/or in vitro recorded cardiac monophasic action potentials could be critical for the future development and validation of cardiac devices.', 'title': 'The Ability to Reproducibly Record Cardiac Action Potentials From Multiple Anatomic Locations: Endocardially and Epicardially, <italic>In Situ</italic> and <italic>In Vitro</italic>', 'embedding': []}, {'id': 14776, 'abstractText': 'Objective: Although HIFU has been successfully applied in various clinical applications in the past two decades for the ablation of many types of tumors, one bottleneck in its wider applications is the lack of a reliable and affordable strategy to guide the therapy. This study aims at estimating the therapeutic beam path at the pre-treatment stage to guide the therapeutic procedure. Methods: An incident beam mapping technique using passive beamforming was proposed based on a clinical HIFU system and an ultrasound imaging research system. An optimization model was created to map the cross-like beam pattern by maximizing the total energy within the mapped area. This beam mapping technique was validated by comparing the estimated focal region with the HIFU-induced actual focal region (damaged region) through simulation, in-vitro, ex-vivo and in-vivo experiments. Results: The results of this study showed that the proposed technique was, to a large extent, tolerant of sound speed inhomogeneities, being able to estimate the focal location with errors of 0.15 mm and 0.93 mm under in-vitro and ex-vivo situations respectively, and slightly over 1 mm under the in-vivo situation. It should be noted that the corresponding errors were 6.8 mm, 3.2 mm, and 9.9 mm respectively when the conventional geometrical method was used. Conclusion: This beam mapping technique can be very helpful in guiding the HIFU therapy and can be easily applied in clinical environments with an ultrasound-guided HIFU system. Significance: The technique is non-invasive and can potentially be adapted to other ultrasound-related beam manipulating applications.', 'title': 'Acoustic beam mapping for guiding HIFU therapy in vivo using sub-therapeutic sound pulse and passive beamforming', 'embedding': []}, {'id': 14777, 'abstractText': 'Quantitative PET image reconstruction requires an accurate map of photon attenuation coefficients (μ-map) in order to correct the PET emission data. Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients. In this study we examine the differences in the quantification of <sup>18</sup>F-FDG standardized uptake values (SUV) in head and neck cancer, using two different MR imaging sequences for MR-based attenuation correction (MRAC): a zero echo time (ZTE) sequence which can image bone directly (ZTE-MRAC), and a vendor-provided 2-point Dixon sequence that neglects bone (Dixon-MRAC). The μ-maps from each MRAC techniques were compared to CT-based attenuation correction (CTAC) maps. Percent SUV-mean and SUV-max differences in relevant regions of interest (ROIs) were calculated for three patients. Relative to Dixon-MRAC, we observed 15±7% and 14±8% increase of SUV-mean and SUV-max, respectively, when ZTE-based bone information was incorporated in the attenuation map and using Dixon-based attenuation map, respectively. We also observed that use of Dixon-MRAC led to 7±7% and 8±8% underestimation of SUV-mean and SUV-max, respectively, whereas with ZTE-MRAC led to 6±8% and 5±8% higher SUV-mean and SUV-max, respectively, compared to CTAC. This study is the first demonstration of ZTE-based attenuation correction in the head and neck region and compared with CTAC as a gold standard with the goal of improving PET quantitation. The study shows that incorporation of bone information on μ-maps has a significant impact on SUV quantitation in head and neck cancer lesions.', 'title': 'Evaluation of Zero-TE-based attenuation correction methods on PET quantification of PET/MRI head and neck lesions', 'embedding': []}, {'id': 14778, 'abstractText': \"Various sensors can be attached and added to autonomous vehicles, included visual cameras, radar, LiDAR (Light Detection And Ranging), and GNSS (Global Navigation Satellite System). These sensors have been studied in many research areas, in particular studies on building precise 3D maps. It is essential for autonomous driving to create an accurate 3D map of the surrounding scene. However, creating an accurate static 3D map is difficult due to changes in moving objects or dynamic environments. Spurious objects on the 3D map can be handled by removing or ignoring them for 3D mapping. Following this idea, we propose an object segmentation and inpainting network. The proposed network called SAM-Net, addresses the object duplication issue by segmenting the objects and inpainting them with the segmentation results. Conventional inpainting research has dealt with RGB images. No matter how well such approaches reconstruct holes or corrupted images, they do not establish 3D points' relationship with the point cloud frame. Therefore, we suggest a depth inpainting method for outdoor object segmentation and inpainting tasks that utilizes a high-precision depth range sensor (Velodyne HDL-64E), which is not suggested before. Unfortunately, no dataset exists for the outdoor depth inpainting task. Thus, to train our model, we generate a new dataset by locating objects on a clean static background. Moreover, our proposed method shows outstanding depth performance compared to the previous visual inpainting method. Our dataset will be available at: ``https://github.com/JunhyeopLee/lidar_inpainting''.\", 'title': 'SAM-Net: LiDAR Depth Inpainting for 3D Static Map Generation', 'embedding': []}, {'id': 14779, 'abstractText': 'Accurately monitoring forest dynamics in the tropical regions is essential for ecological studies and forest management. In this study, images from phase-array L-band synthetic aperture radar (PALSAR), PALSAR-2, and Landsat in 2006-2010 and 2015 were combined to identify tropical forest dynamics on Hainan Island, China. Annual forest maps were first mapped from PALSAR and PALSAR-2 images using structural metrics. Those pixels with a high biomass of sugarcane or banana, which are widely distributed in the tropics and subtropics and have similar structural metrics as forests, were excluded from the SAR-based forest maps by using phenological metrics from time series Landsat imagery. The optical-SAR-based forest maps in 2010 and 2015 had high overall accuracies (OA) of 92-97% when validated with ground reference data. The resultant forest map in 2010 shows good spatial agreement with public optical-based forest maps (OA = 88-90%), and the annual forest maps (2007-2010) were spatiotemporally consistent and more accurate than the PALSAR-based forest map from the Japan Aerospace Exploration Agency (OA = 82% in 2010). The areas of forest gain, loss, and net change on Hainan Island from 2007 to 2015 were 415 000 ha (+2.17% yr<sup>-1</sup>), 179 000 ha (-0.94% yr <sup>-1</sup>), and 236 000 ha (+1.23% yr<sup>-1</sup>), respectively. About 95% of forest gain and loss occurred in those areas with an elevation less than 400 m, where deciduous rubber, eucalyptus plantations, and urbanization expanded rapidly. This study demonstrates the potential of PALSAR/PALSAR-2/Landsat image fusion for monitoring annual forest dynamics in the tropical regions.', 'title': 'Mapping Forest and Their Spatial–Temporal Changes From 2007 to 2015 in Tropical Hainan Island by Integrating ALOS/ALOS-2 L-Band SAR and Landsat Optical Images', 'embedding': []}, {'id': 14780, 'abstractText': 'In search and rescue missions, time is an important factor; fast navigation and quickly acquiring situation awareness might be matters of life and death. Hence, the use of robots in such scenarios has been restricted by the time needed to explore and build a map. One way to speed up exploration and mapping is to reason about unknown parts of the environment using prior information. While previous research on using external priors for robot mapping mainly focused on accurate maps or aerial images, such data are not always possible to get, especially indoor. We focus on emergency maps as priors for robot mapping since they are easy to get and already extensively used by firemen in rescue missions. However, those maps can be outdated, information might be missing, and the scales of rooms are typically not consistent. We have developed a formulation of graph-based SLAM that incorporates information from an emergency map. The graph-SLAM is optimized using a combination of robust kernels, fusing the emergency map and the robot map into one map, even when faced with scale inaccuracies and inexact start poses. We typically have more than 50% of wrong correspondences in the settings studied in this paper, and the method we propose correctly handles them. Experiments in an office environment show that we can handle up to 70% of wrong correspondences and still get the expected result. The robot can navigate and explore while taking into account places it has not yet seen. We demonstrate this in a test scenario and also show that the emergency map is enhanced by adding information not represented such as closed doors or new walls.', 'title': 'SLAM auto-complete: Completing a robot map using an emergency map', 'embedding': []}, {'id': 14781, 'abstractText': 'This paper presents a weather map prediction method using RGB metaphorical feature extraction for atmospheric pressure patterns. In the field of meteorological science, predicting weather based on the analysis of observational data and the knowledge of weather experts is crucial. Weather experts draw weather maps based on air pressure distribution; hence, we believe that weather maps entail the interpretations of weather experts. In this study, we improved the prediction accuracy by using machine learning to recognize patterns of qualitative expert interpretations that cannot be predicted by analyzing observed data alone. The proposed method can be realized via two steps. The first is developing a module for extracting pressure pattern features from a weather map. Certain features, such as tropical cyclones or atmospheric high/low pressure distributions, are emphasized in weather maps to facilitate better understanding of the weather features. Therefore, we can predict weather features based on the knowledge of weather experts using data that contain their interpretations, particularly weather maps. The developed module extracts the atmospheric pressure features from the current weather map as an RGB metaphorical gradation map. The second step is developing a module to design a predicted weather map using the extracted features. The weather map of the following day is predicted using pix2pix. To the best of our knowledge, our method for extracting features from weather maps is the first to create a predicted weather map automatically.', 'title': 'Weather Map Prediction Using RGB Metaphorical Feature Extraction for Atmospheric Pressure Patterns', 'embedding': []}, {'id': 14782, 'abstractText': 'A radio map is a collection of signal fingerprints labeled with their collected locations. It is known that the performance of a fingerprint-based positioning systems is closely related to the precision and accuracy of the underlying radio maps. However, little has been studied on the performance of radio maps in relation to the fingerprint collection methods and the radio map models, which determine the accuracy and precision of radio maps, respectively. This paper evaluates the performance of various radio map construction methods in both indoor and outdoor environments. Four radio map construction methods, i.e., a point-by-point manual calibration, a walking survey, a semisupervised learning-based method, and an unsupervised learning-based method, have been compared. We also evaluate the performance of various types of radio map models that represent the characteristics of collected fingerprints. To demonstrate the importance of the radio map model, a new model named signal fluctuation matrix (SFM) was developed, and its performance was compared with that of the three conventional radio map models, respectively. The evaluation revealed that the performance of the radio maps was very sensitive to the design of radio map models and the number of fingerprints collected at each location. The performance achieved by SFM-based positioning was comparable with that of the other models despite using a small number of fingerprints.', 'title': 'Performance Evaluation of Radio Map Construction Methods for Wi-Fi Positioning Systems', 'embedding': []}, {'id': 14783, 'abstractText': 'Upscaling techniques have been extensively used to produce upscaled maps to fill data gaps serving various Earth observation models by providing area and landscape pattern information. Base maps as input for upscaling techniques inevitably have mapping errors that greatly impact the upscaling performance. However, the influence of mapping error on the representation of landscape pattern of upscaled maps has rarely been explored. To address this issue, the Crop Data Layer (CDL) data for two study sites were first used to generate agricultural maps as the base maps. A probability-based Monte Carlo algorithm was then used to simulate different error levels for the base maps. Two upscaling techniques, Fusing class Membership probability and Confidence level probability (FMC) and a conventional upscaling method (i.e., Majority Rule Based, MRB), were conducted. The results highlight that higher mapping error results in higher change of landscape pattern for upscaled maps. Overall, this work extends our understanding of the influence of mapping error on the upscaling performance. Also, it suggests that next generation upscaling techniques should greatly consider the mapping error and how to accurately present landscape pattern.', 'title': 'Comparing the impact of mapping error on the representation of landscape pattern on upscaled agricultural maps', 'embedding': []}, {'id': 14784, 'abstractText': 'Optimizing a network of maps among a collection of objects/domains (or map synchronization) is a central problem across computer vision and many other relevant fields. Compared to optimizing pairwise maps in isolation, the benefit of map synchronization is that there are natural constraints among a map network that can improve the quality of individual maps. While such self-supervision constraints are well-understood for undirected map networks (e.g., the cycle-consistency constraint), they are under-explored for directed map networks, which naturally arise when maps are given by parametric maps (e.g., a feed-forward neural network). In this paper, we study a natural self-supervision constraint for directed map networks called path-invariance, which enforces that composite maps along different paths between a fixed pair of source and target domains are identical. We introduce path-invariance bases for efficient encoding of the path-invariance constraint and present an algorithm that outputs a path-variance basis with polynomial time and space complexities. We demonstrate the effectiveness of our formulation on optimizing object correspondences, estimating dense image maps via neural networks, and 3D scene segmentation via map networks of diverse 3D representations. In particular, our approach only requires 8% labeled data from ScanNet to achieve the same performance as training a single 3D semantic segmentation network with 30% to 100% labeled data.', 'title': 'Path-Invariant Map Networks', 'embedding': []}, {'id': 14785, 'abstractText': 'In this work, we continue our study on analyzing student created mind maps automatically by providing a new methodology to select the technical vocabulary that students use in their mind maps. The basis of our previous experiments is an instructor chooses a set of twenty words used within the course that will be the set of words to test in mind maps. The instructor then creates their own mind map with this set of words, which is called the criterion map. Next, students create their mind maps using the same twenty words, and the criterion map and student map are analyzed with each other using various algorithms to produce metrics that quantify how similar the two maps are. When this activity is repeated longitudinally over a semester we can show that students are learning if their metrics of similarity are improving over time. One challenge, however, is which twenty words should be selected by the teacher. Similarly, will the set of twenty words impact the quality of observed learning. In 2011 and 2012, we collected our mind map data based on twenty words selected with no methodology (random). In 2013 and 2014, we created a methodology where approximately forty words are initially chosen, and these forty words are reduced down to 20 by creating a larger mind map and picking the words that have low connectivity. The hypothesis here is that less connectivity in the mind map will make it easier for the student to create their own quality maps. Our results show that this new methodology improves the arithmetic average of one of our best comparison metrics for all data points by a worse case of 2.4% better and best case 75% better.', 'title': 'Improved method for creating criterion maps for automatic mind map analysis', 'embedding': []}, {'id': 14786, 'abstractText': 'This paper presents preliminary simulations and analyses done to assess the feasibility of performing Map Relative Localization (MRL) with the Europa Lander LiDAR being developed for the Europa Lander Pre-Phase A concept. Map Relative Localization is the process of determining the horizontal position of a lander with respect to an onboard, a-priori map, by comparing the map to sensor observations of the terrain during deorbit, descent, and landing (DDL). Although kilometer-scale position knowledge is commonly available during DDL, landing in hazard-rich environments requires position errors of 100 m or less. Prior knowledge in the case of Europa Lander will be visual and topographic maps collected by the upcoming Europa Clipper mission. The Mars 2020 Lander Vision System (LVS) uses images from a camera to localize with respect to visual maps. This technology, as well as a 3D imaging LiDAR in development for hazard detection, is currently baselined for the Europa Lander Pre-Phase A concept. This paper investigates the potential use of the hazard detection LiDAR to perform MRL with respect to a 3D digital elevation model (DEM) provided by the Europa Clipper mission, as an alternative or backup solution to passive optical MRL. Compared to passive optical MRL, one advantage of LiDAR-based localization is that it is insensitive to lighting conditions, potentially relaxing requirements on synchronizing map acquisition and landing time of day. To analyze LiDAR based MRL performance, six representative terrains are synthetically up-sampled from Galileo-derived maps of Europa to a resolution of 0.5 m/px and covering an area of 4 km by 4 km. These maps are used as ground-truth to generate simulated noisy a-priori onboard topographic maps expected from Europa Clipper as well as simulated LiDAR DEMs generated at an altitude of 5 km during Europa Lander DDL. The simulated LiDAR DEM is matched against the simulated map via 2D normalized cross-correlation, exploiting the accurately known spacecraft attitude to avoid the need for more computationally intensive algorithms such as Iterative Closest Point (ICP). Two sources of measurement error are identified for analysis: 1) additive Gaussian noise in the range measurements from the Europa Lander LiDAR and the Europa Clipper derived maps and 2) errors in the LiDAR DEM induced by errors in the Europa Lander state estimate which is used to de-warp the LiDAR scan data into a DEM format. We assess the effect of each of these types of errors independently on matching performance as well as the overall performance when all types of error are introduced. Additionally, we present the result of a sensitivity study to terrain frequency content.', 'title': 'LiDAR-Based Map Relative Localization Performance Analysis for Landing on Europa', 'embedding': []}, {'id': 14787, 'abstractText': 'Disaster area mapping is critical to guiding evacuees to safety and aiding responders in decision-making. During disasters however, Cloud-based mapping services cannot be relied upon, because network infrastructures may have been damaged. In this study, we propose a disaster area mapping system that functions under challenged-network environments in a disaster area. The system infers a pedestrian map with walking speed information from data gathered by civilians and responders with mobile devices. To generate the map, the system addresses the following challenges: how to collect disaster area data, how to share data without continuous end-to-end networks, and how to generate maps without Cloud-based mapping services. First, the system leverages human mobility to collect disaster area data. Civilians and responders with mobile devices function as sensor nodes and log their GPS and velocity traces while moving based on the Post-Disaster Mobility Model. Second, the system uses mobile devices to establish a Delay-Tolerant Network, through which nodes opportunistically share data. Finally to generate the map, the collected data are routed to Computing Nodes: devices with more computational resources than mobile devices that are spatially-distributed across the disaster area. The Computing Nodes infer the map from the data and share it with evacuees. Through experimental evaluations and computer simulations, we found that the system significantly decreases the time required to generate and deliver a map to an evacuee, compared to a case without the system. Furthermore, the overall reduction in time increases as the size of the data required to generate the map and the number of DTN nodes increase.', 'title': 'Disaster area mapping using spatially-distributed computing nodes across a DTN', 'embedding': []}, {'id': 14788, 'abstractText': 'For the complex semantic features of objects in a geological map, maintaining a hierarchy between different geological objects is a big challenge when automatically generalizing the geological map. The typical methods focus on automation of geological map generalization or pay more attention to the hierarchical relation between geological objects, which reduces the accuracy of the geological map generalization result. Therefore, a conceptual framework that focuses on both the automated process and the geological objects is particularly important in developing an efficient software designed for automated generalization of geological maps. In this paper, we design a compound conceptual framework for automated generalization of geological maps based on multiple agents and workflow. In this framework, the process is divided into three stages: structure analysis, map generalization, and style standardization. The map objects in the source geological map are abstracted as diverse agents with different properties and behaviors, and the agents can communicate with each other when they are activated. Thus, the relationship of the map objects is coordinated in geological map generalization, avoiding the conflict between the different operation levels. The workflow technology is used to manage the automated process. We discuss the task, modeling method, and specific operation in every stage based on the current conceptual framework and the characteristics of a geological map. Finally, we use a simple geological map for experimental studies that verify the proposed conceptual framework. The result shows that it is advantageous to design the software for automated generalization of geological maps based on the proposed compound conceptual framework.', 'title': 'A Conceptual Framework for the Automated Generalization of Geological Maps Based on Multiple Agents and Workflow', 'embedding': []}, {'id': 14789, 'abstractText': 'Land use and land cover change (LULCC) can influence regional climate by altering the surface roughness, soil moisture, and heat flux partition. LULCC information therefore is important for providing a better understanding of land-atmosphere interaction. A key requirement for such data for use in climate studies is the generation of land use maps on at least an annual basis. However, current continuous annual land use and land cover maps are only available from 2001 to present, limiting the span of the period that can be studied. This study applied a random forests classifier based on nineteen phenological metrics to produce land use and land cover maps of China from 1982 to 2012 using Advanced Very High Resolution Radiometer (AVHRR) third generation NDVI (NDVI3g) dataset, and Moderate-resolution Imaging Spectroradiometer (MODIS) land cover type product (MCD12Q1). The overall accuracy is 86.3%, which indicates the reliability of the maps.', 'title': 'Continuous annual land use and land cover mapping using AVHRR GIMMS NDVI3g and MODIS MCD12Q1 datasets over China from 1982 to 2012', 'embedding': []}, {'id': 14790, 'abstractText': 'This paper presents a case study of flood mapping in Panamaram village, Wayanad, Kerala, India using SAR dataset. Rapid flood mapping can prove to be critical in decision making during flood due to high socio-economic loss associated. In this study, ground range detected (GRD) images of Sentinel-l mission, which are substantially less utilized, are exploited. Pre-event and post-event GRD images are analyzed in Sentinel Application Platform (SNAP) and extent of flooded area is mapped in GIS framework. The generated results are further validated by comparison of cumulative rainfall data during the flood period provided by Indian Meteorological Department, Thiruvananthapuram. Overall, study looks promising in developing flood maps subject to availability of real-time images.', 'title': 'RAPID FLOOD MAPPING USING SENTINEL-1A IMAGES: A CASE STUDY OF FLOOD IN PANAMARAM, KERALA', 'embedding': []}, {'id': 14791, 'abstractText': \"First name to gender mappings have been widely recognized as a critical tool to complete, study and validate data records in a range of different areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 6 million people, provided by a car insurance. We then study how naming conventions have changed over time and how they differ by nationality. Second, we build a probabilistic first name to gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in a two label and three label setting and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies' outcomes and find that our mapping produces high precision results. We validate that the additional information of nationality and year of birth improve the recall scores of name to gender mappings. Therefore, it constitutes an efficient process to improve data quality of organizations' records, whenever the attribute gender is missing or unreliable.\", 'title': 'Improving data quality through high precision gender categorization', 'embedding': []}, {'id': 14792, 'abstractText': 'The main task in navigation of an autonomous vehicle is to have accurate and robust localization. There are variety of localization techniques in study using particle filter; for localization with or without using map information in the measurements. There is a lack of study in localization methods employing map data in particle filter. This paper summarizes the localization techniques that use map information in particle filter for estimation. The study of various literature in the particle filter for localization using map data shows that, accurate and robust localization can be achieved for an autonomous vehicle provided the map to be used in the measurements is constructed precisely with the necessary features like road curbs, edges, trajectories etc.', 'title': 'Map-Based Particle Filter for Localization: Autonomous Vehicle', 'embedding': []}, {'id': 14793, 'abstractText': \"Accurate bathymetric mapping for shallow-water areas is essential for coastal and maritime engineering applications. However, traditional multibeam or light detection and ranging (LiDAR) survey techniques used to produce high-quality bathymetric maps are expensive. Satellite-derived bathymetry provides a fast and inexpensive method for the large-scale mapping of shallow-water areas and can overcome the complexities of traditional bathymetric mapping methods in these areas. Traditionally, linear regression models, most commonly the Stumpf model, are used for satellite-based bathymetric modeling. However, nonlinear artificial neural network (ANN) models have been recently developed and implemented for satellite-based bathymetric modeling and are under significant investigation to develop the most accurate and optimal model. This article proposes two new hybrid ANN-based models for bathymetric modeling and investigates their performance using satellite imagery data and ``truth'' depth data for a coastal shallow-water study area. Two-hybrid ANN algorithms are developed, namely, particle swarm optimization (PSO)-ANN and optimally pruned extreme learning machine (OPELM), and their results are compared with the traditional Stumpf method and current state-of-the-art ANN model. The study area dataset comprises the ``truth'' depth data from a nautical chart of the Alqumriyah Island study area in Saudi Arabia and the corresponding spectral reflection values of green, blue, and near-infrared bands from the free-of-charge Level-1C product of Sentinel-2A images used to train and validate the two newly developed models and the traditional models. The results show that the developed OPELM method can accurately derive the bathymetry and is superior to the developed PSO-ANN model, the current state-of-the-art ANN model, and the traditional Stumpf model by 12.10%, 18.76%, and 32.46%, respectively. The OPELM model can also be used for bathymetric modeling of shallow-water areas with depths up to 30 m with a high level of accuracy compared with the current state-of-the-art ANN and traditional methods. The significant contribution of this research is that it is the first investigation of the artificial intelligence-based hybrid OPELM method for accurate bathymetric modeling and will certainly encourage further investigations of hybrid models. Moreover, this research explores whether these developed hybrid models can meet the International Hydrographic Organization standards for hydrographic survey applications.\", 'title': 'Hybrid Artificial Neural Networks for Modeling Shallow-Water Bathymetry via Satellite Imagery', 'embedding': []}, {'id': 14794, 'abstractText': 'Context: Mobile computing has emerged as a disruptive technology that has empowered its users with portable, connected and context-aware computation. However, issues such as resource poverty, energy efficiency and specifically data security and privacy represents the critical challenges for mobile computing. Objective: The objective of this work is to systematically identify, taxonomically classify and map the state-of-research on adaptive security (a.k.a. self-protection) for mobile computing. Methodology: We followed evidence based software engineering method to conduct a systematic mapping study of 43 qualitatively selected studies-published from 2003 to 2017-on adaptive security for mobile computing. Results and Conclusions: Classification and mapping of the research highlights three prominent themes that support adaptive security for (i) Mobile Device Data and Resources, (ii) Mobile to Mobile Communication, and (iii) Mobile to Server Communication. Mapping analysis suggests that security of mobile device data and resources is the most researched theme. The mapping study highlights that active and futuristic research trends are primarily focused on security as a service, whereas; the frequent research challenges relate to self-protecting mobile devices, user-driven privacy decisions and context-aware security. The results of the mapping study facilitate knowledge transfer that can benefit researchers and practitioners to understand the role of adaptive and context-aware security in mobile computing environments.', 'title': 'Classification and Mapping of Adaptive Security for Mobile Computing', 'embedding': []}, {'id': 14795, 'abstractText': 'This paper investigates spaceborne multiple multispectral data-fusion and blending to generate an integrated data with higher spatio-spectral resolution and spectral coverage in order to obtain improved geological mapping. A hybrid approach using Gram-Schmidt pan-sharpening and Inverse Distance Weighting (IDW) based downsampling technique is developed to generate integrated data from multiple multispectral data. In this study, Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER), Landsat 8, and Sentinel-2 data have been used to evaluate the developed approach for lithological mapping. Liikavaara to Puoltikasvaara including Nautanen and nearby-mining area, in the Gällivare district of Norrbotten county, Sweden, is chosen as a case study. Lithological map of the study area is produced using Support Vector Machine (SVM) classifier. Bedrock geological map from the Geological Survey of Sweden (SGU) is used for classification accuracy assessment. The results show that integrated data produced better accuracy than original individual spaceborne multispectral data for lithological mapping of the study area.', 'title': 'Multiple Multi-Spectral Remote Sensing Data Fusion and Integration for Geological Mapping', 'embedding': []}, {'id': 14796, 'abstractText': 'This paper is about an algorithm that displays the elevation and temperature of a surveillance area in the form of a map using a mobile robot platform equipped with a Lidar and a thermal imaging camera, and determines whether it is abnormal. Localization is performed using Lidar, and the point cloud and thermal distribution at the corresponding location are expressed as a top projection type map. The collected elevation and temperature distribution maps are checked for anomalies using the first-stage auto-encoder and second-stage CNN algorithm. We collected data on normal situations and abnormal situation DB produced by various size boxes and gas burners to study. As a result of the study, it was possible to detect 74.4% for the first stage, 94.5% for the second stage, and 80.2% for the first stage and 99.3% for the second stage for the thermal map. The results of this study will be used in areas such as security robots and guide robots.', 'title': 'Anomaly Detection using Elevation and Thermal Map for Security Robot', 'embedding': []}, {'id': 14797, 'abstractText': 'The evolution of coherent waves in two-dimensional systems with a random potential can give rise to an interesting phenomenon known as Branched Flow. In this process, a wave scatters from a weak random potential with correlation length longer than the wavelength, and forms focused channels that keep dividing as the wave propagates, creating a pattern resembling the branches of a tree. This phenomenon was observed for electrons [1-3], for a specific example of microwaves [4] and for ocean waves [5], but thus far never for light at optical frequencies. Furthermore, the statistical features of branched flow were predicted theoretically but were never observed in any experimental system. Here, we present the first observation of branched flow in optics, prove that the experiments represent branched flow, and study the statistical features. In our experiments, we couple an optical beam to a thin liquid soap film and observe its evolution within this thin membrane. The light experiences scattering from thickness variations in the soap film, which acts as a two-dimensional medium with a random potential. The beam propagates and scatters from the random thickness variations, forming focused branches that keep dividing, ending up in a pattern that resembles the branches of a tree, as shown in Fig.1a. To view the thickness variations directly, we construct a white light microscope illuminating the thin soap film from above, and observe the colorful map shown in Fig. 1b. The colors in Fig. 1b are true colors, and they emerge due to the reflection of white light from the thin soap film, indicating the local thickness (Fig.1c). The colors are mapped to the thickness map shown in Fig. Id, and from that to a two-dimensional map of effective refractive index landscape. The formation of the branches is the result of the appearance of caustics in the random field, following the variations in the refractive index. The theory of Branched Flow [6] gave rise to several predictions, most of which have never been studied in experiments. One example for such a prediction is the distance to the first caustic, d<sub>0</sub>, which was predicted by never observed. From the experiments, we extract d<sub>0</sub> using the Scintillation Index - the variance of the intensity of the fluctuations. It is a convenient notion, because caustics give rise to the highest intensity fluctuations, hence the scintillation index is a measure of the steepness of the caustics. The first caustic is the steepest one; hence, it corresponds to the peak in the scintillation index shown in Fig. 1e. The distance to the first caustic is ~1 in units of ζε<sup>-2/3</sup>, correlation length and potential strength, as shown in that figure. In this regime, the phenomenon of branching of caustics perfectly matches the model of branched flow. To support our experiments, we also carry out simulations (Fig. 1e), of a coherent beam launched into a random 2D potential constructed from the actual experimental image. The beam splits and divides by the potential variations, displaying branched flow as in experiments. From the experiments, we extract additional statistical features of branched flow that were predicted but have never been observed, such as the statistics of the extreme events. In addition, we also study branched flow in curved space (where the soap membrane is curved) and the nonlinear behavior occurring when the light exerts forces on the soap bubble.', 'title': 'Observation of Branched Flow of Light', 'embedding': []}, {'id': 14798, 'abstractText': 'This paper presents the dynamic analysis of two discrete logistic chaotic maps versus the conventional map. The first map is the fractional logistic map with the extra degrees of freedom provided by the added number of variables. It has two more variables over the conventional one. The second map is the double-humped logistic map. It is a fourth-order map which increases the non-linearity over the conventional one. The dynamics of the three maps are discussed in details, including mathematical derivations of fixed points, stability analysis, bifurcation diagrams and the study of their chaotic regions. The chaotic behavior of the three maps, is investigated using the Maximum Lyapunov exponent (MLE).', 'title': 'Dynamics of fractional and double-humped logistic maps versus the conventional one', 'embedding': []}, {'id': 14799, 'abstractText': 'Time-of-Flight (TOF) PET data determines the attenuation map up to a constant. MLAA (Maximum Likelihood Activity and Attenuation Estimation) was proposed for this purpose. However, in real systems the estimated attenuation map usually results in bias and artifacts, due to various factors such as non-uniform timing resolution, detector timing drift, and biased scatter estimation. Moreover, MLAA has much higher computational cost than conventional PET reconstruction. Improving the practical performance of MLAA is important. We proposed an efficient and robust emission-attenuation joint estimation framework, based on the condition that regions with almost uniform attenuation coefficients are segmented. Following the derivation, the update of attenuation map only requires a few weighted additions in sinogram space, which reduces overall computational cost significantly compared with conventional MLAA, as the latter demands at least two backward projections in each iteration. Furthermore, as the parameter space for the attenuation map is reduced and that the math model encourages an averaging effect in reach region, the bias and artifacts due to various factors above can be reduced. We used clinical TOF PET data to evaluate the performance of our proposed method. In each iteration, the computation time for updating the attenuation map was less than 20% of that for conventional MLAA, leading to significant improvement in overall computation efficiency (twice as fast as conventional MLAA). More importantly, the method resulted in high quantitative accuracy. In population study, SUV computed with the estimated attenuation map and the CT based attenuation map had maximum relative error less than 5.7% in multiple VOIs including the spine, liver, kidney, and heart. Our method can be used as a robust and efficient solution to estimate the attenuation map for quantitative PET image reconstruction, based on a single assumption that the attenuation coefficients are similar in each segmented regions. The numerical error caused by treating attenuation coefficients as uniform within each segmented region is acceptable.', 'title': 'Ultra efficientand robust estimation of the attenuation map in PET imaging', 'embedding': []}, {'id': 14800, 'abstractText': \"This study aimed to assess the potential of GLAS (Geoscience Laser Altimeter System) LiDAR data to overcome the saturation at high AGB values of existing AGB map on Madagascar (Vieilledent's AGB map [1]). First, spatially distributed estimations of AGB were obtained from GLAS data. Second, the difference between the Vieilledent's AGB map and GLAS derived AGB at each GLAS footprints location was calculated and a spatially distributed additional correction factors were obtained. Thanks to the spatial structure of these additional correction factors, an ordinary kriging interpolation was thus performed to provide a continuous correction factor map. Finally, the existing and the correction factor map were summed to improve the Vieilledent's AGB map. Results showed that the integration of GLAS data overcome the saturation at high AGB of Vieilledent's AGB map and allow AGB estimation until 650 t/ha (maximum AGB values from Vieilledent AGB map was 550 t/ha).\", 'title': 'Integration of spaceborne lidar data to improve the forest biomass map in madagascar', 'embedding': []}, {'id': 14801, 'abstractText': 'Temporary of preservation regional planning map need to evaluate and renewable execute so deviate possibilities can be prevented. This study is conducted to evaluate the regional planning map in East Lombok based on potential land-use using directive and existing land-use map. Directive land-use map is made by overlaying of some parameters, such as the slope, soils, and rainfall using GIS. Existing land-use map produced by visual interpretation is utilizing Landsat 8 OLI satellite imagery. The field survey did use stratified random sampling to validate parameters, accuracy field survey is about 68% of the existing land-use map. Result based on spatial and regional planning map evaluation of East Lombok using directive land-use and the existing land-use map of suitability is good enough reach 80.15% and not suitable reach 19.85%.', 'title': 'Evaluation of spatial and regional planning map using remote sensing and GIS in East Lombok Indonesia', 'embedding': []}, {'id': 14802, 'abstractText': 'This study is about the disastrous flooding of an Indian metropolitan area of Chennai when the rain had nearly broken the record of 100-years with 374 mm rain falling on December 1, 2015, virtually breaking the November monthly average of 407.4 mm in a day. This city with a population of approximately 6.7 million people came to a standstill. Astonishingly, one of the biggest software development hubs in India was struggling for data and tools to identify which parts of the city were most affected and vulnerable to such climate phenomena. A group of software engineers quickly came up with an idea of using a Flood Map tool managed through crowdsourcing to help the citizens of Chennai and prevent further casualties. They developed a map-based tool called Flooded Streets to report flooded streets using OpenStreetMap (OSM) data. Using this Flood Map tool, anyone in the crowd could click on a street if they knew it was flooded and update the map information. Within the following 24 hours, over 2,500 streets had been reported as flooded by the citizens of Chennai using the Flood Map tool. An ordinary citizen could zoom into a locality, visualize which streets are reported as flooded and decide their next course of action. This map was also a great aid tool for relief and aid workers to track the flooded paths and provide appropriate aids in that area. The map consists of a base layer of low-lying areas created using elevation models from ISRO and NASA, and flooded areas from UNITAR. The map interactivity was built using Mapbox GL and hosted on GitHub. This crowdsourced sensing system is an extraordinary example of disaster response using the crowdsourcing concept, which potentially helped millions of people with the minimum time and resources but with great crowd contributions from both experts and non-experts.', 'title': 'Flooded streets — A crowdsourced sensing system for disaster response: A case study', 'embedding': []}, {'id': 14803, 'abstractText': \"Chaotic dynamics is an important source for generating pseudorandom binary sequences (PRBS). Much efforts have been devoted to obtaining period distribution of the generalized discrete Arnold's Cat map in various domains using all kinds of theoretical methods, including Hensel's lifting approach. Diagonalizing the transform matrix of the map, this paper gives the explicit formulation of any iteration of the generalized Cat map. Then, its real graph (cycle) structure in any binary arithmetic domain is disclosed. The subtle rules on how the cycles (itself and its distribution) change with the arithmetic precision e are elaborately investigated and proved. The regular and beautiful patterns of Cat map demonstrated in a computer adopting fixed-point arithmetics are rigorously proved and experimentally verified. The results can serve as a benchmark for studying the dynamics of the variants of the Cat map in any domain. In addition, the used methodology can be used to evaluate randomness of PRBS generated by iterating any other maps.\", 'title': \"The Graph Structure of the Generalized Discrete Arnold's Cat Map\", 'embedding': []}, {'id': 14804, 'abstractText': 'The significant capability of synoptic coverage of satellite remote sensing data at the time of advent providing accurate and immediate valuable information on various aspects. The progressive development of remote sensing increases its capability to identify and map the precious and valuable materials and minerals. Spectral Angle Mapper (SAM) is classifiers of supervised classification for mapping and classification. The characteristic of SAM is based on similarity between image spectra and reference spectra on the behalf of tolerance level of specified maximum angle of threshold. In this research work SAM algorithms applied for mapping of Chhabadiya talc mineral. In this research work SAM algorithms defines the applicability of similarity of angle and value of threshold parameters of spectral angle which show capability to interpret or map the maximum and minimum abundance of talc minerals. For this research work Hyperion hyperspectral remote sensing data used for study of applicability and efficiency of SAM algorithms for mineral mapping. The quality and abundance of minerals completely depend on the minimum degree of SAM threshold parameter. Maximum spectral angle shows maximum mapping area with minimum similarity, but low spectral angle shows small and more abundant mapping area with maximum similarity in the hyperspectral image. Conclusion of this research work verify the identification of minerals are depend on the spectral and spectral characteristics of hyperspectral remote sensing data and mapping with qualitative abundance of minerals depend on the lower value of spectral angle and threshold of SAM algorithms or maximum similarity of spectra.', 'title': 'Comparative Evaluation Threshold Parameters of Spectral Angle Mapper (SAM) for Mapping of Chhabadiya Talc Minerals, Jahajpur, Bhilwara, India using Hyperion hyperspectral Remote Sensing Data', 'embedding': []}, {'id': 14805, 'abstractText': \"This paper shares an innovative approach of using mind map to support active leaning for electrical &amp; electronics engineering students. It is also aimed at exploring students' experiences in using mind maps. In this study, 80 students were asked to respond to 2 online surveys; one, prior to the mind map activity and the other after. The findings for the survey prior to the mind map activity showed that, majority of the students are familiar with mind map and are aware that mind map does help them to learn and that most are willing to put in some effort to use mind map for learning purposes. The second survey's findings showed that even though some respondents have pessimistic attitude, generally the response have been positive and well received and some students ask for more guidance and training to be able to master the mind map learning technique.\", 'title': 'An Innovative Approach of Using Mind Map to Support Active Learning for Engineering Students', 'embedding': []}, {'id': 14806, 'abstractText': 'Generally robots need to use maps while navigating from one place to another. Metric (grid-based) and topological (node-based) paradigms are utilized to produce maps. In this work, topological map of a large-scale indoor environment is constructed from its metric map. To do this, we follow three stages: 1) Construction of metric map; 2) Determination of nodes; 3) Connecting the nodes to produce the topological map. Apart from the studies in literature, only the new cells in the metric map are considered to determine the nodes. As a result, the topological map grows in an online manner and computational cost of spectral clustering and extended Voronoi graph is reduced. The methods are tested in a simulated model of ESOGU Electrical Engineering Laboratory building in Gazebo simulation environment by using ROS.', 'title': 'A comparative study for topological map construction methods from metric map (In English)', 'embedding': []}, {'id': 14807, 'abstractText': 'With the rapid development of solar distribution, solar panel mapping is becoming increasingly valuable to decision-makers. Weakly supervised methods have been developed to reduce the cost in training sample collection, and the most successful ones follow the alternative training scheme, which first generates coarse object localizations as pseudo labels (PLs) and then utilizes these PLs to train an end-to-end network for object extraction. As remote sensing images are typically characterized by multiple occurrences of objects and complicated background, the alternative training scheme suffers from low mapping accuracy and deficient boundary maintenance due to the varying quality of PLs. In this paper, we focus on addressing these problems by adaptively adjusting the contributions of quality-varying PLs and propose a novel self-paced residual aggregated network (SP-RAN) for solar panel mapping. Specifically, with the initial PLs generated by gradient-weighted class activation mapping, a residual aggregated network is designed for target mapping with a special consideration for the capability in producing complete and well-shaped mapping results. Considering the inconsistent quality of PLs, an effective confidence-aware (CA) loss is developed to emphasize the contribution of high-quality PLs and alleviate the negative impacts brought by the bad-quality ones in the training phase. Moreover, to concentrate on boundary maintenance, a novel self-paced label correction (SP-LC) strategy is proposed to selectively update PLs by considering their reliability. Extensive experimental comparison with state-of-the-art methods and ablation study on two aerial data sets and a remote sensing data set demonstrate the superiority of the proposed method.', 'title': 'SP-RAN: Self-paced Residual Aggregated Network for Solar Panel Mapping in Weakly Labelled Aerial Images', 'embedding': []}, {'id': 14808, 'abstractText': 'In vehicle and robot navigation low-level tasks such as path planning, obstacle avoidance and autonomous operation are extensively studied nowadays. Most of these task require map building. In this paper a map representation is discussed with the focus for the singular domain of our Neobotix MP500 mobile robot. Among others the state of the art map building techniques will be introduced such as topological map, line map, landmark-based map and of course in more detail the occupancy grid based map. The probabilistic representation of the occupancy grid will be examined as a map building problem for the given mobile robot.', 'title': 'Probabilistic occupancy grid map building for Neobotix MP500 robot', 'embedding': []}, {'id': 14809, 'abstractText': 'Most navigation aids for blind people require users to equip various equipment and multi-sensor subsystems for environmental detection. This study integrates the visual simultaneous localization and mapping system with pre-established maps to develop a navigation aid system for blind people. The system operates similarly to how Google Map uses pre-established environmental maps and landmarks. While the user walks, the system reads the map in the database and synchronizes feature points from the visual simultaneous localization and mapping system onto the environment map. In addition, the aid employs audio cues to notify users of signpost information and enables users to make inquiries on specific destinations via human-machine interaction.', 'title': 'A Navigation Aid for Blind People Based on Visual Simultaneous Localization and Mapping', 'embedding': []}, {'id': 14810, 'abstractText': \"The elastic map, or generalized Hooke's Law, associates stress with strain in an elastic material. A symmetry of the elastic map is a reorientation of the material that does not change the map. We treat the topic of elastic symmetry conceptually and pictorially. The elastic map is assumed to be linear, and we study it using standard notions from linear algebra—not tensor algebra. We depict strain and stress using the ‘beachballs’ familiar to seismologists. The elastic map, whose inputs and outputs are strains and stresses, is in turn depicted using beachballs. We are able to infer the symmetries for most elastic maps, sometimes just by inspection of their beachball depictions. Many of our results will be familiar, but our versions are simpler and more transparent than their counterparts in the literature.\", 'title': 'Elastic symmetry with beachball pictures', 'embedding': []}, {'id': 14811, 'abstractText': 'Most autonomous mobile systems require pre- generated detailed maps that are expensive to prepare. In this study, we proposed an autonomous mobile system that does not rely on a detailed map; instead, it uses a simple map for robot navigation. The robot has two maps; one is a detailed map created by sensor observation during movement, and the other is a simple map provided as pre-information. The robot performs the matching between detailed and simple maps and converts waypoints on the simple map to waypoints on the detailed map. Matching is performed based on straight line matching and is optimized by genetic algorithm. Experiments were conducted in buildings. Our method is compared with linear transformations and conditions that our method works effectively or not are confirmed.', 'title': 'Transformation Between Simple and Detailed Maps Based on Line Matching for Robot Navigation', 'embedding': []}, {'id': 14812, 'abstractText': 'Recent work in data visualization has demonstrated that small, perceptually-distinct color palettes such as those used in categorical mapping can connote significant affective qualities. Data that are mapped or otherwise visualized are also often emotive in nature, either inherently (e.g., climate change, disease mortality rates), or by design, such as can be found in visual storytelling. However, little is known about how the affective qualities of color interact with those of data context in visualization design. This paper describes the results of a crowdsourced study on the influence of affectively congruent versus incongruent color schemes on categorical map-reading response. We report both objective (pattern detection; area comparison) and subjective (affective quality; appropriateness; preference) measures of map-reader response. Our results suggest that affectively congruent colors amplify perceptions of the affective qualities of maps with emotive topics, affective incongruence may cause confusion, and that affective congruence is particularly influential in maps of positive-leaning data topics. Finally, we offer preliminary design recommendations for balancing color congruence with other design factors, and for synthesizing color and affective context in thematic map design.', 'title': 'Affective Congruence in Visualization Design: Influences on Reading Categorical Maps', 'embedding': []}, {'id': 14813, 'abstractText': 'The shadowing property has an important significance in terms of theory and application. In this paper, we study the shadowing property of product map in the two-dimensional product space. By means of properties of zero density sets and product map, we give the following conclusions. (1)The product map f × g has the pseudo orbit tracing property if and only if the map f and g has the pseudo orbit tracing property. (2)The product map f × g has the periodic pseudo orbit tracing property if and only if the map f and g has the periodic pseudo orbit tracing property. (3)The product map × g has the asymptotic average shadowing property if and only if the map f and g has the asymptotic average shadowing property. These results enrich the theory of the shadowing property of the product map in two-dimensional product spaces.', 'title': 'Various Shadowing Property of Product Map', 'embedding': []}, {'id': 14814, 'abstractText': 'Identification of targets for catheter ablation of arrhythmias remains a significant challenge. Traditional mapping techniques often neglect the tissue repolarization, hampering the detection of pro-arrhythmic regions. We have recently developed a novel mapping procedure, termed the Reentry Vulnerability Index (RVI), which incorporates both activation (AT) and repolarization (RT) times to identify ablation targets. Despite showing promise in a series of experiments, the RVI requires further development to enable its incorporation into a clinical protocol. The goal of this study was to use computer simulations to optimize the RVI procedure for its future usage within the clinic. A 2D sheet model was employed to investigate the behavior of the RVI algorithm under mapping catheter recordings resembling clinical conditions. Conduction block following premature stimulation was induced and mapped in a cardiac tissue model including repolarization heterogeneity. RVI maps were computed based on the difference between RTs and ATs between successive pairs of electrodes within a given search radius. Within 2D sheet models we show that RVI maps computed on irregular sparse recording sites were in good agreement with high resolution maps. We concude that the RVI algorithm performed well under clinically-relevant mapping conditions and may be used to guide ablation.', 'title': 'Optimization of a Novel Activation-Repolarization Metric to Identify Targets for Catheter Ablation', 'embedding': []}, {'id': 14815, 'abstractText': 'Due to the similarity of the radar backscatter in flooded and unflooded conditions over particular areas, it is not possible to carry out a comprehensive SAR-based flood mapping at large scale. In this paper, an additional information layer derived from Sentinel-l time series data, called Exclusion map (EX-map), is introduced. Its aim is to enhance and complement the results of automatic change detection-based flood mapping methods. The EX-map aims at delineating areas where observed variations of SAR backscatter do not allow detecting the appearance of floodwater. The EX-map is mainly composed of the following land cover classes: topographic shadow/layover, double bounce and smooth tarmac in urban areas, arid areas, dense vegetation and permanent water bodies. The method is evaluated over six study sites across the globe and tested for different flood events. The EX-map not only increases the classification accuracy of change detection-based flood maps derived from Sentinel-l data from 95.92% to 97.02%, but also enables a better interpretation of any SAR-based floodwater map.', 'title': 'Deriving an Exclusion Map (Ex-Map) from Sentinel-l Time Series for Supporting Floodwater Mapping', 'embedding': []}, {'id': 14816, 'abstractText': 'Recently, various types of robots have developed for the exploration and monitoring in unknown and dynamic environment. Especially, the expectation of robots used in disasters is increasing to prevent the second disaster. Especially, it is very important to extract the environmental information related for remote control and monitoring of mobile robots. Simultaneous localization and mapping (SLAM) is an important methodology to deal with environmental information. Various types of methods for SLAM have been proposed such as Extended Kalman Filter (EKF) SLAM, Graph SLAM, visual SLAM, and cooperative SLAM. In general, there are two main approaches of grid mapping and topological mapping in the study on SLAM. In this talk, we focus on topological mapping methods to extract environmental features from a 3D point cloud. Various types of unsupervised learning methods based on topological mapping have been proposed to deal with environmental features in unknown environments. One of them is Growing Neural Gas (GNG) that can dynamically change the topological structure composed of nodes and edges. The advantage of GNG is in the incremental learning capability of nodes and edges according to target data distribution. We have proposed several different topological mapping methods based on GNG to extract the environmental features from a 3D point cloud until now. In this talk, we explain the research background of SLAM, the learning algorithm of GNG, and experimental results of GNG for SLAM in various environments. Next, we explain multi-layer GNG to extract hierarchical features in environmental maps as a multi-scale approach, and batch learning algorithm for GNG (GL-GNG) to improve the convergence property. Furthermore, we explain the modified method of GNG-utility (GNG-U), that we called GNG-U2. GNG-U2 can improve the real-time adaptability of extracting topological structure in non-stationary data distribution. Next, show experimental results of SLAM based on GNG-U2 in dynamic environments. Finally, we show several other application examples of topological approaches, and discuss the applicability and future direction of topological approaches in robotics.', 'title': 'Topological approaches for simultaneous localization and mapping', 'embedding': []}, {'id': 14817, 'abstractText': \"Landsat imagery can be used to establish spatial habitat suitability modeling for prediction habitat quality and evaluation potential habitat. However, the imagery has to limit information for specific habitat characteristics such as turtles' habitat. Chelodina mccordi is an endemic turtle from Roti Island but many previous studies have been done either biological or non-spatial studies. This study uses Landsat-8 OLI and TIRS as actual data and support with Landsat-5 TM as historical data, through a Geographic Information System (GIS). This study aims to create a spatial habitat suitability map model of C. mccordi and its mapping accuracy. The method that is used for modeling is overlaying indicative parameter maps use of logistic regression to presents the habitat suitability index (HSI). There are nine parameters used for modeling in this study, i.e. normalized difference water index (NDWI), land surface temperature (LST), slope, the topographic wetness index (TWI), distance from high canopy density, distance from settlement and agriculture, distance from freshwater, distance from the sea or salty, and distance from the street. We found three parameters that have strong contribution i.e. the NDWI, the LST, and the TWI. The result of spatial habitat suitability model of C. mccordi based on Landsat-8 OLI/TIRS and GIS analysis presents the value of mapping accuracy is about 75%.\", 'title': 'Spatial Habitat Suitability Modeling of the Roti Snake-Necked Turtle (Chelodina Mccordi) Based on Landsat-8 Imagery and GIS', 'embedding': []}, {'id': 14818, 'abstractText': \"The influence of rocks on the microwave thermal emission (MTE) of the lunar regolith has not been fully studied with the four-channel microwave radiometer (MRM) data onboard Chang'e-1/2 satellites. To highlight the influence of the rocks on the MTE of the regolith, the Hertzsprung basin located near the lunar equator in highland regions is selected as the study area. The comparison between the brightness temperature (TB) maps derived from the Chang'e-2 MRM data and rock abundance (RA) map derived from the Diviner data postulates three special issues about the correlation between the MTE features and the regolith with rocks. Then, aimed to interpret the issues, two new layered regolith models and the corresponding radiative transfer models are constructed. The main results are as follows. First, the observation and the simulation both verify that the regolith with rocks will provide a cold TB anomaly at night and at low frequencies at daytime, but result in a hot anomaly at high-frequency at daytime. Moreover, the temperature profiles of the regolith with surface and hidden rocks are evaluated with the theoretical model. Second, the simulation results verify the existence of the hidden rocks in the lunar regolith assumed when studying the TB performances of the Hertzsprung basin. Third, the rock distribution revealed by the TB maps shows a different view compared to that estimated by the Diviner data in space and values, and the change of the TB with frequencies postulates a new view about the variation of the RA with depth. This study hints that the MRM data probably provide a new way to quantitatively estimate the RA values of the lunar regolith, and the results will be meaningful to improve understanding of the evolution of the impact craters.\", 'title': 'Re-Evaluating Influence of Rocks on Microwave Thermal Emission of Lunar Regolith Using CE-2 MRM Data', 'embedding': []}, {'id': 14819, 'abstractText': 'Background: Mining Software Process (MSP) helps distill important information about software process enactment from software data repositories. An increasing amount of research effort is being dedicated to MSP. These studies differ in various aspects (e.g., topics, data, and techniques) of MSP. Objective: We aim to study the state of the art on MSP from following aspects, i.e., research topics, data sources, data types, mining techniques, and mining tools. Method: We conducted a systematic mapping study on the research relevant to MSP at both microprocess and macroprocess levels. Results: Our mapping study identified 40 relevant studies that can be grouped into microprocess and macroprocess levels. The identified mining techniques have been mapped onto the associated mining tools that fall into four types. Driven by the three research questions which represented in a meta-model, the findings revealed the correlations among the research topics, data sources, data types, mining techniques, and mining tools. Conclusion: It is observed that in order to discover the software process model or map, the main data source is from industrial project. Current mining techniques for microprocess research are mostly business process mining or sequence mining techniques used to recover descriptive software process. In addition, various machine learning algorithms and novel proposed methods are used to improve the accuracy of macroprocess level factors (e.g., software effort estimation).', 'title': 'A Mapping Study on Mining Software Process', 'embedding': []}, {'id': 14820, 'abstractText': 'Summary form only given. Quick response to a large-scale natural disaster such as earthquake and tsunami is vital to mitigate the further loss. Remote sensing especially the spaceborne sensors provide the solution to monitor a huge area in a short time and with regular revisit circle. Damage ranges and damage levels of the destructed urban areas are extremely important information for rescue planning after an event. Rapid mapping of the urban damage levels with synthetic aperture radar (SAR) is still challenging. Compared with single-polarization SAR, fully polarimetric SAR (PolSAR) has the better potential to understand the urban damage from the viewpoint of scattering mechanism investigation. In radar polarimetry, dominant double-bounce scattering mechanism in urban area is primarily induced by the ground-wall structures and can reflect the changes of these structures. In this sense, urban damage level in terms of destroyed ground-wall structures can be indicated by the reduction of the dominant double-bounce scattering mechanism, which is the basis of this study. This work first establishes and validates the linear relationship between the urban damage level and the established polarimetric damage index using polarimetric model-based decomposition. Then, efforts are focused on the development of a rapid urban damage level mapping technique which mainly includes two steps of urban area extraction and polarimetric damage level estimation. The 3.11 East Japan Earthquake and Tsunami inducing great-scale destructions selected for study using multi-temporal spaceborne PolSAR data. Experimental studies demonstrate that the estimated damage levels are closely consistent to the ground-truth. The final urban damage level map for the full scene is generated thereafter. Results achieved in this study further validate the necessity of exploring fully polarimetric technique for damage investigation.', 'title': 'Urban damage mapping using fully polarimetric SAR data with scattering mechanism modeling and interpretation technique', 'embedding': []}, {'id': 14821, 'abstractText': \"The main aim of this study was to compare and discuss the effective brain connectivity maps of dyslexic and control groups in terms of differences and similarities. The differences and similarities that may be found in the study, could provide information for the related future studies. There were total of 58 subjects which were 27 control and 31 dyslexics data in this study. All the data according to the groups were averaged with respect to time and group data were generated. Then, effective connectivity maps for both dyslexic and control groups were constructed using the Dynamic Bayesian Networks algorithms. The symmetric connectivities which we call strong connections between electrode pairs were extracted and mainly used for discussion. It was observed that there are differences in effective connectivity maps of both groups in terms of connections' density and directions. The results should also be checked by brain anatomy experts anatomically and by comparing with literature of medicine, their validity and usabilities could be tested.\", 'title': 'The interpretation of the effective connectivity maps obtained by using Dynamic Bayesian Networks on EEG data', 'embedding': []}, {'id': 14822, 'abstractText': 'Landslide is usually happening anywhere in Malaysia without any warning. Most of landslide occurs at manmade slope and natural slopes according to their slope gradient. Recently, Unmanned Aerial Vehicle (UAV) has been widely used for mapping purpose. The first objective of this study is to assess capabilities of UAV in the production of digital photogrammetric products based on landslide risk area along Jeli-Gerik highway. The study area covered is along Jeli-Gerik highway and its surrounding area. Primary data of ground control points (GCP) and check points (CP) were established using real time kinematic technique of Global Positioning System (GPS). All the aerial photographs were processed using digital photogrammetric software and the output in the form of digital elevation model (DEM) and orthophoto were produced. The second objective is to produce landslide risk area map using UAV technology. The risk area map separates the landslide area into three groups which are Low Risk, Medium Risk and High Risk. The parameters that were used to generate the landslide risk area map are slope, aspect and elevation. GIS based Landslide Hazard Zonation models helps not only to map and monitor landslides but also to predict future slope failures. In addition, data collected by UAV can be used to produce accurate ground surface surveys. As conclusion, UAV system has potential use for large scale mapping and could be used by public work department in order to monitoring of landslide area with low cost, less manpower and faster. The ability to detect landslide scarps will lead to a better understanding of landslide mechanisms for the propose area, thus leading to an enhanced identification of the most likely failure sites within a landslide-prone area.', 'title': 'UAV Based Multi-spectral Imaging System for Mapping Landslide Risk Area Along Jeli-Gerik Highway, Jeli, Kelantan', 'embedding': []}, {'id': 14823, 'abstractText': \"This paper explains the details about the development and implementation of various map presentation techniques during a development of virtual reality (VR) application. Maps presentation refers to the method of how the development is being visualized to the users including its orientation. In VR, map presentation is often considered as a secondary 2-Dimensional display which shows the location and destination points. In addition, this paper explains the technical experience as well as the findings that have been discovered. These map presentation techniques are tested using a head-mounted display to visualize a virtual world. In this study, a number of participants are invited to try and give feedback upon completing the completion of the experiment. Findings collected during the experiment shows that a proper map presentation technique increases end-users' VR experience. In addition to a map presentation, this study also addresses issues related to map activation and orientation techniques towards improving the effectiveness during a navigation process.\", 'title': 'Virtual Reality Mini Map Presentation Techniques: Lessons and experience learned', 'embedding': []}, {'id': 14824, 'abstractText': 'Georeferenced social media data are gaining increased application in creating near real-time flood maps needed to improve situational awareness in data-starved regions. However, there is growing concern that the georeferenced locations of flood-related social media contents do not always correspond to the actual locations of the flooding event. But to what extent is this true? Without this knowledge, it is difficult to ascertain the accuracy of flood maps created using georeferenced social media contents. This study aims to improve understanding of the extent to which georeferenced locations of social media flood reports deviate from the actual locations of floods. The study analyses flood-related tweets acquired as part of the PetaJakarta.org project implemented in the coastal mega-city of Jakarta and provides insight into the level of accuracy expected with using georeferenced social media data for flood mapping. Importantly, the results reveal that the accuracy of flood maps generated with georeferenced social media data reduces with increase in the size of the minimum mapping unit of the flood map. Finally, an approach is recommended for creating more accurate real time flood maps from crowdsourced social media data.', 'title': 'Investigating the accuracy of georeferenced social media data for flood mapping: The PetaJakarta.org case study', 'embedding': []}, {'id': 14825, 'abstractText': 'The feasibility of using normalized cumulative difference attenuation (NCDA) map for tracking the spatial and temporal evolution of temperature during microwave hyperthermia experiment on in-vitro phantoms is explored in this study. The NCDA maps were estimated from the beamformed ultrasound radio frequency (RF) data using a regularized log spectral difference (RLSD) technique. The NCDA maps were estimated at different time instants for the entire period of the experiment. The contour maps of the NCDA and the ground truth temperature map, obtained using an infra-red(IR) thermal camera corresponding to the ultrasound imaging plane, showed that NCDA was able to locate the axial and lateral co-ordinates of the hotspot with the error of &lt;; 1.5 mm axially and &lt;; 0.1 mm laterally. The error in the estimated hotspot area was less than 8 %. This preliminary in-vitro study suggests that NCDA maps estimated using RLSD may have potential in evaluating the spatio-temporal evolution of temperature and may help in the development of ultrasound-based image-guided temperature monitoring system for microwave hyperthermia.', 'title': 'Ultrasound-Based Regularized Log Spectral Difference Method For Monitoring Microwave Hyperthermia', 'embedding': []}, {'id': 14826, 'abstractText': 'The abnormal point has an important significance in terms of theory and application. In order to study dynamical properties of the abnormal point in the high dimensional space, we will give a new concept of strongly descendible map according to the definition of the descendible map. We will study the abnormal point of strongly descendible map by means of properties of interval map and product map and it is respectively given that equivalence conclusion of strongly descendible map with abnormal points and without abnormal points in the n -dimensional monomer. These results further generalize the research of Professor of Xiong and Zhou.', 'title': 'Dynamical Properties of Strongly Descendible Map on N Dimensional Monomer', 'embedding': []}, {'id': 14827, 'abstractText': 'Converting probability maps derived from indicator cokriging (ICK) to a specific land cover classification map is the second step of super-resolution mapping (SRM) under the geostatistical framework. In this study, two image segmentation strategies, namely mathematical morphology and region growing, were applied on the ICK-derived probability maps in order to take into account spatial characteristics such as shape and connectivity. A case study in South Carolina (USA) showed that the thematic map created by the proposed method had an overall accuracy improved by 2% and Kappa improved by 6% compared to the map derived from the existing sequential generation process. This indicates our methodology as a promising alternative that can be embedded into SRM tasks.', 'title': 'Integration of region growing and morphological analysis with super-resolution land cover mapping', 'embedding': []}, {'id': 14828, 'abstractText': 'Active learning strategies such as mind mapping have been proven to be effective in helping retain memory. Given that mind mapping has seen notable success in assessment and exam preparation, this study investigated the effectiveness of mind mapping as an active learning tool for university students. In addition, this study also looked into areas of active learning that could possibly require further improvement. Data collected from 22 Strategic management students was analysed using quantitative and qualitative methods. Results from the one-sample t-test suggest that the mind map technique enhances students learning ability. Results from the qualitative phase revealed areas that students feel could assist them to better utilize the mind map technique. Future improvements for the use of mind map technique for students are also discussed.', 'title': 'A Comparative Study of Active Learning with and Without Using Mind Mapping Approach', 'embedding': []}, {'id': 14829, 'abstractText': 'Over the last decade, the horticulture sector has become one of the important driving forces for the rapid development of agriculture in India. Geospatial technology is being operationally used in India for mapping and monitoring horticultural plantations. In general, per pixel or OBIA techniques have been used for mapping while use of deep learning techniques for analysis of high-resolution data for these plantation crops is limited. In the present study, seven deep learning models have been attempted for mapping of coconut plantations to overcome some of the challenges faced by conventional classifiers. U-Net and Siamese architectures are used to develop hybrid models for improved classification. These hybrid models have been implemented with batch normalization and the results indicated that the developed hybrid models showed reasonable IoU score in comparison to other DL models with better than 90.0 per cent classification accuracy for mapping coconut plantations. The trained DL model has been tested in different geographic regions for achieving reasonable accuracy for mapping of coconut plantations. The present study demonstrated the use of DL methods for mapping of coconut plantations using Cartosat-2 MX data within acceptable accuracy and trained DL model could be developed with larger no. of labelled training samples for operational applications.', 'title': 'Performance of Different U-Net Architectures for Inventory of Coconut Plantations Using Cartosat-2 Multispectral Data', 'embedding': []}, {'id': 14830, 'abstractText': \"Structured query language (SQL) is difficult to master because the execution process of SQL statements is invisible. When learning to construct an SQL query, learners must visualise the evolution process of the intermediate datasets of the SQL statement in working memory, which may burden learners' cognitive load and consequently jeopardise learning outcomes. This study describes the execution process of SQL statements by using concept maps to improve learners' understanding of SQL. An empirical experiment was conducted using two database courses, namely concept map-based and conventional instruction, to examine the relationship between concept maps and the understanding of SQL from a cognitive load theory perspective. The experimental results demonstrated the superiority of concept map-based instruction over conventional instruction because concept map-based instruction reduces extraneous load but increases germane load. Concept map construction facilitated learner engagement and promoted meaningful learning. Studying the instructors' concept maps helped learners follow the cognitive structures used by instructors to perform SQL queries, and enabled them to perceive the execution process of SQL queries relatively easily. These results potentially help educators understand the learning difficulties caused by the declarative nature of SQL and motivating researchers to resolve the inherent problem by considering learners' cognitive processes.\", 'title': 'Structured Query Language Learning: Concept Map-Based Instruction Based on Cognitive Load Theory', 'embedding': []}, {'id': 14831, 'abstractText': 'This paper examines the accuracy of the limitations of using drone to generate contour maps. Contour mapping involves the location of elevation datum referenced in the x, y and z planes to accurately depict the contours of an area and for generating topographic maps. Traditionally, this was conducted by using Total Station and other surveying tools which is based on manually locating points on the site using the angle and distance method often referred to as trigonometric leveling. This method is usually labor intensive and relatively time consuming. Several “Drone surveying systems” were introduced to the market. These systems were advertised to replace the traditional surveying method. This case study examines practicality and accuracy of creating topographic maps using unmanned aerial systems (drones). As a case study a 5.522 acres site was surveyed by traditional methods and by Kespry© 2 drone to collect the surface data required to generate a topographic map. The results showed the drone readings are within an average accuracy of 0.0075 ft and 0.0175 for the northing and easting distances respectively with a standard deviation of 0.022ft. and 0.029 ft. and a range of 0.02 ft. The Elevation accuracy average was 0.332 ft with a standard deviation of 0.278 ft. and a range of 0.77 ft. The difference in the calculated cut and fill volume between the drone system and the traditional system was 8.2% The results indicated that topographic mapping using drones can generate topographic maps with acceptable accuracy for general site grading. The Drone system also saves time and reduce human error. However, it usually requires subcontracting the work to a company with special training personnel who have access to special analytical software and relatively expensive equipment. Special Drone flight permissions is also required for some sites.', 'title': 'Examining the practicality and accuracy of Unmanned Aerial System Topographic Mapping (Drones) Compared to Traditional Topographic Mapping', 'embedding': []}, {'id': 14832, 'abstractText': 'Breast cancer is a high incidence of malignancy in women, with a higher mortality rate. Accurate screening is helpful to early detection and improve the treatment success rate and patient survival rate. This study is based on low-cost ultrasound, using ultrasound multifeature maps based on the original radiofrequency (RF) signals and radiomics analysis method to evaluate the benign and malignant of breast tumors. The three ultrasound multifeature maps of breast tumor are composed of direct energy attenuation coefficient (AC), standard deviation of image intensity (SD) and Rician distribution parameters (RD). From the above multifeature maps, high-throughput radiomics features were extracted, then sparse representation method was used for feature selection, and then support vector machine was used to predict the benign and malignant of breast tumors. Eight groups of comparative experiments were established by using ultrasound gray-scale image, single ultrasound feature map and two ultrasound feature maps. The results from 164 patients with breast tumor showed that the AUC, accuracy and sensitivity of the radiomics classification model with feature maps of AC, SD and RD can reach 93.61%, 93.94% and 100%, respectively. The use of RF based ultrasound multifeature maps combined with radiomics could effectively predict the benign and malignant of breast tumors in this study.', 'title': 'Breast tumor diagnosis using radiofrequency signals based ultrasound multifeature maps combined with radiomics analysis', 'embedding': []}, {'id': 14833, 'abstractText': 'Regions within the atria with sustained rapid reentrant or focal activity have been defined as a mechanism of persistent atrial fibrillation (AF). However, the mechanism behind the anchoring of these sites and their stability over time is unknown. We tested the hypothesis that fibrosis anchors sites of high frequency activation during AF and that these sites can be non-invasively determined using cardiac T1 Mapping with MRI.A canine rapid atrial paced model of persistent AF was used (n=12, including 6 controls) for the study. Whole heart T1 Mapping was performed prior to an electrical mapping study. Spatial maps of high dominant frequency (DF) probability were constructed to determine stability of the highest DF sites. These sites were then correlated with fibrotic regions determined by T1 Mapping.The chronic AF animals had at least one site of stable, high DF for at least 22.5 (75%) of 30 minutes of AF. Regions of stable high DF bordered regions offibrosis as determined by T1 Mapping MRI 82% of the time (p&lt;; 0.05).Heterogeneous atrial remodeling, specifically fibrosis, arising from chronic AF may provide a substrate that anchors sites of high DF. Cardiac T1 Mapping with MRI may determine such sites non-invasively.', 'title': 'Regions of High Dominant Frequency in Chronic Atrial Fibrillation Anchored to Areas of Atrial Fibrosis', 'embedding': []}, {'id': 14834, 'abstractText': 'Wetlands are important natural resources which provide many benefits to the environment. Consequently, mapping and monitoring wetlands has gained a considerable attention in recent years among remote sensing experts. Wetlands undergo a considerable change within a year. Thus, it is important to study how much various wetland types are distinguishable at different dates. This will help in choosing an appropriate image for wetland classification. On the other hands, combining various satellite images acquired on different dates is a promising approach to obtain a more accurate classified map compared to the map obtained by single-date satellite imagery. In this study, wetlands within a pilot sites, located in Newfoundland were first classified using each of the several available Landsat 8 data, captured in the three seasons of Spring, Summer, and Fall. By doing this, the separability of the wetland classes in each season was analyzed. Then, these multi-temporal data were integrated to obtain a more accurate map of wetlands. The overall classification accuracy of the final map was 88%, proving that using multi-temporal remote sensing data was necessary to obtain a more reliable and accurate map of the dynamic wetlands in the province.', 'title': 'Evaluation of multi-temporal landsat 8 data for wetland classification in newfoundland, Canada', 'embedding': []}, {'id': 14835, 'abstractText': 'Forest fires occur throughout the year in rainforests and deserts of Australia. The disastrous bush fire event occurred during November 2019, and lasted until February 2020, destroying more than 46 million acres of land. Burn area mapping is a major parameter in carrying out mitigation measures and regrowth activities by forest officials or fire managers post fire event. In this study, multi-temporal satellite datasets such as images acquired from Sentinel-2 (S2) and Landsat-8 (L8) missions are used to map the burn areas. Two thematic indices such as Differenced Normalized Burn Ratio (dNBR) and Relativized Burn Ratio (RBR) are implemented on the study area. The entire analysis, i.e., accessing the datasets, preprocessing, and calculation of indices for brunt area mapping is carried out on Google Earth Engine cloud platform. Rather than ground survey, the active fire product VIIRS product (VNP14IMGTDL) is used as a proxy for the actual fire indices in accuracy assessment. Results revealed that RBR showed better accuracy than dNBR for both the datasets (S2 and L8). S2 burn severity maps of dNBR and RBR showed better accuracy than L8 burn severity maps because of S2 having a higher spatial resolution. Thus, S2 datasets can be useful for rapid mapping of burn areas with improved spatial as well as temporal resolution.', 'title': 'Burn area mapping in Google Earth Engine (GEE) cloud platform: 2019 forest fires in eastern Australia', 'embedding': []}, {'id': 14836, 'abstractText': 'In 2011 A.I. EL Maghrabi and A.M. Mubaraki introduced and studied the notions of Y - open and Y - closed sets in general topology as well as presented some characterizations of these notions. We introduce and investigate several properties and characterizations of a new class of maps between topological spaces called Y - open maps, Y - closed maps, Y - continuous maps and Y - irresolute maps. We also introduce slightly Y - continuous, totally Y - continuous and almost Y - continuous maps between topological spaces and establish several characterizations of these new forms of maps. Furthermore, we introduce and study the notions of Y - separated sets and Y - connectedness in topological spaces.', 'title': 'Y – Continuity and Y – Connectedness in Topological Space', 'embedding': []}, {'id': 14837, 'abstractText': \"In this paper, we address the problem of quantifying the reliability of computational saliency for videos, which can be used to improve saliency-based video processing algorithms and enable more reliable performance and objective risk assessment of saliency-based video processing applications. Our approach to quantify such reliability is twofold. First, we explore spatial correlations in both the saliency map and the eye-fixation map. Then, we learn the spatiotemporal correlations that define a reliable saliency map. We first study spatiotemporal eye-fixation data from the public CRCNS data set and investigate a common feature in human visual attention, which dictates a correlation in saliency between a pixel and its direct neighbors. Based on the study, we then develop an algorithm that estimates a pixel-wise uncertainty map that reflects our supposed confidence in the associated computational saliency map by relating a pixel's saliency to the saliency of its direct neighbors. To estimate such uncertainties, we measure the divergence of a pixel, in a saliency map, from its local neighborhood. In addition, we propose a systematic procedure to evaluate uncertainty estimation performance by explicitly computing uncertainty ground truth as a function of a given saliency map and eye fixations of human subjects. In our experiments, we explore multiple definitions of locality and neighborhoods in spatiotemporal video signals. In addition, we examine the relationship between the parameters of our proposed algorithm and the content of the videos. The proposed algorithm is unsupervised, making it more suitable for generalization to most natural videos. Also, it is computationally efficient and flexible for customization to specific video content. Experiments using three publicly available video data sets show that the proposed algorithm outperforms state-of-the-art uncertainty estimation methods with improvement in accuracy up to 63% and offers efficiency and flexibility that make it more useful in practical situations.\", 'title': 'Unsupervised Uncertainty Estimation Using Spatiotemporal Cues in Video Saliency Detection', 'embedding': []}, {'id': 14838, 'abstractText': 'Land degradation by salinity is one of the main environmental hazards threatening soil sustainability especially in arid and semi-arid regions of the world characterized by low precipitation and high evaporation. Geo-statistical approaches and remote sensing (RS) techniques have provided fast, accurate and economic prediction and mapping of soil salinity within the last two decades. Obtaining multi-temporal data via satellite images in different spatial domains with various scales is one of the key developments of monitoring spatial variability of soil salinity. In addition, geo-statistical methods have the capability of producing prediction surfaces from limited sample data. This study aims to map spatial distribution of soil salinity in the selected pilot area which is located in the western part of Urmia Lake Basin, Iran, by applying geo-statistical methods. A kriging based map and three different co-kriging based maps were produced using electrical conductivity (EC) measurements as primary variable and three different soil salinity index values as secondary variable. Three soil salinity indices were created by using Sentinel-2A image that were acquired in the same date of field measurements to generate 3 various soil salinity prediction maps. Salinity maps obtained from geo-statistical methods were compared and validated to understand the performance of these approaches for soil salinity prediction. The results of this study demonstrated that co-kriging can provide promising estimation of spatial variability of soil salinity especially when there is relevant and abundant set of secondary data derived from satellite images.', 'title': 'Characterizing the spatial variability of soil salinity in Lake Urmia Basin by applying geo-statistical methods', 'embedding': []}, {'id': 14839, 'abstractText': 'This research presents a novel topology preserving map (TPM) called Weighted Voting Supervision -Beta-Scale Invariant Map (WeVoS-Beta-SIM), based on the application of the Weighted Voting Supervision (WeVoS) meta-algorithm to a novel family of learning rules called Beta-Scale Invariant Map (Beta-SIM). The aim of the novel TPM presented is to improve the original models (SIM and Beta-SIM) in terms of stability and topology preservation and at the same time to preserve their original features, especially in the case of radial datasets, where they all are designed to perform their best. These scale invariant TPM have been proved with very satisfactory results in previous researches. This is done by generating accurate topology maps in an effectively and efficiently way. WeVoS meta-algorithm is based on the training of an ensemble of networks and the combination of them to obtain a single one that includes the best features of each one of the networks in the ensemble. WeVoS-Beta-SIM is thoroughly analyzed and successfully demonstrated in this study over 14 diverse real benchmark datasets with diverse number of samples and features, using three different well-known quality measures. In order to present a complete study of its capabilities, results are compared with other topology preserving models such as Self Organizing Maps, Scale Invariant Map, Maximum Likelihood Hebbian Learning-SIM, Visualization Induced SOM, Growing Neural Gas and Beta- Scale Invariant Map. The results obtained confirm that the novel algorithm improves the quality of the single Beta-SIM algorithm in terms of topology preservation and stability without losing performance (where this algorithm has proved to overcome other well-known algorithms). This improvement is more remarkable when complexity of the datasets increases, in terms of number of features and samples and especially in the case of radial datasets improving the Topographic Error.', 'title': 'A Novel Ensemble Beta-Scale Invariant Map Algorithm', 'embedding': []}, {'id': 14840, 'abstractText': 'Mangrove forest (MF) extents and distributions are fundamental for conservation and restoration efforts. According to previous studies, both the commercial Gaofen-2 (GF-2) imagery (0.8 m spatial resolution and 4 spectral bands) and freely accessed Sentinel-2 (S2) imagery (10 m spatial resolution and 13 spectral bands) have been successfully used to map MFs. However, the efficiency and accuracy of MF mapping based on these two data is not clear, especially for large-scale applications. To address this issue, first, we developed a robust classification approach by integrating object-based image analysis (OBIA) and random forest (RF) algorithm; and then, applied this approach to GF-2 and S2 images to map the extents of MF along the entire coasts of Guangxi, China, respectively; at last, compared the efficiency and accuracy of GF-2 and S2 imagery in MF mapping. Results showed that: first, based on OBIA and RF integrated classification approach both MF maps derived from GF-2 and S2 obtained high mapping accuracies (the overall accuracy was 96% and 94%, respectively); second, areal extent of MFs in Guangxi extracted from GF-2 and S2 images was 8182 and 8040 ha, respectively; third, GF-2 imagery has extraordinary abilities in detecting fragmented MF patches located along landward and seaward edges; and finally, S2 imagery performed better in detecting seaward submerged MFs and separating MF from terrestrial vegetation. Results and conclusions of this study can provide basic considerations for selecting appropriate data source in MF or wetland vegetation mapping tasks.', 'title': 'A Comparison of Gaofen-2 and Sentinel-2 Imagery for Mapping Mangrove Forests Using Object-Oriented Analysis and Random Forest', 'embedding': []}, {'id': 14841, 'abstractText': 'Given the explosive growth of Internet of Things (IoT) devices ranging from the 2D ground to the 3D space, it is a necessity to establish a 3D spectrum map to comprehensively present and effectively manage the 3D spatial spectrum resources in smart city infrastructures. By leveraging the popularity and location flexibility of unmanned aerial vehicles (UAVs), we are able to execute spatial sampling with these emerging flying spectrum-monitoring devices (SMDs) at will. In this article, we first present a brief survey to show the state-of-the-art studies on spectrum mapping. Then we introduce the 3D spectrum mapping model. Next, we propose a 3D spectrum mapping framework that is composed of pre-sampling, spectrum situation estimation, UAV deployment, and spectrum recovery. Therein we develop a region-of-interest-driven UAV deployment scheme, which selects new sampling points of the highest estimated interest and the lowest energy cost iteratively. Meanwhile, we slice the entire 3D spectrum map into a series of \"images\" and \"repair\" those unsampled locations. Furthermore, we provide an exemplary case study on 3D spectrum mapping, where, for example, an important event is being held, and the entire spectrum situation needs to be monitored in real time to deal with malicious interference sources. Lastly, the challenges and open issues are discussed.', 'title': '3D Spectrum Mapping Based on ROI-Driven UAV Deployment', 'embedding': []}, {'id': 14842, 'abstractText': 'Online maps play an important role in providing traffic information services for users. The complexity of urban road network and the increase of traffic information have encouraged diversified demands for traffic maps, and provoked great conflict between growing visualization requirements of complex information and the limited map display space. Based on a comparison of road classification and attribute integration of typical online map platforms in different scales, this study provided an optimized design at the aim of exploring better visual integration methods of multiple traffic information for online maps. A user test was conducted, which proved the validity of the integrating design methods in showing that the design strategy of this study can represent more traffic information in different dimensions in a user-accepted manner. Both the design scheme and other results can provide reference for the elements configuration and visual integration of future online map design.', 'title': 'Visual Integration of Multiple Traffic Information for Online Map Design', 'embedding': []}, {'id': 14843, 'abstractText': 'SAR or Microwave remote sensing today has proved itself to be a paradigm shift in the field of remote sensing. With its all-weather availability, it has found variety of applications in several fields today. Till now, and mainly, floods have been mapped using optical remote sensing data since a long time. Floods have been a common and yearly catastrophe which the people of areas situated in lesser elevations and on the foothills of mountains in the Terai area must face. Proper mapping of floods can help to delineate the areas with higher loss of life and property thus aiding the relief measures. This study aims to analyze and exploit the potential of SAR Interferometry (InSAR) for flood inundation mapping using a pair of C-Band dual PolSAR datasets from Sentinel-1. The datasets were preprocessed for speckle removal and radiometric corrections. Thereafter, subset of study area was taken, and the datasets were co- registered followed by interferometric processing. The results showed low phase shift in and around the river and to some extent in the urban areas which generally does not happen unless there is an increased single bounce scatter. The topographic phase removed interferogram showed high phase variations in the core of urban areas while coherence map showed an acute loss of coherence from permanent features as well which showed high degree of flood water accumulation. This flood was mapped by masking out the low values of high coherence areas from the image using a proper threshold value. High degree of flooding was observed in Urban areas and much of agricultural land was seen to have been submerged under water once the resulting inundation map was overlaid on Google earth.', 'title': 'C-band SAR Interferometry based flood inundation mapping for Gorakhpur and adjoining areas', 'embedding': []}, {'id': 14844, 'abstractText': 'Geothermal energy is one of the renewable energy sources that can be utilized to replace fossil fuels. The conventional geophysical methods have been applied to determine geothermal potential areas in the globe. These existing methods are costly and time-consuming for explorations in a wide range and unreachable volcano area. Thus, an alternative and challenging approach for identifying a geothermal prospecting area is remotely sensed thermal infrared imagery. This study aims to explore and evaluate the use of a sensor platform of Thermal Infrared Scanner (TIRS) and Operational Land Imager (OLI), Landsat 8 for mapping geothermal resource in Peut Sagou volcano. The mountain is an active volcano in Aceh Province, Indonesia that is possible to develop a geothermal power plant by an energy estimation of 100 MWe. The several processing techniques have been applied such as composite bands for lithological mapping that related to the geological aspect of the volcano, Normalize Deferential Vegetation Index (NDVI) threshold methods of spectral emissivity used for mapping the land cover area, and the Land Surface Temperature (LST) by split windows algorithm method to study the temperature distribution related to the activity of geothermal resources in the sub-surface. The composite band shows clearly the lineament of the faults, and alluvium deposits in the manifestation area, which is specifically located in the top of the volcano. NDVI data shows a low vegetation index (-0.4 - 0.5) in the manifestation area, while the Land Surface Emissivity (LSE) also allows mapping of a weak zones area that is characterized by low radiation index &lt;; 0.98. The high-temperature value (21 - 30 C) from the LST map is obtained at the top of the volcano in response to the conduit zone area, which functions as a route for fluid discharge from the reservoir to the surface. The LST data also shows some areas with relatively high temperatures (19 - 20 C) in response to the weak zone areas such as faults and the distribution of surface manifestations. We conclude that TIRS dan OLI sensors from Landsat 8 demonstrate high efficiency for potential mapping of geothermal resources in the Peut Sagou volcano.', 'title': 'OLI and TIRS Sensor Platforms for Detection the Geothermal Prospecting in Peut Sagoe Volcano, Aceh Province, Indonesia', 'embedding': []}, {'id': 14845, 'abstractText': 'Automatic generation of level maps is a popular form of automatic content generation. In this study, a recently developed technique employing the do what’s possible representation is used to create open-ended level maps. Generation of the map can continue indefinitely, yielding a highly scalable representation. A parameter study is performed to find good parameters for the evolutionary algorithm used to locate high quality map generators. Variations on the technique are presented, demonstrating its versatility, and an algorithmic variant is given that both improves performance and changes the character of maps located. The ability of the map to adapt to different regions where the map is permitted to occupy space are also tested.', 'title': 'Automatic Generation of Level Maps with the Do What’s Possible Representation', 'embedding': []}, {'id': 14846, 'abstractText': 'Due to recent artificial intelligence (AI) technology progress, more and more applications present all-to-all, irregular or unpredictable communication patterns among compute nodes in high-performance computing (HPC) systems. Traditional communication infrastructures, e.g., torus or fat-tree interconnection networks, may not handle well their matchmaking problems with these newly emerging applications. For these typical non-random network topologies, there are already many communication-efficient application mapping algorithms. However, for the above unpredictable communication patterns, it is difficult to efficiently map their applications onto the non-random network topologies. In this case, a simple optimization is to map their applications with small diameter or average shortest path length (ASPL) among the assigned compute nodes. In this context, we recommend to use random network topologies as the communication infrastructures, which have drawn increasing attention for the use of HPC interconnects. In this study, we make a comparative study to analyze the performance impact of application mapping on non-random and random network topologies. We list several application mapping policies, and compare their job scheduling performances assuming that the communication patterns are unpredictable to the computing system. Evaluations with a large compound application workload show that, when compared to non-random topologies, random topologies can reduce the average turnaround time up to 39.3% by a random connected mapping method and up to 72.1% by a diameter/ASPL-based mapping method.', 'title': 'The Impact of Application Mapping on Non-Random and Random Network Topologies', 'embedding': []}, {'id': 14847, 'abstractText': 'Accurate mapping of urban land cover is still a fundamental challenge in remote sensing communities due to the great spectral variability of urban environments. This study presents an application of multiple criteria spectral mixture analysis (MCSMA) approach to map vegetation, impervious surfaces, and soil (V-I-S) components in a highly urbanized city of Chengdu, China, using the Landsat-8 Operational Land Imager (OLI) surface reflectance product. Unlike its counterparts which rely on single indicator in the mapping process, MCSMA uses multiple indicators to better address the problem of spectral variability. Our results showed that MCSMA produced accurate V-I-S maps that well matched the actual distributions. The vegetation map presented higher accuracies than impervious surfaces and soil maps in root mean square error, mean absolute error and systematic error. Results of this study demonstrate the potential of MCSMA in accurate urban land cover mapping.', 'title': 'Mapping Urban Land Cover Using Multiple Criteria Spectral Mixture Analysis: A Case Study in Chengdu, China', 'embedding': []}, {'id': 14848, 'abstractText': 'Visual search can be time-consuming, especially if the scene contains a large number of possibly relevant objects. An instance of this problem is present when using geographic or schematic maps with many different elements representing cities, streets, sights, and the like. Unless the map is well-known to the reader, the full map or at least large parts of it must be scanned to find the elements of interest. In this paper, we present a controlled eye-tracking study (30 participants) to compare four variants of map annotation with labels: within-image annotations, grid reference annotation, directional annotation, and miniature annotation. Within-image annotation places labels directly within the map without any further search support. Grid reference annotation corresponds to the traditional approach known from atlases. Directional annotation utilizes a label in combination with an arrow pointing in the direction of the label within the map. Miniature annotation shows a miniature grid to guide the reader to the area of the map in which the label is located. The study results show that within-image annotation is outperformed by all other annotation approaches. Best task completion times are achieved with miniature annotation. The analysis of eye-movement data reveals that participants applied significantly different visual task solution strategies for the different visual annotations.', 'title': 'An Evaluation of Visual Search Support in Maps', 'embedding': []}, {'id': 14849, 'abstractText': \"As of today, SAR imagery represents the most commonly used data source for remote sensing-based flood mapping. The data are characterized by a good sensitivity to water and are available day and night, regardless of cloud cover. Many studies have demonstrated that SAR systems are suitable tools for flood mapping on bare soils and scarcely vegetated areas. In spite of the progress in the development of Near Real Time SAR based flood mapping algorithms, the detection of inundation in urban areas still represents a critical issue. Here we propose a methodology for identifying floods that heavily affected the city of Houston (Texas) during the 2017 hurricane season. Our approach takes advantage of the Interferometric SAR coherence feature to detect the presence of floodwater in urbanized areas. In particular, data provided by the Sentinel-1 mission in both, Strip Map and Interferometric Wide Swath modes, have been used, with a geometric resolution of 5m and 20m, respectively. The algorithm takes fully advantage of the Sentinel-1 mission's repeat cycle of six days, thereby providing an unprecedented possibility to develop an automatic, high frequency flood mapping application that is suitable for complex environments. The test of the algorithm for the Houston case study showed promising results for mapping flood in urban areas.\", 'title': 'Monitoring Urban Floods Using SAR Interferometric Observations', 'embedding': []}, {'id': 14850, 'abstractText': 'Several works have focused on Simultaneous Localization and Mapping (SLAM), which is a topic that has been studied for more than a decade to meet the needs of robots to navigate in an unknown environment. SLAM is an essential perception functionality in several applications, especially in robotics and autonomous vehicles. RGB-D cameras are among the sensors commonly used with recent SLAM algorithms. They provide an RGB image and the associated depth map, making it possible to solve scale drift with less complexity and create a dense 3D environment representation. Many RGB-D SLAM algorithms have been studied and evaluated on publicly available datasets without considering sensor specifications or image acquisition modes that could improve or decrease localization accuracy. In this work, we deal with indoor localization, taking into account the sensor specifications. In this context, our contribution is a deep experimental study to highlight the impact of the sensor acquisition modes on the localization accuracy, and a parametric optimization protocol for a precise localization in a given environment. Furthermore, we apply the proposed protocol to optimize a depth-related parameter of the SLAM algorithm. The study is based on a publicly available dataset in an indoor environment with a depth sensor. The reconstruction results’ analysis is founded on the study of different metrics involving translational and rotational errors. These metrics errors are compared with those obtained with a state-of-the-art stereo vision-based SLAM algorithm.', 'title': 'Enhancing RGB-D SLAM Performances Considering Sensor Specifications for Indoor Localization', 'embedding': []}, {'id': 14851, 'abstractText': 'In the last decade, as an emerging technique for business processes management, process mining (PM) has been applied in many domains, including manufacturing, supply-chain, government, healthcare, and software engineering. Particularly in healthcare, where most processes are complex, variable, dynamic, and multi-disciplinary in nature, the application of this technique is growing yet challenging. Several literature reviews, as secondary studies, reveal the state of PM applications in healthcare from different perspectives, such as clinical pathways, oncology processes, and hospital management. In this article, we present the results of a systematic mapping (SM) study which we conducted to structure the information available in the primary studies. SM is a well-accepted method to identify and categorize research literature, in which the number of primary studies is rapidly growing. We searched for studies between 2005 and 2017 in the electronic digital libraries of scientific literature, and identified 172 studies out of the 2428 initially found on the topic of PM in healthcare. We created a concept map based on the information provided by the primary studies and classified these studies according to a number of attributes including the types of research and contribution, application context, healthcare specialty, mining activity, process modeling type and notation/language, and mining algorithm. We also reported the demographics and bibliometrics trends in this domain; namely, publication volume, top-cited papers, most contributing researchers and countries, and top venues. The results of mapping showed that, despite the healthcare data and technique related challenges, the field is rapidly growing and open for further research and practice. The researchers who are interested in the field could use the results to elicit opportunities for further research. The practitioners who are considering applications of PM, on the other hand, could observe the most common aims and specialties that PM techniques are applied.', 'title': 'Systematic Mapping of Process Mining Studies in Healthcare', 'embedding': []}, {'id': 14852, 'abstractText': 'Deep convolutional neural networks have demonstrated superior performance in natural image denoising. Trainable network weights have been typically optimized by minimizing a loss function that computes pixel-wise discrepancies between the noisy image and the clean target image. In this study, we investigate an alternative solution that utilizes more prior knowledge to highlight the features of interest by modifying their contributions to the global loss function. We propose a feature-oriented deep convolutional neural network (FeaOri-DCNN) for PET image denoising that uses weight maps in order to steer the training toward contrast preservation for small features. To obtain the weight maps, we first manually segment the lesions in the target images to create lesion masks. Lesion voxels are assigned stronger weights than the background voxels followed by a Gaussian smoothing. This weight map is then incorporated into the loss function optimization. We first trained the proposed FeaOri-DCNN and a conventional DCNN built on a five-layer residual network architecture with simulated and phantom images containing hot spheres with various size and contrast. We then trained an eight-layer network with 8 patient studies and 1 phantom study. We evaluated the five-layer network on phantom studies and the eight-layer network on 2 patient studies inserted with GATE simulated lesions. The results of the phantom studies show that FeaOri-DCNN improved contrast recovery on small and low contrast spheres by up to 36% while performing similarly in terms of noise reduction in the background. Similar results were also observed in the patient studies.', 'title': 'Feature Oriented Deep Convolutional Neural Network for PET Image Denoising', 'embedding': []}, {'id': 14853, 'abstractText': 'The decision tree (DT) represents a nonparametric estimation method that has been mostly used for both classification and regression problems. DTs were adopted for software development effort estimation (SDEE) generally for their simplicity of use and interpretation contrary to other learning methods. Nevertheless, to our self-knowledge, no systematic mapping has been devoted especially to decision trees. The aim of this study is to elaborate a systematic mapping study that classifies DTs papers in conformity with the succeeding criteria: research approach, contribution type, techniques employed in combination with DT methods besides identifying publication channels and trends. An automated search of five digital libraries was made to carry out a systematic mapping of DT studies mainly devoted to SDEE that were published in the period 1985-2017. We identify 46 relevant studies. Basically, the results revealed that most researchers focus on technique contribution type. In addition, the majority of papers deal with improving the existing DT models while few studies have proposed novel models to improve the reliability of SDEE. Furthermore, solution proposal and case study are the most frequently used approaches.', 'title': 'Decision Trees Based Software Development Effort Estimation: A Systematic Mapping Study', 'embedding': []}, {'id': 14854, 'abstractText': 'The rapid development of the field of geoinformation has paved the way for great hope in the framework of economic development and infrastructure in the world. This study deals with the publication of the geo-information to document the historical monuments of Turkish tourism in Khartoum State. The motives of the study are the geomorphic possibilities in the evaluation of spatial features, the lack of studies and research in the field, Descriptive and spatial features of Turkish monuments as one of the important historical landmarks in the state of Khartoum. The objective of the study is to construct the historical spatial data model GeoHistorical Data Model, the design of the historical spatial database of the geocapital in Gezpatial Database, the implementation of a historical spatial information system application for the Turkish features in Khartoum state, the design of a basic map of the Turkish features in Khartoum State Historical Thematic Map, documenting the Turkish monuments in the state of Khartoum. The importance of the study is to highlight the geomorphic possibilities in documenting the spatial parameters in Khartoum.', 'title': 'GeoSpatial Technology Documental Historical Tourism Site: Turkey in Khartoum', 'embedding': []}, {'id': 14855, 'abstractText': 'Clear-cutting and logging operations are the most drastic and wide-spread changes that affects the hydrological and carbon-balance properties of forested areas. A long time series of Sentinel-1 images are used to study the potential for mapping logged areas in areas in boreal zone region and in tropical forest. In the first case study in southern Finland, the time series covered a full year starting in October 2014, in 200km-by-200-km study site. The Sentinel-1 images were acquired in Interferometric Wide-swath (IW), dualpolarized mode (VV+VH). All scenes were acquired in the same orbit configuration. In the second case study the potential of Sentinel-1 time series for mapping logged areas was studied over tropical forest in Mexico. Acquisitions were made in the time frame between November 2014 and September 2015. The temporal behavior of the C-band backscatter was studied for areas representing: 1) areas clear-cut during the acquisition of the Sentinel-1 time-series, 2) areas remaining forest during the acquisition of the Sentinel-1 time-series, and 3) areas that had been clear-cut before the acquisition of the Sentinel-1 time-series. Algorithms for mapping the spatial extent of logged areas were developed and tested, showing potential of long time series of Sentinel-1 data for successful delineation of clear-cuts despite high sensitivity to seasonal and weather conditions.', 'title': 'Mapping forest disturbance using long time series of Sentinel-1 data: Case studies over boreal and tropical forests', 'embedding': []}, {'id': 14856, 'abstractText': 'Information Security has still the aim of many agencies and organisations. They have great attention to cryptosystems to ensure the security of their information. So, scientists and researchers had proposed many cryptosystems and improve their performance via new methods and techniques. Most recent studies depended on the chaotic map with cipher systems which had gained most researchers to improve its security and robustness. The continued studies had mentioned several methods to strength these chaotic based ciphers and the randomness of its keystream generation. Some of the studies had proposed successful cipher techniques while others had not or advised further improvement. Another side, this paper tests chaotic maps which had used in the cipher algorithms to evaluate its randomness. In general, this paper gives a review of recent stream cipher based on chaotic maps. It also shows the randomness evaluation of chaotic maps. The survey recommended that Chen map is the most random method.', 'title': 'Stream Cipher Based on Chaotic Maps', 'embedding': []}, {'id': 14857, 'abstractText': 'In the present study, we develop a new characterization methodology allowing to directly convert standard electroluminescence (EL) images of a given PV module into absolute quantitative performance maps. The experiments and results were obtained based on both multi-crystalline Al-BSF and mono PERC bifacial silicon modules. After acquiring two EL images, we show that by applying the generalized reciprocity relations, it is possible to optically extract key performance indicator maps. We demonstrate the ability to extract an optical I-V characteristics whereby we optically map the current collection efficiency, the current density, the series resistance across the studied multi-crystalline silicon modules. In addition, we are able to provide a first luminescence-based experimental map of the bifaciality coefficient of the studied mono PERC silicon modules. The results were finally validated by comparing the mean central values extracted from the optically determined maps with the values extracted from electrical I(V) measurements. Such agreement opens the potential of using the presented work for a better control of module reliability, testing accuracies and optimization of manufacturing processes.', 'title': 'On the use of electroluminescence-based reciprocity relations for quantitative mapping of PV modules performance', 'embedding': []}, {'id': 14858, 'abstractText': \"The microwave radiometer (MRM) on-board the Chinese Chang'e-2 (CE-2) lunar probe measures the lunar brightness temperature (also referred to as TB) data that are large-scale scientific data. In order to construct lunar TB map, the optimized hierarchical MK splines method is proposed, which uses a hierarchy of coarse-to-fine control lattices to generate a fine control lattice. The computation of the TB construction function is limited to the small number of control points in the merged control lattice, and then the desired high-resolution TB maps are constructed. At the same time, some basis relations between the lunar TB and frequencies are also analyzed based on the constructed TB maps. It can be found that the high-frequency TB map shows lunar topographic features with close similarity. Furthermore, to express the TB distribution features quantitatively, the lunar TB distribution models, including the global TB model of the Moon, the TB model of the lunar far side, and the TB model of the lunar near side, are established based on the constructed TB maps, and the obtained TB distribution models are log-normal distributions. The establishment of the lunar TB distribution model is important to reasonably select the color layer and intensity of color for the lunar TB maps, and is helpful for studying the lunar TB distribution law. In addition, the topographic data measured by the lunar orbiter laser altimeter are selected to discuss the influence of elevation on the lunar TB, and the CE-2 TB data combined with the FeO and TiO<sub>2</sub> abundances are used to study the microwave thermal emission features of the lunar regolith. The research results have important implications for studying the thermal radiation of the Moon.\", 'title': 'Lunar Brightness Temperature Map and TB Distribution Model', 'embedding': []}, {'id': 14859, 'abstractText': \"The use of the internet today cannot be separated from people's lives. The available information is getting bigger and easier to obtain. Such information can be found in blog articles, news sites, and even statuses on social media. However, the available information sometimes cannot be utilized properly due to lack of a better understanding of the obtained information itself. The purpose of this research is to develop an automatic mind map generator application that can create a mind map from the input of news articles automatically. This is expected to help users understand the contents of the article. The study will take a case study of natural disaster news articles. In its application, researchers used the Support Vector Machine (SVM) method of multi-label classifier one vs rest with linear kernel to classify sentences in the news into the 5W+1H class (what, when, where, who, why, how). Beside classification, this research also includes summarization task as a preliminary task. To get the accurate model, we conducted some experimental study by examining combination of filtering features and candidate features. Our classification model raises F1-scores of 75%. The classification maps the word or phrase into each class, each class is determined as each node in mind map visualization with the root node is the image which shows the title of news article. The usability of our application was evaluated using The System Usability Scale and got the score of 78.5. This mind map generator also provides model evaluation by users, each user can review the classification result and if they agree with the result, they can update the model. By this scheme, the accuracy of our model is getting more accurate and lets us grab new data set automatically.\", 'title': 'Building automatic mind map generator for natural disaster news in Bahasa Indonesia', 'embedding': []}, {'id': 14860, 'abstractText': 'In language learning contexts, reading comprehension is an important learning activity. In EFL reading comprehension learning, one of the frequent styles of reading is the sentence-by-sentence style, in which learners can understand the text as separate sentences only, not as a whole structure. This study focus on the structural understanding of test and map making process from the viewpoints of paragraph. The assumption in this study is that map making in KB mapping does not follow sentence order but focus sets of meanings formed by paragraphs. This study investigates the relation between map making process in KB and SB mapping and paragraph structure of text.', 'title': 'Analysis of the concept mapping style in EFL reading comprehension comparison between kit-build and scratch-build concept mapping from the viewpoint of paragraph structure of text', 'embedding': []}, {'id': 14861, 'abstractText': 'In this work, we study the problem of assimilating high resolution Precipitable Water Vapor (PWV) maps using the Weather Research and Forecast 3D Variational Data assimilation system (WRF-3DVar). The PWV maps are obtained using the Sentinel-1 Synthetic Aperture Radar (SAR) images and the SAR interferometry (InSAR) technique. The influence of the high resolution PWV data on the initial condition of WRF and during the next 12 hours is studied. We demonstrate that the assimilation of InSAR PWV maps increases both the water vapor concentration and temperature over areas affected by extreme weather events so correctly generating localized convection cells. The PWV forecast, after the assimilation of InSAR maps, are compared with the PWV estimates provided by a dense GNSS network. The precipitation pattern and amount are compared to meteorological radar measurements. The case study of the extreme weather event that affected the city of Adra, Spain, on 6<sup>th</sup> September 2015, is used to demonstrate how the assimilation of high resolution PWV maps.', 'title': 'Assimilation of Insar-Derived PWV Maps Exhibit Potential for Atmosphere Convective Storm Characterization', 'embedding': []}, {'id': 14862, 'abstractText': 'High definition (HD) map is a critical part of highly automated driving (HAD) technology and shows potential for high precision vehicle localization when GNSS signals are not available. The current study of using HD map for localiz ation is mostly based on Simultaneous Localization and Mapping (SLAM) technique, which requires high computing power, huge storage space, and quick data transmission ability. Therefore, a study of a new HD map based vehicle localization method which requires less computation is necessary. Geometry is one key component that affects the quality of localization, including accuracy, reliability, and separability. Analysing the geometry can provide reference for designing a localization system to meet the quality requirement of HAD, but is rarely studied. This paper aims to design a high precision and reliable localization system using HD map as a sensor, and the influence of geometry is also explored. Geometric strength is evaluated under different scenarios considering three factors, including feature distribution type, feature number, and distance between vehicle and feature. The results show Minimum Detectable Bias (MDB) and Minimal Separable Bias (MSB) are mostly affected by feature number and distance between vehicle and feature. Randomly distribution, more detected features and close distance between the host vehicle and the features may all contribute to good quality of vehicle position estimation.', 'title': 'High definition map-based vehicle localization for highly automated driving: Geometric analysis', 'embedding': []}, {'id': 14863, 'abstractText': 'Urban buildings are essential components of cities and an indispensable source of urban geographic information. While there are many research efforts focused on urban buildings extraction, there are few studies on large-scale urban building mapping based on satellite images. In this research, a large-scale urban building mapping scheme based on Gaofen-2 satellite (GF-2) images is proposed based on a hierarchical approach. In this hierarchical approach, urban buildings are regarded as a mixture of dense low-rise buildings (DLBs) and sparse independent buildings (SIBs) stacked in space, which are extracted by a semantic segmentation model and an instance segmentation model, respectively. In this study, GF-2 images and OpenStreetMap data were used to extract DLB using <inline-formula><tex-math notation=\"LaTeX\">$U^2$</tex-math></inline-formula>-Net with focal loss. GF-2 images were used to extract SIB using an improved CenterMask model with a deformable convolution network and a spatial coordinate attention module. The main urban area within the 5th ring road of Beijing was selected as the study area. With the trained model, the GF-2 image tiles of Beijing input into the models to first derive coarse maps of DLB and SIB. Postprocessing optimization was performed after combining the maps. The accuracy assessment shows that the overall accuracy of large-scale urban building mapping using the hierarchical approach proposed in this article reaches 91.5%, which is 4.8% higher than that with a traditional method. Overall, the hierarchical approach proposed in this article is effective in large-scale urban building mapping and provides new application opportunities.', 'title': 'A Large-Scale Mapping Scheme for Urban Building From Gaofen-2 Images Using Deep Learning and Hierarchical Approach', 'embedding': []}, {'id': 14864, 'abstractText': 'Summary form only given. Traditional immunostaining techniques utilize antibodies to probe the expression of specific proteins in the brain, but this only allows for two-dimensional mapping that cannot recapitulate the complex circuit and regional interactions that occur in the brain. To achieve a more advanced level of circuitry mapping, we combine classic immunofluorescence staining with a tissue clearing technique, immunolabelling-enabled three-dimensional imaging of solvent-cleared organs (iDISCO), to map neurocircuits in the intact brain. Coupling these technologies permits analysis of neuronal circuits under a variety of conditions, including responses to visual and auditory stimuli, as well as in response to pharmacological agents or drugs of abuse. We apply these technologies to study the neuronal activity changes in the basal ganglia driven by selective dopamine-D1 receptor agonist 2-chloro-APB hydrobromide in wild type and Slc35d3 heterozygous mice and correlated behavioral changes with differences in circuit response. We also map tyrosine hydroxylase positive projection neurons in the whole brains of methamphetamine-injected rats. Based on these studies, we can target specific neuronal populations stimulated in response to these compounds to modulate circuit activation as a potential intervention in basal ganglia function and in drug abuse.', 'title': 'Integrating Immunostaining with Tissue Clearing Techniques for Whole Brain Mapping in Basal Ganglia and Drug Addiction', 'embedding': []}, {'id': 14865, 'abstractText': \"Small-sized unmanned aerial vehicles (UAVs) have been widely investigated for use in a variety of applications such as remote sensing and aerial surveying. Direct three-dimensional (3D) mapping using a small-sized UAV equipped with a laser scanner is required for numerous remote sensing applications. In direct 3D mapping, the precise information about the position and attitude of the UAV is necessary for constructing 3D maps. In this study, we propose a novel and robust technique for estimating the position and attitude of small-sized UAVs by employing multiple low-cost and light-weight global navigation satellite system (GNSS) antennas/receivers. Using the “redundancy'' of multiple GNSS receivers, we enhance the performance of real-time kinematic (RTK)-GNSS by employing single-frequency GNSS receivers. This method consists of two approaches: hybrid GNSS fix solutions and consistency examination of the GNSS signal strength. The fix rate of RTK-GNSS using single-frequency GNSS receivers can be highly enhanced to combine multiple RTK-GNSS to fix solutions in the multiple antennas. In addition, positioning accuracy and fix rate can be further enhanced to detect multipath signals by using multiple GNSS antennas. In this study, we developed a prototype UAV that is equipped with six GNSS antennas /receivers. From the static test results, we conclude that the proposed technique can enhance the accuracy of the position and attitude estimation in multipath environments. From the flight test, the proposed system could generate a 3D map with an accuracy of 5 cm.\", 'title': 'Robust UAV Position and Attitude Estimation using Multiple GNSS Receivers for Laser-based 3D Mapping', 'embedding': []}, {'id': 14866, 'abstractText': 'Fuzzy cognitive maps are studied from the quantitative human-scientific standpoint. The concepts and weights of these maps are thus examined as statistical random variables based on probability distributions in order to make more feasible interpretations on these entities. First, the available fuzzy cognitive maps are considered. Second, our statistical approach is introduced which assumes that the concepts and weights are uniformly or normally distributed random variables. Third, the dynamics and application possibilities of these fuzzy cognitive maps are studied.', 'title': 'A Statistical Random Variable Approach to Fuzzy Cognitive Map Modeling', 'embedding': []}, {'id': 14867, 'abstractText': 'The knowledge of the dynamical state of galaxy clusters allows to alleviate systematics when observational data from these objects are applied in cosmological studies. Evidence of correlation between the state and the morphology of the clusters is well studied. The morphology can be inferred by images of the surface brightness in the X-ray band and of the thermal component of the Sunyaev–Zel’dovich (tSZ) effect in the millimetre range. For this purpose, we apply, for the first time, the Zernike polynomial decomposition, a common analytical approach mostly used in adaptive optics to recover aberrated radiation wavefronts at the telescopes pupil plane. With this novel way, we expect to correctly infer the morphology of clusters and so possibly their dynamical state. To verify the reliability of this new approach, we use more than 300 synthetic clusters selected in the three hundred project at different redshifts ranging from 0 up to 1.03. Mock maps of the tSZ, quantified with the Compton parameter, y-maps, are modelled with Zernike polynomials inside R<inf>500</inf>, the cluster reference radius. We verify that it is possible to discriminate the morphology of each cluster by estimating the contribution of the different polynomials to the fit of the map. The results of this new method are correlated with those of a previous analysis made on the same catalogue, using two parameters that combine either morphological or dynamical-state probes. We underline that instrumental angular resolution of the maps has an impact mainly when we extend this approach to high-redshift clusters.', 'title': 'The Three Hundred project: quest of clusters of galaxies morphology and dynamical state through Zernike polynomials', 'embedding': []}, {'id': 14868, 'abstractText': \"Indoor localization has been recognized as a promising research around the world, and fingerprint-based localization method which leverages WIFI Received Signal Strength (RSS) has been extensively studied since widespread deployment of Access Points (APs) makes WIFI signals omnipresent and easily be obtained. A primary weakness of WIFI-based fingerprinting localization approach lies in its vulnerability under environmental changes and alteration of AP deployment. Despite some studies focus on dealing with effects of AP alterations and low-dynamic environmental factors, such as humidity, temperature, etc., influences of high-dynamic factors, such as changes of crowds' density and position, on WIFI radio map have not been sufficiently studied. In this work, we propose OWUH, an Online Learning-based WIFI Radio Map Updating service considering influences of high-dynamic factors. OWUH utilizes sensors in smart phones as the source of RSS datasets, and it combines historical and newly collected RSS data and purposeful probe data as dataset to incrementally update radio map, which means, compared with traditional methods, the OWUH approach requires a smaller number of RSS data for frequent updating of radio map. Moreover, in order to further enlarge our dataset, we take static data and low-dynamic data into account. An improved online learning method is proposed to recognize periodic pattern and update current radio map. Extensive experiments with 15 volunteers across 10 days indicate that OWUH effectively accommodates RSS variations over time and derives accurate prediction of fresh radio map with mean errors of less than 5dB, outperforming existing approaches.\", 'title': 'Online Learning-Based WIFI Radio Map Updating Considering High-Dynamic Environmental Factors', 'embedding': []}, {'id': 14869, 'abstractText': 'Abstract A matrix convex set is a set of the form <tex>${{\\\\cal S}_n}$</tex> (where each <tex>$d$</tex> is a set of <tex>$n \\\\times n$</tex>-tuples of <tex>${M_n}$</tex> matrices) that is invariant under unital completely positive maps from <tex>${M_k}$</tex> to <tex>${\\\\cal S} = \\\\mathop \\\\cup \\\\limits_{n \\\\ge 1} {{\\\\cal S}_n},$</tex> and under formation of direct sums. We study the geometry of matrix convex sets and their relationship to completely positive maps and dilation theory. Key ingredients in our approach are polar duality in the sense of Effros and Winkler, matrix ranges in the sense of Arveson, and concrete constructions of scaled commuting normal dilation for tuples of self-adjoint operators, in the sense of Helton, Klep, McCullough, and Schweighofer. Given two matrix convex sets <tex>${\\\\cal T} = \\\\mathop \\\\cup \\\\limits_{n \\\\ge 1} {{\\\\cal T}_n}$</tex> and <tex>${\\\\cal S}$</tex>, we find geometric conditions on <tex>${\\\\cal T}$</tex> or on <tex>${{\\\\cal S}_1} \\\\subseteq {{\\\\cal T}_1}$</tex>, such that <tex>${\\\\cal S} \\\\subseteq C{\\\\cal T}$</tex> implies that <tex>$C$</tex> for some constant <tex>${\\\\cal S}$</tex>. For instance, under various symmetry conditions on <tex>$C$</tex>, we can show that <tex>$d$</tex> above can be chosen to equal <tex>$C = d$</tex>, the number of variables. We also show that <tex>${{\\\\cal W}^{\\\\max }}(\\\\overline {{\\\\BBB}{_d}} )$</tex> is sharp for a specific matrix convex set <tex>${\\\\BBB}{_d}$</tex> constructed from the unit ball <tex>${\\\\cal D}$</tex>. This led us to find an essentially unique self-dual matrix convex set <tex>$C = \\\\sqrt d $</tex>, the self-dual matrix ball, for which corresponding inclusion and dilation results hold with constant <tex>${\\\\cal T}$</tex>. For a certain class of polytopes, we obtain a considerable sharpening of such inclusion results involving polar duals. An illustrative example is that a sufficient condition for <tex>${^{(d)}} = \\\\mathop \\\\cup \\\\limits_n \\\\left\\\\{ {({T_1}, \\\\ldots ,{T_d}) \\\\in M_n^d:{T_i} \\\\le 1} \\\\right\\\\}$</tex> to contain the free matrix cube <tex>$\\\\{ x \\\\in {\\\\BBR}{^d}:\\\\sum |{x_j}| \\\\le 1\\\\} \\\\subseteq {1 \\\\over d}{{\\\\cal T}_1}$</tex>, is that <tex>${1 \\\\over d}{{\\\\cal T}_1}$</tex>, that is, that <tex>${[ - 1,1]^d} = _1^{(d)}$</tex> contains the polar dual of the cube <tex>$d$</tex>. Our results have immediate implications to spectrahedral inclusion problems studied recently by Helton, Klep, McCullough and Schweighofer. Our constants do not depend on the ranks of the pencils determining the free spectrahedra in question, but rather on the “number of variables” <tex>$$\\\\matrix{ {{1 \\\\over M}\\\\mathop \\\\sum \\\\limits_{M \\\\le m \\\\lt 2M} {1 \\\\over H}\\\\left| {\\\\mathop \\\\sum \\\\limits_{m \\\\le n \\\\lt m + H} {e^{2\\\\pi iP(n)}}\\\\nu (n)} \\\\right|0} \\\\cr } $$</tex>. There are also implications to the problem of existence of (unital) completely positive maps with prescribed values on a set of operators.', 'title': 'Dilations, Inclusions of Matrix Convex Sets, and Completely Positive Maps', 'embedding': []}, {'id': 14870, 'abstractText': \"An attempt has been made to study the effectiveness of concept maps in physics education through quasi-experimental design. The post-test is used as a measuring instrument and the post-test scores are statistically analysed. The results of Levene's test and t-test reveal that mean of post-test scores of the experimental and control groups are statistically significant. This confirms the Alternate Hypothesis that students who studied using concept maps (experimental group) attained higher learning compared to the students who did not use the concept maps (control group). The learner achievement score reveals that 17.7 % higher learning has been attained by experimental group compared with control group. Questionnaires were used to know the students perceptions on the use of concept maps. The results of questionnaire analysis showed that students find concept maps are useful in teaching and learning. Further details of this study are discussed in the article.\", 'title': 'Concept maps in teaching physics concepts applied to engineering education: An explorative study at the Middle East College, Sultanate of Oman', 'embedding': []}, {'id': 14871, 'abstractText': 'Global forest aboveground biomass (AGB) is very important in quantifying carbon stock, and, therefore, it is necessary to estimate forest AGB accurately. Many studies have obtained reliable AGB estimates by using light detection and ranging (LiDAR) data. However, it is difficult to obtain LiDAR data continuously at regional or global scale. Although many studies have integrated multisource data to estimate biomass to compensate for these deficiencies, few methods can be applied to produce global time series of high-resolution AGB due to the complexity of the method, data source limitations, and large uncertainty. This study developed a new method to produce a global forest AGB map using multiple data sources-including LiDAR-derived biomass products, a suite of high-level satellite products, forest inventory data, and other auxiliary datasets-to train estimated models for five different forest types. We explored three machine learning methods [artificial neural network, multivariate adaptive regression splines, and gradient boosting regression tree (GBRT)] to build the estimated models. The GBRT method was the optimal algorithm for generating a global forest AGB map at a spatial resolution of 1 km. The independent validation result showed good accuracy with an R<sup>2</sup> value of 0.90 and a root mean square error value of 35.87 Mg/ha. Moreover, we compared the generated global forest AGB map with several other forest AGB maps and found the results to be highly consistent. An important feature of this new method is its ability to produce time series of high-resolution global forest AGB maps because it heavily relies on high-level satellite products.', 'title': 'A New Method for Generating a Global Forest Aboveground Biomass Map From Multiple High-Level Satellite Products and Ancillary Information', 'embedding': []}, {'id': 14872, 'abstractText': 'Paddy rice serves as one of the most important crop food globally. The spatial distribution of paddy rice fields plays a fundamental role in describing the rural landscapes, and the precise location and extent of paddy rice fields are of key importance to analyze the subsequent resource allocation, rice yield prediction, and food security. Paddy rice has a distinct character relative to other crops in that the paddy fields need flooding during the initial period of rice seeds preparing and rice transplanting. In order to map paddy rice, remote sensing techniques have long been used to extract and monitor rice crops. Throughout many approaches, phenology-based paddy rice mapping algorithms have been introduced and tested in coarser remote sensing images such as MODIS (Moderate Resolution Imaging Spectroradiorneter), AVHRR (Advanced Very High Resolution Radiometer) and Landsat images. However, the average size of paddy rice fields is commonly smaller than 0.09 ha (e.g., the area of a Landsat pixel). Therefore, a large number of mixed pixels exist, which leads to misclassification. Meanwhile, the phenological indicators used in the previous studies such as LSWI (Land Surface Water Index), and MNDWI (Modified Normalized Difference Water Index) are not feasible to detect the land surface water content at the initial stage of paddy rice seeds preparation and transplanting. Therefore, this study first proposes a new index PMI (Perpendicular Moisture Index) to identify the irrigation in paddy rice fields, and then combines other Vegetation Indices to map paddy rice in Jianghan Plain using time stacks of Sentinel-2 imagery. The results indicates that the proposed PMI can be effectively used as phenological indicator for paddy rice mapping and the Sentinel-2 images provide more detailed spatial distributions. The accuracy assessment suggests that our method has a high accuracy with Overall Accuracy (&gt;97%) and Kappa (&gt;93%). This study suggests that the Sentinel-based automated paddy rice mapping algorithm could potentially and effectively be applied at large spatial scales to monitor paddy rice agriculture.', 'title': 'Automated Paddy Rice Extent Extraction with Time Stacks of Sentinel Data: A Case Study in Jianghan Plain, Hubei, China', 'embedding': []}, {'id': 14873, 'abstractText': \"Concept map can explicitly represent complex ideas and the relationships among them. However, there are few empirical studies on deploying it as a supporting tool for inquiry learning. This pilot study explored the effects of concept mapping on students' group task performance, attitudes towards inquiry learning, and motivational and emotional experiences, in an online learning module. A class of 49 eleventh-grade students participated in the study. They learned in small groups to explore a fish death problem, and constructed concept maps to support their inquiry. The group inquiry task performance was satisfying, with a total mean score of 3.60 out of 5. Survey results showed that students had positive attitude towards inquiry learning (M = 4.31, SD = 0.60), and consensus building (M = 4.40, SD = 0.55). Interviews also revealed that concept mapping enabled students to learn how to think in a logical way, and thus improved their inquiry abilities. The findings demonstrate the promising effects of concept mapping on supporting inquiry learning.\", 'title': 'A Pilot Study of Concept Mapping Mediated Inquiry Learning in an Online Environment', 'embedding': []}, {'id': 14874, 'abstractText': 'Trend Assessment and Scenario Development Analysis approach was carried out in this study. This approach is a framework specifically designed to look at the direction of technological development and possible conditions that occur in the future. First stage of the framework consists of the Delphi study, which is a method for obtaining issues related to maritime and possible conditions in the future. Second stage of the framework, scenario planning, which is a method to illustrate the possibility of future conditions by taking into account the main driving factors; then technology road-mapping illustrate the steps that must be taken to achieve the ideal conditions planned. The Delphi study is applied to develop a consensus-based, prioritized research agenda for Indonesia Foresight Maritime Research Topics in year 2018-2045. Expert investigation of the web-based Delphi method was employed to develop online survey. In this study, a three round modified Delphi survey was conducted. A total of 45 study participants were participated in the third round of study. Some interview study was designed to validate some of the findings. The top 10 priority themes were determined by the web-based Delphi method result and in-depth interview of experts. The results of this study indicate that among the Indonesian maritime researcher and maritime technology expert in this study, they have a high degree consensus on some research topics. In the second stage of the framework, we used “scenario planning” approach. Scenario planning is intended to understand the perception of management in recognizing future alternatives so that appropriate decisions can be taken. Experts and stakeholders have better understanding of the issues pertaining the development of Maritime Frontier Research Topics in Indonesia, including those related to tourism, ocean and fisheries, infrastructures, energy and mineral etc. This study then analyzes the dynamics of strategic environment changes (socio-political-economic context) that determine the construction of Maritime Frontier Research Topics. By using a scenario planning analysis framework, the driving factors and uncertainty factors are formulated which are then developed into a strategic environmental scenario. There are 40 experts participated in two scenario planning workshops. The results of scenario planning exercises map 4 (four) scenarios in 2030 and 4 (four) scenario in 2045. The framework for this foresight technology activities has produced the documents for the development of the Foresight Maritime Research topics that use the combination of Delphi study and scenario planning approach.', 'title': 'Combining Delphi Study and Scenario Planning for Indonesia Research Priorities in Maritime Sector', 'embedding': []}, {'id': 14875, 'abstractText': 'Agile Software Development (ASD) is a knowledge intensive and collaborative activity. The key of acquiring, creating and co-operating the knowledge depends on Knowledge Management (KM). The field of knowledge management helps to improve the productivity of the whole agile software development process from the beginning to the end of the phase. Therefore, investigation of various aspects such as purposes, types of knowledge, technologies and research type is essential. The goal of this study is to conduct a literature review on existing researches on KM initiatives in ASD in order to identify the-state-of-the-art in the area as well as future research opportunities. A mapping study was performed by searching six electronic databases, and we considered studies published until December 2017. The initial resulting set comprised of 404 studies. From this set, a total of 10 studies were selected. for these 10, we performed snowballing and direct search of publications of researchers and research groups that accomplished these studies. Finally, we identified 12 reviewed studies addressing KM in Agile software development in order to extract relevant information on a set of research questions. Although few studies were found that addressed KM in ASD, the mapping shows an increasing attention and interest in the topic in the recent years. Reuse of knowledge of the team is a perspective that has received more attention. Moreover, as a main conclusion, the results show that KM is pointed out as an important strategy for increasing the effectiveness in ASD.', 'title': 'Knowledge Management in Agile Software Development- A Literature Review', 'embedding': []}, {'id': 14876, 'abstractText': 'Since lane geometry information can be used for controlling the pose of an intelligent vehicle, a lane geometry map that contains the lane geometry information should have reliable accuracy. For generating the reliable lane geometry map, lane curve which is detected from a lane detector is an useful information because the lane geometry information can be obtained directly. However, since the detected lane curve contains an uncertainty caused by the noise of the lane detector, the accuracy of the lane geometry map can be degraded. In previous studies, a near point on each detected lane is sampled at each time stamp and accumulated for reducing the noise effects of the lane detector. However, these sampled points also contain the sensing noise of the lane detector and the density of accumulated points depends on the distance interval of data acquisition. In this article, we proposed the probabilistic lane smoothing-based generation method for the reliable lane geometry map. In the probabilistic lane smoothing, the lane geometry map is modeled as the nodes with the uncertainty of its position obtained from the sensor error model. Each node of the lane geometry map is smoothed based on the Bayesian filtering scheme. The evaluation results show that the lane geometry map can be generated by reducing the noise of the detected lane curve. Additionally, the generated lane geometry map is more reliable than the sampling point-based generated map in terms of the accuracy of the distance and heading angle.', 'title': 'Probabilistic Smoothing Based Generation of a Reliable Lane Geometry Map With Uncertainty of a Lane Detector', 'embedding': []}, {'id': 14877, 'abstractText': \"Intelligent Cushion (iCushion) technology is recently booming with embedded pressure array sensors to enable individual-specific sitting experiences. iCushion has the build-in functionality to identify users throughout its use in a continuous and non-intrusive manner. Due to the variability in sitting posture and the angle of seated deflection, the accuracy of user identification remains unstable or unclear with existing solutions. Aiming at this problem, this study develops a two-stage pressure map algorithm based on robust spatial-temporal features. First, pressure maps are collected constantly without limiting the user's posture, based on which an accumulated identity library is established for sitting postures by extracting features from pressure maps. To be specifically, we create a decision tree to classify maps by distances between both ischia and then variances in both areas around ischia in maps are analyzed. Second, the similarity between both maps are measured by the Euclidean distance between feature vectors around ischia for matching maps data. A k-NN voting mechanism is developed to achieve reliability of identification. The resulted iCushion prototype has successfully identified 92.2% of maps with three randomly chosen individuals through four-hour non-stop testing. It holds potentials of non-intrusive and reliable activity recognition in other pervasive applications.\", 'title': 'iCushion: A Pressure Map Algorithm for High Accuracy Human Identification', 'embedding': []}, {'id': 14878, 'abstractText': 'Existing studies on indoor navigation often require such a pre-deployment as floor map, localization system and/or additional (customized) hardwares, or human motion traces, making them prohibitive when the situation deviates from these requirements (e.g., navigating a crowd of panicking people where no localization system or motion traces are available). The main observation inspiring our work without reliance on such pre-deployment is that when there are sufficient participants (e.g., a crowd of panicking people), the WiFi signatures collected by participants can serve as the fingerprints (referred to as location fingerprints) of their unknown locations. By computing relative positions of these location fingerprints we can connect them to form a global map. Such a map reflects the topology of the underlying walkable space and thus holds the potential of offering a navigation path for any intended users. Based on this observation, we design Fly-Navi, a crowdsourcing based indoor navigation system via on-the-fly map generation, and primarily designed for indoor environments with rectilinear and narrow corridors. Specifically, each participant uploads sensory data, and the server then generates a global map (on-the-fly map) through a series of operations such as local map generation, local map stitch and edge computation. On top of the global map, Fly-Navi computes a navigation path to the given destination and tracks the progress. We implement the prototype of Fly-Navi and our experiments show that Fly-Navi can quickly generate a correct global map with the 80-percentile of between-fingerprint distance error less than 3 meters, which is important for computing turning points of the map and hereon offering turn-by-turn instructions, and correctly navigate the intended users to their destinations.', 'title': 'Fly-Navi: A Novel Indoor Navigation System With On-the-Fly Map Generation', 'embedding': []}, {'id': 14879, 'abstractText': 'Coherent mapping becomes important resources in ontology matching process. A process called mapping repair is carried out to ensure the output alignment does not contain incoherent mapping. The goal of repair is to change the incoherent alignment into coherent alignment by removing some unwanted mapping from the alignment. The removal of mapping should be as minimal as possible in order to avoid a major change in input alignment. This article aims to build a mapping repair system that restore incoherent into coherent mapping (or alignment). This study implements mapping weighting as the heuristic function and A* Search method to produce coherent output alignment. Some experiments were conducted, the results showed that proposed system can produce output alignment with zero conflict and 100% coherence degree.', 'title': 'Optimal Path Finding Algorithm Using Weighted Based Heuristic for Incoherent Mapping Repair', 'embedding': []}, {'id': 14880, 'abstractText': 'Virtual network embedding (VNE) is the key technology in network virtualization and has been proven NP-hard. The purpose of VNE is to find the optimal mapping of virtual nodes and links, and minimize the utilization of resources. However, many particle swarm optimization approaches to VNE separate VNE into two independent subproblems (i.e., node mapping and link mapping) and ignore the coordination between node mapping and link mapping. In this paper, a one-stage and dual-heuristic particle swarm optimization (DH-PSO) is devised to solve VNE. To coordinate node mapping and link mapping, firstly, DH-PSO updates positions of particles step by step, and nodes and links are mapped in one stage. Secondly, DH-PSO devises the dual-heuristic strategy to further improve the optimizing capability. The first heuristic strategy is to construct a candidate set and the second strategy is to find the best solution from the candidate set. Hence, not only the network resources but the network paths are taken into account to construct solutions. DH-PSO can be combined with different two-stage approaches to become one-stage. DH-PSO is experimentally studied on different instances. The experimental results verify that the proposed DH-PSO is promising.', 'title': 'One-stage and Dual-heuristic Particle Swarm optimization for Virtual Network Embedding', 'embedding': []}, {'id': 14881, 'abstractText': 'Aiming at the problem that the map operation time occupies more resources in the embedded system. By studying the digital map, then the corresponding map is drawn and the map is optimized. Using the GPS/BEIDOU module to locate and use the Dijkstra algorithm to achieve the shortest path of the navigation module design, the use of the map database to remove the corresponding area of the data mapping map information, in a translation to a certain distance when the scene clear the data, draw a map. In this way, there will be no data left in the view. Since the Dijkstra algorithm is based on the graph, it is necessary to link the roads in the area to create an indirect graph to find the shortest path to the end point. If the point of the line as an indirect node, the composition of the indirect map will be very large. Moreover, the use of the line as an indirect node, we can reduce the number of nodes without the map, to improve the efficiency of Dijkstra algorithm traversal. Based on the open system platform and data standard, this paper designs and develops a secure, stable and low cost embedded GPS/BEIDOU navigation and navigation system. The experimental results are optimized. The results show that the proposed method is efficient for navigation.', 'title': 'An Open Source Map Optimization Platform for Efficient Navigation', 'embedding': []}, {'id': 14882, 'abstractText': 'Single-robot visual SLAM has problems such as slow mapping speed. This study proposed a multi-robot collaborative SLAM and scene reconstruction system based on an RGB-D camera. The system adopts a centralized structure. While several client robots collect RGB-D image data respectively and transmit the data to the server, the server runs a stand-alone visual SLAM system based on ORB-SLAM2 for each client, and stores the real-time mapping data in the map manager. At the same time, it detects whether the maps in the map managers meet the fusion conditions at a certain frame rate, and uses the map fusion algorithm to fuse when the conditions are met. In order to solve the problem that ORB-SLAM2 can only do semi-dense mapping, the system uses the 7-DoF poses of each client output by collaborative SLAM to reconstruct the dense map. We evaluated the performance of the system through simulations. Compared with the single-robot system, the collaborative SLAM in this system has a significant improvement in speed and can maintain high accuracy. We verified the effectiveness of the scene reconstruction algorithm through the scene reconstruction simulation and further proved that high-precision camera poses can be obtained in collaborative SLAM through the reconstruction results.', 'title': 'Multi-robot collaborative SLAM and scene reconstruction based on RGB-D camera', 'embedding': []}, {'id': 14883, 'abstractText': \"Semantic information without spatial characteristics can also be visualised in the form of a map. This map type is usually called map-like visualisation; it is not a representation of a real geo-entity but borrows the map metaphor. In current map-like visualisation works, the manipulation of the cartographic process is poor, and there is a considerable amount of randomness and uncertainty in the map results. For instance, in the process of converting semantic objects to map objects, the map shape is usually uncertain and has many possibilities. Frequently, however, we need shapes to follow certain principles, such as the principles of being close to a certain shape and having a simple geometry, to facilitate recognition and mapping. To achieve a balance between uncertain shapes and design needs, this study enhances the realisation of the cartographer's design idea by improving the controllability in the cartographic process. An optimisation technology, the simulated annealing algorithm, is introduced to control the determination of shape to improve mapping efficiency. Experiments based on real data show that based on this method, the map-like representation not only creates a whole outline close to the pre-defined shape but also realises internal regions with a simple shape.\", 'title': 'Shape Decision-Making in Map-Like Visualization Design Using the Simulated Annealing Algorithm', 'embedding': []}, {'id': 14884, 'abstractText': 'In this article, a novel dense underwater 3-D mapping paradigm based on pose graph simultaneous localization and mapping (SLAM) using an acoustic camera mounted on a rotator is proposed. The demands of underwater tasks, such as unmanned construction using robots, are growing rapidly. In recent years, the acoustic camera, which is a state-of-the-art forward-looking imaging sonar, has been gradually applied in underwater exploration. However, distinctive imaging principles make it difficult to gain an intuitive perception of an underwater environment. In this study, an acoustic camera with a rotator was used for dense 3-D mapping of the underwater environment. The proposed method first applies a 3-D occupancy mapping framework based on the acoustic camera rotating around the acoustic axis using a rotator at a stationary position to generate 3-D local maps. Then, scan matching of adjacent local maps is implemented to calculate odometry without involving internal sensors, and an approximate dense global map is built in real time. Finally, based on a graph optimization scheme, offline refinement is performed to generate a final dense global map. Our experimental results demonstrate that our 3-D mapping framework for an acoustic camera can achieve dense 3-D mapping of underwater environments robustly and accurately.', 'title': 'Acoustic Camera-Based Pose Graph SLAM for Dense 3-D Mapping in Underwater Environments', 'embedding': []}, {'id': 14885, 'abstractText': 'Heterogeneous MPSoCs consisting of integrated CPUs and GPUs are suitable platforms for embedded applications running on hand- held devices such as smart phones. As the handheld devices are mostly powered by battery, the integrated CPU and GPU MPSoC is usually designed with an emphasis on low-power rather than performance. In this paper, we are interested in exploring a power- efficient layer mapping of convolution neural networks (CNNs) deployed on integrated CPU and GPU platforms. Specifically, we investigate the impact of layer mapping of YoloV3-Tiny (i.e., a widely-used CNN in both industry and academia) on system power consumption through numerous experiments on NVIDIA board Jetson TX2. The experimental results indicate that 1) almost all of the convolution layers are not suitable for mapping to CPU, 2) the pooling layer can be mapped to CPU for reducing power consumption, but the mapping may lead to a decrease in inference speed when the layer’s output tensor size is large, 3) the detection layer can be mapped to CPU as long as its floating-point operation scale is not too large, and 4) the channel and upsampling layers are both suitable for mapping to CPU. These observations obtained in this study can be further utilized to guide the design of power-efficient layer mapping strategies for integrated CPU and GPU platforms.', 'title': 'Power-Efficient Layer Mapping for CNNs on Integrated CPU and GPU Platforms: A Case Study', 'embedding': []}, {'id': 14886, 'abstractText': 'A highlighted map, where objects with unique shapes are highlighted, has been studied for mobile robot localization. This map improves the localization accuracy without adding any sensors or online computations for localization. In addition, it can be used in various particle-filter-based localization algorithms. For generating a highlighted map, reinforcement learning has been used. Since this method generates the highlighted map by utilizing a limited number of the actual sensor measurement data, the generated map is vulnerable to unexpected sensor measurement noise. In this paper, the robustification method of a highlighted map is proposed. Our proposed method introduces a virtual obstacle that causes measurement noise, and learns both the worst-case obstacle behavior and the optimal highlighted map simultaneously based on adversarial reinforcement learning. We perform a numerical simulation to verify the robustness of the map.', 'title': 'Adversarial Reinforcement Learning Based Robustification of Highlighted Map for Mobile Robot Localization', 'embedding': []}, {'id': 14887, 'abstractText': \"Chaotic dynamics is widely used to design pseudo-random number generators and for other applications, such as secure communications and encryption. This paper aims to study the dynamics of the discrete-time chaotic maps in the digital (i.e., finite-precision) domain. Differing from the traditional approaches treating a digital chaotic map as a black box with different explanations according to the test results of the output, the dynamical properties of such chaotic maps are first explored with a fixed-point arithmetic, using the Logistic map and the Tent map as two representative examples, from a new perspective with the corresponding state-mapping networks (SMNs). In an SMN, every possible value in the digital domain is considered as a node and the mapping relationship between any pair of nodes is a directed edge. The scale-free properties of the Logistic map's SMN are proved. The analytic results are further extended to the scenario of floating-point arithmetic and for other chaotic maps. Understanding the network structure of a chaotic map's SMN in digital computers can facilitate counteracting the undesirable degeneration of chaotic dynamics in finite-precision domains, also helping to classify and improve the randomness of pseudo-random number sequences generated by iterating the chaotic maps.\", 'title': 'Dynamic Analysis of Digital Chaotic Maps via State-Mapping Networks', 'embedding': []}, {'id': 14888, 'abstractText': 'Autonomous vehicle self-positioning based on 3D light detection and ranging (Lidar) has become popular recently due to disadvantages of global navigation satellite system (GNSS) in urban areas. As LiDAR-based simultaneous localization and mapping (SLAM) methods suffer from error accumulation, state-of-the-art approaches match the point cloud data acquired by LiDAR to the priori known 3D point cloud map to obtain the position of the vehicle within the map. However, 3D point cloud map is very expensive to store and download as it contains an enormous amount of data even for a small area (around 300 million points per km<sup>2</sup>). In this study, rather than using 3D point cloud directly as a map, we focused on the planar surfaces which are mostly available in urban areas, easy to extract, and at the same time clearly observable by LiDAR. Therefore, in our proposed map, we extract the planar surfaces from the 3D point cloud and calculate its uncertainty (deviation) and store them as a prior map. Accordingly, in this map, we can abstract several thousands of points by only one plane. As a result, we can extremely shrink the map size (25 million points to around 1000 planes). Later in the localization phase, we reconstruct Gaussian mixture model for each planar surface based on previously stored deviation, and match LiDAR data to it to obtain the precise location of the vehicle. Experiments conducted in one of the urban areas of Tokyo show that even though we extremely shrank the map size, we could preserve the mean error of the localization less than 43cm comparing to other point cloud based methods.', 'title': 'Autonomous vehicle self-localization based on probabilistic planar surface map and multi-channel LiDAR in urban area', 'embedding': []}, {'id': 14889, 'abstractText': 'Coarse-grain reconfigurable arrays (CGRAs) are emerging accelerators that promise low-power acceleration of compute-intensive loops in applications. The acceleration achieved by CGRA relies on the efficient mapping of the compute-intensive loops by the CGRA compiler, onto the CGRA architecture. The CGRA mapping problem, being NP-complete, is performed in a two-step process, namely, scheduling and mapping. The scheduling algorithm allocates timeslots to the nodes of the data flow graph, and the mapping algorithm maps the scheduled nodes onto the processing elements of the CGRA. On a mapping failure, the initiation interval (II) is increased and a new schedule is obtained for the increased II. Most previous mapping techniques use the iterative modulo scheduling (IMS) algorithm to find a schedule for a given II. Since IMS generates a resource-constrained as-soon-as-possible (ASAP) scheduling, even with increased II, it tends to generate a similar schedule that is not mappable. Therefore, IMS does not explore the schedule space effectively. To address these issues, this article proposes CRIMSON, compute-intensive loop acceleration by randomized IMS and optimized mapping technique that generates random modulo schedules by exploring the schedule space, thereby creating different modulo schedules at a given and increased II. CRIMSON also employs a novel conservative test after scheduling to prune valid schedules that are not mappable. From our study conducted on the top 24 performance-critical loops (run for more than 7% of application time) from MiBench, Rodinia, and Parboil, we found that previous state-of-the-art approaches that use IMS, such as RAMP and GraphMinor could not map five and seven loops, respectively, on a 4×4 CGRA, whereas CRIMSON was able to map them all. For loops mapped by the previous approaches, CRIMSON achieved a comparable II.', 'title': 'CRIMSON: Compute-Intensive Loop Acceleration by Randomized Iterative Modulo Scheduling and Optimized Mapping on CGRAs', 'embedding': []}, {'id': 14890, 'abstractText': 'Depth maps obtained by commercial depth sensors are always in low-resolution, making it difficult to be used in various computer vision tasks. Thus, depth map super-resolution (SR) is a practical and valuable task, which up-scales the depth map into high-resolution (HR) space. However, limited by the lack of real-world paired low-resolution (LR) and HR depth maps, most existing methods use down-sampling to obtain paired training samples. To this end, we first construct a large-scale dataset named \"RGB-D-D\", which can greatly promote the study of depth map SR and even more depth-related real-world tasks. The \"D-D\" in our dataset represents the paired LR and HR depth maps captured from mobile phone and Lucid Helios respectively ranging from indoor scenes to challenging outdoor scenes. Besides, we provide a fast depth map super-resolution (FDSR) baseline, in which the high-frequency component adaptively decomposed from RGB image to guide the depth map SR. Extensive experiments on existing public datasets demonstrate the effectiveness and efficiency of our network compared with the state-of-the-art methods. Moreover, for the real-world LR depth maps, our algorithm can produce more accurate HR depth maps with clearer boundaries and to some extent correct the depth value errors.', 'title': 'Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark Dataset and Baseline', 'embedding': []}, {'id': 14891, 'abstractText': 'The first Canadian wetland inventory (CWI) map, which was based on Landsat data, was produced in 2019 using the Google Earth Engine (GEE) big data processing platform. The proposed GEE-based method to create the preliminary CWI map proved to be a cost, time, and computationally efficient approach. Although the initial effort to produce the CWI map was valuable with a 71% overall accuracy (OA), there were several inevitable limitations (e.g., low-quality samples for the training and validation of the map). Therefore, it was important to comprehensively investigate those limitations and develop effective solutions to improve the accuracy of the Landsat-based CWI (L-CWI) map. Over the past year, the L-CWI map was shared with several governmental, academic, environmental nonprofit, and industrial organizations. Subsequently, valuable feedback was received on the accuracy of this product by comparing it with various in situ data, photo-interpreted reference samples, land cover/land use maps, and high-resolution aerial images. It was generally observed that the accuracy of the L-CWI map was lower relative to the other available products. For example, the average OA in four Canadian provinces using in situ data was 60%. Moreover, including reliable in situ data, using an object-based classification method, and adding more optical and synthetic aperture radar datasets were identified as the main practical solutions to improve the CWI map in the future. Finally, limitations and solutions discussed in this study are applicable to any large-scale wetland mapping using remote sensing methods, especially to CWI generation using optical satellite data in GEE.', 'title': 'Evaluation of the Landsat-Based Canadian Wetland Inventory Map Using Multiple Sources: Challenges of Large-Scale Wetland Classification Using Remote Sensing', 'embedding': []}, {'id': 14892, 'abstractText': 'Task mapping has been a hot topic in multiprocessor system-on-chip software design for decades. During the mapping process, load balance (LB) and communication optimization have been two important performance optimization factors. This paper studies the relations between LB, interprocessor communications, and communication pipeline technique during the mapping process, and proposes an integer linear programming (ILP)-based static task mapping approach, which considers both LB and communication optimization. The approach consists of an optimized ILP model for task mapping with fewer variables compared to previous ILP mapping works. Moreover, to enhance the scalability of the ILP task mapping, the task-processor-cluster algorithm is proposed to reduce the scale of the task graph and the number of processors and then solve the coarse-grained input by the ILP mapping. To increase the adaptability of the ILP task mapping, the improved augmented E-constraint method is further integrated with the ILP formulations to select the best mapping for different applications. Experimental results on a 2/4/8/16/24-CPU platform of both synthetic and real-life benchmarks demonstrate the efficiency of the proposed approach.', 'title': 'A Scalable and Adaptable ILP-Based Approach for Task Mapping on MPSoC Considering Load Balance and Communication Optimization', 'embedding': []}, {'id': 14893, 'abstractText': 'Pre-surgical mapping of sensorimotor and language functions is crucial to reduce neurological deficits in epilepsy and tumor resection surgery. As non-invasive mapping, both resting-state and task-evoked functional MRI has been explored in pre-surgical mapping. In lack of standardized test paradigm, the reliability of fMRI mapping is still a concern for clinical use. In this study, to improve the reliability of fMRI based mapping, task fMRI data from all available task paradigms (motor movement, word repeating and picture naming) were low-pass filtered in the band of resting-state fMRI (0.01-0.08Hz) and concatenated to get more time points. With K-means clustering, it was shown that the sensorimotor network could be reliably parcellated into hand and tongue sub-regions. The resulted parcellations were further verified with invasive ECoG and ECS mapping. Both the accuracy and specificity were better than using the motor-task fMRI only. Especially, for those patients who failed in task fMRI mapping, our method was able to provide accurate mapping as well. Our results also indicate that cortical sensorimotor network pattern is intrinsic and always present during various tasks, which supports the physiological link between the spontaneous and the task-evoked BOLD signals.', 'title': 'Sensorimotor network parcellation for pre-surgical patients using low-pass filtered fMRI', 'embedding': []}, {'id': 14894, 'abstractText': 'Inverse tone mapping is an important topic in High Dynamic Range technology. Recent years, deep learning based image inverse tone mapping methods have been extensively studied and perform better than classical inverse tone mapping methods. However, these methods consider the inverse tone mapping problem as a domain transformation problem from LDR domain directly to HDR domain and ignore the relationship between LDR and HDR. Besides, when using these deep learning based methods to transform frames of videos, it will lead to temporal inconsistency and flickering. In this work, we propose a new way to consider the inverse tone mapping problem and design a deep learning based video inverse tone mapping algorithm to reduce the flickering. Different from previous methods, we first transform LDR resources back to approximate real scenes and use these real scenes to generate the HDR outputs. When generating HDR outputs, we use 3D convolutional neural network to reduce the flickering. We also use methods to further constrain the luminance information and the color information of HDR outputs separately. Finally, we compare our results with existing classical video inverse tone mapping algorithms and deep image inverse tone mapping methods to show our great performance, and we also prove the necessity of each part of our method.', 'title': 'Deep Video Inverse Tone Mapping', 'embedding': []}, {'id': 14895, 'abstractText': \"A quantitative evaluation of a 2D map is important and becomes a challenging task for indoor survey mapping. The paper presents the method for comparison of built 2D maps generated from Google's Cartographer. We focus on the comparison of map quality without ground truth data between a reference map from a set of LiDAR with IMU and wheel-encoder and compared maps from a set of LiDAR with IMU, wheel-encoder, and/or UWB. Three metrics for comparison the map quality are presented in this paper including RMSE, blur detection, and drifts in translation and rotation. The finding shows that a set of LiDAR with IMU and UWB can be comparable to a set of LiDAR with IMU and wheel-encoder in terms of map quality. Moreover, the proposed method can be further used for the relative evaluation of a feasibility study.\", 'title': 'Quality Evaluation Method of 2D SLAM', 'embedding': []}, {'id': 14896, 'abstractText': 'Accurate protein contact map prediction is essential for de novo protein structure prediction. Over the past few years, deep learning has brought a significant breakthrough in protein contact map prediction and optimized deep learning architectures are highly desired for performance improvement. As an emerging deep learning architecture, the generative adversarial network (GAN) has shown the powerful capability of learning intrinsic patterns, which inspires us to comprehensively exploit GAN for predicting accurate protein contact maps. In this study, we present GANcon, a novel GAN-based deep learning architecture for protein contact map prediction, which to the best of our knowledge is the first GAN-based approach in this field. Instead of using a single neural network, GANcon is composed of two competitive networks that are evolving through adversarial learning. The generator network employs a dedicated encoder-decoder architecture that can efficiently capture the underlying contact information from versatile protein features to generate contact maps, while the discriminator network learns the differences between generated contact maps and real ones and promotes the generator network to produce more accurate contact maps. Moreover, to deal with the imbalance problem and take into account the symmetry of contact maps, we also propose a novel symmetrical focal loss, which can further enhance the effectiveness of adversarial learning for better performance. The experimental results on several datasets demonstrate that GANcon outperforms many state-of-the-art methods, indicating the effectiveness of our method for predicting protein contact maps. GANcon is freely available at https://github.com/melissaya/GANcon.', 'title': 'GANcon: Protein Contact Map Prediction With Deep Generative Adversarial Network', 'embedding': []}, {'id': 14897, 'abstractText': 'We present a study on sketch-map interpretation and sketch to robot map matching, where maps have nonuniform scale, different shapes or can be incomplete. For humans, sketch-maps are an intuitive way to communicate navigation information, which makes it interesting to use sketch-maps for human robot interaction; e.g., in emergency scenarios. To interpret the sketch-map, we propose to use a Voronoi diagram that is obtained from the distance image on which a thinning parameter is used to remove spurious branches. The diagram is extracted as a graph and an efficient error-tolerant graph matching algorithm is used to find correspondences, while keeping time and memory complexity low. A comparison against common algorithms for graph extraction shows that our method leads to twice as many good matches. For simple maps, our method gives 95% good matches even for heavily distorted sketches, and for a more complex real-world map, up to 58%. This paper is a first step toward using unconstrained sketch-maps in robot navigation.', 'title': 'Using sketch-maps for robot navigation: Interpretation and matching', 'embedding': []}, {'id': 14898, 'abstractText': 'High resolution 3D mapping of road systems is currently being carried out by expensive Mobile Mapping Systems (MMS) but coverage is limited. Recently Low Cost Sensor (LCS) systems have been developed which use common, low cost, internal MEMS position sensors from mobile phones, but such sensors come with a reduced absolute and relative positional accuracy. This study investigates the registration of LCS maps within MMS maps to improve map coverage and lower costs. MMS and LCS maps of a real world environment are made and registration is performed using feature matching and Iterative Closest Point alignment. Accuracy of ICP alignment is approximately (10cm) and local convergence is possible up to (1m). A combination of feature matching and ICP is used to demonstrate accurate alignment from an initial error of (10m). An example of a LCS map aligned within a MMS map is presented to confirm the use of LCS systems to extend 3D mapping coverage.', 'title': 'Registration of Low Cost Maps within Large Scale MMS Maps', 'embedding': []}, {'id': 14899, 'abstractText': 'Maps can greatly improve vehicle localization using perception sensors that detect features georeferenced in the map. This relies on two assumptions. Firstly, the detected features and the elements of the map have to be correctly associated. Secondly, the features of the map have to be accurately referenced. In this paper, solutions regarding these issues are presented. The case study of localization using a camera detecting road markings is considered. A Kalman smoothing process is used to obtain the best possible estimate of the trajectory that enables to evaluate the reliability of markings stored in the map. A likelihood maximization technique is used to best associate the observed markings to those referenced in the map. By using these two methods, map errors are detected after a first passage in an area and can be mitigated in later passes. Experimental results are reported to evaluate the performance of this approach. It is shown that mapping errors can be correctly handled.', 'title': 'Estimating the reliability of georeferenced lane markings for map-aided localization', 'embedding': []}, {'id': 14900, 'abstractText': \"In order to generate real‐time motion plans, the planning and control module can combine perception inputs, which detect dynamic obstacles in real time, localization inputs, which generate real‐time vehicle poses, and mapping inputs, which capture road geometry and static obstacles. Digital maps, such as Google map, Bing map, and Open Street Map (OSM), were developed for humans instead of for machines, as these digital maps rely heavily on human knowledge and observations. This chapter discusses the details of digital mapping by using OSM as an example. It also discusses details of building high definition (HD) maps and presents a case study on PerceptIn's mapping technology, in which existing OSM is extended for autonomous robot and vehicle navigation. HD maps for autonomous driving systems need to have high precision, usually at centimeter level.\", 'title': 'Mapping', 'embedding': []}, {'id': 14901, 'abstractText': 'Signal map is of great importance, especially in the dawn of 5G network, for site spectrum monitoring, location-based services (LBS), network construction, and cellular planning. Despite its significance, the traditional signal map construction, e.g., through full site survey, could be time-consuming and labor-intensive as the signal varies frequently over time and the accuracy requirement grows rapidly with the emergence of new applications. Even with crowdsourcing scheme, the participants tend to be unevenly distributed in space while the encouragement budgets for the participants could be far from enough to collect adequate high-quality measurements. Therefore, the signal map constructed by crowdsourcing is often sparse and incomplete. To this end, in this paper, we study how to effectively reconstruct and update the signal map in the case of partially measured signal maps with minimum cost and propose an auto-encoder-based active signal map reconstruction method (AER). Our method is mainly innovative in three parts. Firstly, AER can effectively update the signal map with only a small number of observations while also fully using the incomplete historical signals to effectively update the signal map online. Secondly, AER consists of an active query mechanism which quantitatively evaluates the most valuable measurement site for reconstruction, which further reduces the measurement cost to a large extent. Thirdly, to cope with the measurement dynamics, we give a new signal map model describing not only the signal strength but also the signal dynamics, based on which an advanced AER algorithm is proposed. The simulation results demonstrate the advantages and effectiveness of our approach in both accuracy and cost.', 'title': 'Cost-Effective Signal Map Crowdsourcing with Auto-Encoder Based Active Matrix Completion', 'embedding': []}, {'id': 14902, 'abstractText': 'Human visual cortex is organized into several functional re-gions/areas. Identifying these visual areas of the human brain (i.e., V1, V2, V4, etc) is an important topic in neurophysiology and vision science. Retinotopic mapping via functional magnetic resonance imaging (fMRI) provides a noninvasive way of defining the boundaries of the visual areas. It is well known from neurophysiology studies that retino-topic mapping is diffeomorphic within each local area (i.e. locally smooth, differentiable, and invertible). However, due to the low signal-noise ratio of fMRI, the retinotopic maps from fMRI are often not diffeomorphic, making it difficult to delineate the boundaries of visual areas. The purpose of this work is to generate diffeomorphic retinotopic maps and improve the accuracy of the retinotopic atlas from fMRI measurements through the development of a specifically designed registration procedure. Although there are sophisticated existing cortical surface registration methods, most of them cannot fully utilize the features of retinotopic mapping. By considering unique retinotopic mapping features, we form a qua-siconformal geometry-based registration model and solve it with efficient numerical methods. We compare our registration with several popular methods on synthetic data. The results demonstrate that the proposed registration is superior to conventional methods for the registration of retinotopic maps. The application of our method to a real retinotopic mapping dataset also results in much smaller registration errors.', 'title': 'Diffeomorphic Registration for Retinotopic Mapping Via Quasiconformal Mapping', 'embedding': []}, {'id': 14903, 'abstractText': 'Object-based image analysis (OBIA) technique has been representing an evolving paradigm of remote sensing application, along with more high-resolution satellite images available. However, too many derived features from segmented objects also present a new challenge to OBIA applications. In this paper, we present a supervised and adaptive method for ranking and weighting features for object-based classification. The core of this method is the feature weight maps for each land type resulted from prior thematic maps and their corresponding satellite images of study areas. Specifically, first, satellite images to be classified are segmented using an adaptive multiscale algorithm, and the multiple (spectral, shape, and texture) features of segmented objects are calculated. Second, we extract distance maps and feature weight vectors for each land type from the prior thematic maps and corresponding satellite images, to generate feature weight maps. Third, a feature-weighted classifier with the feature weight maps, is applied on the segmented objects to generate classification maps. Finally, the classification result is evaluated. This approach is applied on a Sentinel-2 multispectral satellite image and a Google Map image to produce objected-based classification maps, compared with the traditional feature selection algorithms. The experimental results illustrate that the proposed method is practically efficient to select important features and improve classification performance.', 'title': 'Supervised and Adaptive Feature Weighting for Object-Based Classification on Satellite Images', 'embedding': []}, {'id': 14904, 'abstractText': \"This paper presents the basis of the system for automation of the military trafficability maps development. The system is based on determining the index of trafficability (IOP) for square primary fields of 1 km by 1 km size. It is defined with use of data on land cover elements, obtained from both military and civilian spatial databases. To determine the trafficability, various methods have been used: artificial neural networks (multilayer perceptron and Self Organizing Maps) and GIS multi-criteria spatial analyses. Tests were performed for both military (Vector Map Level 2) and civilian spatial databases (Corine Land Cover and Open Street Map). The article presents a comparison of used methods. In addition to the basic statistical analyses (average value of IOPs, Pearson's correlation matrices, etc.), spatial statistics such as spatial autocorrelation coefficients (Moran I) were calculated. The key experiment was also to compare generated maps to the map of trafficability made by the methods commonly used in the Armed Forces. The result of conducted experiments is therefore choosing the optimal map of trafficability and the answer to the question: which method and data should be used for the best trafficability map elaboration? It was found that the most useful and most faithful map showing the conditions of trafficability is the study elaborated using the Vegetation Roughness Factor method and using Corine Land Cover data.\", 'title': 'Comparison of the military maps of trafficability developed by different methods', 'embedding': []}, {'id': 14905, 'abstractText': 'Vehicle self-Iocalization based on the matching of Light detection and ranging (LiDAR) scans to the normal distribution (ND) map become more popular in recent years due to the price down and miniaturization of the LiDARs. In such methods, the source of self-Iocalization error can be divided into input scan quality, matching algorithm and map. In this work, we focus on the map, as one ofthe high potential sources of error. By investigating the erroneous scenarios in the map and comparing their characteristics, we come up with some criteria and requirements for the map to be able to perform self-Iocalization with a needed error. In this work, we propose four factors for quantified evaluation ofthe map requirements. These factors are feature count factor, layout factor, normal entropy factor, and local similarity factor ofthe map. We evaluated these four factors in a different part ofthe map with different scenarios by comparing them with the self-Iocalization error. Experimental results show that the local similarity factor with 0.59 of correlation with the maximum error has the highest contribution to the Iocalization error. For normal entropy factor, feature count factor, layout factor, correlations are 0.42, 0.36, and 0.34 respectively. By applying these four factors, maximum Iocalization error can be modeled with RMSE and R-squared (R<sup>2</sup>) of 0.44 and 0.598 respectively. Result of this study can be applied to the dynamic determination of the abstraction ratio of the map and sensor fusion as well.', 'title': 'Evaluation of Digital Map Ability for Vehicle Self-Iocalization', 'embedding': []}, {'id': 14906, 'abstractText': 'Multi-destination maps are a kind of navigation maps aimed to guide visitors to multiple destinations within a region, which can be of great help to urban visitors. However, they have not been developed in the current online map service. To address this issue, we introduce a novel layout model designed especially for generating multi-destination maps, which considers the global and local layout of a multi-destination map. We model the layout problem as a graph drawing that satisfies a set of hard and soft constraints. In the global layout phase, we balance the scale factor between ROIs. In the local layout phase, we make all edges have good visibility and optimize the map layout to preserve the relative length and angle of roads. We also propose a perturbation-based optimization method to find an optimal layout in the complex solution space. The multi-destination maps generated by our system are potential feasible on the modern mobile devices and our result can show an overview and a detail view of the whole map at the same time. In addition, we perform a user study to evaluate the effectiveness of our method, and the results prove that the multi-destination maps achieve our goals well.', 'title': 'Generating Multi-Destination Maps', 'embedding': []}, {'id': 14907, 'abstractText': 'Despite having many similar characteristics with cryptography, existing chaotic systems have many security issues that negatively affect the chaos-based cryptographic algorithms that utilize them. This paper proposes a new chaotification method that enhances the chaotic complexity of existing chaotic maps to surmount these issues. The proposed method uses a cosine function alongside a chaotic map in a cascade system. To depict its advantages, we apply it to enhance logistic and Henon maps before analyzing their chaotic properties. Results and comparisons indicate that the new chaotic maps have a wider chaotic range, elevated sensitivity, complex characteristics, high nonlinearity, and an extended cycle length as compared to the original (seed) maps as well as other chaotic maps. We then utilize the modified maps (and their corresponding seed maps) to design simple pseudorandom number generators to study their feasibility when used in cryptographic algorithms. We perform comparisons between the generators derived from both the original and seed maps. Results show that generators based on the new maps outperform their seed counterparts in nearly every aspect. This finding demonstrates the capability of the proposed method in improving the performance of chaos-based cryptographic algorithms.', 'title': 'Digital Cosine Chaotic Map for Cryptographic Applications', 'embedding': []}, {'id': 14908, 'abstractText': 'Map images (e.g., illustrated maps, historical maps, and geographic maps) have been published around the world, not only for giving location but also to attract tourists or hand down the histories of locations. The management of map data, however, has been an open issue for several research fields, including digital library, humanities, and tourism studies. This paper explores an approach for classifying diverse map images by their themes using map content features. Specifically, we present a novel strategy for preprocessing text data that are positioned inside the map images, which are extracted using OCR. The activation of the textual feature-based model is joint with the visual features in an early fusion manner. Finally, we train a classifier model comprising a convolutional layer and a fully connected layer, which predicts the belonging class of the input map. In experiments conducted on a new labeled dataset of map images, we demonstrate that our approach that uses the fused features achieved the best classification performance over single modality. We have made our dataset available on the Internet to facilitate this new task.', 'title': 'A Deep Multimodal Approach for Map Image Classification', 'embedding': []}, {'id': 14909, 'abstractText': \"Opportunistic signals (e.g., WiFi, magnetic fields, and ambient light) have been extensively studied for low-cost indoor localization, especially via fingerprinting. We present an automatic site survey approach to build the signal maps in space-constrained environments (e.g., modern office buildings). The survey can be completed by a single smartphone user during normal walking, say, with a little human intervention. Our approach follows the classical GraphSLAM framework: the front end constructs a pose graph by incorporating the relative motion constraints from the pedestrian dead-reckoning (PDR), the loop-closure constraints by magnetic sequence matching with the WiFi signal similarity validation, and the global heading constraints from the opportunistic magnetic heading measurements; and the back end generates a globally consistent trajectory via graph optimization to provide ground-truth locations for the collected signal fingerprints along the survey path. We then build the signal map (also known as fingerprint database) upon these location-labeled fingerprints by the Gaussian processes regression (GPR) for later online localization. Specifically, we exploit the pseudowall constraints from the GPR variance map of magnetic fields and the observations of ceiling lights to correct the PDR drifts with a particle filter. We evaluate our approach on several data sets collected from both the HKUST academic building and a shopping mall. We demonstrate the real-time localization on a smartphone in an office area, with 50th percentile accuracy of 2.30 m and 90th percentile accuracy of 3.41 m. Note to Practitioners - This paper was motivated by the problem of the efficient signal map construction for fingerprinting-based localization on smartphones. The conventional manual site survey method, known to be time-consuming and labor-intensive, hinders the penetration of fingerprinting methods in practice. This paper suggests a GraphSLAM-based approach to automate this signal map construction process by reducing the survey overhead significantly. A surveyor is merely asked to walk through an indoor venue with an Android smartphone held in hand with a little human intervention. Meanwhile, opportunistic signals (e.g., WiFi and magnetic fields) are captured by smartphone sensors. We construct a GraphSLAM engine to first identify the measurement constraints from these signal observations and then recover the surveyor's walking trajectory by the graph optimization. We can generate signal maps using the captured signals alongside the recovered trajectory. In this paper, we propose a WiFi signal similarity validation method to reduce false positive loop-closures and exploit the magnetic headings to improve the trajectory optimization performance. In addition, we propose to use the generated magnetic field variance map and the lights distribution map for localization. The efficacy of the proposed site survey approach is proven through field experiments, and real-time localization is demonstrated on a smartphone using the generated signal maps. The localization experiment was conducted by a single user with the same Android smartphone that was used in the site survey. Therefore, the usability of signal maps on other devices and the generality to other users have not yet been testified. We will leave these issues in our future work.\", 'title': 'An Automatic Site Survey Approach for Indoor Localization Using a Smartphone', 'embedding': []}, {'id': 14910, 'abstractText': 'Agriculture is one of the most affected sectors by the flood. Spaceborn remote sensing is widely used for flood mapping and monitoring in recent decades. Some applications such as flood crop loss assessment require data with fine temporal resolution to monitor short-lived flood. MODIS is providing remote sensing data with 1-2 days temporal resolution which has frequently been used for flood mapping for a large area. However, incapability to penetrate through the cloud hindered the application of optical remote sensing in flood mapping in many cases. Thus, radar remote sensing especially synthetic aperture radar (SAR) already shows the capability for the flood mapping in cloud condition. However, monitoring of short-lived flood is not possible using freely available SAR data because of the long revisit capacity of these SAR systems. Therefore, microwave remote sensing with fine temporal resolution might be helpful for flood inundation mapping. Soil Moisture Active Passive (SMAP) is a microwave remote sensing initiative which is providing 3-hourly soil moisture data. Therefore, this study tries to map agriculture flood based on SMAP soil moisture data and soil physical properties. Soil moisture above the filed capacity might be the indication of soil inundation. Moreover, It has been observed that there is an increment in soil moisture during the flood. Therefore, this approach considered three conditions to map the flooded pixel: a minimum of 0.05 increment in soil moisture, a soil moisture threshold 0.40 (moisture above the field capacity) and the 72 consecutive hours. To avoid the random increment in soil moisture a 3-day moving window is applied to the time series data. The flood map extracted from SMAP data is validated with FEMA declared inundated crop land. The overall accuracy is 60% and about 32% of commission error. The over estimation of the flood by SMAP data due to the coarse spatial resolution (9km) of SMAP data.', 'title': 'Agriculture flood mapping with Soil Moisture Active Passive (SMAP) data: A case of 2016 Louisiana flood', 'embedding': []}, {'id': 14911, 'abstractText': 'Automotive players have recently shown an increasing interest in high-precision mapping, with the aim of enhancing vehicles safety and autonomy. Nevertheless, the acquisiton, processing and updates of accurate maps remains an economic challenge. Collaborative mapping through vehicles crowdsourcing represents a promising solution to tackle this problem. However, the potential accuracy provided by such an approach has yet to be assessed. In this paper, we evaluate the performances of crowdsourced mapping in real conditions. We build a map of geolocalized landmarks by crowdsourcing observations from multiple vehicles, and applying several successive map updates. We evaluate the map quality through a field-test with multiple vehicle passings. Furthermore, we study the benefits of crowdsourced mapping for vehicles positioning.', 'title': 'Graph-based Approach for Crowdsourced Mapping: Evaluation through Field Experiments', 'embedding': []}, {'id': 14912, 'abstractText': \"Many pervasive computing applications depend upon maps for navigation and support of location based services. Maps are commonly available for outdoor pervasive applications from a variety of sources. An individual can determine their location outdoors on these maps via GPS. Indoor pervasive applications may also need to know the layout of rooms, doorways and hallways of buildings, and the objects and obstacles within them, however indoor maps of buildings are less prevalent. Moreover, indoor maps may need to be dynamic and updated regularly since the layout changes when objects and obstacles are added or removed by people within the building. In this paper, we present iFrame, a dynamic approach that leverages existing mobile sensing capabilities for constructing indoor floor plans. We explore how iFrame users may collaborate and contribute to constructing 2-dimensional indoor maps by merely carrying smartphones or other mobile devices, and to allow their mobile devices to share information with other users' devices. The iFrame approach consists of four steps: 1) Abstract the unknown indoor map as a matrix; 2) Leverage collaborating mobile devices that incorporate three mobile sensing technologies - accelerometers to support dead reckoning, Bluetooth RSSI detection, and WiFi RSSI detection; 3) Combine the three methods by Curve Fit Fusion (CFF), and 4) Extend iFrame from one room to a whole building by shadow rates and anchor points analysis. We conducted a deployment study that shows iFrame is a light-weight and unattended approach that provides a skeleton map of a real building effectively and automatically. The layouts of 12 rooms are reconstructed within 5-10 minutes. Changes of layout in indoor maps can be detected and the resolution of the reconstructed indoor floor plans can be improved when there is an increase in the number of cooperating users.\", 'title': 'iFrame: Dynamic indoor map construction through automatic mobile sensing', 'embedding': []}, {'id': 14913, 'abstractText': \"Massive maps have been shared as Web Map Service (WMS) from various providers, which could be used to facilitate people's daily lives and support space analysis and management. The theme classification of maps could help users efficiently find maps and support theme-related applications. Traditionally, metadata is usually used in analyzing maps content, few papers use maps, especially legends. In fact, people usually considers metadata, maps and legends together to understand what maps tell, however, no study has tried to exploit how to combine them. This paper proposes a method to fuse them with the purpose of classifying map themes, named latent feature based multimodality fusion for theme classification (LFMF-TC). Firstly, a multimodal dataset is created that supports the supervised classification on map themes. Secondly, textual and visual features are designed for metadata, maps, and legends using some advanced techniques. Thirdly, a latent feature based fusion method is proposed to fuse the multimodal features on the feature level. Finally, a neural network classifier is implemented using supervised learning on the multimodal dataset. In addition, a web-based collaboration platform is developed to facilitate users in labeling multimodal samples through an interactive Graphical User Interface (GUI). Extensive experiments are designed and implemented, whose results prove that LFMF-TC could significantly improve the classification accuracy. In theory, the LFMF-TC could be used for other applications with few modifications.\", 'title': 'A Latent Feature-Based Multimodality Fusion Method for Theme Classification on Web Map Service', 'embedding': []}, {'id': 14914, 'abstractText': 'Probing seismic anisotropy of the lithosphere provides valuable clues on the fabric of rocks. We present a 3-D probabilistic model of shear wave velocity and radial anisotropy of the crust and uppermost mantle of Europe, focusing on the mountain belts of the Alps and Apennines. The model is built from Love and Rayleigh dispersion curves in the period range 5–149 s. Data are extracted from seismic ambient noise recorded at 1521 broad-band stations, including the AlpArray network. The dispersion curves are first combined in a linearized least squares inversion to obtain 2-D maps of group velocity at each period. Love and Rayleigh maps are then jointly inverted at depth for shear wave velocity and radial anisotropy using a Bayesian Monte Carlo scheme that accounts for the trade-off between radial anisotropy and horizontal layering. The isotropic part of our model is consistent with previous studies. However, our anisotropy maps differ from previous large scale studies that suggested the presence of significant radial anisotropy everywhere in the European crust and shallow upper mantle. We observe instead that radial anisotropy is mostly localized beneath the Apennines while most of the remaining European crust and shallow upper mantle is isotropic. We attribute this difference to trade-offs between radial anisotropy and thin (hectometric) layering in previous studies based on least-squares inversions and long period data (&gt;30 s). In contrast, our approach involves a massive data set of short period measurements and a Bayesian inversion that accounts for thin layering. The positive radial anisotropy (V<inf>SH</inf> &gt; V<inf>SV</inf>) observed in the lower crust of the Apennines cannot result from thin layering. We rather attribute it to ductile horizontal flow in response to the recent and present-day extension in the region.', 'title': 'Evidence for radial anisotropy in the lower crust of the Apennines from Bayesian ambient noise tomography in Europe', 'embedding': []}, {'id': 14915, 'abstractText': 'In this study, we conducted a test with 20 sighted users to track the work described in earlier studies in an expanded scenario. As we have tried to answer whether adding multi-reference mapping of sonification to auditory graphs could improve the point estimation accuracy in nonvisual condition. We also emphasize the efficiency of the performance of multi-reference graphs using timbre to make them as efficient as mapping using a single pitch. The design of the study was different from the earlier study, who found that multi-reference mapping took a long time against single pitch mapping in auditory graphs. The results help provide empirical evidence that the multi-reference mode provides more accurate results than the single-pitch mode. The evaluation confirms that adding contexts to auditory graphs such as markers could improve the perception of auditory graphs.', 'title': 'Investigation of Multi Reference Point Estimation with Timbre for Effective Mobile Auditory Devices', 'embedding': []}, {'id': 14916, 'abstractText': 'We present an ALMA study of the ∼180 brightest sources in the SCUBA-2 850-μm map of the COSMOS field from the S2COSMOS survey, as a pilot study for AS2COSMOS – a full survey of the ∼1000 sources in this field. In this pilot study, we have obtained 870-μm continuum maps of an essentially complete sample of the brightest 182 sub-millimetre sources (<tex>$S_{850\\\\, \\\\mu \\\\rm m}\\\\gt $</tex> 6.2 mJy) in COSMOS. Our ALMA maps detect 260 sub-millimetre galaxies (SMGs) spanning a range in flux density of <tex>$S_{870\\\\, \\\\mu \\\\rm m}$</tex> = 0.7–19.2 mJy. We detect more than one SMG counterpart in 34 ± 2 per cent of sub-millimetre sources, increasing to 53 ± 8 per cent for SCUBA-2 sources brighter than <tex>$S_{850\\\\, \\\\mu \\\\rm m}\\\\gt $</tex> 12 mJy. We estimate that approximately one-third of these SMG–SMG pairs are physically associated (with a higher rate for the brighter secondary SMGs, <tex>$S_{870\\\\, \\\\mu \\\\rm m}\\\\gtrsim$</tex> 3 mJy), and illustrate this with the serendipitous detection of bright [C II] 157.74-μm line emission in two SMGs, AS2COS 0001.1 and 0001.2 at z = 4.63, associated with the highest significance single-dish source. Using our source catalogue, we construct the interferometric 870-μm number counts at <tex>$S_{870\\\\, \\\\mu \\\\rm m}\\\\gt $</tex> 6.2 mJy. We use the extensive archival data of this field to construct the multiwavelength spectral energy distribution of each AS2COSMOS SMG, and subsequently model this emission with MAGPHYS to estimate their photometric redshifts. We find a median photometric redshift for the <tex>$S_{870\\\\, \\\\mu \\\\rm m}\\\\gt $</tex> 6.2 mJy AS2COSMOS sample of z = 2.87 ± 0.08, and clear evidence for an increase in the median redshift with 870-μm flux density suggesting strong evolution in the bright end of the 870-μm luminosity function.', 'title': 'An ALMA survey of the brightest sub-millimetre sources in the SCUBA-2–COSMOS field', 'embedding': []}, {'id': 14917, 'abstractText': 'Crowdsourcing is a multidisciplinary research area that represents a rapidly expanding field where new applications are constantly emerging. Research in this area has investigated its use for citizen science in data gathering for research and crowdsourcing for industrial innovation. Previous studies have reviewed and categorized crowdsourcing research using qualitative methods. This has led to the limited coverage of the entire field, using smaller discrete parts of the literature and mostly reviewing the industrial aspects of crowdsourcing. This study uses a scientometric analysis of 7059 publications over the period 2006–2019 to map crowdsourcing research to identify clusters and applications. Our results are the first in the literature to map crowdsourcing research holistically. In this article, we classify its usage in the three domains of innovation, engineering, and science, where 11 categories and 26 subcategories are further developed. The results of this article reveal that the most active scientific clusters where crowdsourcing is used are environmental sciences and ecology. For the engineering domain, it is computer science, telecommunication, and operations research. In innovation, idea crowdsourcing, crowdfunding, and crowd creation are the most frequent areas. The findings of this study map crowdsourcing usage across different fields and illustrate emerging crowdsourcing applications.', 'title': 'A Scientometric Exploration of Crowdsourcing: Research Clusters and Applications', 'embedding': []}, {'id': 14918, 'abstractText': \"Apollo basin is located within the large South Pole- Aitken (SPA) basin. The study on Apollo basin will provide interesting information about the basic geologic issues about the lunar farside. In this paper, the normalized brightness (TB) temperature (nT<sub>B</sub>) maps and the TB difference (dT<sub>B</sub>) maps are generated with the Chang'E-2 microwave sounder data to study the microwave thermal emission features of Apollo basin. The results are as follows. First, the mare volcanism in Apollo basin is re-understood according to the nT<sub>B</sub> performances at noon, and they should be originated from the southern part of the Apollo basin and strongly altered by the later impact ejecta. Second, the nT<sub>B</sub> maps indicate that there exists a special material from Dryden crater to Chaffee crater, whose thickness is more than 31 cm but less than 76.9 cm. Third, the similar dT<sub>B</sub> performances at 3.0 GHz indicate the homogeneous regolith thermophysical parameters of Apollo basin in the lateral direction. Fourth, the dTB maps and the discovered cold TB anomaly indicate the homogeneity of the SPA basin at least in the microwave thermophysical parameters. Our study also shows that the scientific study about the lunar surface is not sufficient only by visible data.\", 'title': 'MTE Features of Apollo Basin and Its Significance in Understanding the SPA Basin', 'embedding': []}, {'id': 14919, 'abstractText': 'Cellular automata can be used to rapidly generate complex images, but controlling the character of those images can be difficult. This study continues experimentation with fashion-based cellular automata that generate cavern-like level maps and provides the beginning of a mathematical theory. Fashion-based automata are defined by a competition matrix with different cell states competing to capture territory. This study co-evolves pairs of competition matrices to permit the evolution of automata rules that can be spatially morphed to provide substantially more diverse types of maps than earlier systems using fashion-based cellular automata. As in earlier studies, the cellular automata rules function in local neighborhoods, meaning that the level generation system scales smoothly to any desired level map size. This reusability also permits variation of the type of morph used: a variety of spatial morphing styles are tested with the evolved rules. The theoretical treatment includes the derivation of a normal form for the cellular automata rules that informs the design of the fitness function and has application to understanding the fitness landscape of fashion based automata.', 'title': 'Automatic Generation of Diverse Cavern Maps with Morphing Cellular Automata', 'embedding': []}, {'id': 14920, 'abstractText': \"Volunteered Geographic Information (VGI) is a kind of geographic information which is created, edited, managed and maintained on the basis of common handheld GPS terminals, high resolution remote sensing images and personal spatial cognition. VGI has received extensive attention from scholars at home and abroad for its advantages of wide data coverage, strong current situation, and free use. OpenStreetMap (OSM) is simple in its way of generating and uploading data, and it has a large amount of storage data, which has become a widely used case model in VGI. Since there is no effective classification and management of data, how to extract valuable information from OSM data effectively becomes a bottleneck restricting the application of OSM data. Thematic maps of agricultural information can visually reflect the current status of agriculture in a region and have important reference value for government departments in formulating agricultural policies and economic plans. At present, thematic map production of agricultural information is mainly completed using satellite remote sensing and ground surveys, which requires higher costs. But agriculture is a low-value-added industry. How to reduce the production cost of agricultural thematic maps is an issue that needs urgent solution. In this study, Hapcheon County in South Korea is selected as the research area. First, study the threshold setting for the area of blocks, the area of buildings in the blocks, the density of road lines, the density of cores, and the density of road nodes, etc., and use the recent historical OSM data of the study area to extract the area of cropland; Secondly, the farmland area extracted from the OSM data was compared with the cultivated land area of the combined Unmanned Aerial Vehicle (UAV) and RapidEye image data to determine the optimal threshold; Finally, the farmland information based on the optimal threshold value and OSM's own agricultural geographic information overlay are integrated to realize the agricultural geographic information thematic map production based on OSM data. The research results show that setting thresholds for various impact factors, such as block size, can improve the extraction accuracy of farmland area. Based on the information of cultivated land area extracted by remote sensing image of UAV, the information of farmland based on OSM data has good accuracy (The error is about 15%). The study can achieve rapid acquisition of geographical information of farmland at various regional scales and provide data support for dynamic monitoring of farmland area and agricultural production management.\", 'title': 'Extraction of Farmland Geographic Information Using OpenStreetMap Data', 'embedding': []}, {'id': 14921, 'abstractText': 'In this study, it is aimed to monitor and control the plant production in greenhouses, and increase the efficiency of greenhouses by doing interdisciplinary research in the fields of agriculture, electronics, robotics, and data mining. In the small and outnumbered greenhouses used by farmers, to collect data using static sensor systems is both costly and impractical. In this study, an autonomous mobile robot has been developed and used to collect information from the greenhouse and mapping the greenhouse. By using the autonomous mobile robot, that is designed and produced for the project needs, sensory data such as, RGB-D map of the greenhouse, moisture, temperature, and light, were obtained. As well as the mobile robot could autonomously navigate in the greenhouse, it can be manually controlled by an operator. On the robot, robot operating system (ROS) is used. By using RGB-D mapping and SLAM packages in the ROS, robot can find its position in the greenhouse can measure the temperature, the moisture, and the light density of this location, and generates a three dimensional map of the greenhouse. In this study, data about how it changed the greenhouse environment and the plants grown in the greenhouse was obtained with measurements made at regular intervals of time. The next step of this study is process the data gathered by the robot in real time to get information such as the number of the crops, the phonological phase of the crops or condition of the greenhouse.', 'title': 'Data acquisition from greenhouses by using autonomous mobile robot', 'embedding': []}, {'id': 14922, 'abstractText': 'Different communication-aware mapping techniques were proposed in recent years for improving the performance of distributed systems based on both, off-chip and on-chip networks. Some of these proposals were based on heuristic search for finding pseudo-optimal assignments of tasks and processing elements. However, the technology integration improvements have allowed a significant increase in the number of network nodes, requiring the acceleration of the heuristic search. In this paper, we propose a comparative study of the local search method used in a communication-aware mapping technique, when implemented on different parallel architectures. We compare the performance provided by a version of the local search method when executed on a single Graphics Processing Unit (GPU) with the one provided by the MPI version executed on a supercomputer with the same theoretical performance of the GPU platform, in order to study a fair scenario. We have considered a GPU based on the Fermi architecture, evaluating the improvements achieved by some new architectural features of this platform. The results show that a mixed parallel implementation on a single GPU outperforms the MPI implementation of the local search method. These results validate the GPU implementation as a very cost-effective accelerator for the local search method.', 'title': 'On the Use of GPU for Accelerating Communication-Aware Mapping Techniques', 'embedding': []}, {'id': 14923, 'abstractText': 'Many resource allocation problems in the cloud can be described as a basic Virtual Network Embedding Problem (VNEP): the problem of finding a mapping of a request graph (describing a workload) onto a substrate graph (describing the physical infrastructure). Applications range from mapping testbeds, over the embedding of batch-processing tasks to the embedding of service function chains and come with different mapping restrictions for nodes and edges. The restrictions studied most often are node and edge capacities, node mapping, edge routing and latency restrictions. While the VNEP has been studied intensively, complexity results are only known for specific models and this paper provides a first comprehensive study of the computational complexity of the VNEP by systematically analyzing its hardness for any combination of the above stated mapping restrictions. For all studied variants the NP-completeness of the respective decision problems is shown. Furthermore, NP-completeness results for finding approximate embeddings, which may, e.g., violate capacity constraints by certain factors, are derived. Lastly, it is also shown that all these results pertain when restricting the request graphs to planar and degree-bounded graphs. While theoretic in nature, our results have severe practical implications. Firstly, any optimization variant of the VNEP is NP-hard and cannot be approximated for any of the studied restrictions, unless P = NP. Secondly, we uncover structural hardness properties: the VNEP is NP-hard and inapproximable even if, e.g., only node placement and edge routing restrictions are considered.', 'title': 'On the Hardness and Inapproximability of Virtual Network Embeddings', 'embedding': []}, {'id': 14924, 'abstractText': 'Context: Security bug reports are reports from bug tracking systems that include descriptions and resolutions of security vulnerabilities that occur in software projects. Researchers use security bug reports to conduct research related to software vulnerabilities. A mapping study of publications that use security bug reports can inform researchers on (i) the research topics that have been investigated, and (ii) potential research avenues in the field of software vulnerabilities. Objective: The objective of this paper is to help researchers identify research gaps related to software vulnerabilities by conducting a systematic mapping study of research publications that use security bug reports. Method: We perform a systematic mapping study of research that use security bug reports for software vulnerability research by searching five scholar databases: (i) IEEE Xplore, (ii) ACM Digital Library, (iii) ScienceDirect, (iv)Wiley Online Library, and (v) Springer Link. From the five scholar databases, we select 46 publications that use security bug reports by systematically applying inclusion and exclusion criteria. Using qualitative analysis, we identify research topics investigated in our collected set of publications. Results: We identify three research topics that are investigated in our set of 46 publications. The three topics are: (i) vulnerability classification; (ii) vulnerability report summarization; and (iii) vulnerability dataset construction. Of the studied 46 publications, 42 publications focus on vulnerability classification. Conclusion: Findings from our mapping study can be leveraged to identify research opportunities in the domains of software vulnerability classification and automated vulnerability repair techniques.', 'title': 'Security Bug Report Usage for Software Vulnerability Research: A Systematic Mapping Study', 'embedding': []}, {'id': 14925, 'abstractText': 'Vegetation phenology identification is significance to the exploration of vegetation growth and is also conducive to the impact of phenology on the ecological environment. Recently, vegetation phenology detection is based on a time series of vegetation phenology to index simulation of vegetation growth time indirectly. In this study, we identify the vegetation phenology of deciduous broad-leaved forest through the deep learning method within a single PhenoCam image. The result of the phenology identification of growing regions, the accuracy MAP of daily identification in daily scales mAP up to 10.2%, which could identify the growing period of most deciduous broad-leaved forests. The identification accuracy mAP in the 8-day scale is up to 69%, and the identification mAP accuracy of vegetation could reach 98.2% when it was divided into four categories. The purpose of this study is to detect the phenological growth period of deciduous broad- leaved forest with rapid development, high precision, and fast deep learning methods. It has a great improvement on the current method of calculating the vegetation phenology period by using the traditional measurement and related mathematical and physical models. While obtaining the phenology period more quickly, it can automatically and accurately obtain the growth area and growth period of the study area, making a certain contribution to the study of vegetation phenology.', 'title': 'Vegetation phenology detection of deciduous broad-leaf forest using YOLOv3 from PhenoCam', 'embedding': []}, {'id': 14926, 'abstractText': 'Over the last two decades, high-resolution (HR) mapping has emerged as a powerful technique to study normal and abnormal bioelectrical events in the gastrointestinal (GI) tract. This technique, adapted from cardiology, involves the use of dense arrays of electrodes to track bioelectrical sequences in fine spatiotemporal detail. HR mapping has now been applied in many significant GI experimental studies informing and clarifying both normal physiology and arrhythmic behaviors in disease states. This review provides a comprehensive and critical analysis of current methodologies for HR electrical mapping in the GI tract, including extracellular measurement principles, electrode design and mapping devices, signal processing and visualization techniques, and translational research strategies. The scope of the review encompasses the broad application of GI HR methods from in vitro tissue studies to in vivo experimental studies, including in humans. Controversies and future directions for GI mapping methodologies are addressed, including emerging opportunities to better inform diagnostics and care in patients with functional gut disorders of diverse etiologies.', 'title': 'Methods for High-Resolution Electrical Mapping in the Gastrointestinal Tract', 'embedding': []}, {'id': 14927, 'abstractText': 'EEG signal classification is an important task to build an accurate Brain Computer Interface (BCI) system. Many machine learning and deep learning approaches have been used to classify EEG signals. Besides, many studies have involved the time and frequency domain features to classify EEG signals. On the other hand, a very limited number of studies combine the spatial and temporal dimensions of the EEG signal. Brain dynamics are very complex across different mental tasks, thus it is difficult to design efficient algorithms with features based on prior knowledge. Therefore, in this study, we utilized the 2D AlexNet Convolutional Neural Network (CNN) to learn EEG features across different mental tasks without prior knowledge. First, this study adds spatial and temporal dimensions of EEG signals to a 2D EEG topographic map. Second, topographic maps at different time indices were cascaded to populate a 2D image for a given time window. Finally, the topographic maps enabled the AlexNet to learn features from the spatial and temporal dimensions of the brain signals. The classification performance was obtained by the proposed method on a multiclass dataset from BCI Competition IV dataset 2a. The proposed system obtained an average classification accuracy of 81.09%, outperforming the previous state-of-the-art methods by a margin of 4% for the same dataset. The results showed that converting the EEG classification problem from a (1D) time series to a (2D) image classification problem improves the classification accuracy for BCI systems. Also, our EEG topographic maps enabled CNN to learn subtle features from spatial and temporal dimensions, which better represent mental tasks than individual time or frequency domain features.', 'title': 'EEG Signal Classification Using Convolutional Neural Networks on Combined Spatial and Temporal Dimensions for BCI Systems', 'embedding': []}, {'id': 14928, 'abstractText': 'Mapping urban dynamics at the global scale becomes a pressing task with the increasing pace of urbanization and its important environmental and ecological impacts. In this study, we proposed a new approach to mapping global urban areas from 2000 to 2012 by applying a region-growing support vector machine classifier and a bidirectional Markov random field model to time-series nighttime light data. In this approach, both spectrum and spatial-temporal contextual information are employed for an improved urban area mapping. Our results indicate that at the global level, the urban area increased from 625,000 to 1,039,000 km<sup>2</sup> during 2000-2012. Most urban areas are concentrated in the region between 30°N and 60°N latitudes. The latitudinal distribution of urban areas from this study is consistent with three land-cover products, including European Space Agency Climate Change Initiative Land Cover dataset, Finer Resolution Observation and Monitoring Global Land Cover, and 30-m Global Land Cover dataset. We found that for several major cities, such as Shanghai, urban areas from our study contain some nonurban land-cover types with intensive human activities. The validation using Landsat 7 ETM+ imagery indicates that the overall accuracies of the mapped urban areas for 2000, 2005, 2008, and 2010 are 86.0%, 88.6%, 89.8%, and 88.7%, respectively, and the Kappa coefficients are 0.72, 0.77, 0.79, and 0.78, respectively. This study also demonstrates that the integration of the spatial-temporal contextual information and the use of bidirectional Markov random field model are effective in improving the accuracy and temporal consistency of urban area mapping using time-series nighttime light data.', 'title': 'Mapping Global Urban Areas From 2000 to 2012 Using Time-Series Nighttime Light Data and MODIS Products', 'embedding': []}, {'id': 14929, 'abstractText': 'Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey 88 ML4VIS studies, aiming to answer two motivating questions:what visualization processes can be assisted by MLandhow ML techniques can be used to solve visualization problemsThis survey reveals seven main processes where the employment of ML techniques can benefit visualizations:Data Processing4VIS, Data-VIS Mapping, InsightCommunication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling. The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations.Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this paper can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at https://ml4vis.github.io', 'title': 'A Survey on ML4VIS: Applying MachineLearning Advances to Data Visualization', 'embedding': []}, {'id': 14930, 'abstractText': 'The purpose of this study is to develop a PET detector with a 0.5 mm crystal size for high-resolution preclinical PET imaging. The methods to improve the decoding performances of the detectors, including the wedge-shaped light guides with different thicknesses, different methods to suppress the dark count noises in the SiPMs and different methods to calculate the flood maps, were investigated. The experimental results show that the PET detector developed in this study achieved an extraordinary decoding performance. The ratio of background “noise” in the flood map to the total events is about 40%. The simulation study shows that the intrinsic intra-crystal Compton scatters contribute to 30.32% of the background “noise”. Thus, the optical design of the detector and the electronics and algorithms to generate the flood maps only introduced 10% the background “noise”. We are currently constructing a high-resolution preclinical PET imager using 12 detectors.', 'title': 'Preliminary Optimized Design of a High-resolution PET Detector with a 0.5 mm Crystal Size', 'embedding': []}, {'id': 14931, 'abstractText': 'Abstract Password schemes based on online map locations are an emerging topic in authentication research. GeoPass is a promising such scheme, as it provides satisfactory resilience against online guessing and showed high memorability (97%) in a single-password laboratory study. In this article, we investigate more deeply into the potential of GeoPass through four separate studies. First, in a 2-month-long field study, we found that users in a real-world setting remembered their location passwords 96.1% of the time and showed improvement with more login sessions. Then, in a study of interference effects in Geopass, in which each participant had to remember four separate location passwords, we found that memorability was &lt;70%, with 41.5% of login failures due to interference. Based on these findings, we propose to address interference issues in GeoPass with mental stories, where users are asked to create a meaningful association between their location password and the corresponding account. We tested the efficacy of this approach through a second interference study, where the memorability rate for GeoPass was &gt;97%, with only 3.4% of login attempts failing due to interference. We also conducted a shoulder-surfing study to examine the resilience of GeoPass against this attack. Based on our results, we identify the promising aspects of location passwords that should be further studied in future research.', 'title': 'Exploring the Potential of GeoPass: A Geographic Location-Password Scheme', 'embedding': []}, {'id': 14932, 'abstractText': 'Understanding the current situation of natural disaster damages is a critical step for an effective natural disaster responses, and many pictures uploaded after natural disasters are valuable resources for this purpose. However, many pictures are not associated with location information and it is not easy to connect the image content to locations on a map. There are two reasons for this difficulty. First, pictures are different in terms of directions and heights. Second, the situation of damaged areas in the picture may differ from before the natural disaster. Therefore, there is a mismatch between map fragments and the pictures taken after a disaster. This paper explores the potential of human computation to solve this problem. For this study, we asked people to draw a birds-eye view map of the place in a picture and compared the map with the correct map fragments to see the characteristics of such drawn maps.', 'title': 'Analysis of Hand-drawn Maps of Places in Natural Disaster Pictures', 'embedding': []}, {'id': 14933, 'abstractText': 'This study aims to improve users’ positive experiences in wayfinding in virtual environments through empirical research on the influence of different designs of landmarks on overview maps. The experiment adopted a four (landmark) x two (gender) between-subjects design. Landmarks with symbols, symbols and images, symbols and text, and symbols, images and text were examined. Fifty-six participants were invited to complete three wayfinding tasks and fill out questionnaires. The generated results indicated that: (1) Landmark presentation styles significantly affect wayfinding performance on overview maps in virtual environments. (2) Concerning subjective rationality, system usability and perceived usefulness, the use of text in landmark design can significantly improve users’ evaluations of overview maps. (3) In terms of gender, females’ system usability evaluations and subjective intentions of using overview map with landmarks are significantly more positive than males’.', 'title': 'Wayfinding in Virtual Environments With Landmarks on Overview Maps', 'embedding': []}, {'id': 14934, 'abstractText': 'Autonomous robots are increasingly used in many fields to perform some special tasks. Autonomous transfer vehicles (ATV) in smart factories are one of the most critical components of industry 4.0. Mapping is the key process for the long term successful operation of ATVs. Although, robotic mapping is enough for the autonomous navigation of an ATV, high definition (HD) maps are required to perform the perfect behavior and ensure better interactions with the environment. HD maps can be created manually, but updating the maps is an open problem for sustainability of the performance. In this study, an HD-map update strategy is proposed for ATVs that operate in smart factories.', 'title': 'Updating HD-Maps for Autonomous Transfer Vehicles in Smart Factories', 'embedding': []}, {'id': 14935, 'abstractText': 'Car counting on drone-based images is a challenging task in computer vision. Most advanced methods for counting are based on density maps. Usually, density maps are first generated by convolving ground truth point maps with a Gaussian kernel for later model learning (generation). Then, the counting network learns to predict density maps from input images (estimation). Most studies focus on the estimation problem while overlooking the generation problem. In this paper, a training framework is proposed to generate density maps by learning and train generation and estimation subnetworks jointly. Experiments demonstrate that our method outperforms other density map-based methods and shows the best performance on drone-based car counting.', 'title': 'Drone-Based Car Counting via Density Map Learning', 'embedding': []}, {'id': 14936, 'abstractText': 'This paper addresses a vehicle localization method that fuses aerial maps and lidar data in urban canyon environments where global positioning system (GPS) signals are inaccurate. The boundaries of buildings are extracted from the aerial map and they are matched to point cloud data provided by the lidar. However, most aerial maps contain perspective projection distortions which can be significant in urban canyons with tall buildings. In this study, a new method to correct such projection distortion is proposed and it is applied to precise localization by fusing the corrected map and lidar data. In order to achieve this, the semantic segmentation of an aerial image is performed using a convolutional neural network, and the mutual information between the lidar measurements and the building boundaries is obtained to measure their similarity. A particle filter framework is employed to localize the vehicle and match the map using the mutual information as the weight of a particle. An experimental dataset is then used to validate the feasibility of the proposed method.', 'title': 'Fusing Lidar Data and Aerial Imagery with Perspective Correction for Precise Localization in Urban Canyons', 'embedding': []}, {'id': 14937, 'abstractText': 'Coral reefs are referred to as \"tropical rainforests of the sea\" to provide for richness of biological diversity. However, coral reefs are seriously degrading for global warming effects. Mapping coral reefs is important to spatially interpret the current coral distribution. Now, coral reefs mapping is generally created by visual interpretation with aerial photos in Japan. For effectively mapping coral reefs cover, the study is to understand availability if satellite-based remote sensing technique is able to be substituted in the near future. The paper introduces how to map coral reefs distribution by aerial photo-based visual interpretation and satellite-based analytical process. The result by satellite data is evaluated with the ground truth data and the visible interpretation method. In the error matrix utilizing 73 ground survey points, the overall accuracy was achieved 78%, which indicated similar trend with the ground truth. In error matrix with rasterized visible interpretation map, the overall accuracy was 55%. The reason is why \"less than 5% Coral cover on Sand and Rock bottom\" and \"5-50% Coral cover\" were caused misclassification in the offshore. However, the inshore area in the analytical result is able to detail coral structure, and the satellite image was able to penetrate deeper by the water column correction. Therefore, satellite-based remote sensing is appreciable to apply for the coral reefs mapping. In the future, we need to consider the classification class types and pro-processing to achieve higher accuracy.', 'title': 'Evaluation of Coral Reefs Mapping in Kerama Islands by Satellite-Based Classification', 'embedding': []}, {'id': 14938, 'abstractText': 'Hydrocarbon contamination introduced during point, line and map analyses in a field emission electron probe microanalysis (FE-EPMA) was investigated to enable reliable quantitative analysis of trace amounts of carbon in steels. The increment of contamination on pure iron in point analysis is proportional to the number of iterations of beam irradiation, but not to the accumulated irradiation time. A combination of a longer dwell time and single measurement with a liquid nitrogen (LN<inf>2</inf>) trap as an anti-contamination device (ACD) is sufficient for a quantitative point analysis. However, in line and map analyses, contamination increases with irradiation time in addition to the number of iterations, even though the LN<inf>2</inf> trap and a plasma cleaner are used as ACDs. Thus, a shorter dwell time and single measurement are preferred for line and map analyses, although it is difficult to eliminate the influence of contamination. While ring-like contamination around the irradiation point grows during electron-beam irradiation, contamination at the irradiation point increases during blanking time after irradiation. This can explain the increment of contamination in iterative point analysis as well as in line and map analyses. Among the ACDs, which are tested in this study, specimen heating at 373 K has a significant contamination inhibition effect. This technique makes it possible to obtain line and map analysis data with minimum influence of contamination. The above-mentioned FE-EPMA data are presented and discussed in terms of the contamination-formation mechanisms and the preferable experimental conditions for the quantification of trace carbon in steels.', 'title': 'Quantitative FE-EPMA measurement of formation and inhibition of carbon contamination on Fe for trace carbon analysis', 'embedding': []}, {'id': 14939, 'abstractText': 'Sea ice lead area fraction, distribution of lead length and orientation of leads are subject of this study. Leads are classified from dual-band Sentinel-1 SAR data with an automatic supervised learning classification algorithm. Binary maps are combined from scenes acquired within a three-day interval to provide an Arctic-wide composite lead map. Resolution of these binary maps is 80 meters. Based on these binary maps, lead area fraction is calculated on a 12 km grid. Regional maps of lead area fraction for the Beaufort Sea and the Fram Strait are calculated on a 4 km grid. The Hough transform is used to detect linear features on binary lead maps. The lead length distribution and the orientation of leads are calculated for the Fram Strait region and the Beaufort Sea. Most of the detected leads have length below 15 km. Two pronounced peaks on the lead orientation are found at around 50 and 130 degrees.', 'title': 'Sea Ice Leads Detected From Sentinel-1 SAR Images', 'embedding': []}, {'id': 14940, 'abstractText': 'We use a convolutional neural network to study cosmic string detection in cosmic microwave background (CMB) flat sky maps with Nambu–Goto strings. On noiseless maps, we can measure string tensions down to order 10<sup>−9</sup>, however when noise is included we are unable to measure string tensions below 10<sup>−7</sup>. Motivated by this impasse, we derive an information theoretic bound on the detection of the cosmic string tension Gμ from CMB maps. In particular, we bound the information entropy of the posterior distribution of Gμ in terms of the resolution, noise level and total survey area of the CMB map. We evaluate these bounds for the ACT, SPT-3G, Simons Observatory, Cosmic Origins Explorer, and CMB-S4 experiments. These bounds cannot be saturated by any method.', 'title': 'Information theoretic bounds on cosmic string detection in CMB maps with noise', 'embedding': []}, {'id': 14941, 'abstractText': 'The integrated crosstalk noise (ICN) has been widely used as an alternative to the insertion crosstalk ratio (ICR) for channel crosstalk evaluation in the IEEE 802.3ba standard. In this work, the differential ICN mitigation scheme by using idea of orthogonality is implemented in two adjacent differential pairs first. In the full pin map area of SerDes channel, new pin map patterns based on such scheme are prosed and compared with the conventional pan map patterns. The new pin maps mitigate the differential ICN drastically, yet maintain the G:s ratio. A preliminary study is conducted on fan-out trace routing to maintain the benefit from new pin map patterns.', 'title': 'Differential Integrated Crosstalk Noise (ICN) Mitigation in the Pin Field Area of SerDes Channel', 'embedding': []}, {'id': 14942, 'abstractText': 'Dot map is one of quantitative cartographic presentation methods used for geovisualization. Traditionally, it was a task of a qualified cartographer to distribute dots in the map. Nowadays, when maps are prepared not only by specialists, it is necessary to develop methods for automatic map production, which will be accessible for a wide range of GIS users. The author presented a new approach to automation of dot maps production. In the study, a building object class from Database of Topographic Objects BDOT10k was combined with statistical data on population and average useful floor area of dwelling in given administrative units. Size and value of dots were calculated with use of hexagonal tessellation. The author also introduced an algorithm for hexagons clusterization with use of dot value. Examples of maps prepared with use of the described method were prepared for a couple of districts of Małopolskie Province.', 'title': 'Automation of Dot Maps Production Supported by BDOT10k Database', 'embedding': []}, {'id': 14943, 'abstractText': 'Designers are increasingly using online resources for inspiration. How to best support design exploration without compromising creativity? We introduce and study Design Maps, a class of point-cloud visualizations that makes large user interface datasets explorable. Design Maps are computed using dimensionality reduction and clustering techniques, which we analyze thoroughly in this paper. We present concepts for integrating Design Maps into design tools, including interactive visualization, local neighborhood exploration and functionality to integrate existing solutions to the design at hand. These concepts were implemented in a wireframing tool for mobile apps, which was evaluated with actual designers performing realistic tasks. Overall, designers find Design Maps supporting their creativity (avg. CSI score of 74/100) and indicate that the maps producing consistent whitespacing within cloud points are the most informative ones.', 'title': 'Interactive Exploration of Large-Scale UI Datasets with Design Maps', 'embedding': []}, {'id': 14944, 'abstractText': 'Philosophical principles are very useful in customization of Linux kernel, e.g., the answer for the question: \"For the pointer to the start address of page table, is it a physical address or a virtual address?\" can be derived by one simple philosophical principle: the depth of recursion is limited. This is because if the pointer were a virtual address, there would be another new page table to store the translation information of this virtual address, but who was responsible for storing the translation information of the start address of this new page table? This would result an infinite recursion. So the pointer definitely is a physical address. In fact, the usefulness of philosophical principles comes from the reduction of searching space. And this reduction is very important in customization of Linux kernel, for it could cut down the size of the new code needed to be read. This is especially valuable when considering that Linux kernel is continuously updating and huge now. Another example to further demonstrate the reduction of searching space in customization is showed in the following: in customization of file system in kernel version 3.10, the question: \"Does the Linux kernel itself maintain the consistency between the buffer cache and the page cache?\". This is a hard problem in practice, for without any guidance of philosophical principle, a developer has to read all of the code in Linux kernel to get a precise answer. The tricky part of this question is that if the developer only read a part of the codes and doesn\\'t find any mechanisms for maintenance of cache consistency, the conclusion of non-existence of such mechanisms still can not be drawn, for there\\'s still a possibility that such mechanisms exist in the codes not explored. Besides, if the developer search internet to find the answer, assume that the developer is lucky enough, he/she finally finds one program example on a web page shows that the inconsistency may raise between buffer cache and page cache. He/she still can not get the conclusion that Linux kernel does not maintain such consistency, because that program example maybe is only valid in a specific scenario, e.g. in kernel version 2.26, not 3.10. But we can get a satisfied answer by using the philosophical principle: the cost of management process should be far less than the value created by being managed process. By this principle, it can be drawn that Linux kernel doesn\\'t maintain the consistency between the buffer cache and page cache in kernel 3.10. This is because that the data in buffer cache and page cache is highly dependent on application logic, so if Linux kernel wanted to maintain such consistency, it would have to track all these applications, which cost was much higher than the benefits that these applications could produce. However, the successful application of philosophical principles depends on two factors: firstly, establishment of a mapping between concepts in Linux system and well-known concepts in human society. This is not a new idea, e.g. the word of \"cost\" is a concept first appeared in human society, not in computer science, but nowadays, developers establish a mapping between this concept and concepts in computer science. Although the idea is very old, it is still very effective. Since well-known concepts in human society are familiar to most developers and are what they have in common, the cost of applying philosophical principles is reduced. Besides this, already existing cause-effect relations among concepts in human society can be highly possible to be reused in philosophical deduction in Linux kernel. E.g, in the mapping we established, process is treated as a human and since in religion of human society, God creates humankind, it is natural to derive that there\\'s one process that creates all other processes in Linux system with high probability. Secondly, a concrete model with many qualitative and quantitative details should be the basis of philosophical deduction. We build such model according to our past experiences and the construction of the model follows the philosophical principle: unfold the complexity only when it is necessary. E.g., in this model, for a specific detail, it is covered only when it is required in practice. This is to lower down the cost of modelling huge and continuously evolving Linux kernel. This model is very important, without it, philosophical deduction is impossible. But it is really a hard work, according to our experiences, it needs at least 6-years of work on Linux kernel for one developer to build it. Although philosophical principles are very useful in practice, there\\'s a big gap on the recognition of philosophical principles between academic researchers and industry practioners. E.g., some academic researcher seriously doubts whether the mapping above, which mentioned God, is helpful. In fact, it is, for by this mapping, a developer will know that the existence of the process, which is the origin of all other processes, is highly possible and also that process maybe is not easily observed. This is true, for that process is the process which PID is zero and that process can not be observed by Linux command: \"ps -e\". That process is a very valuable point of customization, e.g., by modifying that process, all processes in the Linux will be affected. Why does this big gap exist? We believe there\\'re at least three reasons: i. The bias on philosophical principles. This usually comes from the observation that some developers establish wrong mapping between the philosophical principles and the objects in real world. But is that true for those that has been verified many times in practice? ii. Wrong expectations. E.g., hope to get the precise answer when applying philosophical principles, instead of reducing the searching space. iii. Some academic researchers do not realize that a good philosophical principle usually is the result of a deep learning process of many years by human brain. Finally, we suggest that more efforts should be put on the studying of philosophical principles in program understanding and we believe that in the near future, the philosophical principles plus AI will be a trend in program understanding.', 'title': 'Application of Philosophical Principles in Linux Kernel Customization', 'embedding': []}, {'id': 14945, 'abstractText': 'While the accuracy requirement for simulation-based efficiency map becomes high for traction motors that have high efficiency over wide drive range. Even a small improvement in efficiency is critical, the accuracy level of the simulation-based efficiency map has not been well studied.To evaluate the accuracy of a finite element analysis (FEA)based efficiency map, efficiency maps were generated with different loss calculation methods for an interior permanent magnet (IPM) motor. The simulation results were compared with a measurement-based efficiency map.The comparison indicated that FEA can reproduce an efficiency map with an error of less than 1% when losses were calculated taking into account high fidelity factors which are; minor hysteresis loops, AC loss, manufacturing degradations, and stray losses. Also, the cause and effect of the loss generation are analyzed by the observation of the magnetic hysteresis and the loss density distribution in the steel sheet and the windings.', 'title': 'Loss Analysis of a Permanent Magnet Traction Motor in a Finite Element Analysis based Efficiency Map', 'embedding': []}, {'id': 14946, 'abstractText': 'Spectrum maps represent the spatial distribution of signal strength in a particular area and are necessary in spectrum management applications such as frequency reuse and prediction of coverage. This paper studies the problem of spectrum map complementation under the condition of limited observations. First, the relationship among the number of radiation sources, the rank of the spectrum map, and the complement performance is discussed. Then, for the problem of complementation performance of spectrum map being not ideal when there are too many radiation sources, an iterative completion method of spectrum map based on difference of observation values is proposed. This method is used to reconstruct the variation of the frequency spectrum map at the adjacent time. Simulation results show that the proposed method is superior to the direct complement method in accuracy and complement performance.', 'title': 'The iterative completion method of the spectrum map based on the difference of measurement values', 'embedding': []}, {'id': 14947, 'abstractText': 'Recently \"entertainment computing\" (EC) technology becomes a hit term in Japan. There is a well-known \"projection mapping\" in this EC. Projection mapping is a video technique that synchronizes real and video, and the fascinating world view that the fusion of both produces attracts worldwide attention. \"Mapping\" of projection mapping has a meaning of pasting a material called video on the surface layer of the projection target. In mapping, by giving images such as light and shadow while utilizing information such as the design and unevenness of the object, realistic multi-dimensional feeling and space feeling can be expressed. In this study, we prototyped an interactive projection mapping that changes according to user\\'s movement by projecting to the user. This time we focused on sports like baseball and soccer, and we projected the ball to the user by projection mapping so that we can experience pitching and lifting.', 'title': 'A Proposal of Interactive Projection Mapping Using Kinect', 'embedding': []}, {'id': 14948, 'abstractText': 'The likelihood of transitions between pairs of land cover and land use classes in a given time interval and environmental context can be used to impose classification restrictions on an image or to evaluate results. This study presents a methodology for using the likelihood of transitions between classes to improve land cover classification, given a base map (a supposedly accurate map for the same area in another date) and a set of previously classified images. These improved land cover classified images were named conditioned classified images. We aimed to classify one Synthetic Aperture Radar image and an optical one, both from June 2010, using two land cover legends in different level of detail for a region in the Brazilian Amazon. We used both a classified image from 2008 (also in two legends levels) and the data from the Programme for the Estimation of Deforestation in Brazilian Amazon (PRODES) from 2008 as base maps, and presented the likelihood of transitions between the considered classes. The proposed methodology resulted in conditioned classified images with higher Overall Accuracy than the one that does not consider the base maps and the likelihood of transitions. The conditioned classified images presented unlabeled areas due to classification errors in the input data. It is important to highlight that these areas are probably misclassified in maps obtained without using likelihood transition and base maps, since they are impossible to occur in the field.', 'title': 'The use of land cover change likelihood for improving land cover classification', 'embedding': []}, {'id': 14949, 'abstractText': 'Recently, wireless localization has attracted great interest. However, a type of localization problem has not received much attention and has not been well studied, in which all locations of nodes are predetermined, but when nodes are deployed to these positions, the relationship between identifiers and locations of nodes is unknown. Applications with such localization problems include smart lighting, structural health, smart roads and industrial monitoring, etc. In this paper, we investigate how to map node identifiers to known locations. Although the locations of nodes are given, it is still challenging to map the node identifier to known location because the combination of mapping possibilities is huge, and incorrect mapping may occur when several known locations are close to each other. To address this problem, we first propose a localization algorithm, called Q-Hop, in which the distance of two nodes is measured by a fine-grained hop count. Then, we propose a mapping method that maps the node positions estimated by Q-Hop to known locations. The simulation results demonstrate that our solution achieves satisfactory mapping accuracy, which will provide helpful insights for applications with such localization problems.', 'title': 'Wireless Localization of Mapping Node Identifiers to Known Positions', 'embedding': []}, {'id': 14950, 'abstractText': 'In an autonomous scientific exploration system, the terrain map generated from mapping process integrates sensing information from multiple aspects and lays the base for decision making processes. With the increasing challenges in planetary exploration, equipping planetary rovers with the principles of terramechanics is becoming more and more common, especially on rough or intricate terrain. However, it is difficult for conventional maps with elevation information only to reflect terrain mechanical properties, which play important roles in terramechanics-based simulation or motion control. This study extracts the dominant parameters in terrain bearing and shearing models, and presents a multi-layered grid map with fundamental geometric and mechanical elements. A corresponding mapping scheme based on dense visual input is designed to reconstruct elevation in the map and predict terrain mechanical parameters of the entire visual field. Experiments are conducted to verify the practicability of the approach proposed in a Mars emulation yard with a rover prototype.', 'title': 'Mapping for Planetary Rovers from Terramechanics Perspective<sup>*</sup>', 'embedding': []}, {'id': 14951, 'abstractText': 'Modern genomic tree breeding studies and forest health monitoring programs demand accurate mapping of tree phenotype. However, conventional field-based approaches to map phenotype are costly in terms of time and money. The recent phenomenal advances in the low-flying Unmanned Airborne Vehicle (UAV) remote sensing platforms together with the availability of high-resolution hyperspectral cameras allow to periodically capture a huge amount of crown spectral details of individual trees. These details can be exploited to map phenotypic class of tree genotypes. State-of-the-art methods that maps tree phenotype often underexploit the information in hyperspectral time-series data by using only a few specific band-based remote sensing (RS) indices to model phenological tree parameters that define the phenotypic response. Thus, we propose a wavelet-based approach to map tree phenotype of trees that a) maximally exploits the spectral and temporal information in band-groups by addressing data redundancy problem, and b) uses spectro-temporal/phenological information in the hyperspectral time-series data to map phenotype class of tree genotype. The improved performance of the proposed method over a RS index-based state-of-the-art one to map trees to a phenotypic class, on a set of 100 trees from 10 genotypes, proves the method to be performing.', 'title': 'A Band Grouping Based Approach for Phenotype-Class Mapping of Tree Genotypes Using Spectro-Temporal Information in Hyperspectral Time-Series UAV Data', 'embedding': []}, {'id': 14952, 'abstractText': 'We propose a deep-learning approach based on generative adversarial networks (GANs) to reduce noise in weak lensing mass maps under realistic conditions. We apply image-to-image translation using conditional GANs to the mass map obtained from the first-year data of Subaru Hyper Suprime-Cam (HSC) Survey. We train the conditional GANs by using 25\\xa0000 mock HSC catalogues that directly incorporate a variety of observational effects. We study the non-Gaussian information in denoised maps using one-point probability distribution functions (PDFs) and also perform matching analysis for positive peaks and massive clusters. An ensemble learning technique with our GANs is successfully applied to reproduce the PDFs of the lensing convergence. About <tex>$60{{\\\\ \\\\rm per\\\\ cent}}$</tex> of the peaks in the denoised maps with height greater than 5σ have counterparts of massive clusters within a separation of 6 arcmin. We show that PDFs in the denoised maps are not compromised by details of multiplicative biases and photometric redshift distributions, nor by shape measurement errors, and that the PDFs show stronger cosmological dependence compared to the noisy counterpart. We apply our denoising method to a part of the first-year HSC data to show that the observed mass distribution is statistically consistent with the prediction from the standard ΛCDM model.', 'title': 'Noise reduction for weak lensing mass mapping: an application of generative adversarial networks to Subaru Hyper Suprime-Cam first-year data', 'embedding': []}, {'id': 14953, 'abstractText': 'Recent advances in Open Source mapping software are opening up new possibilities for online visualization of satellite data. Bhuvan, Indian Geo-platform of ISRO, hosts multi-temporal, multi-sensor and multi-resolution satellite data over internet and is used for various remote sensing applications. Serving satellite images over internet with good quality and faster response is essential for better user experience and increases the use of satellite data online. Map rendering software is generally used for serving satellite data as web map tiles over internet. This software will process satellite images and generate fixed size tiles based on parameters like zoom level, projection and area of interest. In this study, we explore open source map tile rendering software Mapnik and MapServer. The overall implementation process for hosting satellite data as web map tiles is discussed in detail. Using these two software, satellite data is published with different re-sampling methods as map layers. The published map layers are compared for web tile quality and performance under different load conditions. We also discuss various possible methods for improving the performance of tile creation and rendering.', 'title': 'Study of open source web map rendering software for rendering Bhuvan High Resolution Satellite data', 'embedding': []}, {'id': 14954, 'abstractText': 'Aim: To parallelize scanning and save time and cut costs of preclinical studies we have designed a new hotel holding 4 rats in the HRRT, which has a spatial resolution close to that of preclinical PET scanners. In this work we test the quantitative accuracy on phantoms in the hotel using different attenuation corrections methods on the HRRT. Material and Methods: The rat hotel has 4 compartments made of acrylic plastic with an 8 mm base plate and a 3 mm half-cylinder lid. Four 50 ml syringes filled with [<sup>18</sup>F]-FDG in water were used as phantoms and scanned in the rat hotel for 20 min. on the HRRT and a high statistics speed 10 transmission scan was acquired. Three μ-map processing/reconstruction methods - MAP-TR with either human head (HH) or water phantom (WP) prior and TXTV - were used and μ-maps and PET images reconstructed with each of the 3 μ-maps evaluated. Results: The μ-maps all underestimated the LAC of the acrylic plastic material as compared to CT, and the base plate thickness was underestimated. Activity concentrations were thus also underestimated: -4.6% using HH -8.7% using TXTV and -13.8% with WP. No noteworthy local variations were found. Conclusion: We found a global underestimation of PET activity, which was within a ±5% acceptance range using MAP-TR with the human head prior and a long transmission scan (speed 10). Fine tuning HH or TXTV parameters might give further improvements.', 'title': 'Quantification accuracy of a new HRRT high throughput rat hotel using transmission-based attenuation correction: A phantom study', 'embedding': []}, {'id': 14955, 'abstractText': 'Increasing the low spatial resolution of hyperspectral images (HSIs) improves the performance of applications in which the HSIs are used. In this study, a fusion based method is proposed to increase the resolution of HSIs. In the proposed method, low resolution (LR) HSI is fused with the high resolution (HR) RGB image to obtain the HR HSI. In this approach, instead of using the spectral images as in the conventional methods, RGB image is used with the abundance maps of the HSI estimated from the linear unmixing and the spatial resolution is enhanced using these maps. In this method, firstly, endmembers are estimated and LR abundance maps are obtained. Then, HR abundance maps are obtained by minimizing an energy function, which is constructed from the LR abundance maps with the HR RGB image. Finally, HR HSI is obtained from these HR abundance maps. The method is tested with real HSIs. Main contribution of the method is converting fusion problem to a quadratic optimization problem in the abundance map domain without any assumption or prior knowledge. The proposed method solves the fusion problem with a computational time much lower than the state-of-the-art fusion based methods with a competing performance.', 'title': 'Fusion based resolution enhancement in hyperspectral images', 'embedding': []}, {'id': 14956, 'abstractText': 'A highly accurate and robust real-time localization process is crucial for autonomous driving applications. Numerous methods for localization have been proposed, which combine various kinds of input, such as data from environmental sensors, inertial measurement units (IMU), and the Global Positioning System (GPS). Because reliance on a single environmental sensor is a vulnerable approach, the use of multiple environmental sensors is a better alternative. However, the fusion methods from previous studies have not adequately compensated for the drawbacks due to the lack of sensor diversity nor have the methods considered the fail-safe issue. In this paper, we propose a multi-modal fusion-based localization framework that uses multiple map matching sources. The framework contains two independent map matching sources and integrates them in a stochastic situational analysis model. By applying a probabilistic model, the more reliable map matching between the multiple sources is determined and the system stability is verified via a fail-safe action. A number of experiments with autonomous vehicles within actual driving environments have shown that combining multiple map matching sources yield more robust results than the use of a single map matching.', 'title': 'Fail-Safe Multi-Modal Localization Framework Using Heterogeneous Map-Matching Sources', 'embedding': []}, {'id': 14957, 'abstractText': 'Predictive map of geoelectric sections of the North Eurasia, necessary for calculation of propagation of VLF-MF radio waves, is constructed. Taking into account the layered structure of the underlying medium, this map is capable of increasing the accuracy of electromagnetic field calculations by 1.5-3 times as compared to the Morgan-Maxwell map and ITU-R Recommendation P.832-2. The methodology of the geoelectric mapping is described. The studies of electrical properties of layered media by combined radio and geophysical methods in a variety of natural and geological conditions, and the proposed method of geoelectric mapping have resulted in the construction of a new generation of maps showing the electrical properties of the underlying medium that account for the layered structure of the crust and have no analogues in the world.', 'title': 'Predictive map of geoelectric sections of North Eurasia and its application for the radiowaves propagation calculations', 'embedding': []}, {'id': 14958, 'abstractText': 'The need to reduce water pumps size to achieve compact designs adapted to multiple working points opens new fields of study. PMSMs are the preferred choice due to outstanding torque-speed range capabilities. This paper presents a methodology to design and optimize PMSMs by defining the desired torque-speed-efficiency map, adapting its performance to the hydraulic characteristics of the water pump. Once the hydraulic efficiency is known, an initial PMSM reference torque-speed-efficiency map is defined according to the objective motor performance, including the distribution of power losses and the power rating of the selected application. The designer has full freedom to define the efficiency levels and distribution along the torque-speed map. The design optimization algorithm achieves the PMSM characteristics which adjust as much as possible to the defined performance. This methodology uses ultra-fast finite element analysis by applying magneto-static computations and a time-space conversion to compute the iron losses, reducing the computational requirements. The torque-speed-efficiency map is calculated by applying a direct-quadrature electrical model. The objective function uses a novel image comparison technique that allows comparing the similarity between the objective and optimized maps. The methodology is validated experimentally by designing and testing a PMSM adapted to a real WP application', 'title': 'Customized PMSM Design and Optimization Methodology for Water Pumping Applications', 'embedding': []}, {'id': 14959, 'abstractText': 'In this work, we investigated the effect of map presentations and landmarks on wayfinding performance. We carried out an experiment in virtual reality, participants were asked to navigate inside a 3D environment to find targets shown on the maps. We studied two kinds of maps: Skymap, a world-scale, and world-aligned head-up map and a track-up bird’s eye view map. Results showed that neither SkyMap nor landmarks did improve target finding performances. In fact, participants performed better with the track-up map.', 'title': 'Map Displays And Landmark Effects On Wayfinding In Unfamiliar Environments', 'embedding': []}, {'id': 14960, 'abstractText': \"Network function virtualization (NFV) provides an effective way to decouple network functions from the proprietary hardware, allowing the network providers to implement network functions as virtual machines running on standard servers. In the NFV environment, an NFV service request is provisioned in the form of a service function chain (SFC). The SFC defines the exact sequence of actions or virtual network functions (VNFs) that the data stream from the service request is subjected to. These actions or VNFs need to bemapped onto specific physical networks to provide network services for end users. In this paper,we investigate the problem of dependence-aware service function chain (D_SFC) design and mapping. We study how to efficiently map users' service requests over the physical network while taking into consideration the computing resource demand, function dependence of the VNFs, and the bandwidth demand for the service request. We propose an efficient algorithm, namely, Dependence-Aware SFC Embedding With Group Mapping (D_SFC_GM), which integrates the proposed techniques of dependence sorting, independent grouping, adaptive mapping, and tetragon remapping to jointly design and map users' service requests. The proposed D_SFC_GM algorithm takes advantage of VNF's dependence relationships and the available resources in the physical network to efficiently design the chain and reserve the computing/bandwidth in the physical network. The extensive performance analysis in both IP and physical networks shows that the proposed D_SFC_GM significantly outperforms the traditional approach based on topological sorting and sequential embedding.\", 'title': 'Embedding dependence-aware service function chains', 'embedding': []}, {'id': 14961, 'abstractText': 'Local mapping plays an important role in outdoor intelligent vehicle applications and multi-vehicle cooperative local mapping which takes advantage of vehicular communication can bring considerable benefits to this important task. In this paper, a multi-vehicle cooperative local mapping architecture using split covariance intersection filter (Split CIF) is proposed. In the proposed method, a vehicle can flexibly perform cooperative local mapping with other vehicles in decentralized way, without complicated monitoring and controlling of data flow among vehicles; fused maps can be shared freely among vehicles. An efficient and accurate implementation of the Split CIF is also introduced. A simulation-based comparative study demonstrates the potential and advantage of the proposed multi-vehicle cooperative local mapping architecture using Split CIF.', 'title': 'Multi-Vehicle Cooperative Local Mapping Using Split Covariance Intersection Filter', 'embedding': []}, {'id': 14962, 'abstractText': 'Feature maps in Convolutional neural networks are extracted automatically with some initialization methods and training strategies, which greatly economizes the cost of feature engineering. However, correlation between feature maps are not considered in common networks, resulting in the increase of redundant feature maps with the networks becoming more complicated. In this work, we proposed the correlation layer and designed the correlation loss, which can compute the correlation coefficient matrix of the feature maps in the last convolutional layer and optimize the weights distribution respectively. In the training phase, 2 strategies, namely the supervision and initialization are studied with Gaussian and He initialization methods for the baseline. The experimental results on CIFAR-10 dataset demonstrated that the supervision strategy for the multi-task training could efficiently reduce the correlation between the feature maps learned and increase the classification accuracy from 0.39% to 1.14% on the test set.', 'title': 'Feature Correlation Loss in Convolutional Neural Networks for Image Classification', 'embedding': []}, {'id': 14963, 'abstractText': 'Rivers are important elements of the Earth’s ecosystem, and their spatial distribution information is critical for the study of hydrological and biogeochemical processes. Moderate Resolution Imaging Spectroradiometer (MODIS) imagery has been widely used for river mapping due to its high temporal resolution and long-term observation records, which are essential to capture the rapid fluctuation of rivers. However, when the conventional hard classification methods are used, the accuracy of the river maps produced from MODIS data (and especially for those rivers with narrow widths) is often limited because mixed pixels are common in MODIS imagery, due to the coarse spatial resolution. In this paper, a cascaded spectral-spatial information combined deep convolutional neural network (CNN) model for super-resolution river mapping (DeepRivSRM) is proposed to produce Landsat-like fine-resolution river maps from MODIS images. In DeepRivSRM, a CNN made up of a spectral unmixing module and a super-resolution mapping (SRM) module is introduced to handle the spectral and spatial information simultaneously. Moreover, for training the DeepRivSRM model, an adaptive cross-entropy loss function incorporating the fraction information of the rivers is designed to improve the performance of the DeepRivSRM model for small rivers. The proposed method was evaluated with MODIS images from three test sites and was compared with hard classification, a conventional SRM method, and a CNN-based SRM method. The results show that DeepRivSRM can generate more accurate river maps by effectively learning the subpixel-scale spatial-spectral information in MODIS imagery.', 'title': 'A Cascaded Spectral-Spatial CNN Model for Super-Resolution River Mapping With MODIS Imagery', 'embedding': []}, {'id': 14964, 'abstractText': \"Landslide susceptibility maps are vital for natural disaster mitigation activities. It can be a basic information to make appropriate mitigation plan. In the present study, we propose ensemble fuzzy clustering to produce the landslide susceptibility map for Ponorogo. The mapping process is based on five factors which play a dominant role in the occurrence of landslide. The factors are rainfall, land use, slope angle, geology, and elevation. As a result, from 316 areas in Ponorogo, 202 areas were mapped in very low level, 94 areas in medium level and 20 areas in the high level of vulnerability. Finally, the validation of landslide susceptibility map was carried out using Pearson's chi-square test. The chi-square value shows higher value than the critical value. It shows that the model has good accuracy in predicting the landslide susceptibility in Ponorogo. The map was visualized on web-based to make it easier to use and can be used for mitigation activities.\", 'title': 'Landslide susceptibility mapping using ensemble fuzzy clustering: A case study in ponorogo, east java, Indonesia', 'embedding': []}, {'id': 14965, 'abstractText': 'Along the season crop classification based on satellite data is challenging task for Ukraine because of a big diversity of different agricultural crops with different phenology (crop calendars). Taking into account the availability for free of high resolution (10 to 30 meter) optical and SAR data from different satellite, the most resource consuming task is ground data collecting. That is why the proper time of ground surveys and crop classification maps developing is very important. In the study we propose to build three crop classification maps for JECAM Ukraine test site in Kyiv region during the vegetation season. The first one is built in the middle of May to classify winter cereals and rapeseeds. The next crop classification map is developing in July to discriminate major summer crops (spring cereals, maize, soybeans, sunflowers). The final crop map is built in autumn to refine summer crops and sugar beet discrimination. Time series of multi-temporal satellite images with restored missing (clouded and shadowed) data are classified using neural network approach, in particular ensemble of multi-layer perceptrons (MLPs). It is shown, that addition of satellite data from the end of previous year to the spring imagery allows to significantly improve the accuracy of winter crops classification. In July it is possible to deliver the map with major summer crops with overall accuracy higher than 87%, and the overall accuracy of final map at the end of the season is 94%.', 'title': 'Along the season crop classification in Ukraine based on time series of optical and SAR images using ensemble of neural network classifiers', 'embedding': []}, {'id': 14966, 'abstractText': \"This study presented a votable concept mapping approach to promoting students' attentional behavior based on the evidence of mining sequential behavioral patterns. Twenty-nine students from an Educational Research Methodology course were recruited as participants. In the first week, a group-polling method was introduced in class; in the second week, participants were asked to draw concept maps using pen and paper (PnP concept mapping); and in the third week, the polling system and concept maps were integrated (votable concept mapping) and applied. The results showed that the votable concept mapping method was effective to stimulating students' attention during class. It was therefore suggested that teachers adopt methods integrating concept maps and polling tools to stimulate students' attention and thereby promote a positive cycle of attentional behavior in the classroom.\", 'title': \"A Votable Concept Mapping Approach to Promoting Students' Attentional Behavior: Based on the Evidence of Mining Sequential Behavioral Patterns\", 'embedding': []}, {'id': 14967, 'abstractText': 'Dynamic cerebral perfusion computed tomography (DCPCT) imaging has the ability to detect ischemic stroke via hemodynamic maps. However, due to multiple acquisitions protocol, DCPCT scanning imposes high radiation doses on patients and might increase their potential cancer risks. The DCPCT protocol that decreases DCPCT samples by increasing sampling intervals can greatly reduce radiation dose, but this may introduce bias in the hemodynamic maps estimation, affecting the diagnosis. To address this issue, in this study, we present a deep learning network to determine the DCPCT protocol to realize the dose-reduction task, i.e., decreasing DCPCT samples, and the diagnosis-quality task, i.e., improve hemodynamic maps accuracy. Specifically, one interpolation convolutional neural network is fully designed to estimate the DCPCT images at the sampling interval, termed as dynamic cerebral perfusion interpolation network (DCPIN). The present network treats the DCPCT measurements as a \"video\" to characterize the maximum temporal coherence of spatial structure among phases, and interpolates a frame at any arbitrary time step between any two frames. First, a flow computation network is used to estimate the bi-directional optical flow between two input DCPCT frames by linearly fusing to approximate the required intermediate optical flow. Second, another flow interpolation network is designed to refine the flow approximations and predict soft visibility maps. Finally, the estimated flow approximations and visibility maps are merged together to jointly predict the intermediate DCPCT frame. Experimental results on patient data clearly demonstrate that the present DCPIN can achieve promising reconstruction performance, i.e., high-quality DCPCT images and high-accuracy hemodynamic maps.', 'title': 'Task-driven Deep Learning Network for Dynamic Cerebral Perfusion Computed Tomography Protocol Determination', 'embedding': []}, {'id': 14968, 'abstractText': 'This work in progress paper presents the preliminary step towards developing a new educational framework titled “Tracking Evolution Architecture of Cognitive Hierarchy Maps for Engaged Classrooms” (TEACH ME). In this paper, we track the scaffolding of the pre-requisites needed in high school level physics concepts and use this information to design the higher-level scaffolding of the same concepts in a university freshmen physics course. This tracking is performed as a meta-cognitive activity inside the classrooms, using a well-established concept visualization tool called “concept maps”. Further, these maps will be extended to include higher-level concepts in the engineering courses that have the physics course as a pre-requisite. This paper presents the insights from a pilot implementation of this technique in a freshman physics course. We used concept map as an additional instruction method to aid the students to visually connect the pre-requisite and the newly delivered concepts. This will allow students to understand what they know and what they need to learn. The effectiveness of this method is measured using the data collected from class quizzes and tests. The paper summarizes the initial results from this study along with a discussion on how such earlier maps developed at freshmen level can be extended to higher-level engineering classes for creating engaged classrooms using a novel teaching-learning frame work under development titled “Tracking Evolution Architecture of Cognitive Hierarchy Maps for Engaged Classrooms” (TEACH ME).', 'title': 'The first step towards a pre-requisite knowledge tracking architecture for engineering programs', 'embedding': []}, {'id': 14969, 'abstractText': 'The topological G -conjugacy map has an important significance in terms of theory and application. In this paper, we study recurrent point, non-wandering point and periodic shadowing property of topological G -conjugacy on metric G -space. By means of properties of topological G -conjugacy map and by inference, we will give the following conclusions that if Let f: X → X and t: Y → Y be two continuous map of metric G-space X and Y. Suppose the map h: X → Y is a topogical G -conjugacy from f to t and x2 X, then(1)The point x is a G - recurrent point of f if and only if the point h(x) is a G -recurrent point of t. (2)The point x is a G -nonwandering point of f if and only if the point h(x) is a G -nonwandering point of t. (3) The map f has the G -periodic shadowing property if and only if the map t has the G -periodic shadowing property. These results will enrich the theory of the recurrent point, non-wandering point and periodic shadowing property of topological G -conjugacy on metric G-space.', 'title': 'Point Sets and Periodic Shadowing Property of Topological G-Conjugacy on Metric G-Space', 'embedding': []}, {'id': 14970, 'abstractText': 'Precipitable water vapor (PWV) is one of the key parameters in the evolution of extreme weather and climate change. However, current data fusion methods (such as Gaussian Processes, Spherical Cap Harmonics and polynomial fitting) can hardly obtain simultaneously the PWV map with high precision and high spatio-temporal resolution. To solve this problem, a two-step based PWV fusion (TPF) method is proposed, in which a Hybrid PWV Fusion Model (HPFM) and a Spatial and Temporal Fusion Model (STFM) are introduced separately. In the first step, HPFM is established by combining the Global Pressure and Temperature 2 wet (GPT2w) model, spherical harmonic functions, and polynomial fitting to obtain the PWV value with high precision at an arbitrary location in the study area. In the second step, STFM is proposed to generate the PWV map with high temporal resolution taking advantage of site-based GNSS-derived PWV. To validate the performance of the proposed method, GNSS observations, ERA-Interim, and ERA5 reanalysis products are selected in Yunnan Province, China to carry out the experiment. Statistical results show that: (1) HPFM has the ability to obtain atmospheric water vapor with an root mean square (RMS) of less than 3 mm in an arbitrary location of the PWV map; (2) STFM can generate PWV maps with the same temporal resolution as GNSS observations, and the accuracy of the obtained PWV values can be guaranteed. Therefore, the proposed TPF method is proved to have the ability to simultaneously retrieve PWV maps with high accuracy and spatio-temporal resolution.', 'title': 'Two-step precipitable water vapor fusion method', 'embedding': []}, {'id': 14971, 'abstractText': 'There is a growing demand for maps from government, enterprises and the pubulic. This paper studies the data management methods of ancient and modern maps to solve the problems of scattered storage, low openness, inconvenient query and use, lack of system inheritance and so on. Based on the digital processing and database management system, the map database was established after the ancient map was edited. Based on SOA service architecture and Browser/Server mode, the spatio-temporal fusion and operation platform for ancient and modern maps is established. The functions of map unified management, visual display, query and analysis, input and multi-mode export are achieved. Using Android and IOS technology to build mobile applications, improve map management and sharing application mode.', 'title': 'Design and Development of Spatio-Temporal Fusion and Operation Platform for Ancient and Modern Maps', 'embedding': []}, {'id': 14972, 'abstractText': '3D maps such as Google Earth and Apple 3D Map that allow users to easily access 3D models of real world become ubiquitously available recently. Currently, most people use the 3D maps on conventional display devices such as monitors and interface devices such as mouse and keyboard. With recent development of mass-market head mounted displays (HMD) where users can experience virtual reality (VR) with more affordable cost, experiencing 3D maps with virtual reality is expected to be an important application. Although one of the main goals of virtual reality technology is to enhance the sense of immersion, the conventional mouse and keyboard interfaces limit the level of immersion, which makes the interfaces often inappropriate for the 3D map navigation. From this motivation, this paper proposes immersive gesture interfaces for 3D map navigation in HMD-based virtual environments. The gestures are recognized using Kinect in real-time. User studies show that the proposed interface improves the level of immersion.', 'title': 'Immersive gesture interfaces for 3D map navigation in HMD-based virtual environments', 'embedding': []}, {'id': 14973, 'abstractText': 'The present study explores the center-of-mass (Hc) of the ionosphere as the effective ionospheric varying shell height (VSH). We presents global ionospheric maps GIM-Hc produced with IRI-Plas model by assimilating the instant GIM-TEC maps provided by JPL from 1994 to present which allows obtaining global maps of the F2 layer critical frequency, GIM-foF2 (or NmF2) and peak height, GIM-hmF2. Ratio TEC/NmF2 represents the slab-thickness model of the ionosphere, GIM-τ, fitted to foF2, hmF2 peak at each cell of a map. The slab-thickness τ is for the first time tied to hmF2 peak height with its components τbot below hmF2 and topside τtop. Equations for evaluating Hc with IRI-Plas and NeQuick models are derived from Ne(h) profile complemented with the total electron content—height profile TEC(h) from the bottom boundary of the ionosphere (65–80 km over the Earth) to varying altitude h up to 20,200 km (GPS orbit). The position of Hc depends on the altitude range selected at the ionosphere and plasmasphere. We determine Hc from Ne(h) and TEC(h) profile within the borders of τ from the bottom side (hmF2 — τbot) to topside (hmF2 + τtop). Regression linear model of Hc variation with hmF2 is derived which allows estimate of Hc from the F2 layer peak height hmF2. Statistical characteristics of GIM-Hc maps can serve for validation and updating the effective shell height with Hc parameter varying over the globe for improved vertical TEC evaluation from the slant TEC observations.', 'title': 'Modeling center-of-mass of the ionosphere from the slab-thickness', 'embedding': []}, {'id': 14974, 'abstractText': 'Fatal accidents can be avoided in autonomous vehicles by detecting the obstacles, and distance estimation accurately and in real-time is crucial. Still, most of the object detection approaches fail to detect debris and other roadside obstacles. The drawbacks of the object detection approaches are countered using active sensors such as LiDAR, RADAR, and SONAR, but expensive. This study introduces a modified architecture to estimate a depth map based on an unsupervised learning framework. Furthermore, integrating depth map information directly into autonomous vehicles is a slightly critical and challenging task. This paper presents a concept for predicting a potential collision based on the estimated depth map. For this purpose, we designed and developed an efficient and robust algorithm to understand the color encoding used in representing the depth map. Thereby, the possibility of collision regions in real-time identified using predefined threshold values for the color encoding of the predicted depth map. This approach emphasizes integrating the estimated depth map with the level of comprehension of the situation awareness to enhance the ability to recognize and process predicted hazards in the environment. It facilitated the possibility to avoid a dangerous situation in real-time capabilities.', 'title': 'Towards Robust Perception Depth Information For Collision Avoidance', 'embedding': []}, {'id': 14975, 'abstractText': 'Land use and land cover (LULC) mapping are required by some government institutions to manage their natural resources sustainably at various temporal and spatial scales. Our study, set in Ecuador, use Sentinel-2 and Landsat-8 from 2017 to 2019 applying an adapted Phenology-Based Synthesis (PBS) method, which originally only classifies land cover, to additionally classify land use categories and eventually obtain LULC map in yearly period with different spatial resolution. Copernicus Global Land Cover map with 100-meter spatial resolution was used as a reference LULC map and provided a critical analysis on PBS classifications, which showed, a consistent representation of the LULC patterns, with vegetation and non-vegetation types well described across the country. Furthermore, we evaluated the LULC changes which showed conversion transition occurred in each class. Finally, entire Ecuador was mapped confirming that adapted PBS mapping approach could be considered as a reliable, replicable and low-cost LULC monitoring tool.', 'title': 'Mapping and Assessment of Land Use and Land Cover for Different Ecoregions of Ecuador Using Phenology-Based Classification', 'embedding': []}, {'id': 14976, 'abstractText': 'Objective: Expanding the clinical research network from China to the international community will greatly facilitate the collaborative efforts to conquer emerging pandemics globally. This study aims to link clinical drugs in China with those in USA and beyond, by mapping concepts of clinical drugs in a Normalized Chinese Clinical Drug (NCCD) knowledge base with RxNorm and the international RxNorm Extension in the OHDSI research network. Methods: At this initial stage, we mainly focused on chemical drugs with a single active ingredient. A hybrid approach combining automated natural language processing technologies and manual review by domain experts was proposed, to normalization of drug names at different specification levels. The statistics of the mapping results, as well as the validation results using top 1000 frequent drugs in clinical settings of China. Results: 2,247 (25.39%) and 588 (6.64%) semantic clinical drugs in NCCD can be mapped to RxNorm and RxNorm Extension, respectively. Moreover, among the top 1000 most frequent clinical drugs in China, 99.6% of chemical drugs can be mapped to RxNorm/Extension at the ingredient level. This high coverage indicates that mapping Chinese chemical drugs to RxNorm/Extension can effectively support downstream clinical research and operations in the near future. Conclusion: The mapping between NCCD to RxNorm/Extension serves as a promising channel to promote transnational clinical research. The mapping terminologies are publicly accessible through OHDSI Athens. (athena.ohdsi. org/).', 'title': 'NCCD-RxNorm: Linking Chinese Clinical Drugs to International Drug Vocabulary', 'embedding': []}, {'id': 14977, 'abstractText': 'Disease similarity is a useful measure that has potential application to various aspects of medicine. One such application is the mapping of diseases in a two-dimensional plane, which can be the foundation of a useful diagnostic reminder method called the “pivot and cluster strategy.” However, the mapping of diseases using a similarity measure has yet to be explored. This article investigates such a mapping, and quantifies its basic characteristics. We first collected mutual similarity data for 1,550 diseases using a machine learning approach. The calculated similarity data were then used to map the diseases using a “multidimensional scaling” algorithm. Quantitative analysis indicated that it is difficult to express all the diseases on the map and yet still show the similarity information between the items. Then, by restricting the input, the algorithm performed well in practice. To our knowledge, this is the first study to investigate the automated mapping of diseases on a plane for use in clinical practice.', 'title': 'Geometrical mapping of diseases with calculated similarity measure', 'embedding': []}, {'id': 14978, 'abstractText': 'Attenuation correction of brain MRI coils used on PET/MR systems can be prone to error emanating from artifacts in CT-based coil attenuation maps. In this study editing was applied to brain-coil CT images to reduce the impact of metalinduced artifacts on attenuation correction for the GE SIGNA PET/MR. Methods for global and material-specific transformation of coil CT images to linear attenuation coefficient (LAC) maps were evaluated. In addition, CT-based attenuation maps were generated for the coil mirrors and four levels of smoothing were applied to the attenuation maps. A uniform phantom experiment was performed to assess absolute quantification, and both axial and transaxial image uniformity. For the three coils tested, the edited CT attenuation maps improved absolute quantification (mean absolute error 1.2% vs. 7.2% in the central axial 15 cm) and axial uniformity by up to 60% (33% on average) compared to the vendor-supplied attenuation maps, although transaxial uniformity deteriorated by up to 38% (16% on average). No single CT-to-LAC conversion method was found to be optimal across coils and performance metrics. Overall, material-specific CT-to-LAC conversion performed best for image quantification (0.6% vs. 1.5%), whereas global transformation narrowly ranked best for uniformity (1.69% vs. 1.71%). The inclusion of the mirror marginally improved axial uniformity in the majority of cases, but also slightly degraded transaxial uniformity. The impact of changing smoothing was minor, with the system default setting of 10 mm FWHM Gaussian producing the best results overall.', 'title': 'Brain MRI Coil Attenuation Map Processing for the GE SIGNA PET/MR: Impact on PET Image Quantification and Uniformity', 'embedding': []}, {'id': 14979, 'abstractText': 'Large-scale mapping of coastal oil spills and their monitoring over time is a major issue that can be adressed by using hyperspectral images and dedicated processing. Previous researches have shown that it is possible to map the polluted coastline caused by the explosion of the Deepwater Horizon (DwH) platform from AVIRIS images (AVIRIS: Airborne Visible/InfraRed Imaging Spectrometer). But the detection processes required either ground truth or laboratory spectra of hydrocarbons or were not fully automatic.In this paper we focused on an AVIRIS image which covers The Bay Jimmy, located south of New Orleans, and particularly impacted by oil pollution. Two automatic methods were developed to detect oiled coasts. In the first one, we have developed a new spectral index able to detect directly hydrocarbon and less sensitive to noise than indices proposed in previous works. The second one extracts endmembers via Orthogonal Subspace Projection, and then sorts the endmembers in terms of hydrocarbon indices scores, in descending order. Then, the detection map or the abundance map corresponding to the best endmember is used to map oiled areas. Both approaches give results consistent with those of studies previously conducted on the same image, and with maps built from field observations.', 'title': 'Automatic Mapping of Hydrocarbon Pollution Based on Hyperspectral Imaging', 'embedding': []}, {'id': 14980, 'abstractText': 'Context: Heuristic optimization has been of strong focus in the recent modeling of the Resource Constrained Project Scheduling Problem (RCPSP), but lack of evidence exists in systematic assessments. New solution methods arise from random evaluation of existing studies. Objective: The current work conducts a secondary study, aiming to systemize existing primary studies in heuristic optimization techniques applied to solving classes of RCPSPs. Method: The systemizing framework consists of performing a systematic mapping study (SM), following a 3-steped protocol. Results: 371 primary studies have been depicted from the multi-stage search and filtering process, to which inclusion and exclusion criteria have been applied. Results have been visually mapped in several distributions. Conclusions: Specific RCPSP classes have been grounded and therefore a rigorous classification is required before performing a systematic mapping. Focusing on recent developments of the RCPSP (2010-2015, a strong interest has been acknowledged on solution methods incorporating AI techniques in meta- and hyper-heuristic algorithms.', 'title': 'Heuristic optimization for the resource constrained Project Scheduling Problem: A systematic mapping', 'embedding': []}, {'id': 14981, 'abstractText': 'Communication plays an important role in Agile Software Development (ASD). In each ASD practice (e.g., stand-up or retrospective meetings), different communication practices and channels are adopted by different companies. Several works have analyzed the impact of communication channels and practices. However, there are no secondary studies summarizing their impact on ASD. This study presents a Systematic Mapping Study (SMS) that aggregates, summarizes, and discusses the results of 25 relevant primary studies concerning the impact of communication channels and practices in ASD. We followed the well-known systematic mapping methodology in software engineering and analyzed empirical studies published before the end of June 2018. The results of our study have yielded several strategies that can be adopted by practitioners. Communication practices are context dependent. In the case of a distributed team, blended usage of rich-media communication tools, such as shared mind-map tools, videoconferencing, and promoting the exchange of team members between teams, is beneficial. In conclusion, communication can be expensive if teams do not apply the right strategies. Future research direction is to understand how to maximize product quality while reducing communication cost and how to identify the most beneficial communication strategy for the different stages of ASD.', 'title': 'Lessons Learned on Communication Channels and Practices in Agile Software Development', 'embedding': []}, {'id': 14982, 'abstractText': 'This article describes the mathematical modeling of the relationship between genotype and phenotype using statistical methods. The results of the study of cells of laboratory mice are used as the initial data. To build a relationship model, methods of correlation analysis, cluster analysis, ANOVA and its modification - QTL analysis are used. Graph theory is used to model the interaction of proteins in cellular processes. The QTL mapping idea is to phenotype observation and identification of the genome region on which the genotype is associated with the phenotype. With the help of molecular-genetic markers, molecular maps of individual chromosomes and genomes are made, genes and QTLs mapping are performed on them. Thus, genes with the greatest connectivity to phenotype were identified. The correlation between genotype and phenotype is studied across the full genome of the individual in this study. The data provided by the Laboratory of Molecular Genetics of the innate immunity of Petrozavodsk State University were the initial data in this research. The essence of the project, carried out jointly with the laboratory, is to study the arrays of genetic information to identify and model the relationships between the genotype and the phenotype of biological organisms. Genotyping and phenotyping were conducted based on sequencing data (determination of amino acid and nucleotide sequence) of matrix RNA. The result of the study is the construction of a mathematical model of the relationship between phenotype and genotype and the identification of groups of significant phenotypes that affect the cells processes.', 'title': 'Mathematical Modeling of Influence of the Genotype on Cell Processes by Statistical Analysis', 'embedding': []}, {'id': 14983, 'abstractText': 'Seismic ambient noise tomography is applied to central and southern Mozambique, located in the tip of the East African Rift (EAR). The deployment of MOZART seismic network, with a total of 30 broad-band stations continuously recording for 26 months, allowed us to carry out the first tomographic study of the crust under this region, which until now remained largely unexplored at this scale. From cross-correlations extracted from coherent noise we obtained Rayleigh wave group velocity dispersion curves for the period range 5–40\\ue251s. These dispersion relations were inverted to produce group velocity maps, and 1-D shear wave velocity profiles at selected points. High group velocities are observed at all periods on the eastern edge of the Kaapvaal and Zimbabwe cratons, in agreement with the findings of previous studies. Further east, a pronounced slow anomaly is observed in central and southern Mozambique, where the rifting between southern Africa and Antarctica created a passive margin in the Mesozoic, and further rifting is currently happening as a result of the southward propagation of the EAR. In this study, we also addressed the question concerning the nature of the crust (continental versus oceanic) in the Mozambique Coastal Plains (MCP), still in debate. Our data do not support previous suggestions that the MCP are floored by oceanic crust since a shallow Moho could not be detected, and we discuss an alternative explanation for its ocean-like magnetic signature. Our velocity maps suggest that the crystalline basement of the Zimbabwe craton may extend further east well into Mozambique underneath the sediment cover, contrary to what is usually assumed, while further south the Kaapval craton passes into slow rifted crust at the Lebombo monocline as expected. The sharp passage from fast crust to slow crust on the northern part of the study area coincides with the seismically active NNE-SSW Urema rift, while further south the Mazenga graben adopts an N-S direction parallel to the eastern limit of the Kaapvaal craton. We conclude that these two extensional structures herald the southward continuation of the EAR, and infer a structural control of the transition between the two types of crust on the ongoing deformation.', 'title': 'Ambient noise tomography of the East African Rift in Mozambique', 'embedding': []}, {'id': 14984, 'abstractText': 'This study provides a technical review of the current state of immersive virtual museum, heritage, and tourism focusing on workflows and challenges involved in realistic asset creation. The workflow includes two parts i.e. virtualization of historic objects and creation of environment. However, in some instances the environment itself is a cultural heritage site e.g. an old castle that can be considered as historic object. Otherwise, the environment is just a conceptual virtual place (created using traditional 3D modeling methods) to mimic museum experience, embedding smaller historic objects, which are virtualized. Although tools and technologies such as photogrammetry, 3D scanning, or aerial 3D mapping have made the process of virtualization of historic/cultural objects considerably easier for basic users, challenges and limitations still remain as these automatic processes are not always accompanied by flawless outcomes. This study addresses some of those challenges and limitations faced during preparation of experimental immersive virtual museum for exhibition purposes. This covers various ranges of topics from lighting, texturing, and topology to limitations related to opacity, dark colors, and small details. This paper also provides a comprehensive overview of the technical details when it comes to preparation of virtual cultural heritage environments specifically for immersive experiences. Areas such as user interaction, navigation, space optimization, quality and viewing distance, access, purpose and objectives, degree of realism, etc. are covered in this review. The major processes illustrated in this study include photogrammetry, aerial 3D mapping, polygon modeling, 3D sculpting, 3D painting, UV Mapping, etc. The major software/tools used in this workflow include Agisoft Photoscan, Autodesk Remake, Pixologic ZBrush, xNormal, Autodesk 3ds Max, Unity, SteamVR, HTC Vive, including other relevant plugins and scripts. However, this study is not a step by step guide or a tutorial, but a reference for the currently available technologies to create immersive virtual museum, cultural heritage, and tourism aiming to distinguish the lines between different levels of processes involved. The objective is to provide a clear understanding of the challenges involved. Based on the literature review done prior to this study, a comprehensive academic reference (covering the mentioned areas) for digital heritage researchers is lacking (to date). The authors believe that due to the increasing availability and affordability of the current immersive virtual reality technologies for basic users this is a proper time for gathering major processes/challenges involved in creation of such environments and present them in form of a comprehensive reference. Although the main focus of this study is on digital heritage, the processes undertaken and explained can be generalized to be used by researchers in other fields where applicable.', 'title': 'Workflows and Challenges Involved in Creation of Realistic Immersive Virtual Museum, Heritage, and Tourism Experiences: A Comprehensive Reference for 3D Asset Capturing', 'embedding': []}, {'id': 14985, 'abstractText': 'The ionosphere has a time-varying electron content. The electron content has a significant effect on the signals emitted in the ionosphere. For this effect to be examined, it is important to estimate and map the total electron content. In addition, historical archiving is important in terms of the typical behaviors shown by the ionosphere with long-term examinations and the extraction of instant disturbances at certain days and times. Although compressive sensing has application in many areas, the studies on the ionosphere are negligible. In addition to the studies for estimating the total electron content, compressive sensing method was used in this study. It has been shown that the ionosphere maps are sparsely structured and are estimated by compressive sensing methods. The estimation performances of random and clustered observation methods is compared.', 'title': 'Construction of TEC Maps Using Compressive Sensing', 'embedding': []}, {'id': 14986, 'abstractText': 'HD\\xa0170582 is an interacting binary of the double periodic variable (DPV) type, showing ellipsoidal variability with a period of 16.87\\xa0d along with a long photometric cycle of 587\\xa0d. It was recently studied by Mennickent et\\xa0al., who found a slightly evolved B-type star surrounded by a luminous accretion disc fed by a Roche lobe overflowing A-type giant. Here we extend their analysis presenting new spectroscopic data and studying the Balmer emission lines. We find orbitally modulated double-peak Hα and Hβ emissions whose strength also vary in the long term. In addition, Doppler maps of the emission lines reveal sites of enhanced line emission in the first and fourth velocity quadrants, the first one consistent with the position of one of the bright zones detected by the light-curve analysis. We find a difference between Doppler maps at high and low stage of the long cycle; evidence that the emission is optically thicker at high state in the stream-disc impact region, possibly reflecting a larger mass transfer rate. We compare the system parameters with a grid of synthetic binary evolutionary tracks and find the best-fitting model. The system is found to be semi-detached, in a conservative Case-B mass transfer stage, with age 7.68 × 10<sup>7</sup>\\xa0yr and mass transfer rate 1.6 × 10<sup>−6</sup> M<inf>⊙</inf>\\u2009yr<sup>−1</sup>. For five well-studied DPVs, the disc luminosity scales with the primary mass and is much larger than the theoretical accretion luminosity.', 'title': 'Doppler tomography of the Double periodic variable HD\\xa0170582 at low and high stage', 'embedding': []}, {'id': 14987, 'abstractText': 'Advance organizer strategies based on cognitive theories have been found to have an impact on learning in the early studies. However, recent studies have found inconclusive results when type and format of the advance organizers were taken into account. This study investigated the impact of different formats of advance organizers (graphic with text explanation, voice narration, and concept map) on learners understanding of science concepts. A total of 136 high school students participated in this study. A factorial design was utilized for the experiment using format of advance organizers as the independent variable while learners achievement on retention and transfer test regarding the learning content as the dependent variables. The findings showed that using graphical advance organizers, especially the concept map, worked effectively in achieving high school students understanding of the learning content. The results showed that using concept map was likely to conceptually anchored viewers attention on key elements of reading content. When designing instructional materials, it is advised to take the advantage of graphical organizers to achieve understanding of abstract science concepts. Implications and suggestions for effective instructional design and for future studies are provided in this paper.', 'title': \"Investigations of the Effect of Format of Advance Organizers on Learners' Achievement on Understanding of Science Concepts\", 'embedding': []}, {'id': 14988, 'abstractText': 'This research full paper characterizes the literature on academic initiatives to foster computational thinking and programming (CT&amp;P) in Brazilian K-12 education. Context: Mapping and analyzing the diversity of experiences and studies that address CT&amp;P in K-12 education can bring valuable data to researchers. This work delimits such study to the Brazilian scenario to allow a more in-depth view, given the Brazilian context. Previous surveys and systematic mapping studies present recent publications in major Brazilian computing journals and conferences. Although they offer important contributions, they do not comprehensively cover the Brazilian literature on CT&amp;P in K-12 education, since they focus the search in the Brazilian Computer Society publications alone. Objective: This work proposes to characterize the literature on CT&amp;P in K-12 education in Brazil. Results: through a systematic mapping study, we collected information on year, venue, type, K-12 education stage and modality, methodological contexts, and used tools and programming languages from 338 selected primary studies from 2001 until 2016. Conclusions: there is a significant increase in the number of Brazilian studies in the latest years, showing a growing interest on this research area as well as several trends and gaps to be addressed by both researchers and practitioners.', 'title': 'A Mapping Study of Computational Thinking and Programming in Brazilian K-12 Education', 'embedding': []}, {'id': 14989, 'abstractText': \"Mapping and quantification of biomass changes is critical to understanding mangrove carbon sequestration, conservation, and restoration. Few previous studies have focused on mangrove biomass changes based on high spatial resolution images, particularly for disturbed and recovering areas. This study developed an effective model to estimate and map mangrove aboveground biomass dynamic change between 2010 and 2016 on Qi'ao Island in South China. The study area includes native Kandelia candel (K. candel) and planted Sonneratia apetala (S. apetala) mangrove species within the largest planted area in China. Models were developed using WorldView-2 images, digital surface models (DSMs), and the random forest algorithm. Accuracies of the model were assessed using multiyear field samples. DSMs were identified as the most important variable for model accuracy, reducing relative error by up to 3.14%. Three models were developed: a model for 2010, another model for 2016, and a combined model for 2010 and 2016. Compared with the 2010 (RMSE = 41.03 t/ha, RMSEr = 24.31%) and 2016 (RMSE = 39.92 t/ha, RMSEr = 23.40%) models, the combined model (RMSE = 50.99 t/ha, RMSEr = 30.48%) only increased the relative error by 6.17% and 7.08%, respectively. Mangrove biomass maps generated from the most accurate models showed total biomass increased from 23270.43 to 39819.03 tons by up to 71.11% over the study period. K. candel total biomass decreased by 36.5% due to Derris trifoliata challenge. S. apetala total biomass increased by 74.79% due to reforestation programs, achieving aboveground biomass accumulation of 4.17 t/ha for stands that existed in 2010. This study provides insights into biomass dynamic change in disturbed and recovering mangrove areas. Future studies should consider using LiDAR techniques to obtain actual tree height applied for biomass estimation instead of DSM.\", 'title': 'Estimating and Mapping Mangrove Biomass Dynamic Change Using WorldView-2 Images and Digital Surface Models', 'embedding': []}, {'id': 14990, 'abstractText': 'Light detection and ranging (LiDAR) data classification provides useful thematic maps for numerous geospatial applications. Several methods and algorithms have been proposed recently for LiDAR data classification. Most studies focused on object-based analysis because of its advantages over per-pixel-based methods. However, several issues, such as parameter optimization, attribute selection, and development of transferable rulesets, remain challenging in this topic. This study contributes to LiDAR data classification by developing an approach that integrates ant colony optimization (ACO) and rule-based classification. First, LiDAR-derived digital elevation and digital surface models were integrated with high-resolution orthophotos. Second, the processed raster was segmented with the multiresolution segmentation method. Subsequently, the parameters were optimized with a supervised technique based on fuzzy analysis. A total of 20 attributes were selected based on general knowledge on the study area and LiDAR data; the best subset containing 12 attributes was then selected via ACO. These attributes were utilized to develop rulesets through the use of a decision tree algorithm, and a thematic map was generated for the study area. Results revealed the robustness of the proposed method, which has an overall accuracy of ~95% and a kappa coefficient of 0.94. The rule-based approach with all attributes and the k nearest neighbor (KNN) classification method were applied to validate the results of the proposed method. The overall accuracy of the rule-based method with all attributes was ~88% (kappa = 0.82), whereas the KNN method had an overall accuracy of &lt;;70% and produced a poor thematic map. The selection of the ACO algorithm was justified through a comparison with three well-known feature selection methods. On the other hand, the transferability of the developed rules was evaluated by using a second LiDAR dataset at another study area. The overall accuracy and the kappa index for the second study area were 92% and 0.90, respectively. Overall, the findings indicate that the selection of a subset with significant attributes is important for accurate LiDAR data classification with object-based methods.', 'title': 'Integration of Ant Colony Optimization and Object-Based Analysis for LiDAR Data Classification', 'embedding': []}, {'id': 14991, 'abstractText': 'Effective assessment of cyber risks in the increasingly dynamic threat landscape must be supported by artificial intelligence techniques due to their ability to dynamically scale and adapt. This article provides the state of the art of AI-supported security risk assessment approaches in terms of a systematic mapping study. The overall goal is to obtain an overview of security risk assessment approaches that use AI techniques to identify, estimate, and/or evaluate cyber risks. We carried out the systematic mapping study following standard processes and identified in total 33 relevant primary studies that we included in our mapping study. The results of our study show that on average, the number of papers about AI-supported security risk assessment has been increasing since 2010 with the growth rate of 133% between 2010 and 2020. The risk assessment approaches reported have mainly been used to assess cyber risks related to intrusion detection, malware detection, and industrial systems. The approaches focus mostly on identifying and/or estimating security risks, and primarily make use of Bayesian networks and neural networks as supporting AI methods/techniques.', 'title': 'A Systematic Mapping Study on Approaches for Al-Supported Security Risk Assessment', 'embedding': []}, {'id': 14992, 'abstractText': 'Geomorphological and geological studies of the seafloor benefit today from both ROV exploration and from acquisition of high resolution bathymetric data. Although both represent significant improvements to study submarine domains, the understanding of the studied objects is made more difficult than on land given the limited visual perception provided by the ROV camera due to the attenuation of light in the water and the need to use artificial illumination. Likewise, mapping can be performed using GIS software for digital elevation models and its derivatives (e.g. slope or shade raster), mostly in a 2D map view only. So, the submarine studies lack the field survey stage performed in classical onshore works that allows clear visualization and appreciation of the studied objects. Our aim is to develop a solution allowing the visualization of Digital Elevation Models (DEM) and 3D models derived from Structure-from-Motion (SfM) within a virtual reality environment, and to use these data for geomorphological and geological analysis. For this, we use an Oculus Rift headset, Touch controllers, and the Unity game engine, with GIS-like interaction capabilities. The free and open Unity package that we are developing allows, at this stage, data visualization and working at a 1:1 scale in a georeferenced system. The user can therefore move freely within a 3D immersive environment that includes custom topographic data. For quantitative observations, we develop tools (ruler, compass) allowing measurements similar to those performed during geomorphological or geological field work. We also add the possibility to map objects. Digitizing in 3D is achieved with a laser pointed towards the data, providing great precision. The user can thus create pseudo shapefiles using the same three graphic primitives, and that are compatible with standard GIS software. Beside these functionalities, we also implement a spatial user interface displaying help and information and a teleportation tool preventing motion sickness. The users that have tested this solution are enthusiastic and agree that it helps to better appreciate and understand the shape and geometry of the studied objects. It was also used to present and explain 3D models of outcrops to master students. Further developments will port the solution for other headsets, facilitate the data import (e.g., standard file formats for 3D objects and DEMs), create and manage of multiple layers of shapefiles, and include multiplayer online gaming capabilities to allow remote co-working with colleague(s) at other distant locations, or a whole classroom.', 'title': 'Performing submarine field survey without scuba gear using GIS-like mapping in a Virtual Reality environment', 'embedding': []}, {'id': 14993, 'abstractText': \"Startup companies are seeking for a business model by means of innovative products and need external investments until they can stabilize this product in the market and then start to grow to become a mature company. Nevertheless, many of these companies fail before the growth phase, emerging the need to seek for methods that help these companies to achieve their goals. This paper intends to identify the techniques, practices and tools used in software development on startups and from the results present a knowledge base that can be transferable to the industry. A systematic mapping study was conducted using a classification schema to attest the primary studies quality as well as a exclusion and inclusion criteria to select them. A total of 112 studies are found at the end of the searches and 19 of them were selected to form the mapping study. From these studies a total of 24 techniques, 31 practices and 37 tools were found. As a result the mapping study presents a technical knowledge base that seeks to fill the research gap and that can also be used as a starting point for both researchers and startups' practitioners.\", 'title': 'Technical Aspects of Software Development in Startups: A Systematic Mapping', 'embedding': []}, {'id': 14994, 'abstractText': \"Urban growth indicates rapid increasing in urban population, city significance level and economic expansion. The present and historical information is necessary in urban spatial analysis study and future urban growth planning. These fast urban sprawls require mapping and monitoring of urban area using geospatial and temporal imagery. This study examines and assesses urban expansion of Bhilwara city using Remote Sensing (RS) images and Geo-graphical Information System (GIS) with help of Shannon's entropy model. In this research, urban growth areas were mapped and analysed for a period of 64 years using Shannon's entropy by multi spatial and temporal RS data. The key advantages of RS data are its availability over large coverage area and to detect temporal changes. The Survey of India (Sol) toposheet and Landsat data (1972, 1981, 1991 &amp; 2001), Liss-III (2011) and Sentinel 2B (2018) are used for preparation of GIS based outputs maps of the Bhilwara City and surrounding areas. The finding of this study will be helpful in the understanding the present and projecting the future growth scenario. Further, this study will also help in efficient planning for systematic and controlled urban sprawl in the study area.\", 'title': 'Urban Growth Assessment Using Remote Sensing, GIS and Shannon’s Entropy Model: A Case Study of Bhilwara City, Rajasthan', 'embedding': []}, {'id': 14995, 'abstractText': 'Timely and accurately monitoring of cropland use is of critical significance to ensure food security. Shaanxi province of China has experienced major land use and land cover changes due to the rapid urban sprawl and the implement of the Grain for Green Project. This paper aimed to investigate cropland use changes in Shaanxi from 2000 to 2015 based on MODIS EVI time-series data by utilizing shape-matching cropping index mapping method. The shape-matching method we formerly developed was proved to be an efficient method in mapping cropland use intensity. We firstly divided the study area into six different geomorphic zones, and then randomly selected training samples in each zone. After that, the cropping status for each sample was visually interpreted and used as reference data to train the optimal thresholds for detect cropping index in each zone with the shape-matching cropping index mapping model. Then the cropland use maps for 2000 and 2015 was derived. Finally, the cropland use changes were obtained by spatial analysis of these two cropland use maps. Results show that: (1) The dominant land use is double cropping and non-grain crop production in Guangzhong Plain both in 2000 and 2015. And most cropland in other zones was used for non-grain crop production in the two study years. (2) There were totally 2 585.3 km<sup>2</sup> cropland newly developed and 7 042.8 km<sup>2</sup> cropland lost in the past fifteen years. Areas with increased use intensity is 4 084.9 km<sup>2</sup>, while areas with decreased use intensity is 10 889.3 km<sup>2</sup> during this period. (3) Extensive cropland use changes were taken places in the Wind Drift Sand Region, the Loess Plateau, the Guanzhong Plain and the Hanjiang Basin. Our findings could provide great support for local agriculture management.', 'title': 'Cropland Use Change Analysis in Shaanxi Province of China Based on the Shape-Matching Cropping Index Mapping Method', 'embedding': []}, {'id': 14996, 'abstractText': 'The study involves development of ultrasound-based regularized Nakagami imaging (RNI) that improves the quality of Nakagami image. The feasibility of RNI to image the spatial and temporal evolution of hotspot during microwave (MW) hyperthermia experiment was explored on in-vitro polyacrylamide gel (PAG)-agar based phantoms. The normalized cumulative differential regularized Nakagami (NCDRN) maps were estimated from the envelope of beamformed ultrasound radiofrequency (RF) data using proposed RNI technique. The NCDRN maps were estimated at different time instants for the entire duration of the experiment. The experiments were carried out on phantoms at power level of 12 W fed to the microwave antenna. The contour maps of the NCDRN and the ground truth temperature map, obtained using an infra-red (IR) thermal camera corresponding to ultrasound imaging plane, showed that NCDRN was able to locate the axial and lateral co-ordinates of the hotspot with an error of &lt;; 1.5 mm axially and &lt;; 0.4 mm laterally. This preliminary in-vitro study demonstrates that NCDRN maps estimated using the regularized Nakagami imaging may have potential in imaging and evaluating the spatio-temporal evolution of hotspot and may help in the development of ultrasound-based image guided hotspot monitoring system for microwave hyperthermia.', 'title': 'Regularized Nakagami Ultrasound Imaging for Microwave Hyperthermia monitoring', 'embedding': []}, {'id': 14997, 'abstractText': \"Transcranial Magnetic Stimulation is a noninvasive magnetic stimulation of the motor cortex and motor pathways. Responses obtained from the target muscles through TMS can be evaluated electrophysiologically. There are 20 healthy volunteers who use the right hand actively in the study. There are active right hand 20 healthy volunteers in the study. Two sEMG electrodes are placed in the Abductor Pollicis Brevis (APB) and Orbicularis Oris (O.oris) muscles. APB muscle constitutes a hand representative in the cortical motor mapping. O.Oris muscle constitutes a face representative in the cortical motor mapping. The position, amplitude and latency values of muscular action potentials induced by magnetic stimulation are determined by signal processing methods. The results are mapped on 3-D human model. When the right-hand representation map is examined, the hot spot of the APB muscle is found as the C1. While the motor potential of the hot spot is 6.89(±1.67) mV, the latency time is 20.98(±0.87) ms. When the right-face representation map is examined, the hot spot of the O.Oris muscle's found as the F5. While the motor potential of the hot spot is 2.83(±1.81) mV, the latency time is 4.95(±0.05) ms. Moreover, the face representation intersects with the hand representation at some points and is active at distances close to the hand representation.\", 'title': 'Mapping of Hand and Face Representations Areas in Cerebral Cortex with Transcranial Magnetic Stimulation', 'embedding': []}, {'id': 14998, 'abstractText': 'To evaluate the ESHERSs and determine their efficiency to measure environmental sustainability, we tackle this problem as a classification assignment. This study benchmark three ESHERSs: UI GreenMetric, Times Higher Education Impact ranking, and STARS (Sustainability Tracking, Assessment Rating System) by AASHE (the association for the advancement of sustainability in higher education). Next, we recruited a group of experts who mapped the ESHERS indicators to the SDGs indicators. Then, we use NLP techniques to classify (map) the ESHERS indicators to the SDGs indicators. Since most of the ESHERS indicators and the SDGs indicators are in the form of short text, we use the query expansion technique to make the NLP techniques more effective. Each ESHERS indicator and its expanded text represents a document. And, each SDG indicator and its expanded text represents a document. We took the expanded text from the description of the ESHERS indicators and the description of SDG indicators, forming the corpus for our study. Then, we used document similarity to find the similarity between every pair of the corpus documents. We used different similarity measures to see the similarity between the forms. Then, we used a voting system to map the ESHERSs indicators to the SDGs indicators. The proposed system was able to automatically map the underlying ranking systems indicators to the UN SDGs with 99% accuracy compared to the experts mapping.', 'title': 'Automated Mapping of Environmental Higher Education Ranking Systems Indicators to SDGs Indicators using Natural Language Processing and Document Similarity', 'embedding': []}, {'id': 14999, 'abstractText': 'Human skin temperature mapping provides abundant information of physiological conditions of human body, which provides supplementary or alternative indicators for disease monitoring or diagnosis. The existing models of temperature mapping or temperature field distribution of human skin are generally established by finite element method. Due to the complexity of biological systems, it is challenging to achieve high accuracy mathematical models of temperature field of human skin. The goal of this study is to establish human skin temperature three-dimensional (3-D) mapping platform by integrating optical fibers and improved genetic algorithm-back propagation (GA-BP) neural network. The proposed data-driven method is capable of acquiring entire human skin temperature 3-D mapping by simply measuring a few points on human skin. Multiple experiments were conducted to validate the proposed method on different areas of human skin in different ambient environments. In each experiment setting, the measured data and the model output data were compared. The mean absolute error in all the validation experiments is 0.11 °C, which is lower than that in the state of the art using physical modeling for skin temperature prediction and more close to clinical accuracy. The results show that the proposed approach is accurate and reliable, which may provide a platform technology for human skin temperature mapping that can be used in both medical and scientific studies as well as home monitoring.', 'title': 'An Optical Fiber-Based Data-Driven Method for Human Skin Temperature 3-D Mapping', 'embedding': []}, {'id': 15000, 'abstractText': 'The new era of digitization increases the impact of IT on the overall success of organizations. Even though the business generally recognizes the importance of IT, business-IT alignment is still considered a major challenge by IT executives. A widely adapted Enterprise Architecture Management tool addressing this challenge is the business capability map. However, there are only few approaches specifying the creation of business capability maps. This paper presents a case study describing the initiation of a business capability map at a medium-sized, state-controlled organization following an approach to business capability map implementation presented by The Open Group. Based on the case study, we detail each phase of the approach presenting necessary activities and resulting artifacts. Additionally, lessons learned are presented with the major finding being that an involvement of the whole business leadership leads to a better business-IT alignment, a common language, and a better understanding between all business units. Furthermore, a business capability map provides a suitable tool for structuring strategy development.', 'title': 'Reporting from the Implementation of a Business Capability Map as Business-IT Alignment Tool', 'embedding': []}, {'id': 15001, 'abstractText': 'Paddy rice is one of the most important crops to offer daily food for human beings and has a strong influence on food security and market stability. Therefore, it is of significance to obtain first-hand information related to rice such as rice planting extent. In order to timely and accurately obtain the spatial distribution and temporal variations of rice plants, remote sensing techniques have been introduced and applied to achieve this goal, among which the phenological method is unsupervised and can be used to automatically extract rice. However, the phenological methods are strongly dependent on the weather conditions, and the resultant extraction results might be damaged or incomplete. Additionally, the machine learning-based methods are less influenced by the weather contaminations but are restricted by the availability of training samples. In consideration of the challenges of each approach, this study combines these two approaches to extract rice by using the initial phenology-based rice samples as the training samples of machine learning models, thus obtaining more complete results and high accuracy. This strategy is tested in the Central Valley of California, USA, one of the main rice-producing areas in the United States, and is compared with the rice maps derived from Cropland Data Layer (CDL) from the U.S. Department of Agriculture (USDA). The results show that through the incorporation of the phenology- and machine learning-based methods, the derived rice maps present high consistency with the CDL-rice map, and high accuracy is achieved in terms of OA and Kappa values reaching up to 0.96 and 0.86 respectively, while the purely phenology-based rice map shows great misclassifications thus leading to a sharp decrease in the Kappa value (0.49). This comparison proves the effectiveness of this strategy. The satisfactory results in this study show the possibility of extending this strategy to other regions for rice mapping.', 'title': 'Rice extraction in the Central Valley using multi-temporal Landsat images', 'embedding': []}, {'id': 15002, 'abstractText': \"Freehand gesture interaction has long been proposed as a `natural' input method for Augmented Reality (AR) applications, yet has been little explored for intensive applications like multiscale navigation. In multiscale navigation, such as digital map navigation, pan and zoom are the predominant interactions. A position-based input mapping (e.g. grabbing metaphor) is intuitive for such interactions, but is prone to arm fatigue. This work focuses on improving digital map navigation in AR with mid-air hand gestures, using a horizontal intangible map display. First, we conducted a user study to explore the effects of handedness (unimanual and bimanual) and input mapping (position-based and rate-based). From these findings we designed DiveZoom and TerraceZoom, two novel hybrid techniques that smoothly transition between position- and rate-based mappings. A second user study evaluated these designs. Our results indicate that the introduced input-mapping transitions can reduce perceived arm fatigue with limited impact on performance.\", 'title': 'Augmented Reality Map Navigation with Freehand Gestures', 'embedding': []}, {'id': 15003, 'abstractText': \"Concept map has been widely employed to promote knowledge understanding in various academic disciplines. This study developed an extended open-ended concept map to enhance meaningful learning through two phases of concept map construction that calls Extended Scratch-Build (ESB). The study included software development process, experimental use, and results analysis. The functionality test showed that this learning support application could run properly. We examined that the proposed design of the ESB concept map can enhance meaningful learning. The experimental results also indicated that the ESB concept map had a significant impact on learners' knowledge understanding, which was represented by using the paired t-test statistical analysis on the pre-test and post-test scores.\", 'title': 'Extended Scratch-Build Concept Map to Enhance Meaningful Learning', 'embedding': []}, {'id': 15004, 'abstractText': 'In this article, the issue of soil passability classification is discussed, using field measurements and soil spatial databases. The most detailed cartographic studies available in Poland were used: a soil-agricultural map at scale 1: 25 000. Maps concerning the passability of soils were developed by conducting field measurements with specialized equipment - an electronic cone penetrometer and the type of combat vehicle with the number of its passes in the analysed area. For this purpose, two indicators were used: the Cone Index and the Vehicle Cone Index. The passability of soils was classified into three classes of passability (GO, SLOW-GO and NO-GO TERRAIN). The obtained maps were analysed, which showed very good conditions of passability in the study area. Nevertheless, this condition significantly changes with the increase in the number of passes by the analysed combat vehicles. The article shows that it is possible to use the existing database of soils in Poland and the results of penetrometric measurements to develop maps of passability, taking into consideration soil conditions. The developed maps may provide geospatial support in planning military operations and crisis management.', 'title': 'Soil passability analysis in the open terrain', 'embedding': []}, {'id': 15005, 'abstractText': 'Hyperspectral rare earth elements detection in space borne and near-field acquired images becomes more and more important for global exploration. In comparison to classic exploration methods, the benefit of hyperspectral surveys is the fast and in-situ generation of spatial information. Current hyperspectral investigations do more and more include rare earth element mappings - one new tool for hyperspectral rare earth mapping is the REEMAP algorithm. So far it is trained for five rare earth elements (erbium, dysprosium, holmium, neodymium and thulium). Previous versions of REEMAP did not map samarium. The here presented study focusses on the extension of REEMAP to identify samarium and presents a detailed mapping of the samarium and dysprosium occurrences of a two-carbonatite units containing outcrop (rauhaugites - dolomitic carbonatites and rødbergites - hematitic carbonatites) at Fen Complex, Norway. Four absorption bands of samarium were scrutinized for their shape characteristics in order to extend REEMAP for the detection of samarium. REEMAP was extended with these newly defined filter parameters. The mapping result for the investigated outcrop show that two absorption bands proved to be robust enough to be used in the REEMAP algorithm. The two remaining absorption bands are superimposed by H<sub>2</sub>O absorptions and are therefore not recommended for space borne or near-field hyperspectral analyses. However, the resulting samarium map shows the two-rock units represented by different samarium concentration levels and revealed a gradual increase of samarium towards the top of the rauhaugites rock unit. This study shows that REEMAP can be trained for the detection of samarium, especially for two of the investigated absorption bands (1250 and 1567 nm), and that REEMAP helps for in-situ interpretations of REE ore distributions.', 'title': 'Rare earth element detection from near-field to space - samarium detection using the REEMAP algorithm', 'embedding': []}, {'id': 15006, 'abstractText': \"The purpose of this preliminary study is to introduce and investigate concept map as a tool for new teaching and learning strategy in power engineering course in higher education system. Participants included 45 students from second year in semester one and two topics from the power engineering syllabus are selected as a pilot study. The students were asked to create their own concept map and allowed to have a discussion in a group. To understand the impact of this learning style, questionnaires are designed and distributed among the students to gauge the students' perception towards concept map. The results reveal that majority of the students perceive that concept map is a useful method to get a big picture before engaging to a new topic. Our results indicate this concept map has the potential to improve the teaching and learning style process in power engineering course.\", 'title': 'A Preliminary Study of the Implementation of Concept Maps for Teaching and Learning Strategies in a Power Engineering Course', 'embedding': []}, {'id': 15007, 'abstractText': 'In positron emission tomography (PET), a subject may be scanned two or more times to monitor longitudinal functional changes. These images often appear very similar except for localised, relatively small regions of change. This observation led to the proposal of the maximum a posteriori simultaneous longitudinal reconstruction (MAP-SLR) method, which reconstructs longitudinal datasets together with regularisation to encourage sparse difference images between pairs of scans. In this work we extend MAP-SLR to application to a multi-scan treatment response simulation study. To do this, five 2D [18F]fluorodeoxyglucose head scan datasets (designed to emulate a brain tumour longitudinal study) were simulated and then reconstructed with MAP-SLR. The resulting images were compared to: maximum likelihood expectation-maximisation (MLEM) reconstructions; longitudinally smoothed MLEM reconstructions; and MLEM applied to a reference dataset with five times the number of counts. When using MAPSLR, the noise (in terms of regional coefficient of variation) in a longitudinally unchanging white matter region was reduced and with sufficient regularisation these noise levels approached the high counts reference case. In the tumour, whilst a longitudinal bias is obtained with MAP-SLR, the bias is much smaller than that obtained when performing a noise-matched longitudinal smooth on MLEM reconstructions. With an appropriate level of regularisation the tumour bias is small enough to produce reconstructed images which preserve the longitudinal changes seen in the independent dataset MLEM reconstructions, but with noise reduction of 40% in regions which do not change. The results suggest that MAP-SLR is a simple and effective way of achieving noise reduction in longitudinal PET imaging. Future work will involve application specific testing and investigation into the inclusion of other longitudinally defined penalties into the simultaneous reconstruction.', 'title': 'Longitudinal Multi-Dataset PET Image Reconstruction', 'embedding': []}, {'id': 15008, 'abstractText': 'This paper aims to study the structure of the singular manifold of a fragment of protein backbone using the method of robot kinematics. By modeling chemical bonds as rigid links, and torsional motion around chemical bonds as revolute joints, a fragment of protein backbone turns into a kinematic linkage. The singular manifold of the forward kinematic map of this linkage contains important information about the structure of its inverse kinematics map, and the results also apply to the study of the conformation space of protein loops. By splitting the forward kinematic map into an orientation and position map, we derive equations of the singular manifolds for both maps and analyze their geometric and combinatorial structures. We discuss the application of singular manifolds in several problems of computational biology, including minimal parametrization of protein conformation space and conformation space sampling of protein loops.', 'title': 'Structure of the Singular Manifold of Protein Backbone and Its Applications', 'embedding': []}, {'id': 15009, 'abstractText': 'The amount of scientific publications is believed to get doubled every five-years. These publications are stored by citation indexes and digital libraries in the form of complete PDF or/and by extracting terms from these documents. This indexing behavior poses several challenges for the scientific community as well as for digital repositories in terms of handling the advanced requirements of a user. For instance, addressing queries like “Give me those papers that contain the term “Pagerank” in their result section” may not be answered unless the papers are indexed section-wise. This issue has been focused by researchers and international prestigious challenges by top venues in the world like Semantic Publishing Challenge in ESWC. One of the important metadata extraction from research papers is the section information such as IMRAD (Introduction, Methodology, Results, and Discussion). Researchers have presented different approaches to identify and map the section-headings to IMRAD sections. The existing studies have employed parameters like dictionary terms, the template of a paper, and in-text citation frequency to map section-headings onto logical sections. The critical analysis of state-of-the-art revealed that some immensely potential features have been ignored, which might result in accurate mapping. In this study, we propose a novel approach that employs new features along with previously well-known features to map sections-headings to IMRAD. The newly proposed features are: (1) variant of In-text Citation count (2) Figure counts, (3) Table counts, and (4) subheading implicit mapping. The employed data set contains 5000 research papers, collected from CiteSeer. The evaluation of the proposed approach and comparisons with state-of-the-art three approaches revealed an improvement of 18.96%, 21.77%, and 9.50% in average precision with Ding et al, Shahid et al, and Habib et al. respectively. This research has significant implications for citation indexes and digital libraries.', 'title': 'A Systematic Approach to Map the Research Articles’ Sections to IMRAD', 'embedding': []}, {'id': 15010, 'abstractText': 'Due to the emergence of dual cameras for smart phones in recent years, synthetic refocusing using stereo data has become an important issue. For synthetic refocusing to produce satisfactory results, it is crucial to render the image with high-quality depth map, which is often obtained through a refinement process. However, most existing depth map refinement algorithms pay little attention to problems related to notorious occlusion and loss of image details. In this paper, we study how the quality of depth map affects the performance of synthetic refocusing and, based on the study, propose a new method that integrates depth information and RGB image for synthetic refocusing. A notable feature of our approach is that it formulates occlusion filling as a labeling problem and solves it by multi-layer alpha matting, resulting in a depth map with edges well-aligned with the RGB image in the occluded area and giving synthetic refocusing a realistic sensation. An evaluation of our method against previous methods is performed by comparing the estimated depth map and the refocusing results.', 'title': 'Occlusion-and-Edge-Aware Depth Estimation From Stereo Images for Synthetic Refocusing', 'embedding': []}, {'id': 15011, 'abstractText': 'Mapping lithological units of an area using remote sensing data can be broadly grouped into pixel-based (PBIA), sub-pixel based (SPBIA) and object-based (GEOBIA) image analysis approaches. Since it is not only the datasets adequacy but also the correct classification selection that influences the lithological mapping. This research is intended to analyze and evaluate the efficiency of these three approaches for lithological mapping in semi-arid areas, by using Sentinel-2A data and many algorithms for image enhancement and spectral analysis, in particular two specialized Band Ratio (BR) and the Independent component analysis (ICA), for that reason the Paleozoic Massif of Skhour Rehamna, situated in the western Moroccan Meseta was chosen. In this study, the support vector machine (SVM) that is theoretically more efficient machine learning algorithm (MLA) in geological mapping is used in PBIA and GEOBIA approaches. The evaluation and comparison of the performance of these different methods showed that SVM-GEOBIA approach gives the highest overall classification accuracy (OA ≈ 93%) and kappa coefficient (K) of 0, 89, while SPBIA classification showed OA of approximately 89% and kappa coefficient of 0, 84, whereas the lithological maps resulted from SVM-PBIA method exhibit salt and pepper noise, with a lower OA of 87% and kappa coefficient of 0, 80 comparing them with the other classification approaches. From the results of this comparative study, we can conclude that the SVM-GEOBIA classification approach is the most suitable technique for lithological mapping in semi-arid regions, where outcrops are often inaccessible, which complicates classic cartographic work.', 'title': 'Pixel and Object-Based Machine Learning Classification Schemes for Lithological Mapping Enhancement of Semi-Arid Regions Using Sentinel-2A Imagery: A Case Study of the Southern Moroccan Meseta', 'embedding': []}, {'id': 15012, 'abstractText': 'The disastrous effects of global warming urge developed and developing countries to invest in Information Technology solutions to reduce flood risks. More recently, flood risk and hazard mapping system is slowly adopted in the Philippines, a developing country. However, acceptance of flood risk and hazard mapping system in the Philippines is underrepresented in the literature. Hence, this paper aims to present users’ acceptance of flood risk and hazard mapping system by using the Technology Acceptance Model, through a survey conducted among different users. Results of the study show an affirmative acceptance of flood risk and hazard mapping system. Also, the relationships between perceived usefulness and behavioral intention, perceived ease of use and behavioral intention, perceived ease of use and perceived usefulness, relevance and perceived usefulness, domain knowledge and perceived usefulness, self-efficacy and perceived ease of use, computer literacy and perceived ease of use are all observed. This study contributes by providing the first empirical evidence of investigation on acceptance of flood risk and hazard mapping system in the Philippines.', 'title': 'User Acceptance of Flood Risk and Hazard Mapping System: A Field Survey in the Philippines', 'embedding': []}, {'id': 15013, 'abstractText': 'Autonomous driving in an urban environment with surrounding agents remains challenging. One of the key challenges is to accurately predict the traversability map that probabilistically represents future trajectories considering multiple contexts: inertial, environmental, and social. To address this, various approaches have been proposed; however, they mainly focus on considering the individual context. In addition, most studies utilize expensive prior information (such as HD maps) of the driving environment, which is not a scalable approach. In this study, we extend a deep inverse reinforcement learning-based approach that can predict the traversability map while incorporating multiple contexts for autonomous driving in a dynamic environment. Instead of using expensive prior information of the driving scene, we propose a novel deep neural network to extract contextual cues from sensing data and effectively incorporate them in the output, i.e., the reward map. Based on the reward map, our method predicts the ego-centric traversability map that represents the probability distribution of the plausible and socially acceptable future trajectories. The proposed method is qualitatively and quantitatively evaluated in real-world traffic scenarios with various baselines. The experimental results show that our method improves the prediction accuracy compared to other baseline methods and can predict future trajectories similar to those followed by a human driver.', 'title': 'Incorporating Multi-Context Into the Traversability Map for Urban Autonomous Driving Using Deep Inverse Reinforcement Learning', 'embedding': []}, {'id': 15014, 'abstractText': \"Short grass grazing lawn patches are significant components of habitat heterogeneity in southern African savannah ecosystems. Accurate maps of grazing lawn distribution is essential to enhance understanding of important ecosystem processes such as mega-herbivore population dynamics, nutrient cycling and plant community composition. The inherent heterogeneity of savannah landscapes however creates significant challenges for accurate discrimination of vegetation components and thus grazing lawn detection. Recent studies favour very high spatial resolution (VHR) multi-spectral imagery for dealing with this challenge. However, such data are costly for use in operational management. Planet Labs, through Norway's International Climate and Forests Initiative (NICFI), now grant free access to high-resolution, analysis-ready mosaics over the tropics, with great potential for fine-scale vegetation mapping. However, the spectral characteristics of these data are limited and fail to resolve the spectral similarity of different savannah vegetation components. We address these issues using Gram-Schmidt transformation to fuse Planet Basemaps and Sentinel-2A images for grazing lawn detection within the Lower Sabie region of Kruger National Park, South Africa. The original and fused images were classified using a random forest approach. Overall, the fused image achieved the best grazing lawn detection accuracy (0.85) and general map accuracy (0.72) results compared to Sentinel-2 (0.67 and 0.62) and Planet basemap (0.64 and 0.62 respectively). Our findings provide a foundation for cost-effective and accurate high spatial resolution vegetation mapping in heterogenous savannah landscapes. Further studies will investigate the potential of multi-temporal fused data and object-based approaches for enhanced savannah vegetation mapping\", 'title': 'Fusion of Sentinel-2 Data with High Resolution Open Access Planet Basemaps for Grazing Lawn Detection in Southern African Savannahs', 'embedding': []}, {'id': 15015, 'abstractText': \"Path planning in a dynamic environment is required not only efficiently but also with high safety. To achieve this, it is necessary to understand people's walking activity in the space, and studies of planning paths focused on walking paths of pedestrian have been widely done. In these studies, information getting from walking paths does not include spatial features of people's walking activity such as dynamics of their walking. A walking behavior is not always smooth from the viewpoints of changes of walking speed and moving directions. For example, people should accelerate or decelerate their walking speed according to environmental settings or their purposes of walking. In addition, moving directions of people's walking also depend on the environmental structures and settings. Therefore, people's walking activity should be understood including walking dynamics. To apply the information of the walking activity to the path planning for mobile robot navigation, there are two requirements; one is that the information should be described in form that a robot system is able to use, and another one is that it should be described without lacking spatial information. To realize these, we propose that an environmental map describing the walking activity of people including walking dynamics. More specifically, we build the environmental map as a grid map that is one of the often-used maps for a mobile robot. From the results of the experiment, it was shown that the proposed environmental map was possible to determine spatial features.\", 'title': 'Environmental Map Building to Describe Walking Dynamics for Determination of Spatial Feature of Walking Activity', 'embedding': []}, {'id': 15016, 'abstractText': 'Speaking style conversion (SSC) is the technology of converting natural speech signals from one style to another. In this study, we aim to provide a general SSC system for converting styles with varying vocal effort and focus on normal-to-Lombard conversion as a case study of this problem. We propose a parametric approach that uses a vocoder to extract speech features. These features are mapped using parallel machine learning models from utterances spoken in normal style to the corresponding features of Lombard speech. Finally, the mapped features are converted to a Lombard speech waveform with the vocoder. A total of three vocoders (GlottDNN, STRAIGHT, and Pulse model in log domain (PML)) and three machine learning mapping methods (standard GMM, Bayesian GMM, and feed-forward DNN) were compared in the proposed normal-to-Lombard style conversion system. The conversion was evaluated using two subjective listening tests measuring perceived Lombardness and quality of the converted speech signals, and by using an instrumental measure called Speech Intelligibility in Bits (SIIB) for speech intelligibility evaluation under various noise levels. The results of the subjective tests show that the system is able to convert normal speech into Lombard speech and that there is a trade-off between quality and Lombardness of the mapped utterances. The GlottDNN and PML stand out as the best vocoders in terms of quality and Lombardness, respectively, whereas the DNN is the best mapping method in terms of Lombardness. PML with the standard GMM seems to give a good compromise between the two attributes. The SIIB experiments indicate that intelligibility of converted speech compared to that of normal speech improved in noisy conditions most effectively when DNN mapping was used with STRAIGHT and PML.', 'title': 'Vocal Effort Based Speaking Style Conversion Using Vocoder Features and Parallel Learning', 'embedding': []}, {'id': 15017, 'abstractText': 'Semantic Mapping is a semantics-based interactive system that enables intuitive virtual content placement for projection mapping in intelligent environments. Our semantic mapping system embeds semantic information of the environment to provide a user with an easy way to control and place projected virtual items in the physical world. In contrast to traditional projection mapping that involves manual adjustments, this semantic mapping system enables efficient manipulation of virtual content through inputs from users via speech or text. To build the system, we first use a commercial depth camera for scene reconstruction and an end-to-end deep learning framework for semantic segmentation at the instance level. We illustrate the system by developing a prototype for a set of proof-of-concept, room-scale applications. The accuracy study and user study results show that the system can provide users with accurate semantic information for effective virtual content placement.', 'title': 'Semantic Mapping: A Semantics-based Approach to Virtual Content Placement for Immersive Environments', 'embedding': []}, {'id': 15018, 'abstractText': 'In this paper, the co-moving Lyapunov exponent estimation problem for a unidirectionally traffic coupled map lattice model with hyperbolic tangent local map is studied. The traffic behavior in the coupled map lattice model shows nonlinear characteristics similar to the car-following model. The nonlinear feedback method is used to study the control of the chaotic system of the unidirectionally traffic coupled map lattice model. The stability of spatiotemporal chaos in the coupled map lattice is realized. The results of numerical simulation show that there is a relationship between control results and control parameters when controlling spatiotemporal chaos to a uniform stable state in a certain phase space compression parameter region.', 'title': 'Spatiotemporal Chaos and Control of a Unidirectionally Traffic Coupled Map Lattice Model', 'embedding': []}, {'id': 15019, 'abstractText': 'The generation of benthic habitat maps relies either on direct in-situ observations made by SCUBA divers swimming in a rectilinear fashion, or on costly remote sensing techniques involving either ROVs or sonar technology. The recent commercialisation of off-the-shelf underwater drones has enhanced benthic mapping possibilities by providing a cost-effective alternative. Despite still requiring ground truthing, such drones do not rely extensively on boat-support services. In this study, the applicability and feasibility of using an underwater high-resolution optical platform to automate the generation of benthic maps is investigated. A PowerVision PowerRay [1] was used to capture underwater imagery in an embayment along the north-east coast of the island of Malta (central Mediterranean). A Machine Learning method based on Self-Organizing Maps was then implemented to automate the classification process. Results produced from this technique were evaluated in terms of their accuracy through comparisons with a benthic habitat map of the same area that was generated through conventional means in a previous study.', 'title': 'Automatic Benthic Habitat Mapping using Inexpensive Underwater Drones', 'embedding': []}, {'id': 15020, 'abstractText': \"We show how a model of visual salience that was originally developed to explain human visual search performance can suggest display design choices that reduce search time for items. The statistical saliency model proposes that the time to find an item on a visual display depends on the similarity between a target item's features and the statistical distribution of display features. In the present study, observers rated the amount of display clutter on a set of MapQuest maps containing colored pushpins. We identified a group of “high-clutter” maps and a group of “low-clutter” maps. Next, we used the statistical saliency model to choose colors for new pushpins placed on those maps. We show that the model's color assignments depend on the colors the display contains. Map designs produced using this method were tested in a visual search experiment. Search time decreased as a pushpin's predicted salience increased. In addition, choosing low salience colors led to slower search times for items on high-clutter displays than for items on low-clutter displays. The method we describe works with real images and does not require any parameter fitting. This study provides evidence that computational models of visual perception have potential as display design tools.\", 'title': 'The Statistical Saliency Model Can Choose Colors for Items on Maps', 'embedding': []}, {'id': 15021, 'abstractText': 'The increase in the crime rate numbers and a rise in the need to find better solutions to handle information about criminality is affected by the ever-changing socio-economical order of the world. Despite the number of solutions implemented for reducing crime (against women), cities continue to have an unsafe environment. The prime drawback lies in the inability to provide a prompt response in real-time when in danger. Thus, the effective utilization of technology in public safety management is important. The present state of the art solutions focus on technological innovations with limited human intervention and are insufficient in ensuring the safety of the women as and when required. To dig deeper into the root cause of preventing a crime from occurring in a particular place, it is vital to analyze the parameters and factors contributing to the crime in a community. This research applies the Information Communication Technologies (ICT) along with harnessing big data tools to identify crime hotspots and patterns. After a comprehensive literature review, it has been noted that there are different social-economic factors affecting the crime in an area. The proposed work aims to integrate the socio-economic attributes leading to increasing crime against women. Interpolation strategies used for thematic maps generation also play a major role in predicting and studying the area affected by a crime. This research initially identifies the various social-economical parameters that affect crime against women. Some of them to mention include unemployment, illiteracy, population, sex ratio, traffic, age, no. of schools, and location of liquor shops. Subsequently, a comparison of major interpolation methods used in crime mapping: Inverse Distance Weighted (IDW), Kriging, and Spline are formulated to understand the overall contribution of socio-economic factors on the crime thematic map to further ascertain if one parameter poses substantially more important than the other. The comparison of different Interpolation techniques used in pixel by pixel error analysis on high definition satellite images of the crime site, of resolution as high as 2.5m × 2.5m, is created using visualization libraries like Matplotlib and Seaborn. Finally, the thematic maps are created using the best Interpolation technique chosen and help in predicting the pattern of the crime. The proposed framework developed using Geographic Information System (GIS) based visualization and big data tools for crime mapping can then be applied in the development of user interactive platforms and designing safety strategies to help the needy in real-time. To validate the methodology, a case study is performed with real data, in the Jhunjhunu district of Rajasthan, India.', 'title': 'Comparison of Different Spatial Interpolation Techniques to Thematic Mapping of Socio-Economic Causes of Crime Against Women', 'embedding': []}, {'id': 15022, 'abstractText': 'Visual relation detection methods rely on object information extracted from RGB images such as 2D bounding boxes, feature maps, and predicted class probabilities. We argue that depth maps can additionally provide valuable information on object relations, e.g. helping to detect not only spatial relations, such as standing behind, but also non-spatial relations, such as holding. In this work, we study the effect of using different object features with a focus on depth maps. To enable this study, we release a new synthetic dataset of depth maps, VG-Depth, as an extension to Visual Genome (VG). We also note that given the highly imbalanced distribution of relations in VG, typical evaluation metrics for visual relation detection cannot reveal improvements of under-represented relations. To address this problem, we propose using an additional metric, calling it Macro Recall@K, and demonstrate its remarkable performance on VG. Finally, our experiments confirm that by effective utilization of depth maps within a simple, yet competitive framework, the performance of visual relation detection can be improved by a margin of up to 8%.', 'title': 'Improving Visual Relation Detection using Depth Maps', 'embedding': []}, {'id': 15023, 'abstractText': 'Cropping patterns are closely related to food production, cropland intensification, water resource management, greenhouse gas emission and regional climate alteration. Timely and accurate mapping of cropping patterns is urgently needed in many disciplines. However, the existing cropland-related datasets are informative at the global level, but lack regional-scale details about cropland utilizations. Thus, there is a need for better information on the area and distribution of cropping patterns at regional scales. In this study, we developed a phenology-based cropping pattern (PBCP) mapping method based on remote sensing vegetation index time series. The new method first extracted vegetation phenological metrics (start of season (SOS), end of season (EOS), growing season length (GSL) and growth amplitude (GA)) from the vegetation index time series. Then, it identified crop seasons by using the minimum crop GSL, the minimum crop GA and the maximum crop GSL, which were derived from the training samples. Finally, cropping patterns were classified based on a set of decision rules. The case study in Henan province of China showed that, the results indicated that: (1) compared with cropping index derived from the supervised classification of Landsat-5 TM images, the PBCP method provided cropping index with satisfactory accuracy of 85.3%. (2) Validation sample analysis indicated that the cropping pattern mapping accuracy was 84% for the PBCP method. Different to current cropping pattern mapping methods, the PBCP method considered crop planting information in three years in deciding the cropping pattern to map the dominant cropping patterns. It can provide new insights in agriculture related land use analysis.', 'title': 'A Phenology-Based Cropping Pattern (PBCP) Mapping Method Based on Remotely Sensed Time-Series Vegetation Index Data', 'embedding': []}, {'id': 15024, 'abstractText': \"The ocean temperature front is a narrow transition zone where the temperature changes dramatically, which can be described by gradient of temperature. The temporal and spatial variations and the patterns of ocean fronts are of great concern to researchers by tedious observation and comparison of many spatial distribution maps of ocean front at different moments. However, a particular number of spatial states may only reflect certain spatial or temporal aspects of ocean front. This study designed a collaborative interactive visualization system to simultaneously integrate temporal and spatial analysis of ocean fronts with experts' knowledge, obtaining higher analysis efficiency and more comprehensiveness. The interactive statistical charts facilitate focus + context selection of points of interest in time and space, while the interactive Map-View and Map-Gallery support spatial analysis from overview to details. Moreover, this paper uses an unsupervised learning model named Self-Organizing Mapping network (SOM) to conduct spatio-temporal cluster analysis on different ocean fronts near the China Sea. The clustering results can be customized by user's colors specification, evaluated and interactively adjusted by researchers' knowledge. The spatio-temporal patterns of clustering result can easily mined by collaborative linkage of multi-graphs including unified distance matrix (U-Matrix), component plane, feature parallel coordinates plot, Map-View and other charts. The effectiveness and usability of the proposed system are demonstrated with two case studies.\", 'title': 'OFViser: An Interactive Visual System for Spatiotemporal Analysis of Ocean Front', 'embedding': []}, {'id': 15025, 'abstractText': 'Purpose: Explore the overview of dust and heavy metal research in typical regions of China and present them in the form of knowledge maps. Through in-depth research and visual analysis, a deeper understanding of the research hotspots of dust and heavy metals is expected to provide some help for later researchers. Method: The key words of “heavy metal exposure” and “heavy metals, dust” were searched in the Chinese Journal Full-text Thematic Database (CNKI), and the relevant literature on dust and heavy metals was gradually selected through extensive reading and intensive reading according to the literature inclusion and exclusion criteria. Relevant documents were summarized and compiled using NoteExpress software. General processing of data was performed by Ucinet and Netdraw software, and the correlation analysis was performed by SPSS25.0 and Stata software. Finally, RevMan software was selected to complete the meta-analysis. Result: A total of 815 documents were hit, and 67 documents were successfully selected, including 4063 sample sizes. A knowledge visualization map of dust and heavy metal health risk assessment in typical domestic regions was established, and the knowledge distribution of dust and heavy metal research was visually displayed and formed a focus. By studying clustering, we obtained the network map of keyword. Through Meta analysis, the analysis results of typical forest map and funnel map are obtained. Conclusion: For the study of heavy metals in dust, the research data in the Southeast area is relatively abundant. According to the results of the meta-analysis, the health risk assessment of typical dust heavy metals in basic research is credible. Degree is higher.', 'title': 'Meta Analysis of Dust Heavy Metals based on Knowledge Map Visualization', 'embedding': []}, {'id': 15026, 'abstractText': 'Intracranial hypertension is an acute, life-threatening neurological condition that can lead to high risk of mortality. Its prompt identification and timely management are key to functional recovery and resuscitation of the patient. The objective of the present study is to propose quantitative measures for the early assessment of intracranial hypertensive (IH) episodes in traumatic brain injured (TBI) patients and to explore the association between intra-individual variability and IH events. To achieve this, we identified fifty-nine IH events in twelve TBI patients, and analyzed intracranial pressure (ICP), mean arterial pressure (MAP) and heart rate (HR). The notion of Granger causal (GC) analysis was adopted to quantify the bi-directional information flow patterns among ICP, MAP and HR. Additionally, the coefficient of variations of GC values was estimated to quantify intra-individual variations. The present study shows that GC values of ICP-to-MAP, MAP-to-ICP and HR-to-ICP decrease during an IH event while the GC value of HR-to-MAP increases during an IH event. Moreover, it was also observed that TBI patients show more inconsistency during ICP elevations. Our findings suggest that directional communications across cardiovascular (MAP and HR) and cerebrovascular (ICP) mechanisms are associated with the onset of intracranial hypertension. These derived GC measures may also be utilized as functional bio-markers in physiological diagnostics.', 'title': 'Variations in Information Flow Patterns Following Intracranial Hypertensive Events in Traumatic Brain Injured Patients', 'embedding': []}, {'id': 15027, 'abstractText': \"Concept mapping is one of the methods of knowledge mapping. The existing studies portray that concept mapping can be very useful for improving students' learning and achievements. Many engineering students taking accounting course find it challenging to understand the accounting topics and it is difficult for them to achieve good results in this course. This study is aimed to review the available literature on concept mapping and its use in teaching accounting to engineering students. Forward and backward snowballing technique has been used to find the relevant literature on concept mapping. The available literature has been divided into three categories. The review results depicted a profound research gap regarding the research questions raised.\", 'title': 'Teaching accounting to engineering students and the use of concept mapping: a preliminary review', 'embedding': []}, {'id': 15028, 'abstractText': 'In this paper, we consider the transformation of laser range measurements into a top-view grid map representation to approach the task of LiDAR-only semantic segmentation. Since the recent publication of the SemanticKITTI data set, researchers are now able to study semantic segmentation of urban LiDAR sequences based on a reasonable amount of data. While other approaches propose to directly learn on the 3D point clouds, we are exploiting a grid map framework to extract relevant information and represent them by using multi-layer grid maps. This representation allows us to use well-studied deep learning architectures from the image domain to predict a dense semantic grid map using only the sparse input data of a single LiDAR scan. We compare single-layer and multi-layer approaches and demonstrate the benefit of a multi-layer grid map input. Since the grid map representation allows us to predict a dense, 360° semantic environment representation, we further develop a method to combine the semantic information from multiple scans and create dense ground truth grids. This method allows us to evaluate and compare the performance of our models not only based on grid cells with a detection, but on the full visible measurement range.', 'title': 'Exploiting Multi-Layer Grid Maps for Surround-View Semantic Segmentation of Sparse LiDAR Data', 'embedding': []}, {'id': 15029, 'abstractText': 'Retinotopic mapping, the mapping of visual input on the retina to cortical neurons, is an important topic in vision science. Typically, cortical neurons are related to visual input on the retina using functional magnetic resonance imaging (fMRI) of cortical responses to slowly moving visual stimuli on the retina. Although it is well known from neurophysiology studies that retinotopic mapping is locally diffeomorphic (i.e., smooth, differentiable, and invertible) within each local area, the retinotopic maps from fMRI are often not diffeomorphic, especially near the fovea, because of the low signal-noise ratio of fMRI. The aim of this study is to develop and solve a mathematical model that produces diffeomorphic retinotopic mapping from fMRI data. Specifically, we adopt a geometry concept, the Beltrami coefficient, as the tool to define diffeomorphism, and model the problem in an optimization framework. Efficient numerical methods are proposed to solve the model. Experimental results with both synthetic and real retinotopy data demonstrate that the proposed method is superior to conventional smoothing methods.', 'title': 'Diffeomorphic Smoothing for Retinotopic Mapping', 'embedding': []}, {'id': 15030, 'abstractText': 'In the aftermath of a disaster, there is an urgent need for base maps to support relief efforts, especially in developing countries. In response to this, the OpenStreetMap project has been leveraged to produce maps of disaster-affected areas in a collaborative way. However, there has been little investigation aimed at explaining the collaborative mapping activity itself. This study presents an exploratory case study on how the collaborative mapping activities that followed the Nepal Earthquake in 2015 were coordinated and structured, i.e. how volunteers were organized, and what were the main outcomes of their activity in the context of disaster management. The results show that a large number of remote contributors spread across the world carried out concerted efforts to support the relief work. Moreover, coordination mechanisms were used by local actors to share their knowledge with remote mappers, and, hence, to improve the accuracy of the map.', 'title': 'Potential of Collaborative Mapping for Disaster Relief: A Case Study of OpenStreetMap in the Nepal Earthquake 2015', 'embedding': []}, {'id': 15031, 'abstractText': 'It is important to reduce the human influence on nature resources by identifying an appropriate land use. Moreover, it is essential to carry out scientific land evaluation. Such kind of analysis allows identifying the main factors for agricultural production and enables decision makers to develop crop managements in order to increase the land capability. The key is to match the type of intensity of land use with its natural capability. Therefore; in order to benefit from these areas and invest them to obtain good agricultural production, they must be organized and managed in full. Lebanon suffers from the unorganized agricultural use. We take South Lebanon as a study area; it is the most fertile ground and has a variety in crops. The study aims to identify and locate the most suitable area to cultivate thirteen types of permanent trees which are: apples, avocadoes, stone fruits in coastal regions and stone fruits in mountain regions, bananas, citrus, loquats, figs, pistachios, mangoes, olives, pomegranates and grapes. Several geographical factors are taken as criterion for selection of the best location to cultivate. Soil, rainfall, PH, temperature, and elevation, are main input to create final map. Input data of each factor is managed, visualized and analyzed using Geographical Information System (GIS). Managements GIS tools were implemented to produce input maps capable of identifying suitable areas related to each index. The combination of the different indices map generates the final output map of the suitable place to get best permanent tree productivity. The output map is reclassified into three suitability classes: low, moderate, and high suitability. Results show different locations suitable for different kinds of trees. Results also reflected the importance of GIS in helping decision makers finding a most suitable location for every tree to get more productivity and a verity in crops.', 'title': 'Geographic Information System-Based Map for Agricultural Management in South-Lebanon', 'embedding': []}, {'id': 15032, 'abstractText': 'We present a user study comparing a pre-evaluated mapping approach with a state-of-the-art direct mapping method of facial expressions for emotion judgment in an immersive setting. At its heart, the pre-evaluated approach leverages semiotics, a theory used in linguistic. In doing so, we want to compare pre-evaluation with an approach that seeks to directly map real facial expressions onto their virtual counterparts. To evaluate both approaches, we conduct a controlled lab study with 22 participants. The results show that users are significantly more accurate in judging virtual facial expressions with pre-evaluated mapping. Additionally, participants were slightly more confident when deciding on a presented emotion. We could not find any differences regarding potential Uncanny Valley effects. However, the pre-evaluated mapping shows potential to be more convenient in a conversational scenario.', 'title': 'Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion', 'embedding': []}, {'id': 15033, 'abstractText': \"Objective: Tumor stiffening in pancreatic adenocarcinoma (PDAC) has been linked to cancer progression and lack of therapy response, yet current elastography tools cannot map stiffness in a whole tumor field-of-view with biologically relevant spatial resolution. Therefore, this study was developed to assess stiffness heterogeneity and geometrical patterns across whole PDAC xenograft ex vivo tumors. Methods: The ex vivo elastography (EVE) mapping system was capable of creating stiffness map at 300-micron spatial resolution under a 5-20 mm field of view relevant to whole tumor assessment. The stiffness value at each location was determined by compression testing and an absolute tumor Young's modulus map was calculated based on the calibration between the system and ultrasound elastography (R<sup>2</sup> = 0.95). Results: Two PDAC tumor lines AsPC-1 and BxPC-3 implanted in xenograft models were assessed to show tumor stiffness and its linear relationship to collagen content (R<sup>2</sup> = 0.59). EVE was able to capture stiffness heterogeneity ranging between 5 and 100 kPa in pancreatic tumors with collagen content up to 25%. More importantly, data shows the inverse relationship of local stiffness to local drug distribution (R<sup>2</sup> = 0.66) and vessel patency (R<sup>2</sup> = 0.61) in both PDAC tumor lines. Conclusion: The results suggested that elastography could be utilized to predict drug penetration in PDAC tumors or assess response to biological modifying adjunct therapies. Significance: This study presents the first attempt to map out stiffness on a biologically relevant spatial scale across whole PDAC tumor slices with spatial resolution in the hundreds of microns.\", 'title': 'High-Resolution <italic>Ex Vivo</italic> Elastography to Characterize Tumor Stromal Heterogeneity In Situ in Pancreatic Adenocarcinoma', 'embedding': []}, {'id': 15034, 'abstractText': \"Recently, super-resolution techniques have been energetically studied for the purpose of reusing the low resolution image contents. Although a lot of approaches to achieve the appropriate super-resolution have been proposed such as non-linear filtering, total variation regularization, deep learning etc., the characteristic of the viewpoint distribution of the observer has not been effectively utilized. Because applying super-resolution to unimportant regions in an image may hinder the observer's attention to seeing the display, it leads to a low subjective evaluation. This paper proposes the region-wise super-resolution algorithm based on the view-point distribution of observer. However, we cannot obtain the viewpoint distribution map for an image without the pre-experiment using the device such as eye mark recorder, therefore, the saliency map is utilized in this paper. Numerical examples show that the proposed algorithm using saliency map achieves a higher subjective evaluation than the previous study based on the non-linear filtering based super-resolution. Furthermore, in numerical examples, the proposed algorithm using the saliency map is shown to give the similar results of the algorithm using the viewpoint distribution map obtained by the pre-experiment using eye mark recorder.\", 'title': 'Region-Wise Super-Resolution Algorithm Based On the Viewpoint Distribution', 'embedding': []}, {'id': 15035, 'abstractText': 'In this paper, we propose and evaluate a simple mechanism to accelerate iterative machine learning algorithms implemented in Hadoop map-reduce (stock), and Apache Spark. In particular, we describe a technique that enables data parallel tasks in map-reduce and Spark to be dynamically and adaptively scheduled on CPU or GPU, based on availability and load. We examine the extent of performance improvements, and correlate them to various parameters of the algorithms studied. We focus on end-to-end performance impact, including overheads associated with transferring data into and out of the GPU, and conversion between data representations in the JVM and on GPU. We also present three optimizations that, in our analysis, can be generalized across many iterative machine learning applications. We present a case study where we accelerate four iterative machine learning applications - multinomial logistic regression, multiple linear regression, K-Means clustering and principal components analysis using singular value decomposition, implemented in three data analytics frameworks - Hadoop Map-Reduce (HMR), IBM Main-Memory Map-Reduce (M3R) and Spark. We observe that the use of GPGPUs decreases the execution time of these applications on HMR by up to 8X, M3R by up to 18X, and Spark by up to 25X. Through our empirical analysis, we offer several insights that can be helpful in designing middleware and cluster managers to accelerate map-reduce and Spark applications using GPUs.', 'title': 'Adaptively Accelerating Map-Reduce/Spark with GPUs: A Case Study', 'embedding': []}, {'id': 15036, 'abstractText': 'Extracting information about variations within urban areas using satellite imagery has generally focused on mapping individual buildings or slum versus non-slum areas. While these data are useful, they can run into issues in very dense urban areas, additionally slums have a subjective definition. In previous research we have found that contextual features are related to population, census variables, poverty, and other values, but have not explored which urban attributes (i.e., buildings and roads) these features represent. In this study we seek to determine the correlation between contextual features calculated on Very High Spatial Resolution (VHSR) satellite data and urban attributes derived from Open Street Map (OSM) for portions of multiple cities in Sri Lanka. Results indicate that individual contextual features are highly correlated with building area, building density, road area, road density, total built up areas and other features. Moreover, when multiple contextual features are combined within a model, they can explain from 70 to 92 percent of the variance of these urban features within the study area. This indicates that contextual features are very strong indicators of urban variability and can be used to map differences within the urban setting. This may allow us to forgo having to map each building and road individually for mapping urban areas in future projects.', 'title': 'Evaluating the Relationship Between Contextual Features Derived from Very High Spatial Resolution Imagery and Urban Attributes: A Case Study in Sri Lanka', 'embedding': []}, {'id': 15037, 'abstractText': \"Recently, various applications including data analytics and machine learning have been developed for geo-distributed cloud data centers. For those applications, the ways of mapping parallel processes to physical nodes (i.e., “process mapping”) could significantly impact the performance of the applications because of non-uniform communication cost in geo-distributed environments. What's more, the different data privacy requirements in geo-distributed data centers pose additional constraints on process mapping solutions. While process mapping has been widely studied in grid/cluster environments, few of the existing studies have considered the problem in geo-distributed cloud environment, which is a challenging task due to the multi-level data privacy constraints, heterogeneous network performance and process failures. In this paper, we introduce the special privacy requirements in geo-distributed data centers and formulate the geo-distributed process mapping problem as an optimization problem with multiple constraints. We develop a new method to efficiently find good process mapping solutions to the problem. Experimental results on real clouds (including Amazon EC2 and Windows Azure) and simulations demonstrate that our proposed approach can achieve significant performance improvement compared to the state-of-the-art algorithms.\", 'title': 'Privacy Regulation Aware Process Mapping in Geo-Distributed Cloud Data Centers', 'embedding': []}, {'id': 15038, 'abstractText': 'IONOLAB is an interdisciplinary research group dedicated for handling the challenges of near earth environment on communication, positioning and remote sensing systems. IONOLAB group contributes to the space weather studies by developing state-of-the-art analysis and imaging techniques. On the website of IONOLAB group, www.ionolab.org, four unique space weather services, namely, IONOLAB-TEC, IRI-PLAS-2015, IRI-PLAS-MAP and IRI-PLAS-STEC, are provided in a user friendly graphical interface unit. Newly developed algorithm for ionospheric tomography, IONOLAB-CIT, provides not only 3-D electron density but also tracking of ionospheric state with high reliability and fidelity. The algorithm for ray tracing through ionosphere, IONOLAB-RAY, provides a simulation environment in all communication bands. The background ionosphere is generated in voxels where IRI-Plas electron density is used to obtain refractive index. One unique feature is the possible update of ionospheric state by insertion of Total Electron Content (TEC) values into IRI-Plas. Both ordinary and extraordinary paths can be traced with high ray and low ray scenarios for any desired date, time and transmitter location. 2-D regional interpolation and mapping algorithm, IONOLAB-MAP, is another tool of IONOLAB group where automatic TEC maps with Kriging algorithm are generated from GPS network with high spatio-temporal resolution. IONOLAB group continues its studies in all aspects of ionospheric and plasmaspheric signal propagation, imaging and mapping.', 'title': 'Space weather studies of IONOLAB group', 'embedding': []}, {'id': 15039, 'abstractText': \"As a meta-heuristic algorithm that simulates the intelligence of gray wolves, grey wolf optimizer (GWO) has a wide range of applications in practical problems. As a kind of local search, chaotic local search (CLS) has a strong ability to get rid of the local optimum due to its integration of chaotic maps. To enhance GWO, CLS is always incorporated into GWO to increase its population diversity and accelerate algorithm's convergence. However, it is still unclear that how may chaotic maps should be used in CLS and how to embed them into GWO. To address these challenging issues, this paper studies both single and multiple chaotic maps incorporated GWOs. Extensive comparative experiments are conducted based on IEEE Congress on Evolutionary Computation (CEC) benchmark test suit. The results show that CLS incorporated GWOs generally perform better than the original GWO, suggesting the effectiveness of such hybridization. Moreover, a remarkable finding of this work is that the piecewise linear chaotic map (PWLCM) and Gaussian map have the most potential to improve the search performance of GWO. Additionally, CLS incorporated GWOs also perform significantly better than some other state-of-the-art meta-heuristic algorithms. This study not only gives more insights into the mechanism of how CLS makes influence on GWO, but also finds that the most suitable choice of chaotic map for it.\", 'title': 'Comparative Study on Single and Multiple Chaotic Maps Incorporated Grey Wolf Optimization Algorithms', 'embedding': []}, {'id': 15040, 'abstractText': 'This chapter demonstrates methods for estimating population and assessing urban environmental quality (UEQ) through case studies. The population estimation study intended to combine the statistical‐based and dasymetric‐based methods and to redistribute census population. The case study of population estimation aimed to compare the effectiveness of the spectral response‐based and the land‐use (LU)‐based methods for population estimation of US census block groups and to produce a more accurate presentation of population distribution by combining the dasymetric mapping with LU‐based methods. The UEQ study intended to evaluate the UEQ changes from 1990 to 2000 in Indianapolis, Indiana, using the integrated techniques of remote sensing and geographic information system. Specifically, the chapter intends to derive an UEQ index based on the synthetic indicators of physical variables extracted from Landsat images and socioeconomic variables derived from US census data and to develop a new method for assessing UEQ change over the time.', 'title': 'Remote Sensing of Socioeconomic Attributes', 'embedding': []}, {'id': 15041, 'abstractText': 'Digital markets and digitalization were developed and were driven by online shopping, consumer lifestyles, and entrepreneurial spirit. It has been research yet the notion about digital market review study which showed the big picture using data from all countries. This research aims to map the status of the digital market studies indexed Scopus using the bibliometrics approach. The study conducted the bibliometrics method and analyzed research data using the service of analyzing search results from Scopus and the VOSviewer application. Research data of 337 academic documents published from 1976 to 2019 obtained from the Scopus database in March 2020. The results revealed about the most productive countries, research institutes, and individual researchers in digital market studies are the United States, Yale University, and Alexander Schachinger. The most intensive subject area and sources of publications in the digital market studies are computer science and Ink World. Besides, Spain was one of the collaborative researcher group map. This research proposed a convergence axis classification consisting of digital market studies to characterize the body of knowledge generated from four decades of studies: Management information system, E-commerce, Digital printing, Data, and Digitalization, abbreviated as MEDDD themes.', 'title': 'A Study of Digital Market Status Using The Bibliometric Approach During Four Decades', 'embedding': []}, {'id': 15042, 'abstractText': 'The development of the field of GIS and remote sensing has greatly contributed to the development and discovery of many archaeological monuments. Therefore, our study discusses the documentation of the historical spatial features of Qary, Sudan using GIS and remote sensing. In addition to the absence of an integrated information application that contributes to the documentation and development of these landmarks through the support of decision makers. The objectives of the study are to collect and classify spatial and descriptive data and information for the Qary area, to design the spatial database of the spatial features of the Qari region, to implement a spatial spatial application of the archaeological spatial features of the Qari region, The study followed descriptive field applied methodology. The study reached several results, the most important of which are: About 20 archaeological sites were recorded and recorded in the Qari area, dating back to different historical periods from the Stone Age to the Islamic Period. Including settlement sites - church tombs. This diversity and richness in the sites shows that the area of Qari has remained a contemporary of all the archaeological periods in the Sudan. (Stone Age (300.00 - 3700 BC)) 8 Post-Merwa (6th century AD) AD 1504 AD (4 monuments, Islamic period (1505 - 1820 AD) 5 monuments, and 2 unidentified teachers)). The spatial database of the spatial and spatial features of the Qari region was constructed. A spatial application of the spatial features of the Qari region was completed. Numerous maps and archaeological sites were developed for the Qari region (map of all the monuments in the Stone Age, the Marwa period, the Christian period, , The study recommends designing the Web and Mobile application to support the decision maker and guide the tourist, and recommends the preparation of a prospective study to implement the three-dimensional area.', 'title': 'GIS &amp; RS-Based Archaeologies Site Documents: Gari Region, Khartoum, Sudan', 'embedding': []}, {'id': 15043, 'abstractText': 'Study on green computing continues to develop but is limited to one country and/or one field. From a bibliometric overview, this study aims to visually study mapping and research trends in the field of green computing on an international scale. This study used bibliometric techniques with secondary data from Scopus. Analyze and visualize data using the VOSViewer program and the analyze search results function on Scopus. This study analyzed 2,596 scientific documents published from 1979 to 2020. According to the research, the Chinese Academy of Sciences, China, and Rajkumar Buyya from the University of Melbourne had the most active organization and individual scientists in green computing research. China and IEEE Access were the most countries and disseminated outlets of green computing. There were five category maps of collaborative researchers from around the world. Based on the identification of a collection of knowledge created from forty-one years of publication, this research proposes a grouping of green computing study themes: Computer science, Environmental management, Mobile computing, Energy, and Sustainability, abbreviated as CEMES study themes.', 'title': 'Four Decades of the Green Computing Study: A Bibliometric Overview', 'embedding': []}, {'id': 15044, 'abstractText': \"Background: Good API documentation facilitates the development process, improving productivity and quality. While the topic of API documentation quality has been of interest for the last two decades, there have been few studies to map the specific constructs needed to create a good document. In effect, we still need a structured taxonomy that captures such knowledge systematically.Aims: This study reports emerging results of a systematic mapping study. We capture key conclusions from previous studies that assess API documentation quality, and synthesise the results into a single framework.Method: By conducting a systematic review of 21 key works, we have developed a five dimensional taxonomy based on 34 categorised weighted recommendations.Results: All studies utilise field study techniques to arrive at their recommendations, with seven studies employing some form of interview and questionnaire, and four conducting documentation analysis. The taxonomy we synthesise reinforces that usage description details (code snippets, tutorials, and reference documents) are generally highly weighted as helpful in API documentation, in addition to design rationale and presentation.Conclusions: We propose extensions to this study aligned to developer utility for each of the taxonomy's categories.\", 'title': 'What should I document? A preliminary systematic mapping study into API documentation knowledge', 'embedding': []}, {'id': 15045, 'abstractText': 'Genetic mapping is an approach in identifying genes and processes. Genetic maps are essential tools for analyzing DNA sequence data, not only providing a blueprint of the genome but also unlocking linkage patterns between genetic markers, chromosomal regions with more than one sequence variant. Studying these linkage patterns enables diverse applications to identifying the biological underlying feature of problems in health, agriculture, and the study of biodiversity. Genetic mapping provides a mean to understand the basis of genetic and biochemical diseases and provides genetic markers. Mapping studies can be done in a single large pedigree; the larger the number of affected individuals sampled the better the estimate of recombination between the gene causing the disease and one or more nearby genetic marker. This work proposes an algorithm for improving the methods to detect breast cancer by analyzing the DNA data and detect the issue in the DNA samples. This work based on the big data and machine learning techniques to get classifications for all samples. All samples will be classified into two main classes. This work evaluates the performance of different classification algorithms on the dataset. It also provides a website application as the tool that can help specialist predict the of breast cancer based on stated genetic mutation.', 'title': 'Machine Learning Model for Breast Cancer Prediction', 'embedding': []}, {'id': 15046, 'abstractText': \"Objective: To enable non-invasive dynamic metabolic mapping in rodent model studies of mitochondrial function using <sup>31</sup>P-MR spectroscopic imaging (MRSI). Methods: We developed a novel method for high-resolution dynamic <sup>31</sup>P-MRSI. The method synergistically integrates physics-based models of spectral structures, biochemical modeling of molecular dynamics, and subspace learning to capture spatiospectral variations. Fast data acquisition was achieved using rapid spiral trajectories and sparse sampling of (k, t, T)-space; image reconstruction was accomplished using a low-rank tensor-based framework. Results: The proposed method provided high-resolution dynamic metabolic mapping in rat hindlimb at spatial and temporal resolutions of 4 × 4 × 2 mm<sup>3</sup> and 1.28 s, respectively. This allowed for in vivo mapping of the time-constant of phosphocreatine resynthesis, a well established index of mitochondrial oxidative capacity. Multiple rounds of in vivo experiments were performed to demonstrate reproducibility, and in vitro experiments were used to validate the accuracy of the estimated metabolite maps. Conclusions: A new model-based method is proposed to achieve high-resolution dynamic 31P-MRSI. The proposed method's ability to delineate metabolic heterogeneity was demonstrated in rat hindlimb. Significance: Abnormal mitochondrial metabolism is a key cellular dysfunction in many prevalent diseases such as diabetes and heart disease; however, current understanding of mitochondrial function is mostly gained from studies on isolated mitochondria under nonphysiological conditions. The proposed method has the potential to open new avenues of research by allowing in vivo and longitudinal studies of mitochondrial dysfunction in disease development and progression.\", 'title': 'High-Resolution Dynamic <sup>31</sup>P-MR Spectroscopic Imaging for Mapping Mitochondrial Function', 'embedding': []}, {'id': 15047, 'abstractText': \"Convolutional neural networks (CNN) are powerful tools in many medical imaging applications including denoising. However, a major concern in deploying a CNN in safety-critical areas is to access its prediction accuracy on out-of-distribution test samples. Our previous study showed that while a CNN could generate similar ensemble standard deviation levels on inputs with different noise levels, the magnitude and location of bias could be substantially different. These observations imply that although CNNs can generate visually high-quality images from each individual dataset, the bias might be higher in one image than the other, which might lead to misdiagnosis. Therefore, it is crucial for us to understand when and where a CNN is uncertain about its prediction in real clinical datasets. In this study, we investigate the use of a Bayesian Convolutional Neural Network (BCNN) to estimate an uncertainty map of its prediction, and assess its correlations with the ensemble bias of its own and a conventional CNN's prediction ensemble bias, respectively. The BCNN is implemented by adding Monte Carlo dropout layers after each convolution and ReLU layer, which is equivalent to imposing a Bernoulli prior distribution on the network weights. At the testing stage, we acquire multiple stochastic forward passes through the network to generate the uncertainty map. We applied BCNN and CNN on two 18F-FDG scans, with one whole-body scan which is considered as an in-distribution test sample, and one brain only study which is considered as an out-of-distribution sample. For the whole-body scan, we generated 10 noise realizations for ensemble analysis using bootstrap. Our results demonstrate that BCNN can generate a prediction uncertainty map that highly correlates with the ensemble bias map. This technique could provide important guidance on interpreting the denoising results from a neural network where ground truth is unknown.\", 'title': 'Estimating Ensemble Bias using Bayesian Convolutional Neural Network', 'embedding': []}, {'id': 15048, 'abstractText': 'Image-based fusion is a state-of-art process to extract vital information by combining the two or more images acquired from different satellite sensors. Recently launched (26th September 2016) ISRO’s (Indian Space Research Organization) Ku-band (13.5 GHz) based Scatterometer Satellite (SCATSAT-1) as an active microwave sensor can offer the day-night, all-weather monitoring services, which are not possible with the optical-based visible and infrared remote sensing satellites. Therefore, the fusion of optical and microwave data offers the cloud-free detection of earth surface transitions and helps in emergency response to natural hazards, security, and defence. The objectives of the proposed framework are (a) nearest-neighbour based fusion (NNF) of ISRO’s SCATSAT-1 and NASA’s (National Aeronautics and Space Administration) moderate resolution imaging spectroradiometer (MODIS) optical data, (b) generation of thematic maps using artificial neural network (ANN) based classification of the fused data, (c) detection of spatiotemporal variations via post-classification comparison (PCC) based change detection, (d) cross-referencing with well-defined fusion methods, i.e. Gram-Schmidt (GS), Brovey Transformation (BT) and Ehlers, and (e) Impact analysis of clouds on the input dataset and fusion methods. This study has been conducted over the Western Himalayas to estimate the snow cover changes under cloudy conditions with two datasets i.e., winter and monsoon. The experimental outcomes confirm the efficacy of the proposed framework in the effective removal of clouds, generation of classified maps, and change maps. The present study includes an exhaustive list of applicative situations for cloud-free monitoring using freely and daily based SCATSAT-1 and MODIS datasets.', 'title': 'Image-Fusion of Ku-band based SCATSAT-1 and MODIS data for Cloud-free Change Detection over Western Himalayas', 'embedding': []}, {'id': 15049, 'abstractText': 'Simultaneous localization and mapping (SLAM) during communication is emerging. This technology promises to provide information on propagation environments and transceivers’ location, thus creating several new services and applications for the Internet of Things and environment-aware communication. Using crowdsourcing data collected by multiple agents appears to be much potential for enhancing SLAM performance. However, the measurement uncertainties in practice and biased estimations from multiple agents may result in serious errors. This study develops a robust SLAM method with measurement plug-and-play and crowdsourcing mechanisms to address the above problems. First, we divide measurements into different categories according to their unknown biases and realize a measurement plug-and-play mechanism by extending the classic belief propagation (BP)-based SLAM method. The proposed mechanism can obtain the time-varying agent location, radio features, and corresponding measurement biases (such as clock bias, orientation bias, and received signal strength model parameters), with high accuracy and robustness in challenging scenarios without any prior information on anchors and agents. Next, we establish a probabilistic crowdsourcing-based SLAM mechanism, in which multiple agents cooperate to construct and refine the radio map in a decentralized manner. Our study presents the first BP-based crowdsourcing that resolves the “double count\" and “data reliability\" problems through the flexible application of probabilistic data association methods. Numerical results reveal that the crowdsourcing mechanism can further improve the accuracy of the mapping result, which, in turn, ensures the decimeter-level localization accuracy of each agent in a challenging propagation environment.', 'title': 'Enabling Plug-and-Play and Crowdsourcing SLAM in Wireless Communication Systems', 'embedding': []}, {'id': 15050, 'abstractText': 'This study presents the results of a field experiment conducted for assessing the crop health status of several barley and oat crop fields in Prince Edward Island, Canada. The crop fields were mapped with an Unmanned Aircraft System (UAS) and the crop health status was assessed through the Green Area Index (GAI) and vegetation indices (VIs). GAI maps were produced from the UAS imagery and VIs using machine learning pipelines with several regression algorithms (Multiple Linear Models, Support Vector Machines, Random Forests, and Artificial Neural Networks) along with a feature selection strategy. The Random Forests algorithm was shown to be the best algorithm for GAI prediction with an average relative Root Mean Square Error of 10.86% and a Mean Absolute Error of 0.67. The resulting GAI maps and the regression feature space were classified with Random Forests to discriminate between vigorous and stressed crop areas. We achieved a mean overall accuracy of 94%. The limits of the study are also presented.', 'title': 'Evaluation of crop health status with UAS multispectral imagery', 'embedding': []}, {'id': 15051, 'abstractText': 'Direct reconstruction of parametric images from raw dynamic PET data has the potential of producing lower noise images than obtained using intermediate frame-based reconstructed images, due to the accurately characterized statistical properties of raw PET data. The goal of this study was to extend a previous direct parametric reconstruction algorithm (PMOLAR). PMOLAR uses the Expectation Maximization (EM) algorithm to estimate the parametric maps. Previous versions of PMOLAR were based on the one-tissue (1T) compartment model (PMOLAR-1T). The 1T model is suitable for some PET tracers, but most reversible PET radioligand kinetics are better described by more complex models, mainly the two-tissue (2T) compartment model. Alternatively, Logan Graphical Analysis (GA) can be applied to all reversible PET tracers. In this study PMOLAR was adapted to a new model based on GA. Two versions of the new reconstruction algorithm were investigated. PMOLAR-GA was evaluated on human data acquired on the High Resolution Research Tomograph (HRRT) after injection of [<sup>11</sup>C]PBR28, a radiotracer used to study neuroinflammation. The new model derived from GA was first compared to previous modeling methods (MA1, LEGA, 2T) suitable for [<sup>11</sup>C]PBR28 on fits of region of interest time-activity curves. Then the two new versions of PMOLAR were evaluated on a human 4D PET data set. PMOLAR-GA parametric maps were compared to frame-based parametric maps. Using routine reconstruction settings, frame based parametric maps were visually very noisy and biased (between 27±4% and 222±121% on a regional level, depending on the modeling method). While the PMOLAR-GA mean regional values were very close to reference values (11%±9% or 8%±9%), with visually lower noise at the voxel level.', 'title': 'Direct EM reconstruction of parametric images from list-mode brain PET using a novel model based on logan graphical analysis', 'embedding': []}, {'id': 15052, 'abstractText': 'Natural calamities triggered by erratic weather conditions like cyclone, earthquakes, hail storms, and flood incurs substantial loss to the infrastructure and crops of the region. Countries across the globe are prone to such natural calamities. In India, specifically coastal parts are vulnerable to tropical cyclones. In 2018 east coast districts of Tamil Nadu and Andhra Pradesh, India were affected by the three cyclones namely Titli (11 Oct. 2018), Gaja (16 Nov. 2018) and Pethai (17 Dec. 2018) causing severe damage to seasonal crops such as Rice, Coconut and Areca Nut plantations. Traditional survey-based methods of crop loss assessment are time-consuming and labor-intensive.This study addresses the problem of near-real-time qualitative crop loss assessment due to tropical Gaja cyclone using the temporal data from Sentinel 1 and 2 satellites. The crop damage assessment study has been undertaken for Gaja cyclone in the affected district of Thanjavur, Tamil Nadu, India. The major crops cultivated in the district are Kharif Rice (locally called as Samba and Late Samba) and Coconut plantations. The study addresses qualitative loss assessment in terms of crop area affected. As a first step, we used time series data of Sentinel1 (VV and VH backscatter) available between Aug.-Nov. 2018 to map the Kharif rice area. Also, cloud-free Sentinel 2 scenes available during Mar.-May. 2018 have been used to map the Coconut area. Field visits were conducted to collect the geo-tagged plot boundaries for the rice crop and coconut plantations. The data collected through field visits was used both for model training and crop loss assessment. Google maps satellite layer was used as a base map for identification of other non-crop classes (i.e., forest, water, settlement, etc.). The overall accuracy of crop area classification was 87.23% for rice and 92.22% for coconut.Further, to estimate the crop loss, crop layers along with the NDVI were considered. Two crop loss scenarios, namely minimum damage and maximum damage, were identified for both the crops. The mean NDVI composite before the event, i.e., 1-15 Nov. 2018 was considered as the base. In case of maximum loss scenario, short term NDVI composite available immediately after the event, i.e., 17-25 Nov. 2018 was selected. After the cyclone, long term NDVI composite of the mean (i.e., 17 Nov.13 Dec. 2018) was used to assess the minimum loss scenario. Using field observations, the crop loss was categorized as severe loss, medium loss, low loss, and no loss. Results showed that the coconut plantations in Pattukkottai, Peravurani, and Papanasam blocks of Tanjavur are affected by the cyclone. The significant rice crop loss has been observed in Thanjavur, Orattanadu, Pattukkottai blocks. We have found the remote sensing based crop loss observations are matching with the government reports based on field observations. The remote sensing observations with human participatory sensing (i.e., field observations) has the potential for near-real-time crop loss assessment.', 'title': 'Near Real Time Crop Loss Estimation using Remote Sensing Observations', 'embedding': []}, {'id': 15053, 'abstractText': 'Sugarcane is a high biomass crop that requires large quantities of water for maximum yield. The study aims to estimate daily actual water consumption or crop evapotranspiration ( ETc) at field scales for practical applications with real data, remote sensing (RS) observation using novel methodologies (ML: machine learning and LSM: land surface model). Northeast Thailand is the study area chosen and three Sugarcane growing seasons (2016 to 2019) is the duration of the study. Similarities between the crop coefficient ( Kc) curve and a satellite-derived leaf area index (LAI) showed potential for estimation of Kc maps. A regression model has been developed to establish LAI vs Kc relation and used to derive daily Kc maps. With a view to computing daily reference evapotranspiration ( ETo) driven by weather, RS data, soil texture, land charac-teric etc, a high resolution LSM has been customized. The results shows ETc distribution low at initial and early development stages, while ETc tends to be high during grand growth and yield formation stages. Significant spatiotemporal variation has been observed across fields. Analysis of 19 fields for complete three seasons has been undergone with regard to yield response to water consumption and outcomes confirm standard yield reduction due to water stress. The daily ETc maps aided to demonstrate the variability of crop water use during growing season at field scales. Further, using ETc maps at the field scale in near real time, growers can supply optimal water to maximize yield, leading to water conservation in scale.', 'title': 'Crop Evapotranspiration Estimates for Sugarcane Based on Remote Sensing and Land Surface Model in Thailand', 'embedding': []}, {'id': 15054, 'abstractText': 'The inversion of remote sensing images is crucial for soil moisture mapping in precision agriculture. However, the large size of remote sensing images complicates their management. Therefore, this study proposes a remote sensing observation sharing method based on cloud computing (ROSCC) to enhance remote sensing observation storage, processing, and service capability. The ROSCC framework consists of a cloud computing-enabled sensor observation service, web processing service tier, and a distributed database tier. Using MongoDB as the distributed database and Apache Hadoop as the cloud computing service, this study achieves a high-throughput method for remote sensing observation storage and distribution. The map, reduced algorithms and the table structure design in distributed databases are then explained. Along the Yangtze River, the longest river in China, Hubei Province was selected as the study area to test the proposed framework. Using GF-1 as a data source, an experiment was performed to enhance earth observation data (EOD) storage and achieve large-scale soil moisture mapping. The proposed ROSCC can be applied to enhance EOD sharing in cloud computing context, so as to achieve soil moisture mapping via the modified perpendicular drought index in an efficient way to better serve precision agriculture.', 'title': 'ROSCC: An Efficient Remote Sensing Observation-Sharing Method Based on Cloud Computing for Soil Moisture Mapping in Precision Agriculture', 'embedding': []}, {'id': 15055, 'abstractText': \"The current COVID-19 pandemic has seen a lot of higher institutions of learning embracing the e-learning systems. Although these e-learning systems promise to deliver solutions to teaching and learning in this pandemic era, a key challenge is motivating the learner to engage with the e-learning system continuously. Most e-learners quickly get bored and lose motivation in the course of learning. While there exist many strategies such as chatrooms and sporadic question and answer sessions to keep learners involved in e-learning platforms, they have always achieved minimal connectedness among e-learners. Facial emotions have been identified as an effective tool for interpreting learning experience in learners. This study, therefore, examines the use of facial emotions expressed by learners to interpret their learning affect in an e-learning session. This work also explores a standardized mapping mechanism between facial emotions exhibited and their respective learning affects. The study identifies the physical changes in the face of a learner and uses it to estimate their facial emotions and then based on the mapping mechanism, maps emotional states to a student's learning affect. Experiments include the use of a convolutional neural network for the classification of seven facial emotions. The research study tests different network architectures to find optimal architecture, using the FER2013 dataset. Results from the mapping are statistically analyzed and compared with responses provided by participants who participated in the live testing of the system. Results show that facial emotions, which are a form of non-verbal communication, can be used to estimate the learning affect of a student and provides a new avenue to enhance the current e-learning platforms.\", 'title': 'Estimating Student Learning Affect Using Facial Emotions', 'embedding': []}, {'id': 15056, 'abstractText': 'The cold dark matter model predicts that dark matter haloes are connected by filaments. Direct measurements of the masses and structure of these filaments are difficult, but recently several studies have detected these dark-matter-dominated filaments using weak lensing. Here we study the efficiency of galaxy formation within the filaments by measuring their total mass-to-light ratios and stellar mass fractions. Specifically, we stack pairs of luminous red galaxies (LRGs) with a typical separation on the sky of 8 h<sup>−1</sup> Mpc. We stack background galaxy shapes around pairs to obtain mass maps through weak lensing, and we stack galaxies from the Sloan Digital Sky Survey to obtain maps of light and stellar mass. To isolate the signal from the filament, we construct two matched catalogues of physical and non-physical (projected) LRG pairs, with the same distributions of redshift and separation. We then subtract the two stacked maps. Using LRG pair samples from the Baryon Oscillation Spectroscopic Survey at two different redshifts, we find that the evolution of the mass in filament is consistent with the predictions from perturbation theory. The filaments are not entirely dark: Their mass-to-light ratios (M/L = 351 ± 137 in solar units in the rband) and stellar mass fractions (M<inf>stellar</inf>/M = 0.0073 ± 0.0030) are consistent with the cosmic values (and with their redshift evolutions).', 'title': 'How dark are filaments in the cosmic web?', 'embedding': []}, {'id': 15057, 'abstractText': 'Knowledge mapping has been studied widely in knowledge management context, but the terminology of knowledge map system (KMSs) is not been much studied and it needs to identify state the art KMSs topic from a literature review to propose the future research. The method refers to the systematic literature review as guidelines from Kitchenham, this research gathers, synthesizes, and analyses some paper based on keyword “knowledge map system” or “knowledge mapping system” and “knowledge management”, where it published from 2008 until 2017 on four international electronic databases and using predefined review protocol. We obtained 9 articles used in this study and find that most design systems use customized with some computational technics. The scope is still a lot of focus on enterprise and virtual community, accordingly it needs to be developed further system for the country as public services and also validation method that combines elements of content and software.', 'title': 'Knowledge Mapping System Implementation in Knowledge Management: A Systematic Literature Review', 'embedding': []}, {'id': 15058, 'abstractText': 'Accurate and timely information of the extent and changes in corn cultivation are of great significance for agricultural production management, food security and global environment change studies. Due to the high temporal repeat interval, near global coverage and rich spectral bands, MODIS time series data has been demonstrated particularly suitable for detecting the seasonal dynamics of different crops. However, their inherently coarse spatial resolution limit the accuracies of corn identification in regions with small fields or complex agricultural landscapes. In this study, we investigate the potential of using the random forest regression (RF-r) model to map sub-pixel corn cultivation based on time-series of MODIS data. Corn in Heilongjiang province, China in year 2011 was selected as a case study. Five time series of vegetation indices (155 features) derived from different spectral channels of MOD09A1 data were used as candidate features for the RF-g model. The out-of-bag strategy and backward elimination approach were applied to select the optimal spectra-temporal feature subset for corn identification. These derived corn maps were assessed in two ways: (1) wall-to-wall pixel comparison with the Landsat-based reference map to evaluate the classification performance in terms of spatial distribution. (2) prefectural- and county-level comparison with census data to evaluate its classification performance in terms of area estimates. Results show 61 optimal spectro-temporal features for corn cultivation were selected, which achieved the highest classification accuracy, with squared R (R<sup>2</sup>) of 0.7586 and the root mean squared error (RMSE) of 0.085. MODIS-derived corn cultivation area had good agreements with the census data, with R<sup>2</sup> of 0.73 and RMSE of 238.07 km<sup>2</sup> across 43 counties, R<sup>2</sup> of 0.83 and RMSE of 1155.57 km<sup>2</sup> across 12 prefectures. These promising results indicate the great potential of RF-g method in mapping sub-pixel crop distributions based on coarse spatial resolution images.', 'title': 'Mapping sub-pixel corn distribution using MODIS time-series data and a random forest regression model', 'embedding': []}, {'id': 15059, 'abstractText': 'Seven years after the beginning of a massive wastewater injection project in eastern Colombia, local earthquake activity increased significantly. The field operator and the Colombian Geological Survey immediately reinforced the monitoring of the area. Our analysis of the temporal evolution of the seismic and injection data together with our knowledge of the geological parameters of the region indicate that the surge of seismicity is being induced by the re-injection of produced water into the same three producing reservoirs. Earthquake activity began on known faults once disposal rates had reached a threshold of ∼2 × 10<sup>6</sup> m<sup>3</sup> of water per month. The average reservoir pressure had remained constant at 7.6 MPa after several years of production, sustained by a large, active aquifer. Surface injection pressures in the seismically active areas remain below 8.3 MPa, a value large enough to activate some of the faults. Since faults are mapped throughout the region and many do not have seismicity on them, we conclude that the existence of known faults is not the only control on whether earthquakes are generated. Stress conditions of these faults are open to future studies. Earthquakes are primarily found in four clusters, located near faults mapped by the operator. The hypocentres reveal vertical planes with orientations consistent with focal mechanisms of these events. Stress inversion of the focal mechanisms gives a maximum compression in the direction ENE-WSW, which is in agreement with borehole breakout measurements. Since the focal mechanisms of the earthquakes are consistent with the tectonic stress regime, we can conclude that the seismicity is resulting from the activation of critically stressed faults. Slip was progressive and seismic activity reached a peak before declining to few events per month. The decline in seismicity suggests that most of the stress has been relieved on the main faults. The magnitude of a large majority of the recorded earthquakes was lower than 4, as the pore pressure disturbance did not reach the mapped large faults whose activation might have resulted in larger magnitude earthquakes. Our study shows that a good knowledge of the local fault network and conditions of stress is of paramount importance when planning a massive water disposal program. These earthquakes indicate that while faults provide an opportunity to dispose produced water at an economically attractive volume–pressure ratio, the possibility of induced seismicity must also be considered.', 'title': 'Seismicity induced by massive wastewater injection near Puerto Gaitán, Colombia', 'embedding': []}, {'id': 15060, 'abstractText': 'We propose a novel hybrid Convolutional Neural Network (CNN) model with one-versus-one approach to forecast solar flare occurrence with the outputs of four classes (No-flare, C, M, and X) within 24 h. We train and test our model using the same data sets as in Zheng, Li &amp; Wang, and then compare our results with previous models using the true skill statistic (TSS) as primary metric. The main results are as follows. (1) This is the first time that the CNN model in conjunction with one-versus-one approach is used in solar physics to make multiclass flare prediction. (2) In the four-class flare prediction, our model achieves quite high mean scores of TSS = 0.703, 0.489, 0.432, and 0.436 for No-flare, C, M, and X class, respectively, which are much better than or comparable to those of previous studies. In addition, our model obtains TSS scores of 0.703 ± 0.070 for ≥C-class and 0.739 ± 0.109 for ≥M-class predictions. (3) This is the first attempt to open the black-box CNN model to study the visualization of feature maps for interpreting the prediction model. Furthermore, the visualization results indicate that our model pays attention to the regions with strong gradient, strong intensity, high total intensity, and large range of the intensity in high-level feature maps. The median gradient and intensity, the total intensity, and the range of the intensity for high-level feature maps increase approximately with the increase of flare level.', 'title': 'Hybrid deep convolutional neural network with one-versus-one approach for solar flare prediction', 'embedding': []}, {'id': 15061, 'abstractText': 'Mangrove usually found on the outfall or along the beach and commonly the area is a mudflats area. Mangroves have many important roles to the environment and to the marine ecosystem and also protect land from the erosion, flood and even tsunami. Mangrove can be a shelter when there is tempest that come from the sea and can reduce the erosion to the land that cause from the wave. Mangrove have main important role for ecosystem, mapping of the mangrove should be done. But to get the information to map the mangrove area might be difficult since the area of the mangrove is a mudflats area and might have constraint to get the information to map the mangrove area. In this study using remote sensing technology, mapping for mangrove area are easier since it is use satellite image and no need to go to the field which might have difficult and probably have to take longer time to get the data. This study is about to detect mangrove area by using high resolution satellite imagery. Since this study is to detect mangrove area by using high resolution satellite imagery and there are many types of satellite imagery are available, Quickbird images have been used to detect mangrove area. Visual interpretation technique being used to choose the training area and supervised and unsupervised classification will be performed.', 'title': 'Mangrove area detection by using high resolution satellite imagery', 'embedding': []}, {'id': 15062, 'abstractText': 'The paper compares the potential of WorldView-3 (WV-3) and Sentinel-2 (S-2) satellite data for mapping naturally occurring asbestos (NOA) outcrops to be used by geologists in the planning phase of environmental monitoring. The wide distribution as well as the variety and extent of asbestos-bearing rocks make the selected area a significant case study for the evaluation of the feasibility of multispectral VNIR-SWIR (0.425-2.330 μm) remote sensing observations for NOA outcrops mapping, in those areas where the density of vegetation allows their spectral identification. Different classification procedures were used to produce NOA outcrops maps for the study area. In our study, we found in general a good agreement (k &gt; 0.8) between the produced NOA outcrops maps and the extensive available in situ data for the accessible locations.', 'title': 'Worldview-3 and Sentinel-2 Imagery for Mapping Naturally Occurring Asbestos (NOA) in Serpentinites Rocks in Southern Italy', 'embedding': []}, {'id': 15063, 'abstractText': 'The present work aimed to map the various requirements engineering techniques and methodologies for the construction of educational technologies through a systematic mapping study. The research protocol of this mapping included the planning, execution, and analysis of the results phases. This research added information about the diversity of methods and techniques of requirements engineering applied in the process of developing educational technologies. However, this research indicated that the phenomena related to the pedagogical practice experienced by teachers and students are sometimes relegated in this process. This work analyzed the primary studies of the last ten years, making it possible to identify gaps in this area of research to propose research and the generation of new forms of work in the development of educational systems.', 'title': 'Requirements Elicitation and Specification for Educational Technology Development: A Systematic Literature Mapping', 'embedding': []}, {'id': 15064, 'abstractText': 'The article presents the results of a study on the spatial distribution of acts of vandalism and alcohol consumption in prohibited areas. The study made use of data on offences committed in Krakow, reported by citizens of the city and illustrated on an interactive map - the National Safety Risk Map. The gathered data was calibrated and generalised. Then, a map was prepared illustrating the spatial distribution of both categories of offences in different sub-districts in Krakow, as well as a choropleth map of both types of offence. Two methods were used for examining the interrelationship between these offences - regression analysis using the Poisson regression model with a linear correction of the dispersion and dual kernel density estimation with a \"hot spot\" analysis. The study constitutes the first systematic attempt to analyse NSRM data with regard to the spatial distribution and spatial co-occurrence of two common offences in Krakow.', 'title': 'A Spatial Analysis of Selected Categories of Offences in Krakow Based on Data from the National Safety Risk Map', 'embedding': []}, {'id': 15065, 'abstractText': 'This article deals with the computational complexity issue of graphbased simultaneous localization and mapping (SLAM). SLAM allows a robot that is navigating in an unknown environment to build a map of this environment while simultaneously determining the robot pose on this map. Graph-based SLAM is a smoothing method that uses a graph to represent and solve the SLAM problem. We first propose a graph construction that takes advantage of the incremental and sparse characteristics of graph-based SLAM. This incremental construction is exploited to perform several algorithmic optimizations. Second, we present a study of using a heterogeneous architecture to implement the graph-based SLAM algorithm. Indeed, the emergence of recent heterogeneous embedded architectures should lead to a great advance in the design of embedded systems-based robotics applications. As a result of this study, an algorithm-architecture mapping is proposed for a central processing unit-graphics processing unit (CPU-GPU)-based architecture. The study also investigates how this kind of architecture can speed up graph-based SLAM by offloading some critical compute-intensive tasks of the algorithm on the GPU. Some common data sets are used to compare our implementations to the state of the art.', 'title': 'Graph-Based Simultaneous Localization and Mapping: Computational Complexity Reduction on a Multicore Heterogeneous Architecture', 'embedding': []}, {'id': 15066, 'abstractText': 'In finite precision implementations of chaotic maps all trajectories are eventually periodic. The goal of this brief is to develop methods for systematic study of effects of finite precision computations on dynamical behaviors of discrete maps and to carry out a study of the logistic map in this context. In particular, we are interested in finding all cycles when the logistic map is implemented in single and double precision and studying properties of these cycles including the size of the basin of attraction, and the maximum and average convergence times.', 'title': 'Periodic Orbits of the Logistic Map in Single and Double Precision Implementations', 'embedding': []}, {'id': 15067, 'abstractText': 'Evolutionary multitasking (EMT) is a newly emerging research topic in the community of evolutionary computation, which aims to improve the convergence characteristic across multiple distinct optimization tasks simultaneously by triggering knowledge transfer among them. Unfortunately, most of the existing EMT algorithms are only capable of boosting the optimization performance for homogeneous problems which explicitly share the same (or similar) fitness landscapes. Seldom efforts have been devoted to generalize the EMT for solving heterogeneous problems. A few preliminary studies employ domain adaptation techniques to enhance the transferability between two distinct tasks. However, almost all of these methods encounter a severe issue which is the so-called degradation of intertask mapping. Keeping this in mind, a novel rank loss function for acquiring a superior intertask mapping is proposed in this article. In particular, with an evolutionary-path-based representation model for optimization instance, an analytical solution of affine transformation for bridging the gap between two distinct problems is mathematically derived from the proposed rank loss function. It is worth mentioning that the proposed mapping-based transferability enhancement technique can be seamlessly embedded into an EMT paradigm. Finally, the efficacy of our proposed method against several state-of-the-art EMTs is verified experimentally on a number of synthetic multitasking and many-tasking benchmark problems, as well as a practical case study.', 'title': 'Affine Transformation-Enhanced Multifactorial Optimization for Heterogeneous Problems', 'embedding': []}, {'id': 15068, 'abstractText': 'Electrocardiogram recordings during opucal mapping experiments in heart tissue are commonly used tu monitor the health of the preparation and to obtain dominant frequencies during arrhythmic and defibrillatory studies. However the use of ECG reconstructed from optical mapping is seldom used and to date it has not been strictly validated. In this manuscript we present the first detailed validation and comparison of Optical Mapping ECG, or OM-ECG, with standard ECG recordings by calculating the electrostatic potential in space as a function of the voltage measured optically and describe the different approximations that can be used to obtain unipolar or bipolar ECG recordings. We found that in small/medium hearts, such as rabbits, leads that are aligned apex to base only require activation recording from one surface (anterior or posterior) for the OM-ECG to match the ECG while leads aligned left to right may require both an anterior and posterior optical mapping recording. The discrepancy between leads is due to symmetries in the ventricular activations. In the case of ischemic hearts where activations even-out more, the match between the OM-ECG and standard ECG may require only one surface recording for both left-right and base-apex leads. We believe that this methodology has two main and direct applications in the study of cardiac dynamics. The first is during studies of defibrillation where information after the shock may be crucial in the development of new strategies, OM-ECGs do not suffer the current artifacts of standard ECGs during shocks and can be calculated during the entire activation. We present examples in rabbit ventricles where even low amplitude pacing artifacts are captured by the ECG but do not appear in the OM-ECG. The second use of this technique is for reconstructions of intramural dynamics in larger hearts where differences between the ECG and OM-ECG obtained from anterior and posterior recordings can be used to derive the intramural activation.', 'title': 'Electrocardiogram reconstruction from high resolution voltage optical mapping', 'embedding': []}, {'id': 15069, 'abstractText': 'The SZZ approach for identifying fix-inducing changes traces backwards from a commit that fixes a defect to those commits that are implicated in the fix. This approach is at the heart of studies of characteristics of fix-inducing changes, as well as the popular Just-in-Time (JIT) variant of defect prediction. However, some types of commits are invisible to the SZZ approach. We refer to these invisible commits as Ghost Commits. In this paper, we set out to define, quantify, characterize, and mitigate ghost commits that impact the SZZ algorithm during its mapping (i.e., linking defect-fixing commits to those commits that are implicated by the fix) and filtering phases (i.e., removing improbable fix-inducing commits from the set of implicated commits). We mine the version control repositories of 14 open source Apache projects for instances of mapping-phase and filtering-phase ghost commits. We find that (1) 5.66%11.72% of defect-fixing commits of defect-fixing commits only add lines, and thus, cannot be mapped back to implicated commits; (2) 1.05%4.60% of the studied commits only remove lines, and thus, cannot be implicated in future fixes; and (3) that no implicated commits survive the filtering process of 0.35%14.49% defect-fixing commits. Qualitative analysis of ghost commits reveals that 46.5% of 142 addition-only defect-fixing commits add checks (e.g., null-ness or emptiness checks), while 39.7% of 307 removal-only commits clean up (unused) code. Our results suggest that the next generation of SZZ improvements should be language-aware to connect ghost commits to implicated and defect-fixing commits. Based on our observations, we discuss promising directions for mitigation strategies to address each type of ghost commit. Moreover, we implement mitigation strategies for addition-only commits and evaluate those strategies with respect to a baseline approach. The results indicate that our strategies achieve a precision of 0.753, improving the precision of implicated commits by 39.5 percentage points.', 'title': 'The Ghost Commit Problem When Identifying Fix-Inducing Changes: An Empirical Study of Apache Projects', 'embedding': []}, {'id': 15070, 'abstractText': 'Electromagnetic (EM) scattering studies from seashore regions are attracting more and more attentions in recent years. As an important part of the seashores, sand beaches should also be treated seriously. In this paper, the statistical simulation of EM scattering from Gaussian-texture sandbeach surface is studies. As the frequency of incident wave is relatively small, with long wavelength, the roughness of the sand scale is neglected and the facet-based small-slope approximation (SSA) method is employed to the EM scattering calculations of the Gaussian-texture sandbeach surface. When the spatial correlated scattering map of small sandbeach area is derived together with its correspond statistical properties by the facet-based EM scattering model, the amplitude of the spatial correlated scattering map is generated through the MNLT method, which is of high efficiency in the generation. From the comparisons of texture feature, statistical characteristics between the results derived from EM model and the statistical approach, it is demonstrated that the statistical approach can give a fast and effective simulation of spatial correlated scattering map.', 'title': 'Statistical Simulation of EM Scattering from Gaussian-Texture Sandbeach Surface', 'embedding': []}, {'id': 15071, 'abstractText': 'Technical analysis is a widely used method for forecasting the price direction on the financial time series data. This method requires the use of different number and types of analysis algorithms (technical indicators) together. Although these algorithms show successful performance on small-scale financial time series data, significant performance decreases are detected when the size of data increased. On the large-scale financial time series data, it is necessary to implement these algorithms based on the Map-Reduce programming model and examine the performance of the algorithms which are implemented based on this model comparatively. For this purpose, seven different indicators are studied within the scope of this study, new versions of these indicators are implemented using Map-Reduce parallel data processing model and performance comparisons are made with these algorithms. As a result of these comparisons on single-node and multi-node, significant performance gains have been obtained using Map-Reduce programming model.', 'title': 'Technical Analysis on Financial Time Series Data Based on Map-Reduce Programming Model: A Case Study', 'embedding': []}, {'id': 15072, 'abstractText': \"In this paper, we propose two different scanning methods, drone scanning method, and mobile scanning method, using a stereo camera for estimation of the tree's diameter at breast height (DBH). The mobile scanning method is widely used in mapping forest environments and for forest inventory due to its cost-effectiveness and accessibility. Drone scanning has gained wide attention in recent years because of the development of unmanned aerial technology. This study evaluates two existing fitting algorithms to determine the most suitable method for DBH extraction, namely, the least-squares ellipse fitting algorithm and the random Hough transform (RHT) circle fitting algorithm. A stereo camera is used to generate 3D point cloud maps of the pine forest area. The trees in the study area are segmented using clustering algorithms before performing DBH extraction. A three-dimensional point cloud map of pine trees in the study area is generated, and DBH of each individual tree is estimated. From the experimental results, the drone scanning method was found to be more appropriate than the mobile scanning method for the application of forest mapping. From our experimental results, the drone scanning method with the RHT circle fitting algorithm achieved an average precision accuracy of 92.67%.\", 'title': 'Estimation of Tree Diameter at Breast Height using Stereo Camera by Drone Surveying and Mobile Scanning Methods', 'embedding': []}, {'id': 15073, 'abstractText': 'Visual defect inspection and classification are significant steps of most manufacturing processes in the semiconductor and electronics industries. Known and unknown defects on wafer maps tend to cluster, and these spatial patterns provide valuable process information for supporting manufacturing in determining the root causes of abnormal processes. In previous studies, data augmentation-based deep learning (DL) techniques were most commonly used for the identification of wafer map defect patterns (WMDP). Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations were manually designed for the WMDP problem. In this study, we propose a DL-based method with automatic data augmentation for the WMDP task. Basically, it focuses on learning effective discriminative features, from wafer maps, through a deep network structure. The network consists of a convolution-based variational autoencoder (CVAE) sequentially. First, we pre-trained the CVAE on large training data in an unsupervised manner. Second, we fine-tuned the encoder of the CVAE, which was followed by a neural network (NN) classifier, in a supervised manner. Additionally, we describe a simple procedure for automatically searching for improved data augmentation policies. The policy mainly consists of five image processing functions: rotation, flipping, shifting, shearing range, and zooming. The effectiveness of the proposed method was demonstrated through experimental results obtained from a simulation dataset and a real-world wafer map dataset (WM-811K). This study provides guidance for the application of deep learning in semiconductor manufacturing processes to improve product quality and yield.', 'title': 'Unsupervised Pre-Training of Imbalanced Data for Identification of Wafer Map Defect Patterns', 'embedding': []}, {'id': 15074, 'abstractText': \"Introduction: Studies of the movement of the chest wall show their potential in the diagnosis of heart diseases. Few studies have focused on mapping these movements especially in the lower inaudible frequency band. Aims: This study evaluates Body Surface Mapping (BSM) as a method for describing mechanical cardiac activity. Methods: The chest wall's velocity was measured with a Laser Doppler Vibrometer (LDV) on six healthy subjects. The measuring procedure was repeated for 30 points positioned in a grid at the subjects chest. An electrocardiogram (ECG) and respiration was measured to support the signal processing. The heart movement were described using amplitude maps, constructed from the integrated LDV signal components of 1-20 Hz. Results: The impact of the cardiac motion on the displacement of the chest wall was shown as a typical pattern of the changes of the amplitude maps as a function of time. Conclusion: The results had a high reproducibility and were in concordance with existing evidence, thus indicating BSM to be a valid method for characterization of the mechanical cardiac activity.\", 'title': 'Body surface mapping of the mechanical cardiac activity', 'embedding': []}, {'id': 15075, 'abstractText': \"Now a day's Unmanned Aerial Vehicle (UAV) replaces the aeroplane flown by a pilot, and a small high-resolution digital camera replaces the large metric camera, the combination of both are used as a platform for acquiring aerial images, so as called UAV photogrammetry. This study concentrates on the use and the capabilities of UAV photogrammetry for producing topographic maps, and to assess the accuracy of these maps. For that, a lightweight fixed-wing UAV eBee - Sensefly was used as a platform for acquiring aerial digital images of the study area. Before the flight mission, ground control points were established, Leica GS15 GPS determined their 3D coordinates in two sessions of static observations. The digital images were processed using Pix4Dmapper software for producing orthophotos and digital surface models. For accuracy assessment, the root mean square errors (RMSE) is used in which 2.0 cm in Easting, 2.1 cm in Northing and 7.5 cm in Elevation were obtained for orthomosaic and DTM respectively. Based on these assessments, the results showed that the accuracy achieved is following (ASPRS Accuracy Standards for Digital Geospatial Data) within the second and third classes of these standards for horizontal and vertical accuracies. In conclusion, this study shows that UAV photogrammetry can be applied for producing digital maps, orthophotos, contour lines, digital terrain model, digital surface model, and line maps all of them complies with international standards.\", 'title': 'Accuracy Assessment of UAV photogrammetry for Large Scale Topographic Mapping', 'embedding': []}, {'id': 15076, 'abstractText': 'In this study, a real-time image stitching method is proposed for cost-effective high resolution wide angle video shooting. In the first stage, the images taken from more than one camera with fixed position are stitched with a classical algorithm. After the parameters calculated in the first stage are stored, the image pixels are mapped using the stored parameters and ArUco markers. The mapping process is used for real-time panoramic video shooting after it has been calculated for that particular camera setup once. The images taken from the multi camera are combined by remapping with a GPU based approach. Therefore, registered mapping can be used in different environments without changing the position and lenses of the cameras. As a case study, real-time panoramic video is shot with two cost-effective cameras in football matches. Deep-learning based autonomous pilot video shooting is then performed on the high resolution panoramic video obtained. In experiments, 36 FPS speed has been reached by using a standard desktop computer and it has been seen that image quality measurements are at reasonable levels.', 'title': 'Real-Time Image Stitching For Multiple Camera Panoramic Video Shoot: A Case Study in Football Matches', 'embedding': []}, {'id': 15077, 'abstractText': 'Analysis and exploration of spatio-temporal data such as traffic flow and vehicle trajectories have become important in urban planning and management. In this paper, we present a novel visualization technique called route-zooming that can embed spatio-temporal information into a map seamlessly for occlusion-free visualization of both spatial and temporal data. The proposed technique can broaden a selected route in a map by deforming the overall road network. We formulate the problem of route-zooming as a nonlinear least squares optimization problem by defining an energy function that ensures the route is broadened successfully on demand while the distortion caused to the road network is minimized. The spatio-temporal information can then be embedded into the route to reveal both spatial and temporal patterns without occluding the spatial context information. The route-zooming technique is applied in two instantiations including an interactive metro map for city tourism and illustrative maps to highlight information on the broadened roads to prove its applicability. We demonstrate the usability of our spatio-temporal visualization approach with case studies on real traffic flow data. We also study various design choices in our method, including the encoding of the time direction and choices of temporal display, and conduct a comprehensive user study to validate our embedded visualization design.', 'title': 'Embedding Spatio-Temporal Information into Maps by Route-Zooming', 'embedding': []}, {'id': 15078, 'abstractText': \"Contributions: A concept-map-based remedial learning system is presented to enhance students' grasp of the learning concepts of the IEEE floating-point standard and microprocessor without interlocked pipeline stages (MIPSs) encoding according to their understanding of these learning concepts. Background: Concept maps have been used to represent the knowledge structures of learning topics. This study presents another usage of concept maps, illustrating the prerequisite relationships between and among the learning concepts in a concept map. Simulation-based systems for learning the IEEE floating-point standard and MIPS encoding have been implemented. A simple remedial learning system that helps students to learn the IEEE floating-point standard and MIPS encoding would therefore be valuable, but had yet to be designed. Intended Outcomes: Students' understandings of the IEEE floating-point standard and MIPS encoding are expected to enhance via studying the remedial materials generated by this system. Application Design: A one-group-pretest-posttest design was utilized. The students first took a pretest to get their grasp of the learning concepts, and then studied the remedial learning materials according to their understanding of the learning concepts in the pretest. Findings: 1) The score progress of the IEEE floating-point learners was significant after they undertook remedial learning; 2) the score progress of the low-achieving students was significantly greater than that of the high-achieving students, in their learning of the IEEE float-pointing standard; 3) the score progress of the MIPS encoding learners was significant after they undertook remedial learning; and 4) the score progress of the low-achieving students was significantly greater than that of the high-achieving students in their learning of MIPS encoding, with weak evidence.\", 'title': 'A Concept Map-Based Remedial Learning System With Applications to the IEEE Floating-Point Standard and MIPS Encoding', 'embedding': []}, {'id': 15079, 'abstractText': 'Great inland freshwater lakes play an important role in regulating inland water resources, and the usage of synthetic aperture radar (SAR) images for the accurate waterline mapping is an effective technical means to study the dynamic changes of great inland lakes. In this article, the Danjiangkou (DJK) reservoir is selected as a study case, and a novel waterline mapping method with four main parts is proposed to monitor the water area dynamically. First, a coarse segmentation method is implemented to extract the initial waterline. Second, a strategy of division in local regions is given to speed up the subsequent processes. Third, a combination of a speckle filter and an improved geometric active contour model is used for refined segmentation. Finally, a change detection method is used to study the changing lake. Furthermore, six SAR images obtained by the Gaofen-3 (GF-3) and Sentinel-1A (S-1A) satellites in the DJK reservoir, Hongze lake and Poyang lake are tested to verify the universality of the proposed water area extraction method. The results demonstrate excellent performances with an accuracy of over 97% and an average contour offset under 0.7 pixels. Besides, the time-series analysis of the DJK reservoir is applied based on the mapped waterlines of 37 SAR images collected from January to December in 2017. Comparing with the changing tendency of the water level surveyed in the DJK reservoir, the waterline mapping results and the filed survey data have great consistency, which further proves the validity of the proposed method, also presents the significant potential of the GF-3 and S-1A SAR images for managing the water resource.', 'title': 'Dynamic Waterline Mapping of Inland Great Lakes Using Time-Series SAR Data From GF-3 and S-1A Satellites: A Case Study of DJK Reservoir, China', 'embedding': []}, {'id': 15080, 'abstractText': 'Local climate zone (LCZ) classification system provides standard urban morphological classification for urban heat island studies and weather and climate modelling. Based on the definition of the LCZ, various semi-supervised classification approaches have been proposed to generate LCZ maps for different cities using available satellite data. Given that the acquisition of training data is labor intensive, it is practical to develop new models that are suitable for LCZ classification for any cities without the need for training data/samples. In this study, a novel domain-adaptation co-training approach with self-paced learning is designed to generate LCZ maps for new cities with which valid training samples from existing cities are explored and transferred to new target cities for classification. Experimental results show that the proposed approach could derive LCZ maps for the four testing cities, with an overall accuracy of 69.8%, which is over 10% more accurate than conventional approaches. Compared with conventional approaches, the novel approach does not need prior knowledge about the target cities, and it can automatically generate worldwide LCZ maps to support urban-climate studies for cities in the world.', 'title': 'A co-training approach to the classification of local climate zones with multi-source data', 'embedding': []}, {'id': 15081, 'abstractText': 'We propose a new cloud gaming platform to address the limitations of the existing ones. We study the rendering pipeline of 2D planar maps, and convert it into the server and client pipelines. While doing so naturally gives us a distributed rendering platform, compressing 2D planar maps for transmission has never been studied in the literature. In this paper, we propose a compression component for 2D planar maps with several parametrized modules, where the optimal parameters are identified through real experiments. The resulting cloud gaming platform is evaluated through extensive experiments with diverse game scenes. The evaluation results are promising, compared to the state-of-the-art x265 codec, our platform: (i) achieves better perceptual video quality, by up to 0.14 in SSIM, (ii) runs fast, where the client pipeline takes ≤ 0.83 ms to render each frame, and (iii) scales well for ultra-high-resolution displays, as we observe no bitrate increase when moving from 720p to 1080p, 2K, and 4K displays. The study can be extended in several directions, e.g., we plan to leverage the temporal redundancy of the 2D planar maps, for even better performance.', 'title': 'Optimizing next-generation cloud gaming platforms with planar map streaming and distributed rendering', 'embedding': []}, {'id': 15082, 'abstractText': \"It is essential use descriptive, visual, and statistical methods to interpret human resource data and processes. This study aims to increase the efficacy in calculating the data analysis of employees through the training of talent mapping to the students of psychology. This study used the design of a non-equivalent control group with pretest and posttest design. The measurement tool was the people analytics-efficacy scale adapted from the combination of Bandura's self-efficacy scale and the concept of people analytics. The result of this study concluded that talent mapping training was able to increase the confidence of psychology students. It means that high people analytics-efficacy do not only perceive themselves to be able to complete talent mapping operations on a larger or more difficult scale but also show higher confidence in their ability to successfully perform existing talent mapping operations.\", 'title': 'Talent Mapping Training to Improve People Analytics Efficacy of University Students', 'embedding': []}, {'id': 15083, 'abstractText': 'Mean aortic pressure (MAP) is a major determinant of perfusion in all organs systems. The ability to forecast MAP would enhance the ability of physicians to estimate prognosis of the patient and assist in early detection of hemodynamic instability. However, forecasting MAP is challenging because the blood pressure (BP) time series is noisy and can be highly non-stationary. The aim of this study was to forecast the mean aortic pressure five minutes in advance, using the 25 Hz time series data of previous five minutes as input. We provide a benchmark study of different deep learning models for BP forecasting. We investigate a left ventricular dwelling transvalvular micro-axial device, the Impella, in patients undergoing high-risk percutaneous intervention. The Impella provides hemodynamic support, thus aiding in native heart function recovery. It is also equipped with pressure sensors to capture high frequency MAP measurements at origin, instead of peripherally. Our dataset and the clinical application is novel in the BP forecasting field. We performed a comprehensive study on time series with increasing, decreasing, and stationary trends. The experiments show that recurrent neural networks with Legendre Memory Unit achieve the best performance with an overall forecasting error of 1.8 mmHg.', 'title': 'Aortic Pressure Forecasting With Deep Learning', 'embedding': []}, {'id': 15084, 'abstractText': 'This research is devoted to the study and analysis of modern methods of building routes. The article describes the data provided by the Open street map, google maps API and ways of working with them, as well as various routing algorithms used in Graphhopper, Yandex.Maps, and other routing services. In the course of the study, based on the material studied about working with maps, graphs, and routing services, a web application was created. It is aimed at solving the problem of optimizing the route for the needs of the user.', 'title': 'Research of Modern Routing Systems', 'embedding': []}, {'id': 15085, 'abstractText': \"Landslide is an activity from balance disruption which triggers the movement of a mass of soil and rock down a sloped section of land. Boyolali Regency is one of 35 regencies in Central Java Province which has high vulnerability to landslide. In order to reduce the number of casualties and property loss, this study aims to create a new model of landslide prone area map using the parameters which cause landslide, such as rainfall, soil types, drainage, slope, and land cover. The parameters are processed and analyzed using the combination of the scoring method and the polygon thiessen method. The scoring method is implemented to determine landslide prone areas, while the polygon thiessen method is applied to do the overlay and spatial mapping of landslide prone areas. The hypothesis proposed in this study is that the combination of scoring method and the polygon thiessen can map landslide prone areas in Boyolali Regency accurately. The result of the study shows that the model of the landslide prone area's accuracy is 83.3%. The landslide prone area map shows the four sub districts in Boyolali Regency which meet the criteria of high landslide vulnerability level are Solo, Ampel, Musuk and Cepogo Sub Districts.\", 'title': 'A New Model of Landslide Prone Map Using a Combination of Scoring and Polygon Thiessen Methods', 'embedding': []}, {'id': 15086, 'abstractText': \"Fast and precise noninvasive evaluation of tissue mechanical properties is of high importance in ultrasound shear wave elastography. In this study, we present an updated, faster version of the local phase velocity-based imaging (LPVI) method used to create images of local phase velocity in soft tissues. The updated LPVI implementation uses 1-D Fourier transforms in spatial dimensions separately in comparison to its original implementation. A directional filter is applied upon the shear wave field to extract the left-to-right (LR) and right-to-left (RL) propagating shear waves. A local shear wave phase velocity map is recovered based on both LR and RL waves. Finally, a 2-D shear wave velocity map is reconstructed by combining the LR and RL phase velocity maps. LPVI performance for shear wave displacement and velocity-wave motion data is examined. A study of LPVI used for only one data acquisition with multiple focused ultrasound push beams is presented. The lesion placement with respect to the pushes and whether two sequential pushes provided different results from two simultaneous radiation force pushes was investigated. The addition of white Gaussian noise to the wave motion data was also tested to examine the LPVI method's performance. Robust and accurate shear wave phase velocity maps are reconstructed using the proposed LPVI method using numerical tissue-mimicking phantoms with inclusions. Results from the numerical phantom study showed that the reconstructed, asymmetric inclusions, for various axial locations, are better preserved for shear wave particle velocity signals compared with particle displacement motion data.\", 'title': 'Fast Local Phase Velocity-Based Imaging: Shear Wave Particle Velocity and Displacement Motion Study', 'embedding': []}, {'id': 15087, 'abstractText': 'In robotics studies, problems can be given as mapping, location estimation, exploration and navigation. Navigation problem; It can be subdivided into path planning, path tracking and obstacle avoidance. In the study, in the Gazebo simulation environment, a 4-wheel robot and a laser distance sensor were used. 2D mapping, path planning and path following with respect to the 2D map are implemented on Robot Operating System (ROS). Potential-based path planning was compared with the A* algorithm. Potential-based path tracking was compared with the dynamic window approach (DWA). In tests conducted in the Gazebo simulation environment, it was observed that the potential-based approach yielded successful results in path planning and path tracking.', 'title': 'Potential Field Path Planning and Potential Field Path Tracking', 'embedding': []}, {'id': 15088, 'abstractText': 'Development of applications in knowledge mapping requires a system architecture approach to be able to explain conceptual models that define structure, behaviour and view of the system. There are many system architectures that are made for knowledge mapping but are very diverse, and there are no general guidelines that help a scholar used as references in making them. Therefore this research proposes a universal architectural system that can be used to create a knowledge mapping system. With the literature study method and content analysis on the system architecture used in previous studies, this study proposes four layers which can generally conduct in creating architectural systems, namely: Acquisition layer, database layer, knowledge mapping process layer and user interface layer.', 'title': 'Toward a Common System Architecture for Knowledge Mapping', 'embedding': []}, {'id': 15089, 'abstractText': \"This proposal is about a study we recently published in the IEEE Transaction of Software Engineering journal [4]. Context: Collaborative software engineering (CoSE) deals with methods, processes and tools for enhancing collaboration, communication, and co-ordination (3C) among team members. CoSE can be employed to conceive different kinds of artifacts during the development and evolution of software systems. For instance, when focusing on software design, multiple stakeholders with different expertise and responsibility collaborate on the system design. Model-Driven Software Engineering (MDSE) provides suitable techniques and tools for specifying, manipulating, and analyzing modeling artifacts including metamodels, models, and transformations. Collaborative MDSE consists of methods or techniques in which multiple stakeholders manage, collaborate, and are aware of each others' work on a set of shared models. A collaborative MDSE approach is composed of three main complementary dimensions: (i) a model management infrastructure for managing the life cycle of the models, (ii) a set of collaboration means for allowing involved stakeholders to work on the modelling artifacts collaboratively, and (iii) a set of communication means for allowing involved stakeholders to exchange, share, and communicate information within the team. Collaborative MDSE is attracting several research efforts from different research areas (e.g., model-driven engineering, global software engineering, etc.), resulting in a variegated scientific body of knowledge on the topic. Objective: In this study we aim at identifying, classifying, and understanding existing collaborative MDSE approaches. More specifically, our goal is to assess (i) the key characteristics of collaborative MDSE approaches (e.g., model editing environments, model versioning mechanisms, model repositories, support for communication and decision making), (ii) their faced challenges and limitations, and (iii) the interest of researchers in collaborative MDSE approaches over time and their focus on the three dimensions of collaborative MDSE. Method: In order to achieve this, we designed and conducted a systematic mapping study on collaborative MDSE. Starting from over 3,000 potentially relevant studies, we applied a rigorous selection procedure resulting in 106 selected papers, further clustered into 48 primary studies, along a time span of nineteen years. A suitable classification framework has been empirically defined and rigorously applied for extracting key information from each selected study. We collated, summarized, and analyzed extracted data by applying scientifically sound data synthesis techniques. Results: In addition to a number of specific insights, our analysis revealed the following key findings: (i) there is a growing scientific interest on collaborative MDSE in the last years; (ii) multi-view modeling, validation support, reuse, and branching are more rarely covered with respect to other aspects about collaborative MDSE; (iii) different primary studies focus differently on individual dimensions of collaborative MDSE (i.e., model management, collaboration, and communication); (iv) most approaches are language-specific, with a prominence of UML-based approaches; (v) few approaches support the interplay between synchronous and asynchronous collaboration. Conclusion: This study gives a solid foundation for a thorough identification and comparison of existing and future approaches for collaborative MDSE. Those results can be used by both researchers and practitioners for identifying existing research/technical gaps to attack, better scoping their own contributions to the field, or better understanding or refining existing ones.\", 'title': 'Collaborative Model-Driven Software Engineering: A Classification Framework and a Research Map [Extended Abstract]', 'embedding': []}, {'id': 15090, 'abstractText': 'Amplification can occur in a graben as a result of strong earthquake-induced ground motion. Thus, in seismic hazard and seismic site response studies, it is of the utmost importance to determine the geometry of the bedrock depth. The main objectives of this study were to determine the bedrock depth and map the depth-to-bedrock ratio for use in land use planning in regard to the mitigation of earthquake hazards in the Eskişehir Basin. The fundamental resonance frequencies (f<inf>r</inf> ) of 318 investigation sites in the Eskişehir Basin were determined through case studies, and the 2-D S-wave velocity structure down to the bedrock depth was explored. Single-station microtremor data were collected from the 318 sites, as well as microtremor array data from nine sites, seismic reflection data from six sites, deep-drilling log data from three sites and shallow drilling log data from ten sites in the Eskişehir Graben. The fundamental resonance frequencies of the Eskişehir Basin sites were obtained from the microtremor data using the horizontal-to vertical (H/V) spectral ratio (HVSR) method. The phase velocities of the Rayleigh waves were estimated from the microtremor data using the spatial autocorrelation (SPAC) method. The fundamental resonance frequency range at the deepest point of the Eskişehir Basin was found to be 0.23–0.35\\ue251Hz. Based on the microtremor array measurements and the 2-D S-wave velocity profiles obtained using the SPAC method, a bedrock level with an average velocity of 1300 m\\ue251s<sup>−1</sup> was accepted as the bedrock depth limit in the region. The log data from a deep borehole and a seismic reflection cross-section of the basement rocks of the Eskişehir Basin were obtained and permitted a comparison of bedrock levels. Tests carried out using a multichannel walk-away technique permitted a seismic reflection cross-section to be obtained up to a depth of 1500–2000\\ue251m using an explosive energy source. The relationship between the fundamental resonance frequency in the Eskişehir Basin and the results of deep drilling, shallow drilling, shear wave velocity measurement and sedimentary cover depth measurement obtained from the seismic reflection section was expressed in the form of a nonlinear regression equation. An empirical relationship between f<inf>r</inf>, the thickness of sediments and the bedrock depth is suggested for use in future microzonation studies of sites in the region. The results revealed a maximum basin depth of 1000\\ue251m, located in the northeast of the Eskişehir Basin, and the SPAC and HVSR results indicated that within the study area the basin is characterized by a thin local sedimentary cover with low shear wave velocity overlying stiff materials, resulting in a sharp velocity contrast. The thicknesses of the old Quaternary and Tertiary fluvial sediments within the basin serve as the primary data sources in seismic hazard and seismic site response studies, and these results add to the body of available seismic hazard data contributing to a seismic microzonation of the Eskişehir Graben in advance of the severe earthquakes expected in the Anatolian Region.', 'title': 'An investigation into the bedrock depth in the Eskisehir Quaternary Basin (Turkey) using the microtremor method', 'embedding': []}, {'id': 15091, 'abstractText': 'We extend the velocity channel analysis (VCA), introduced by Lazarian &amp; Pogosyan, of the intensity fluctuations in the velocity slices of position–position–velocity (PPV) spectroscopic data from Doppler broadened lines to study statistical anisotropy of the underlying velocity and density that arises in a turbulent medium from the presence of magnetic field. In particular, we study analytically how the anisotropy of the intensity correlation in the channel maps changes with the thickness of velocity channels. In agreement with the earlier VCA studies, we find that the anisotropy in the thick channels reflects the anisotropy of the density field, while the relative contribution of density and velocity fluctuations to the thin velocity channels depends on the density spectral slope. We show that the anisotropies arising from Alfvén, slow and fast magnetohydrodynamical modes are different; in particular, the anisotropy in PPV created by fast modes is opposite to that created by Alfvén and slow modes, and this can be used to separate their contributions. We successfully compare our results with the recent numerical study of the PPV anisotropies measured with synthetic observations. We also extend our study to the medium with self-absorption as well as to the case of absorption lines. In addition, we demonstrate how the studies of anisotropy can be performed using interferometers.', 'title': 'Extending velocity channel analysis for studying turbulence anisotropies', 'embedding': []}, {'id': 15092, 'abstractText': 'Software developers build complex systems using plenty of third-party libraries. Documentation is key to understand and use the functionality provided via the libraries APIs. Therefore, functionality is the main focus of contemporary API documentation, while cross-cutting concerns such as security are almost never considered at all, especially when the API itself does not provide security features. Documentations of JavaScript libraries for use in web applications, e.g., do not specify how to add or adapt a Content Security Policy (CSP) to mitigate content injection attacks like Cross-Site Scripting (XSS). This is unfortunate, as security-relevant API documentation might have an influence on secure coding practices and prevailing major vulnerabilities such as XSS. For the first time, we study the effects of integrating security-relevant information in non-security API documentation. For this purpose, we took CSP as an exemplary study object and extended the official Google Maps JavaScript API documentation with security-relevant CSP information in three distinct manners. Then, we evaluated the usage of these variations in a between-group eye-tracking lab study involving N=49 participants. Our observations suggest: (1) Developers are focused on elements with code examples. They mostly skim the documentation while searching for a quick solution to their programming task. This finding gives further evidence to results of related studies. (2) The location where CSP-related code examples are placed in non-security API documentation significantly impacts the time it takes to find this security-relevant information. In particular, the study results showed that the proximity to functional-related code examples in documentation is a decisive factor. (3) Examples significantly help to produce secure CSP solutions. (4) Developers have additional information needs that our approach cannot meet. Overall, our study contributes to a first understanding of the impact of security-relevant information in non-security API documentation on CSP implementation. Although further research is required, our findings emphasize that API producers should take responsibility for adequately documenting security aspects and thus supporting the sensibility and training of developers to implement secure systems. This responsibility also holds in seemingly non-security relevant contexts.', 'title': '\"I just looked for the solution!\" - On Integrating Security-Relevant Information in Non-Security API Documentation to Support Secure Coding Practices', 'embedding': []}, {'id': 15093, 'abstractText': \"Grain is the relationship between the people's livelihood and social stability of the special goods, is the stability of the world's important strategic materials, but also is the human survival and social development of a solid foundation, the world attaches great importance to this. Grain production is closely related to climate factors, especially in the current stage of serious ecological environment destruction. Climate change has become an important factor affecting grain production and security. The scholars speculate that in the next 20–50 years, China's agriculture will be seriously affected by climate factors, and the grain production will be adversely affected. Analysis of meteorological factors affecting grain production, with a view to the development of food policy and the implementation of grain security system control to provide rationalization proposals. Wheat is one of the main grain crops in China, and annual planting area and total yield are second only to rice and maize. Henan is a major grain production province in China, which mainly grow winter wheat. During the whole growth and development of winter wheat, the meteorological factors such as temperature, light and rainfall are important environmental factors that affect the normal growth of winter wheat. Many scholars at home and abroad have done a lot of fruitful work in the field of meteorology and wheat production. Chinese studies on meteorology and grain production can be traced back to the Northern Wei Dynasty. China's outstanding agronomist Jia Sixie, in his book “Qi Min Yao Shu” described in detail the seasons, climates, crops and their mutual relationships. Most of the above research methods are based on the principle of statistics and fuzzy mathematics to study the effect of meteorological factors on the yield of wheat growth period. The grain production system itself is a grey system, which is influenced by many influencing factors. The large sample of data defects, for food production system is no longer applicable, and grey system theory just to make up for the mathematical statistics method defects. The existing literature review focuses on the impact of meteorological factors on the yield of grain during the whole growth period, there is little literature about combining phenology changes to study the influence of meteorological factors on different growth stages of crops, the results are not well explained its various growth stages of the impact mechanism. In order to explore the influence of meteorological factors in different growth stages of crops, this paper firstly chooses winter wheat in Henan province as research object by dividing the winter wheat growth period into nine growth stages according to the phonological map. Then the study selects the yield data and daily average meteorological data of winter wheat in Henan province from 2005 to 2015 and uses the exponential smoothing method to calculate the meteorological yield of winter wheat, and finally establishes the meteorological factors set and the grey relational analysis model to doubly analyze the influence of meteorological factors on winter wheat in different growth stages. The results show that in the case of single analysis, the average relative humidity has the greatest influence on meteorological yield of winter wheat during sowing stage, tillering stage, overwintering stage and grouting stage; the mean wind speed has the greatest influence on meteorological yield of winter wheat during the period of reviving stage, jointing stage and maturing stage; the average daily temperature has the greatest influence on meteorological yield of winter wheat during the seeding stage; the duration of illumination has the greatest influence on meteorological yield of winter wheat during the heading stage. In the case of double quantitative analysis, the meteorological yield of winter wheat was influenced by the average relative temperature and the daily maximum temperature at the seedling stage, the average relative humidity of the tillering stage and the overwintering stage and the mean wind speed of jointing stage. Generally speaking, the results of this study are in agreement with previous studies, And this article more comprehensive and specific study of the whole growth stage of winter wheat different meteorological factors on the impact of meteorological production. Through the study of the influence of meteorological factors on grain yield, it is of great significance to the sustainable development of agriculture in Henan province and to improve the economic strength of Henan province, which is of theoretical significance to the social and economic development of the province and the improvement of grain yield.\", 'title': 'Quantitative analysis of the influence of meteorological factors at different growth stages on yield of winter wheat based on grey relational analysis', 'embedding': []}, {'id': 15094, 'abstractText': 'Estimating a depth map and, at the same time, predicting the 3D pose of an object from a single 2D color image is a very challenging task. Depth estimation is typically performed through stereo vision by following several time-consuming stages, such as epipolar geometry, rectification and matching. Alternatively, when stereo vision is not useful or applicable, depth relations can be inferred from a single image as studied in this paper. More precisely, deep learning is applied in order to solve the problem of estimating a depth map from a single image. Then, that map is used for predicting the 3D pose of the main object depicted in the image. The proposed model consists of two successive neural networks. The first network is based on a Generative Adversarial Neural network (GAN). It estimates a dense depth map from the given color image. A Convolutional Neural Network (CNN) is then used to predict the 3D pose from the generated depth map through regression. The main difficulty to jointly estimate depth maps and 3D poses using deep networks is the lack of training data with both depth and viewpoint annotations. This contribution assumes a cross-domain training procedure with 3D CAD models corresponding to objects appearing in real images in order to render depth images from different viewpoints. These rendered images are then used to guide the GAN network to learn the mapping from the image domain to the depth domain. By exploiting the dataset as a source of training data, the proposed model outperforms state-of-the-art models on the PASCAL 3D+ dataset. The code of the proposed model is publicly available at https://github.com/SaddamAbdulrhman/Depth-and-Viewpoint-Estimation/tree/master.', 'title': 'Adversarial Learning for Depth and Viewpoint Estimation From a Single Image', 'embedding': []}, {'id': 15095, 'abstractText': 'Narrative sensemaking is a fundamental process to understand sequential information. Narrative maps are a visual representation framework that can aid analysts in this process. They allow analysts to understand the big picture of a narrative, uncover new relationships between events, and model connections between storylines. As a sensemaking tool, narrative maps have applications in intelligence analysis, misinformation modeling, and computational journalism. In this work, we seek to understand how analysts construct narrative maps in order to improve narrative map representation and extraction methods. We perform an experiment with a data set of news articles. Our main contribution is an analysis of how analysts construct narrative maps. The insights extracted from our study can be used to design narrative map visualizations, extraction algorithms, and visual analytics tools to support the sensemaking process.', 'title': 'Narrative Sensemaking: Strategies for Narrative Maps Construction', 'embedding': []}, {'id': 15096, 'abstractText': 'Residue contact maps contain important information for understanding the structure and function of proteins, thus contact map prediction is an important problem in the bioinformatics field. In recent years, deep learning has become a very popular tool in many research fields. However, the studies of using deep models to predict residue contact maps are very few. This work attempts to identify contact maps based on a recent breakthrough in deep learning, the residual network. The residual network distinguishes itself from other deep convolutional networks in that it incorporates a structure improvement called identity mapping to enable the neural network to go much deeper without consequent training difficulty. Moreover, dilated convolution is employed into this network to obtain a better performance by enlarging the receptive field of the network. A prediction is made based on the input features of a protein and all the features are input into the network at the same time. The experiments demonstrate that the dilated residual network outperforms the original residual network in contact map prediction. We test the networks on 3 test sets: CAMEO, CASP11 and a self-built independent test set. On top L/5 long-range contacts of the three test sets, the accuracy of the dilated network is higher than the non-dilated one by 5.2 %, 4.6% and 2.9%, respectively. Furthermore, it is confirmed that applying different networks on different features is a worse idea than taking them in together. The accuracy on top L/5 long-range contacts of the latter network is higher than the former one by 9.8% on the CAMEO set, 7.0% on the CASP11 set and 5.9% on the self-built independent test set.', 'title': 'Prediction of protein contact map by fully convolutional dilated residual network', 'embedding': []}, {'id': 15097, 'abstractText': 'We present a novel framework for automatic and efficient synthesis of historical handwritten Arabic text. The main purpose of this framework is to assist word spotting and keyword searching in handwritten historical documents. The proposed framework consists of two main procedures: building a letter connectivity map and synthesizing words. A letter connectivity map includes multiple instances of the various shape of each letter, since a letter in Arabic usually has multiple shapes depends in its position in the word. Each map represents one writer and encodes the specific handwriting style. The letter connectivity map is used to guide the synthesis of any Arabic continuous subword, word, or sentence. The proposed framework automatically generates the letter connectivity map annotation from a several pages historical pages previously annotated. Once the letter connectivity map is available our framework can synthesis the pictorial representation of any Arabic word or sentence from their text representation. The writing style of the synthesized text resembles the writing style of the input pages. The synthesized words can be used in word-spotting and many other historical document processing applications. The proposed approach provides an intuitive and easy-to-use framework to search for a keyword in the rest of the manuscript. Our experimental study shows that our approach enables accurate results in word spotting algorithms.', 'title': 'Automatic Synthesis of Historical Arabic Text for Word-Spotting', 'embedding': []}, {'id': 15098, 'abstractText': 'This paper presents a novel wireframe map structure for resource-constrained robots operating in a rectilinear 2D environment. The wireframe representation compactly represents geometry, in addition to transient situations such as occlusions and boundaries of unexplored regions. We formulate a particle filter to suit this sparse wireframe map structure. Functions for calculating the likelihood of scans, merging wireframes, and resampling are developed to accommodate this map representation. The wireframe structure with the particle filter allows for severe discrete map errors to be corrected, leading to accurate maps with small storage requirements. We show in a simulation study that the algorithm attains a map of an environment with 1 % error, compared to an occupancy grid map obtained with GMapping which attained 23% error with the same storage requirements. A simulation mapping a large environment demonstrates the algorithms scalability.', 'title': 'Wireframe Mapping for Resource-Constrained Robots', 'embedding': []}, {'id': 15099, 'abstractText': 'This study was conducted to create driving episodes using machine-learning-based algorithms that address long-term memory (LTM) and topological mapping. This paper presents a novel episodic memory model for driving safety according to traffic scenes. The model incorporates three important features: adaptive resonance theory (ART), which learns time-series features incrementally while maintaining stability and plasticity for time-series data; self-organizing maps (SOMs), which represent input data as a map with topological relations using self-mapping characteristics; and counter propagation networks (CPNs), which label category maps using input features and counter signals. Category maps represent driving episode information that includes driving contexts and facial expressions. The bursting states of respective maps produce LTM, which is created on ART as episodic memory. Evaluation of the experimentally obtained results show the possibility of using recorded driving episodes with image datasets obtained using an event data recorder (EDR) with two cameras. Using category maps, we visualize driving features according to driving scenes on a public road and an expressway.', 'title': 'Adaptive learning based driving episode description on category maps', 'embedding': []}, {'id': 15100, 'abstractText': 'One of the essential technologies required for environmental recognition of an autonomous vehicle is a localization technique that recognizes the position and orientation of the vehicle. In contrast to previous localization techniques that generate map data from sensor data itself, there is an increasing number of studies using high definition (HD) digital maps. The map-based localization technology consists of predicting the position of the next step through the ego-motion of the vehicle and determining the position through map matching. In this paper, we propose a robust ego-motion estimation and map matching technology for robust vehicle localization. First, we propose a visual odometry (VO) model for robust ego-motion estimation and a vehicle planar motion model based on in-vehicle sensors to improve the robustness of VO in the absence of image features. We also introduce a new line segmentation matching model and a geometric correction method of extracted road marking from an inverse perspective mapping (IPM) for robust map matching techniques. The technology proposed in this paper has been verified in various ways through real autonomous vehicles and successfully acquired the autonomous driving license of the Republic of Korea.', 'title': 'Robust Ego-motion Estimation and Map Matching Technique for Autonomous Vehicle Localization with High Definition Digital Map', 'embedding': []}, {'id': 15101, 'abstractText': 'NAND flash memory has been the default storage component in mobile systems. One of the key technologies for flash management is the address mapping scheme between logical addresses and physical addresses, which deals with the inability of in-place-updating in flash memory. Demand-based page-level mapping cache is often applied to match the cache size constraint and performance requirement of mobile storage systems. However, recent studies showed that the management overhead of mapping cache schemes is sensitive to the host I/O patterns, especially when the mapping cache is small. This paper presents a novel I/O scheduling scheme, called MAP, to alleviate this problem. The proposed scheduling approach reorders I/O requests for performance improvement from two angles: Prioritizing the requests that will hit in the mapping cache, and grouping requests with related logical addresses into large batches. Experimental results show that MAP improved upon traditional I/O schedulers by 30% and 8% in terms of read and write latencies, respectively.', 'title': 'I/O scheduling with mapping cache awareness for flash based storage systems', 'embedding': []}, {'id': 15102, 'abstractText': 'Concept maps are significant tools able to support several tasks in the educational area such as curriculum design, knowledge organization and modeling, students’ assessment and many others. They are also successfully used in learning activities in which students have to represent domain knowledge according to teacher’s assignment. In this context, the development of Learning Analytics approaches would benefit of methods that automatically compare concept maps. Detecting concept maps similarities is relevant to identify how the same concepts are used in different knowledge representations. Algorithms for comparing graphs have been extensively studied in the literature, but they do not appear appropriate for concept maps. In concept maps, concepts exposed are at least as relevant as the structure that contains them. Neglecting the semantic and didactic aspect inevitably causes inaccuracies and the consequently limited applicability in Learning Analytics approaches. In this work, starting from an algorithm which compares didactic characteristic of concept maps, we present an extension which exploits a semantic approach to catch the actual meaning of the concepts expressed in the nodes of the map.', 'title': 'Enriching Didactic Similarity Measures of Concept Maps by a Deep Learning Based Approach', 'embedding': []}, {'id': 15103, 'abstractText': 'Hospitals are known as one of the most important types of infrastructure systems as they are expected to provide health care services continuously, irrespective of the hazardous conditions. Therefore, a framework should be developed to evaluate the existing level of safety of hospitals. As the initial step, the hazards which can affect need to be identified and the level of exposure should be determined. Among these hazards, geological and hydro-meteorological hazards which falling under natural hazards, play a major role towards the safety measures in hospitals. It is because the number of affected people and the affected buildings are higher under geological and hydro-meteorological hazards. Therefore, it is essential to develop exposure maps under these natural hazards. As the first step, hazard maps of Tsunami, floods, landslides, etc. could be obtained. After that, a layer hospitals could be created and then the exposure map could be obtained by locating the hospital layer on the hazard maps. This would illustrates the hazards affecting each hospital in Sri Lanka. This paper discusses the process of developing a framework of the development of exposure maps of Sri Lankan hospitals by superimposing the hospital layer in Sri Lanka on the hazard maps, using a mapping software “ArcGIS” with case studies.', 'title': 'A Framework to Develop Multi-Hazard Maps to Identify the Natural Hazards which Affect the Safety of Sri Lankan Hospitals', 'embedding': []}, {'id': 15104, 'abstractText': 'This paper establishes novel methods for vehicle localization and mapping using a 1D linear automotive radar array in conjuncture with pre-existing lidar maps, and tests if the generated radar map can be made to be 3 dimensional. The basic design of this study was to implement a SLAM (Simultaneous Localization And Mapping) system that co-registers radar data to radar data, and/or register radar data to lidar data. After the execution of experiments, it was established that it is possible to localize the car by relating observed radar data to pre-made lidar maps, and to continually add to a cumulative map made with the radar data that can further aid the localization process. Furthermore, the radar map created using the 1D linear automotive array can be extended to 3D with proposed processing chain, though more experiments to establish the full potential of this capability are recommended.', 'title': 'Localization and 3D Mapping using 1D Automotive Radar Sensor', 'embedding': []}, {'id': 15105, 'abstractText': \"Abstract syntax tree (AST) mapping algorithms are widely used to analyze changes in source code. Despite the foundational role of AST mapping algorithms, little effort has been made to evaluate the accuracy of AST mapping algorithms, i.e., the extent to which an algorithm captures the evolution of code. We observe that a program element often has only one best-mapped program element. Based on this observation, we propose a hierarchical approach to automatically compare the similarity of mapped statements and tokens by different algorithms. By performing the comparison, we determine if each of the compared algorithms generates inaccurate mappings for a statement or its tokens. We invite 12 external experts to determine if three commonly used AST mapping algorithms generate accurate mappings for a statement and its tokens for 200 statements. Based on the experts' feedback, we observe that our approach achieves a precision of 0.98-1.00 and a recall of 0.65-0.75. Furthermore, we conduct a large-scale study with a dataset of ten Java projects containing a total of 263,165 file revisions. Our approach determines that GumTree, MTDiff and IJM generate inaccurate mappings for 20%-29%, 25%-36% and 21%-30% of the file revisions, respectively. Our experimental results show that state-of-the-art AST mapping algorithms still need improvements.\", 'title': 'A Differential Testing Approach for Evaluating Abstract Syntax Tree Mapping Algorithms', 'embedding': []}, {'id': 15106, 'abstractText': 'In this letter, we propose a method for accurately estimating the 6-Degree Of Freedom (DOF) pose in an urban environment when a High Definition (HD) map is available. An HD map expresses 3D geometric data with semantic information in a compressed format and thus is more memory-efficient than point cloud maps. The small capacity of HD maps can be a significant advantage for autonomous vehicles in terms of map storage and updates within a large urban area. Unfortunately, existing approaches failed to sufficiently exploit HD maps by only estimating partial pose. In this study, we present a full 6-DOF localization against an HD map using an onboard stereo camera with semantic information from roads. We introduce an 8-bit representation for road information, which allow for effective bitwise operation when matching between query data and the HD map. For the pose estimation, we leverage a particle filter followed by a full 6-DOF pose optimization. Our experimental results show a median error of approximately 0.3 m in the lateral and longitudinal directions for a drive of approximately 11 km. These results can be used by autonomous vehicles to correct the global position without using Global Positioning System (GPS) data in highly complex urban environments. The median operation speed is approximately 60 msec supporting 10 Hz.', 'title': 'HDMI-Loc: Exploiting High Definition Map Image for Precise Localization via Bitwise Particle Filter', 'embedding': []}, {'id': 15107, 'abstractText': 'Detection of near-ground objects occluded by above-ground vegetation from airborne light detection and ranging (lidar) measurements remains challenging. Our hypothesis is that the probability of obstruction due to objects above ground at any location in the forest environment can be reasonably characterized solely from airborne lidar data. The essence of our approach is to develop a data-driven learning scheme that creates high-resolution two-dimensional (2-D) probability maps for obstruction in the under-canopy environment. These maps contain information about the probabilities of obstruction (clutter map) and lidar undersampling (uncertainty map) in the near-ground space. Airborne and terrestrial lidar data and field survey data collected within the forested mountainous environment of Shenandoah National Park, Virginia, USA are utilized to test and evaluate the proposed approach in this work. A newly developed individual tree detection algorithm is implemented to estimate the undersampled stem contributions to the probability of obstruction. Results show the effectiveness of the tree detection algorithm with an accuracy index (AI) of between 61.5% and 80.7% (tested using field surveys). The estimated clutter maps are compared to the maps created from terrestrial scans (i.e., ground truth) and the results show the root-mean-square error (RMSE) of 0.28, 0.32, and 0.34 at three study sites. The overall framework in deriving near-ground clutter and uncertainty maps from airborne lidar data would be useful information for the prediction of line-of-sight visibility, mobility, and above-ground forest biomass.', 'title': 'Estimation of 2-D Clutter Maps in Complex Under-Canopy Environments From Airborne Discrete-Return Lidar', 'embedding': []}, {'id': 15108, 'abstractText': 'Recent progress in 3D sensor devices and in semantic mapping allows to build very rich HD 3D maps very useful for autonomous navigation and localization. However, these maps are particularly huge and require important memory capabilities as well computational resources. In this paper, we propose a new method for summarizing a 3D map (Mesh)as a set of compact spheres in order to facilitate its use by systems with limited resources (smartphones, robots, UAVs,...). This vision-based summarizing process is applied in a fully automatic way using jointly photometric, geometric and semantic information of the studied environment. The main contribution of this research is to provide a very compact map that maximizes the significance of its content while maintaining the full visibility of the environment. Experimental results in summarizing large-scale 3D map demonstrate the feasibility of our approach and evaluate the performance of the algorithm.', 'title': 'Summarizing Large Scale 3D Mesh', 'embedding': []}, {'id': 15109, 'abstractText': 'This study focuses on effects of uniform and full height map correction methods for dewarping book spread images in an automated book reader design for individuals with visual impairment and blindness. The design concept could also be applied to address the challenging process of book digitization. The method is dependent on the geometry of the book reader setup for acquiring the 3-D maps that yield a high reading accuracy. The experiments were performed on a testing dataset consisting of 142 pages with their corresponding depth maps that were extracted. The accuracy of the book spread images was quantified and measured by introducing the corrected images to an Optical Character Recognition engine. Initially, the book spreads were tested by placing them with a standard alignment which yielded an average accuracy of 95.55% and 96.11% with the uniform maps and the full height maps, respectively. Rotations of the book spreads are introduced in a separate test to see if the two proposed methods are tolerant to unsuspected misaligned placements of the book. These tests yield an average accuracy of 90.63% for the corrections with a uniform map and 94.75% with the full height maps.', 'title': 'Uniform vs Full Height Maps Using a Time of Flight Device for Dewarping Book Spread Images in the Design of an Automated Book Reader', 'embedding': []}, {'id': 15110, 'abstractText': 'This paper considers the method of the winter crop classification map producing in terms of climatic and weather abnormal conditions in 2020. Given that the traditional method of construction involves the use of a training sample, which is collected in ground surveys along the roads. This sample could not be collected under the strict quarantine regime, that is why the classification map was created based on the sample obtained as a result of the photointerpretation. Both, optical Sentine1-2 and SAR Sentinel-l satellite data were used. This is due to the fact, that the period of the winter crop classification map producing fell exactly on the period of time (April and May 2020), when the area of study Odesa region (as well as the whole territory of Ukraine) had a high percentage of cloud cover. At the same time, radar imaging techniques allow us to bypass obstacles such as clouds, but also have lower sampling quality. Therefore, it was decided to combine the obtained classification maps based on radar and optical data by fuzzy logic, considering the degree of belonging of each pixel by the value of the normalized difference vegetation index (NDVI).As a result, the obtained classification maps based on photointerpretation sample have an accuracy close to 95%. The fuzzy logic method allows to increase this value by selecting only the best pixels from classification maps based on radar and optical satellite data.', 'title': 'Losses Assessment for Winter Crops Based on Satellite Data and Fuzzy Logic', 'embedding': []}, {'id': 15111, 'abstractText': 'Almost all of the state-of-the-art object detectors employ convolutional neural network (CNN) to extract feature. However, how to fully utilize spatial information is a challenge. In this paper, we propose an effective framework for object detection. Our motivation is that multi-scale representation and context are extremely important for object detection. For multi-scale representation, our mothed combines hierarchical feature maps to a fusion map, which has abundant spatial information and high-level semantics. For context, we exploit spatial information by stacking multi-region feature maps. The network is learned end-to-end, by minimize an objective function. Our network achieves competitive results, 75.9% mAP on PASCAL VOC 2007, 72.0% mAP on PASCAL VOC 2012 and 23.2% mAP on MS COCO. The speed of the network is 10 fps. Our studies demonstrate that multiscale representation and context can further improve performance of object detection.', 'title': 'Multi-scale Fusion with Context-aware Network for Object Detection', 'embedding': []}, {'id': 15112, 'abstractText': 'In semantic mapping, which connects semantic information to an environment map, it is a challenging task for robots to deal with both local and global information of environments. In addition, it is important to estimate semantic information of unobserved areas from already acquired partial observations in a newly visited environment. On the other hand, previous studies on spatial concept formation enabled a robot to relate multiple words to places from bottom-up observations even when the vocabulary was not provided beforehand. However, the robot could not transfer global information related to the room arrangement between semantic maps from other environments. In this paper, we propose SpCoMapGAN, which generates the semantic map in a newly visited environment by training an inference model using previously estimated semantic maps. SpCoMapGAN uses generative adversarial networks (GANs) to transfer semantic information based on room arrangements to a newly visited environment. Our proposed method assigns semantics to the map of an unknown environment using the prior distribution of the map trained in known environments and the multimodal observations made in the unknown environment. We experimentally show in simulation that SpCoMapGAN can use global information for estimating the semantic map and is superior to previous methods. Finally, we also demonstrate in a real environment that SpCoMapGAN can accurately 1) deal with local information, and 2) acquire the semantic information of real places.', 'title': 'SpCoMapGAN: Spatial Concept Formation-based Semantic Mapping with Generative Adversarial Networks', 'embedding': []}, {'id': 15113, 'abstractText': 'Maps representing the detailed features of the road network are becoming more and more important for autonomous driving and next generation driver assistance systems. The mapping of the road network by specially equipped vehicles through the well-known map providers leads to usually quarterly map updates, which might result in problems encountered by autonomous vehicles in the case that the road information is outdated. Furthermore, the provided maps could lack details relevant to autonomous vehicles and next generation driver assistance systems. As an alternative, road network data can be acquired by common vehicles and subsequently fused at the backend-side to detailed, up-to-date maps. The deduction of maps consisting of point-shaped landmarks by utilizing approaches to SLAM is not trivial but well studied. However, the road network is comprised of many also non-point-shaped landmarks, such as crossroads, roundabouts, sign gantries, traffic islands and crosswalks. The reduction of these complex landmarks to point-shaped ones or the independent consideration of their basic features, such as points and lines, involves a severe loss of useful information and/or accuracy and is, therefore, not appreciated. Instead, we propose a novel approach to the parametric description of complex landmarks as a directed acyclic graph and to handle complex landmark observations by an extended formulation of Bundle-Adjustment-based Full-SLAM. We focus on the challenges imposed by the fusion of complex landmarks, such as attribute interdependencies, dynamic attribute counts, and partial observations of attributes due to occlusions. Furthermore, we show the convergence and consistency of our approach to the fusion of complex landmark observations by a hybrid, real-world scenario consisting of roundabout and traffic sign observations. The approach is presented in a way that allows a straightforward adaptation to other kinds of complex landmark observations by defining an appropriate parametric description.', 'title': 'Parametric fusion of complex landmark observations present within the road network by utilizing bundle-adjustment-based Full-SLAM', 'embedding': []}, {'id': 15114, 'abstractText': 'Spatial sensing is a fundamental requirement for applications in robotics and augmented reality. In urban spaces such as malls, airports, apartments, and others, it is quite challenging for a single robot to map the whole environment. So, we employ a swarm of robots to perform the mapping. One challenge with this approach is merging sub-maps built by each robot. In this work, we use wireless access points, which are ubiquitous in most urban spaces, to provide us with coarse orientation between sub-maps, and use a custom ICP algorithm to refine this orientation to merge them. We demonstrate our approach with maps from a building on campus and evaluate it using two metrics. Our results show that, in the building we studied, we can achieve an average Absolute Trajectory error of 0.2m in comparison to a map created by a single robot and average Root Mean Square mapping error of 1.3m from ground truth landmark locations.', 'title': 'WISDOM: WIreless Sensing-assisted Distributed Online Mapping', 'embedding': []}, {'id': 15115, 'abstractText': \"In this paper, we study extensions to the Gaussian processes (GPs) continuous occupancy mapping problem. There are two classes of occupancy mapping problems that we particularly investigate. The first problem is related to mapping under pose uncertainty and how to propagate pose estimation uncertainty into the map inference. We develop expected kernel and expected submap notions to deal with uncertain inputs. In the second problem, we account for the complication of the robot's perception noise using warped Gaussian processes (WGPs). This approach allows for non-Gaussian noise in the observation space and captures the possible nonlinearity in that space better than standard GPs. The developed techniques can be applied separately or concurrently to a standard GP occupancy mapping problem. According to our experimental results, although taking into account pose uncertainty leads, as expected, to more uncertain maps, by modeling the nonlinearities present in the observation space WGPs improve the map quality.\", 'title': 'Warped Gaussian Processes Occupancy Mapping With Uncertain Inputs', 'embedding': []}, {'id': 15116, 'abstractText': 'This paper proposes a modular teleoperation software platform based on the robot operating system (ROS), which is flexible and portable for different master and slave devices. The main issues for the modularity are to develop its control modules in MATLAB/Simulink and access ROS by the robotics system toolbox, which will facilitate the design, analysis, and adjustment of the control algorithm in teleoperation systems greatly. In addition, a case study for this kind of modular teleoperation system is carried out, which is composed of a Geomagic Touch as the master device and a robotic arm of the Rethink Baxter robot as the slave device. Due to the asymmetric structure property between the master and slave devices, the proper workspace mapping which can cover the whole workspace of the slave robot and simultaneously achieve an excellent mapping accuracy becomes a challenging issue. Different from the traditional methods, a hybrid workspace mapping algorithm is proposed, which uses the joint space mapping to cover the whole workspace of the slave device and the operating space mapping to conduct the elaborate manipulation. A smoothing switch law between the joint space mapping and the operating space mapping is also designed. Comparative experiments are carried out, and the results verify the effectiveness and excellent performance of the proposed design methodology.', 'title': 'Modular Development of Master-Slave Asymmetric Teleoperation Systems With a Novel Workspace Mapping Algorithm', 'embedding': []}, {'id': 15117, 'abstractText': 'In this study, Farmland Fertility Algorithm, which is one of the meta-heuristic optimization algorithms, has been investigated in detail. Three different approaches and different chaotic maps were used to generate the initial population of the algorithm. The chaotic mapped algorithms were tested with quality test functions and the performance of the algorithm was presented and interpreted through comparative tables and graphs. As a result of the tests, it was observed that the results of the approach of Generating the Whole Population and all dimensions with chaotic maps and the third approach in which the population was produced with one dimensional chaotic maps were generally similar or better with the Farmland Fertility Algorithm. In the second approach, it has been observed that the Farmland Fertility Algorithm, in which the tent (Tent) chaotic map is applied, produces more successful results for almost all dimensions and iterations. In the third approach, successful results were obtained in most of the chaotic map algorithms.', 'title': 'Generating Initial Population of Farmland Fertility Algorithm with Chaotic Maps', 'embedding': []}, {'id': 15118, 'abstractText': 'The visualization of the Pareto optimal solution set is one of important issues of the decision-making process on the multi-objective optimization problem. The Pareto optimal solution visualization method using the self-organizing maps (SOM) is one of promising visualization methods. This method has two shortcomings in the Pareto optimal solution representation capability. One is that the maps have incorrect points that represent non-Pareto optimal solutions. The other is that the coverage of the maps for the edge region of the Pareto optimal solution set is not good. This study proposes a Pareto optimal solution visualization method using SOM-NG. In SOM, winner nodes affect neighbor nodes on the map space irrespective of similarity on the input data space. This causes the above-mentioned shortcomings. SOM-NG can form maps with considering similarity on the input space; hence, the shortcomings are expected to be overcome. In addition, in the proposed method, the learning parameter optimization is introduced. The effectiveness of the proposed method is confirmed on the incorrectness and the coverage of maps through numerical experiments.', 'title': 'A Pareto optimal solution visualization method using SOM-NG with learning parameter optimization', 'embedding': []}, {'id': 15119, 'abstractText': \"This paper points out that when a fire occurs in some special semi-closed places, the masses cannot quickly and effectively escape from the site safely, propose a method that self-established maps and target recognition through the robot at the site and guides the masses to escape quickly through motion planning. Self-established maps include three parts: self-exploration, simultaneous localization and mapping and motion planning. Self-exploration uses an active exploration method based on information theory to guide the robot to move the maximum information gain between the current point and each point on the two-dimensional map as the target point; simultaneous localization and mapping apply the core idea of particle filtering. The particle group is scattered at the initial position and the weight of each particle is determined by scanning matching method. The point with the largest particle weight is selected as the current position of the robot. Thus constructing of the dimension raster map; The motion planning is divided into global planning and local planning. The global path planning uses the Dijkstra algorithm to find the global optimal path in the two-dimensional grid map. The local path planning uses the DWA (Dynamic Window Approach) algorithm to find the best speed in the local range. Target recognition uses a template matching algorithm to determine the threshold by multiple experiments to achieve accurate target recognition. This paper analyzes and studies the key technologies of robot' s self-construction and target recognition on the turtlebot experimental platform, and completes the technical feasibility verification of the project, so as to reduce the personal injury in the actual scene application. In addition, all processes can be operated and monitored on the remote computer.\", 'title': 'Feasibility Verification of Independent Mapping Technology under Semi-Closed Fire Conditions', 'embedding': []}, {'id': 15120, 'abstractText': 'Since visualization is a powerful technique to reinforce human cognition, real time robots can be used to study its environment to make a map or a model. Mapping is a fundamental task in navigation through unknown environments. Due to its increased application in robotics, surveillance, tracking system used in security and many other areas researchers have propelled to continuously experiment in more efficient and competitive techniques. In this paper we present an experimental evaluation for mapping system that robustly generates 2D maps using a RGB-D sensor. The evaluation was done for our mobile system which comprises a Kinect. This work mainly presents research on earlier related techniques for mapping evaluation and a comparison of SLAM algorithms. The paper concludes with the analysis of mapping accuracy with the ground truth. Experiment indicate that Gmapping gives an accurate solution for 2D mapping among many ROS compatible algorithms for computationally limited robots.', 'title': 'Accuracy Evaluation of SLAM Algorithms in RGB-D Sensors', 'embedding': []}, {'id': 15121, 'abstractText': 'Since the mid-1980s, eight eruptions have occurred along the Juan de Fuca and Gorda Ridges in the NE Pacific. MBARI has examined seven of them, including the April 2015 eruption at Axial Seamount, using autonomous underwater vehicles (AUVs) to map at 1-m lateral resolution and remotely operated vehicles (ROVs) to observe and sample the lava flows and hydrothermal deposits. We have done similar work on the Alarcon Rise at the mouth of the Gulf of California, where all eruptions pre-date our studies of the ridge system, but are recent enough to host impressive active hydrothermal systems.AUV sonar data were processed with MB-System software and the resulting maps were brought into a geographical information system (GIS) for display and analysis. The highresolution AUV maps were used at sea as basemaps in a real-time GIS to guide our ROV dives with greater efficiency than possible before. From the maps and observations, we defined lava flow boundaries and channel systems, assessed fault activity, calculated flow sizes, evaluated age relationships between flows, and then sampled lavas for chemistry. Those data were coupled with age dates from sediment samples to place changes in eruption styles, lava chemistry, and hydrothermal systems on the ridges into the previously elusive framework of time. Using these tools, we have constructed geologic field maps and volcanic histories of the spreading ridges, much like volcanologists do on land, but all unprecedented for submarine volcanoes. The synergistic high-resolution mapping and targeted ROV sampling of the ridges has permitted better understanding of when, how, how much, and how often spreading ridges erupt, and how their magmatic and hydrothermal systems change over time.', 'title': 'High-resolution AUV mapping and ROV sampling of mid-ocean ridges', 'embedding': []}, {'id': 15122, 'abstractText': 'NAND flash memory has been the default storage component in embedded systems. One of the key technologies for flash management is the address mapping scheme between logical addresses and physical addresses, which deals with the inability of in-place-updating in flash memory. Demand-based page-level mapping cache is often applied to match the cache size constraint and performance requirement of embedded storage systems. However, recent studies showed that the management overhead of mapping cache schemes is sensitive to the host I/O patterns, especially when the mapping cache is small. This paper presents a novel I/O scheduling scheme, called MAP+, to alleviate this problem. The proposed scheduling approach reorders I/O requests for performance improvement from two angles. Prioritizing the requests that will hit in the mapping cache, and grouping requests with related logical addresses into large batches. Batches of requests are reordered to further optimize request waiting time. Experimental results show that MAP+ improved upon traditional I/O schedulers by 48% and 18% in terms of read and write latencies, respectively.', 'title': 'An I/O Scheduling Strategy for Embedded Flash Storage Devices With Mapping Cache', 'embedding': []}, {'id': 15123, 'abstractText': 'High-precision and reliable localization is current research focus in the area of autonomous vehicles. Previous studies rely on either high-cost sensors or some specific characteristics, which means that the methods are limited to only a bit given situations. In this paper, a road DNA based localization method is proposed. It could afford high-precision result and does not have the shortcomings of previous methods at the same time. The scenery on both sides of the roads are used to generate the prior-map. The map is presented as grid map by the joint probability of occupation and reflectivity. With this type of map, different environments show different properties, which means that this method is not limited to specific environments and is effective in most cases. It costs much less memory than the previous maps. The map and live road scene flatting are both generated by data collected by low-cost LIDAR. Normalized Information Distance is utilized to align the live road scene flatting with the road DNA. Experiments show the validation and precision of this method.', 'title': 'Road DNA based localization for autonomous vehicles', 'embedding': []}, {'id': 15124, 'abstractText': \"Supporting blind and visually impaired people with digital maps that can enhance spatial awareness and learning about the surrounding environment is a challenge. This study introduces an audio-tactile you-are-here map system that presents map elements and users' updated location on a mobile pin-matrix display for blind and visually impaired people. In addition to panning and zooming a map, in the system, a set of tactile map symbols consisting of raised and lowered pins have been proposed to present varying map elements. A field test with eight visually impaired subjects and eight blindfolded subjects who did not have experience with tactile maps and Braille was conducted. Subjects would locate surrounding streets easily after a short-time training, as well as nearby point-of-interests (POIs). The results of the evaluation indicated that the mean relative distance error was significantly lower with nearby POIs, which do not require panning while exploring, than far away POIs. Furthermore, it is important to improve the portability of the proposed prototype and develop a one-hand map exploration method.\", 'title': 'Exploration of Location-Aware You-Are-Here Maps on a Pin-Matrix Display', 'embedding': []}, {'id': 15125, 'abstractText': 'In recent years, the core technology enable the rapid rise of autonomous vehicles like the solving the Simultaneous Localization And Mapping (SLAM) problem on the embedded system. And the gmapping package on the ROS platform providing laser-based SLAM is widely used and studied. However, implementing it on embedded system imposes two challenges to us: running ROS requires large CPU consumption and the algorithm of SLAM is computationally intensive. In this paper, we present a system implementation of the gmapping on an ARM based embedded hardware not installing ROS and we also improve the architecture of the mapping system by parallel execution of mapping and estimating pose. The mapping process is running on the embedded system and it can create a 2-D occupancy map from laser and pose data collected by a mobile robot with low CPU load and time consumption. Experimental results carried out with mobile robots in indoor environments illustrate the accuracy of our implementation and the low consumption of time and CPU load of the mapping system.', 'title': 'Indoor mapping using gmapping on embedded system', 'embedding': []}, {'id': 15126, 'abstractText': 'Ventricular tachycardia (VT) is a life-threatening arrhythmia, which can be treated by catheter intervention. Accurate identification of the underlying reentrant circuit is often challenging, yet it is key to successful ablation of the VT. In practice, the cardiologist often uses electrocardiography (ECG) data provided by various catheter mapping techniques, including parameters acquired during sinus rhythm (voltage maps, presence of fragmented/late potentials) and during controlled pacing from different sites of the ventricle, so-called pace-mapping. A novel method is presented here to automatically extract the key information from pace-mapping data with automated detection of paced heartbeats from the surface ECG signals, using wavelet detection of pacing spikes and combined time/energy criteria, and automated delineation of paced beats, QRS peak, and QRS onset. This allows the generation of correlation gradient maps (indicating QRS morphology changes as the catheter is moved) and stimulus-to-QRS maps (sQRS, indicating the delay between pacing and activation of the healthy myocardium). The delineator is shown to be in good agreement with manual annotations from experts in a retrospective study of 18 VT ablation procedures. Paced-QRS detection had 95.2% sensitivity and 98.4% positive predictive value. Resulting sQRS maps had a mean absolute error of 11.1 ms, which was in the same range as the inter-observer errors (9.7 ms). The automatic processing drastically reduces the need for manual annotations. Therefore it makes it feasible to process and visualize, during the procedure, all the relevant parametric maps, which can be analyzed jointly to identify VT circuits and corresponding ablation targets.', 'title': 'A Paced-ECG Detector and Delineator for Automatic Multi-Parametric Catheter Mapping of Ventricular Tachycardia', 'embedding': []}, {'id': 15127, 'abstractText': 'Estimating 3D hand poses from RGB images is essential to a wide range of potential applications, but is challenging owing to substantial ambiguity in the inference of depth information from RGB images. State-of-the-art estimators address this problem by regularizing 3D hand pose estimation models during training to enforce the consistency between the predicted 3D poses and the ground-truth depth maps. However, these estimators rely on both RGB images and the paired depth maps during training. In this study, we propose a conditional generative adversarial network (GAN) model, called Depth-image Guided GAN (DGGAN), to generate realistic depth maps conditioned on the input RGB image, and use the synthesized depth maps to regularize the 3D hand pose estimation model, therefore eliminating the need for ground-truth depth maps. Experimental results on multiple benchmark datasets show that the synthesized depth maps produced by DGGAN are quite effective in regularizing the pose estimation model, yielding new state-of-the-art results in estimation accuracy, notably reducing the mean 3D endpoint errors (EPE) by 4.7%, 16.5%, and 6.8% on the RHD, STB and MHP datasets, respectively.', 'title': 'DGGAN: Depth-image Guided Generative Adversarial Networks for Disentangling RGB and Depth Images in 3D Hand Pose Estimation', 'embedding': []}, {'id': 15128, 'abstractText': 'In making a game, a map is an important component. In making maps, several techniques can be used, one of which uses the Procedural Content Generator (PCG) method. In making maps using PCG can apply the Perlin Noise algorithm, as a generator engine for making maps automatically. The Algorithm Perlin Noise can make a noise gradient of and store values from 1 to 0 in each pixel. This value can be utilized as the height value of a 3D map formed from a point which is then connected to a surface called a mesh. The bigger the mesh, the more detailed a map will be. However, there are obstacles in its formation, namely the burden of the processor in processing the map. The level of detail (LOD) in a mesh will affect the workload of the processor, so we need a dynamic LOD. In this study, game performance measurements were performed using the average FPS with the application of Dynamic LOD and LOD Statistics. The performance test managed to increase processor performance to the maximum extent but did not reduce the overall performance of the game. In table 3 the connectedness calculation uses the person correlation method that the connectedness between the CPU and Vertex has a value of -0.81942 which means that if the CPU goes up the vertices go down, and the value of the connectedness between the CPU and LOD is 0.92299 which means if the CPU performance goes up, the LOD will go up, this indicates The CPU is optimized to run the rendering process and can optimize processor performance.', 'title': 'Optimizing Game Performance with Dynamic Level of Detail Mesh Terrain Based on CPU Usage', 'embedding': []}, {'id': 15129, 'abstractText': 'Concept mapping is a great activity to capture and describe ideas. Several tools that are developed to support concept mapping activities. Several studies show that computer- supported concept mapping tools would contribute to a more efficient collaborative learning performance than using a traditional pencil and paper. The interaction style on how they use and experience with the tool also may differ. A computer- supported concept map authoring tool has been developed in this research to support concept mapping for the Kit-Build Concept Map framework. This research evaluates and analyzes the user experience of two the concept map authoring tools, where one of them incorporates keywords and propositions suggestions support features to construct a concept map from an English reading. The evaluation and analysis are performed by using the short version of the User Experience Questionnaire. The result shows that both the tools are having a positive user experience. The tool that provides support features is hedonically better than the basic tool even though it is not always better pragmatically.', 'title': 'User Experience Evaluation on Computer- Supported Concept Map Authoring Tool of Kit-Build Concept Map Framework', 'embedding': []}, {'id': 15130, 'abstractText': 'Motivated by the concept of circuit design in digital circuit, this paper proposes a one-dimensional (1D) nonlinear model (1D-NLM) for producing 1D discrete-time chaotic maps. Our previous works have designed four nonlinear operations of generating new chaotic maps. However, they focus only on discussing individual nonlinear operations and their properties, but fail to consider their relationship among these operations. The proposed 1D-NLM includes these existing nonlinear operations, develops two new nonlinear operations, discusses their relationship among different nonlinear operations, and investigates the properties of different combinations of these operations. To show the effectiveness of 1D-NLM in generating new chaotic maps, as examples, we provide four new chaotic maps and study their dynamics properties from following three aspects: equilibrium point, stability, and bifurcation diagram. Performance evaluations are provided using the Lyapunov exponent, Shannon entropy, correlation dimension, and initial state sensitivity. The evaluation results show that these new chaotic maps have more complex chaotic behaviors than existing ones. To demonstrate the performance of 1D-NLM in practical applications, we use a pseudo-random number generator (PRNG) to compare new and existing chaotic maps. The randomness test results indicate that new chaotic map generated by 1D-NLM shows better performance than existing ones in designing PRNG.', 'title': 'One-Dimensional Nonlinear Model for Producing Chaos', 'embedding': []}, {'id': 15131, 'abstractText': 'Non-acoustic sensors are widely used in speech signal processing tasks, and their immunity to the background acoustic noise shows great benefits to traditional speech enhancement. To avoid using acoustic speech disturbed by strong noise, spectra mapping from throat microphone (TM) speech to acoustic microphone (AM) speech has been studied. However, there is a distinguished difference between the spectra of the two kinds of speech, and the mapping relationship is different in the low-band and high-band spectra, which limits the performance of the traditional full-band spectra mapping model. In this paper, to improve the perceived quality and intelligibility of TM speech, we investigate the low-band and high-band spectral structure between TM and AM speech, respectively, and propose a spectra-based band-division mapping framework for TM speech enhancement based on the investigation. In the framework, the low-band target spectra of AM speech are mapped based on the equalization method, and the high-band spectra are mapped from the full-band TM speech spectra, which are lack of high-frequency components. The overall framework can be viewed as a combination of spectra equalization in the low band and spectra generation in the high band. Both the objective and subjective evaluation results show clear advantages over the existing TM speech enhancement method based on the full-band spectra mapping.', 'title': 'A Spectra-Based Equalization-Generation Combined Framework for Throat Microphone Speech Enhancement', 'embedding': []}, {'id': 15132, 'abstractText': \"In this study, we propose Salient SLAM, a real-time 3D map generation with saliency information based on human visual characteristics. We used a Fine-Grained technique to generate a saliency map on the spatial domain. Then, we integrated this saliency map to a real-time feature-based Visual SLAM technique. As a result, we can obtain a three-dimensional feature map with saliency information and enable the ability to use this saliency information in robots' autonomous and exploration. Finally, the Salient SLAM map point is leveled into four levels where the higher the level, the higher the score of saliencies in the environment. In the experiments, we investigated and analyzed the characteristics of each saliency level in global feature maps according to the type of objects and visual patterns.\", 'title': 'Integrating Human Visual Perception in Scene Understanding by Saliency Visual SLAM', 'embedding': []}, {'id': 15133, 'abstractText': 'This study proposes to develop, build and implement the IoT of the mobile network robot integrated with features of mapping and location in an internal environment, to trace the best route and obtain a fast and efficient change to detect changes in the environment as an identifier of falls in the elderly. It is observed that it is applied research to aggregate several available algorithms. The robot was provided with internal mapping and location capabilities to map the best route and achieve fast and active movements in a retirement home for the elderly. The mobile robot was also set up to monitor and assist in the transport of medicines and notify the caregiver of any incident with the elderly within its environment. A mobile app controls system and robot development. The main phases are highlighted: definition and acquisition of the model and the components used (mechanical structure, microcontroller, sensors and actuators); application development of user-system interaction; development and construction of a robot, auxiliary modules (environment) and central module; and integration experiments. Monitoring and mapping of the environment are performed using Wall-Following, Simultaneous Location and Mapping (SLAM) and Sensor Fusion techniques. The precise movements of the robot are assured through a combination of navigation and control techniques. Moreover, the robot received internal mapping and location, resources to map the best route and obtain quick and active movements in Institutions of Long Stay for the Elderly (L.T.C.F.). Besides, the performance of the following algorithms was analyzed and compared: Breadth-First Search (B.F.S.), Depth First Search (D.F.S.), and Wall-Following. The B.F.S. algorithm obtained the best results for the minimum path.', 'title': 'Integration of the Mobile Robot and Internet of Things to Monitor Older People', 'embedding': []}, {'id': 15134, 'abstractText': 'In a fuzzy cognitive map-based forecasting model, causal relationships (represented with a weight matrix) are constant. This may hinder the applicability of such a model. In this paper, we propose an adaptive fuzzy cognitive map-based forecasting model. Different from the existing models, the proposed model is made of a collection of fuzzy cognitive maps. Maps are constructed according to the clustering results of the so-called premises covering an entire time series. Subsequently, we use an optimization algorithm to train parameters of each fuzzy cognitive map individually. The proposed model construction procedure allows forming fuzzy cognitive maps that more flexible and, thus, suitable for forecasting of long time series. In experimental studies on synthetic time series and real time series, the proposed model performed very well in comparison with the original fuzzy cognitive map-based forecasting model and another two forecasting models.', 'title': 'A New Adaptive Fuzzy Cognitive Map-Based Forecasting Model for Time Series', 'embedding': []}, {'id': 15135, 'abstractText': 'In multi-label image classification, each image is always associated with multiple labels and labels are usually correlated with each other. The intrinsic relation among labels can definitely contribute to classifier training. However, most previous studies on active learning for multi-label image classification purely mine label correlation based on observed label distribution. They ignore the mapping relation between examples and their labels. This mapping relation also implicates label relationship. Ignoring the mapping relation leads to an uncomprehensive label correlation estimation and results in a bad performance for classification. In this paper, we propose a novel multi-label active learning with low-rank mapping for image classification, called LMMAL, to solve this issue. More precisely, we train a low-rank mapping matrix to signify the mapping relation between the feature space and the label space of a certain multi-label dataset. Using this low-rank mapping relation, we exploit a full label correlation. Subsequently, an effective sampling strategy is designed by integrating this potential information with uncertainty to select the most informative example-label pairs. In addition, we extend LMMAL with automatic labeling (denoted as AL-LMMAL) to further reduce the annotation workload of active learning. Empirical results demonstrate the effectiveness of our approaches.', 'title': 'Multi-label active learning with low-rank mapping for image classification', 'embedding': []}, {'id': 15136, 'abstractText': 'Near realtime flood mapping in densely populated urban areas is critical for emergency response. The strong heterogeneity of urban areas poses a big challenge for accurate near realtime flood mapping. However, previous studies on automatic methods for urban flood mapping perform infeasible in near realtime or fail to generalize well to other floods, for several reasons. First, multitemporal pixel-wise flood mapping requires accurate image registration, hindering the efficiency of large-scale processing. Although automatic image registration has been investigated, precisely coregistered multitemporal image sequence requires time-consuming fine tuning. Additionally, the floods may lead to the loss of many corresponding image points across multitemporal images for accurate coregistration. Second, existing unsupervised methods generally rely on hand-crafted features for floodwater detection. Such features may not well represent the patterns of floodwaters in different areas due to inconsistent weather conditions, illumination, and floodwater spectra. This article proposes a self-supervised learning framework for patch-wise urban flood mapping using bitemporal multispectral satellite imagery. Patch-wise change vector analysis is used with patch features learned through a self-supervised autoencoder to produce patch-wise change maps showing potentially flood-affected areas. Postprocessing including spectral and spatial filtering is applied to these patch-wise change maps to remove nonflood related changes. Final flood maps and parameter sensitivities were evaluated using several performance metrics. Two flood events from areas with differing degrees of urbanization were considered: Hurricane Harvey flood (2017) in Houston, Texas, and Hurricane Florence flood (2018) in Lumberton, North Carolina. The proposed method shows strong performance for self-supervised urban flood mapping.', 'title': 'Urban Flood Mapping With Bitemporal Multispectral Imagery Via a Self-Supervised Learning Framework', 'embedding': []}, {'id': 15137, 'abstractText': 'Localization within high definition maps is a key problem for autonomous navigation as vehicles need to extract information from them. In addition, many navigation tasks are defined with respect to map features. For instance, estimating the cross-track and along-track gaps of a vehicle with respect to a given path is critical for lane keeping or intersection management. Map-based localization is also important for cooperative tasks like platooning in curved roads or lane changing. This work studies different methods to compute map-based coordinates defined as curvilinear abscissa, lateral distance and heading with respect to paths in high definition maps. Four approaches using polylines, lanelets and splines are compared. Thanks to real experiments, the discontinuity issues of polylines used in current high definition maps are evaluated and we discuss advantages and drawbacks of splines-based and lanelet methods. We also report experimental results corresponding to a platoon of two vehicles in curved roads and evaluate the effects of the use of low cost GNSS receivers.', 'title': 'Map-based curvilinear coordinates for autonomous vehicles', 'embedding': []}, {'id': 15138, 'abstractText': \"Wildfire is a significant driver of forest and land cover change in the central interior of British Columbia, Canada. Fuel type maps are a primary input to fire behavior calculations and simulation studies that assess wildfire threat at the landscape level. However, these thematic maps are not easily produced at the scale and speed needed to assess and mitigate wildfire threat on an annual basis. The objective of this research was to explore how an artificial neural network could be used with remotely sensed satellite imagery to map and update fuel types on an annual basis. We applied the artificial neural network over a 40 000-km<sup>2</sup> landscape in central interior British Columbia that burned from a megafire in 2017. Fuel maps were generated for the years 2014-2018, assessed through an independent validation, and evaluated against an existing fuel type map. The highest cross-validation overall accuracy during training was 66.5% and overall accuracy from the independent validation was 63.1%. Generally, the maps had fair agreement with the existing fuel type map (circa 2016), with Cohen's Kappa ranging from 0.28 in 2018 to 0.35 in 2015. Several recommendations are provided for future research using artificial neural networks for fuel typing such as assuring quality of training samples through rigorous standards, designing the network architecture, choosing appropriate cost functions and regularization, incorporating learning of temporal features, and identifying novel fuel types from the output activations.\", 'title': 'FuelNet: An Artificial Neural Network for Learning and Updating Fuel Types for Fire Research', 'embedding': []}, {'id': 15139, 'abstractText': 'With the increased amount of digitized historical documents, information extraction from them gains pace. Historical maps contain valuable information about historical, geographical and economic aspects of an era. Retrieving information from historical maps is more challenging than processing modern maps due to lower image quality, degradation of documents and the massive amount of non-annotated digital map archives. Convolutional Neural Networks (CNN) solved many image processing challenges with great success, but they require a vast amount of annotated data. For historical maps, this means an unprecedented scale of manual data entry and annotation. In this study, we first manually annotated the Third Military Mapping Survey of Austria-Hungary historical map series conducted between 1884 and 1918 and made them publicly accessible. We recognized different road types and their pixel-wise positions automatically by using a CNN architecture and achieved promising results.', 'title': 'Automatic Detection of Road Types From the Third Military Mapping Survey of Austria-Hungary Historical Map Series With Deep Convolutional Neural Networks', 'embedding': []}, {'id': 15140, 'abstractText': 'Extraction of residential areas plays an important role in remote sensing image processing. Extracted results can be applied to various scenarios, including disaster assessment, urban expansion, and environmental change research. Quality residential areas extracted from a remote sensing image must meet three requirements: well-defined boundaries, uniformly highlighted residential area, and no background redundancy in residential areas. Driven by these requirements, this study proposes a global and local saliency analysis model (GLSA) for the extraction of residential areas in high-spatial-resolution remote sensing images. In the proposed model, a global saliency map based on quaternion Fourier transform (QFT) and a global saliency map based on adaptive directional enhancement lifting wavelet transform (ADE-LWT) are generated along with a local saliency map, all of which are fused into a main saliency map based on complementarities. In order to analyze the correlation among spectrums in the remote sensing image, the phase spectrum information of QFT is used on the multispectral images for producing a global saliency map. To acquire the texture and edge features of different scales and orientations, the coefficients acquired by ADE-LWT are used to construct another global saliency map. To discard redundant backgrounds, the amplitude spectrum of the Fourier transform and the spatial relations among patches are introduced into the panchromatic image to generate the local saliency map. Experimental results indicate that the GLSA model can better define the boundaries of residential areas and achieve complete residential areas than current methods. Furthermore, the GLSA model can prevent redundant backgrounds in residential areas and thus acquire more accurate residential areas.', 'title': 'Global and Local Saliency Analysis for the Extraction of Residential Areas in High-Spatial-Resolution Remote Sensing Image', 'embedding': []}, {'id': 15141, 'abstractText': 'The limited scale and genre coverage of labeled data greatly hinders the effectiveness of supervised models, especially when analyzing spoken languages, such as texts transcribed from speech and informal text including tweets and product comments in Internet. In order to effectively utilize multiple labeled datasets with heterogeneous annotations for the same task, this paper proposes a coupled sequence labeling model that can directly learn and infer two heterogeneous annotations simultaneously, using Chinese part-of-speech (POS) tagging as our case study. The key idea is to bundle two sets of POS tags together (e.g., “[NN, n]<sup>n</sup>), and build a conditional random field (CRF) based tagging model in the enlarged space of bundled tags with the help of ambiguous labeling. To train our model on two nonoverlapping datasets that each has only one-side tags, we transform a one-side tag into a set of bundled tags by concatenating the tag with every possible tag at the missing side according to a predefined context-free tag-to-tag mapping function, thus producing ambiguous labeling as weak supervision. We design and investigate four different context-free tag-to-tag mapping functions, and find out that the coupled model achieves its best performance when each one-side tag is mapped to all tags at the other side (namely complete mapping), indicating that the model can effectively learn the loose mapping between the two heterogeneous annotations, without the need of manually designed mapping rules. Moreover, we propose a context-aware online pruning strategy that can more accurately capture mapping relationships between annotations based on contextual evidences and thus effectively solve the severe inefficiency problem with our coupled model under complete mapping, making it comparable with the baseline CRF model. Experiments on benchmark datasets show that our coupled model significantly outperforms the state-of-the-art baselines on both one-side POS tagging and annotation conversion tasks. The codes and newly annotated data are released for research usage.<sup>1</sup>', 'title': 'Coupled POS Tagging on Heterogeneous Annotations', 'embedding': []}, {'id': 15142, 'abstractText': \"In this work, we propose the Road Data Enrichment (RoDE), a framework that fuses data from heterogeneous data sources to enhance Intelligent Transportation System (ITS) services, such as vehicle routing and traffic event detection. We describe RoDE through two services: (i) Route service, and (ii) Event service. For the first service, we present the Twitter MAPS (T-MAPS), a low-cost spatiotemporal model to improve the description of traffic conditions through Location-Based Social Media (LBSM) data. As a case study, we explain how T-MAPS is able to enhance routing and trajectory descriptions by using tweets. Our experiments compare T-MAPS' routes against Google Maps' routes, showing up to 62% of route similarity, even though T-MAPS uses fewer and coarse-grained data. We then propose three applications, Route Sentiment (RS), Route Information (RI), and Area Tags (AT), to enrich T-MAPS' suggested routes. For the second service, we present the Twitter Incident (T-Incident), a low-cost learning-based road incident detection and enrichment approach built using heterogeneous data fusion. Our approach uses a learning-based model to identify patterns on social media data which is then used to describe a class of events, aiming to detect different types of events. Our model to detect events achieved scores above 90%, thus allowing incident detection and description as a RoDE application. As a result, the enriched event description allows ITS to better understand the LBSM user's viewpoint about traffic events (e.g., jams) and points of interest (e.g., restaurants, theaters, stadiums).\", 'title': 'Road Data Enrichment Framework Based on Heterogeneous Data Fusion for ITS', 'embedding': []}, {'id': 15143, 'abstractText': 'With increasing of the spatial resolution of satellite imaging sensors, object-based image analysis (OBIA) has been gaining prominence in remote sensing applications. However, scale selection in multi-scale segmentation and OBIA remains a challenge, which directly reduces efficiency of land cover mapping. In this study, we presented an object-based land cover mapping using adaptive scale segmentation. Central to our method is the use of inherent features of segmented objects to determine whether an object should be segmented with a small scale in a top-down segmentation procedure. We firstly used inherent features of a segmented object to determine whether this object should be segmented with a smaller scale in a top-down segmentation procedure, producing a segmentation map with optimal scales. Then, an object-based SVM classifier was applied on the adaptive-scale segmentation map to yield a land-cover map. We have applied this method on a ZY-3 multi-spectral satellite image to produce land cover map, compared with the results using the traditional mean shift algorithm with fixed scales. The experimental results illustrate that the proposed method is practically helpful and efficient to improve the performance of land cover mapping.', 'title': 'Object-based land cover mapping using adaptive scale segmentation from ZY-3 satellite images', 'embedding': []}, {'id': 15144, 'abstractText': 'Solar panel mapping has gained a rising interest in renewable energy field with the aid of remote sensing imagery. Significant previous work is based on fully supervised learning with classical classifiers or convolutional neural networks (CNNs), which often require manual annotations of pixel-wise ground-truth to provide accurate supervision. Weakly supervised methods can accept image-wise annotations which can help reduce the cost for pixel-level labelling. Inevitable performance gap, however, exists between weakly and fully supervised methods in mapping accuracy. To address this problem, we propose a pseudo supervised deep convolutional network with label correction strategy (PS-CNNLC) for solar panels mapping. It combines the benefits of both weak and strong supervision to provide accurate solar panel extraction. First, a convolutional neural network is trained with positive and negative samples with image-level labels. It is then used to automatically identify more positive samples from randomly selected unlabeled images. The feature maps of the positive samples are further processed by gradient-weighted class activation mapping to generate initial mapping results, which are taken as initial pseudo labels as they are generally coarse and incomplete. A progressive label correction strategy is designed to refine the initial pseudo labels and train an end-to-end target mapping network iteratively, thereby improving the model reliability. Comprehensive evaluations and ablation study conducted validate the superiority of the proposed PS-CNNLC.', 'title': 'Pseudo Supervised Solar Panel Mapping based on Deep Convolutional Networks with Label Correction Strategy in Aerial Images', 'embedding': []}, {'id': 15145, 'abstractText': 'Flood is causing devastating damages every year all over the world. One way to improve the readiness of stakeholders (res-cue authorities, policy makers, and communities) is by providing flood extent maps promptly after the disaster, preferably in an automated way and with a minimum number of satellite imagery to reduce costs. The web application developed in this paper aims to address this problem by mapping the flood extent automatically from SAR images. This web application is portable since it runs on the internet browser, and allows to perform the classification of the flooding in an automated fashion. Another strong point is the rapidity of the processing: the whole processing time was around 3 to 5 minutes for a subset of 20 million pixels. The inundation map returned by our algorithm was validated against vector files mapped by the United Nations Institute for Training and Research (UNITAR) for the same flood event. Regarding the dataset needed in this study, a pair of a preflood SAR image and an optical image of the same area were used to build a training dataset of water and non-water classes. The learning phase is immediately followed by the classification of the post-flood SAR image into a binary flood map. The web application described in this paper was built with open-source Python libraries which are backed by large communities (Django, Scikit-learn among others). The flood map was eventually displayed on OpenStreetMap maps provided by Mapbox.', 'title': 'A Web Application for the Automatic Mapping of the Flood Extent on Sar Images', 'embedding': []}, {'id': 15146, 'abstractText': 'Rapid development of affordable and portable consumer depth cameras facilitates the use of depth information in many computer vision tasks such as intelligent vehicles and 3D reconstruction. However, depth map captured by low-cost depth sensors (e.g., Kinect) usually suffers from low spatial resolution, which limits its potential applications. In this paper, we propose a novel deep network for depth map super-resolution (SR), called DepthSR-Net. The proposed DepthSR-Net automatically infers a high-resolution (HR) depth map from its low-resolution (LR) version by hierarchical features driven residual learning. Specifically, DepthSR-Net is built on residual U-Net deep network architecture. Given LR depth map, we first obtain the desired HR by bicubic interpolation upsampling and then construct an input pyramid to achieve multiple level receptive fields. Next, we extract hierarchical features from the input pyramid, intensity image, and encoder-decoder structure of U-Net. Finally, we learn the residual between the interpolated depth map and the corresponding HR one using the rich hierarchical features. The final HR depth map is achieved by adding the learned residual to the interpolated depth map. We conduct an ablation study to demonstrate the effectiveness of each component in the proposed network. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art methods. In addition, the potential usage of the proposed network in other low-level vision problems is discussed.', 'title': 'Hierarchical Features Driven Residual Learning for Depth Map Super-Resolution', 'embedding': []}, {'id': 15147, 'abstractText': \"The continuous memristor models have been applied to various chaotic circuits. However, the discrete memristor models and their applications to discrete maps haven't attracted much attention, yet. In this brief, we first present a discrete memristor model and analyze its characteristics. By coupling the model into the Logistic map, a memristive Logistic map is further achieved. Due to the existence of line fixed point, the memristive Logistic map can be unstable or critically stable, depending on its control parameters and initial state. Using several analysis methods, we study the control parameters-relied dynamical behaviors of the memristive Logistic map and disclose its hyperchaotic attractors. The numerical results show that the discrete memristor can efficiently improve the chaos complexity in the Logistic map. In addition, digital experiments are designed to validate the numerical results.\", 'title': 'Memristor-Coupled Logistic Hyperchaotic Map', 'embedding': []}, {'id': 15148, 'abstractText': 'Heart failure is associated with substantial mortality and morbidity and remains the most common diagnosis in older patients. Based on experimental electrophysiologic studies, cardiac resynchronization therapy (CRT) for heart failure results in a maximum resynchronization effect when applied to the most delayed left ventricular (LV) site. Current clinical practice is to identify the optimal site using separate visualisation of scar and activation information. These must be mentally mapped into 3D, which is challenging and time-consuming for the electrophysiologist. The aim of this work is to improve patient planning for CRT by mapping propagation of mechanical activation from cardiac magnetic resonance (CMR) onto a three-dimensional plus time (3D+t) model map to assist the cardiologist in determining the optimal LV pacing site. Automatic motion analysis of the 16-segment patient-specific LV anatomical model, automatically segmented from cine MR data, was done and regional volume change curves as a function of the cardiac cycle along with intraventricular dyssynchrony indices were extracted. The regional volume information computed was then mapped onto all phases of the 3D+t CMR data, which provides a 3D+t mechanical activation map over the whole cardiac cycle. This workflow was tested on 7 patients and 3 healthy volunteers. This mapping of the regional change of volume across the LV during ventricular pacing could facilitate the selection of the optimum pacing segment at the planning stage of the procedure, and consequently decrease the number of inadequate responders to CRT.', 'title': 'Dynamic mapping of ventricular function from cardiovascular magnetic resonance imaging', 'embedding': []}, {'id': 15149, 'abstractText': 'Several humanoid robots will require to navigate in unsafe and unstructured environments, such as those after a disaster, for human assistance and support. To achieve this, humanoids require to construct in real-time, accurate maps of the environment and localize in it by estimating their base/pelvis state without any drift, using computationally efficient mapping and state estimation algorithms. While a multitude of Simultaneous Localization and Mapping (SLAM) algorithms exist, their localization relies on the existence of repeatable landmarks, which might not always be available in unstructured environments. Several studies also use stop-and-map procedures to map the environment before traversal, but this is not ideal for scenarios where the robot needs to be continuously moving to keep for instance the task completion time short. In this paper, we present a novel combination of the state-of-the-art odometry and mapping based on LiDAR data and state estimation based on the kinematics-inertial data of the humanoid. We present experimental evaluation of the introduced state estimation on the full-size humanoid robot WALK-MAN while performing locomotion tasks. Through this combination, we prove that it is possible to obtain low-error, high frequency estimates of the state of the robot, while moving and mapping the environment on the go.', 'title': 'A Study on Low-Drift State Estimation for Humanoid Locomotion, Using LiDAR and Kinematic-Inertial Data Fusion', 'embedding': []}, {'id': 15150, 'abstractText': 'Image segmentation in robotics is an ongoing research field in which neural networks have shown promising performance. In this paper, we introduce MapSegNet, a deep convolutional neural network for indoor map segmentation, which is able to segment the indoor maps into smaller units, including rooms, corridors, windows, and furniture. The proposed model consists of an encoder phase to capture context and a corresponding decoder phase to increase the resolution of feature maps to the original resolution. A design for skip connections is introduced with fused multi-scale feature maps between the encoder and the decoder phases. The proposed skip connection increases the flow of information between the phases and improves the model generalization. We evaluate empirical studies based on abstract maps and detailed maps. While abstract maps include empty rooms and corridors, the detailed maps contain indoor space with more objects such as furniture. We investigate the effectiveness of the proposed method by employing various indoor maps and compare its performance with similar neural network models on multiple datasets. The results show that the proposed model is able to achieve more accurate recognition or lower computation cost compared to other state-of-the-art segmentation techniques.', 'title': 'MapSegNet: A Fully Automated Model Based on the Encoder-Decoder Architecture for Indoor Map Segmentation', 'embedding': []}, {'id': 15151, 'abstractText': 'Objective: This study demonstrates how functional connectivity (FC) patterns are affected in direct relation to the lobe that is mostly affected by seizures. Methods: The novel idea of penalized FC (pFC) maps is compared against standard FC maps in the four fundamental EEG frequency sub-bands. The FC measure between any two specific electrodes is scaled depending on the probability of true FC between them, and their power content with respect to the two electrodes of maximum power within each frequency sub-band. The algorithm is automated and introduces adaptive power penalization based on the power distribution of the different sub-bands. Results: The pFC maps were found to be more effective at suppressing the local connectivity in the lobes that are less affected by the interictal epileptiform discharges (IEDs). More precisely, given the least amount of power penalization, pFC maps of the theta sub-band reveal statistical significance in terms of increased local connectivity margin of the affected region as compared to the standard FC maps. However, they cannot be solely relied upon as other sub-bands could alternatively show high local connectivity across different patients within the region of interest. Conclusion: penalized functional connectivity maps intrinsically provide more information regarding the whole brain network in context to regions of interest where the active lobe is determined by the neurologists to contain the focal source. Significance: Findings suggest that (1) the significant sub-band varies from patient to patient while remaining relatively consistent within the IED segments of a same patient, and (2) the pFC maps have an advanced capability in terms of pinpointing to a region of interest of the active lobe, and as such can play a critical role in providing insight as to a region of interest where the 3D source might be located when solving the ill-posed inverse problem.', 'title': 'Penalized Functional Connectivity Maps for Patients With Focal Epilepsy', 'embedding': []}, {'id': 15152, 'abstractText': 'Zoning Improvement Plan (ZIP) Code polygon maps, obtained from different data sources, do not match, thus creating uncertainty in spatial analysis. In this dissertation, we want to combine multiple source of ZIP-code polygon data into a coherent system that maps a given location to a ZIP-code. For this purpose, we want to harness the wisdom of the crowd by combining various polygon map data sets with various sources of volunteered geographical information. The system that we want to build will employ traditional classification methods to map a given spatial coordinate to a distribution of ZIP-codes using the (not publicly available) United States Postal Service (USPS) map as an authoritative ground truth. In our first studies, we train a Naïve Bayes classifier using multiple (publicly available) ZIP Code polygon maps. In addition, we enrich our classification by using a lazy K-Nearest Neighbor classifier to predict the ZIP Codes for a given location using Twitter. Feeding the result of this classifier to the Naïve Bayes classification our experimental evaluation shows an improvement in classification accuracy, compared to using only map data.', 'title': 'ZIP-Code Classification Using Spatial and Crowdsourced Data', 'embedding': []}, {'id': 15153, 'abstractText': \"Geomagnetic signals are attractive media for indoor localization since they have less noise than RF and don't require additional equipment installation for signal generation. The fingerprinting technique used in geomagnetic field based indoor positioning systems (IPS) estimates the position by matching the magnetic vector sampled from the current location with the magnetic vectors recorded in the magnetic field map. However, since the magnetic field is represented by a 3-dimensional vector, the values of a magnetic vector may change depending on the user's orientation or the grip position. Thus, the sampled magnetic vector may have different values from the vector values stored in the magnetic field map depending on the sensor's orientation. This may substantially lower the positioning accuracy. To avoid this problem, the existing studies use only the magnitude of a magnetic vector, but this reduces the uniqueness of the fingerprint, which may also degrade the positioning accuracy. In this paper we propose a vector calibration algorithm which can adjust the sampled magnetic vector to the vector of the magnetic field map by using the parametric equation of a circle. This can minimize mismatching with the magnetic field map. To implement this, we need to compute the relative rotation angle from the moving direction of the current user to the moving direction during the field map collection. Since we can measure the moving direction by using the gyroscope and accelerometer, we can compute this relative rotation angle dynamically. To evaluate our vector calibration algorithm, we compare the value mismatches with and without vector calibration for 6 random-walk paths in our campus testbed of 2470 square meters. Our results show that with the calibration, we can decrease the difference between the sampled magnetic vector and the magnetic field map vectors from 17.61$\\\\mu$T to 2.38 $\\\\mu$T in x dimension, from 17.24$\\\\mu$T to 2.59 $\\\\mu$T in y dimension, and from 6.86 $\\\\mu$T to 2.16 $\\\\mu$T in z dimension on average. This translates to 83% reduction in the map mismatch compared to the numbers without calibration. In addition, we also demonstrate the effectiveness of the calibration by applying the algorithm to our long short-term memory (LSTM)-based IPS.\", 'title': 'Magnetic Vector Calibration for Real-Time Indoor Positioning', 'embedding': []}, {'id': 15154, 'abstractText': 'Map matching is an important part of map routing in modern transportation applications. In the map matching process, the effect of two main components, i.e. (i) the features of road networks, and (ii) the design aspects of geolocation services, is still not well understood. In this paper, using a combination of probabilistic analysis and simulations we study the effects of the above factors on the map matching process. Using a Maximum Aposteriori Probability (MAP) estimator, we offer some design guidelines that could improve the performance of map matching algorithms.', 'title': 'A probabilistic study of map matching for transportation applications', 'embedding': []}, {'id': 15155, 'abstractText': 'This paper presents novel methods for computing fixed points of positive concave mappings and for characterizing the existence of fixed points. These methods are particularly important in planning and optimization tasks in wireless networks. For example, previous studies have shown that the feasibility of a network design can be quickly evaluated by computing the fixed point of a concave mapping that is constructed based on many environmental and network control parameters such as the position of base stations, channel conditions, and antenna tilts. To address this and more general problems, given a positive concave mapping, we show two alternative but equivalent ways to construct a matrix that is guaranteed to have spectral radius strictly smaller than one if the mapping has a fixed point. This matrix is then used to build a new mapping that preserves the fixed point of the original positive concave mapping. We show that the standard fixed point iterations using the new mapping converges faster than the standard iterations applied to the original concave mapping. As exemplary applications of the proposed methods, we consider the problems of power and load estimation in networks based on the orthogonal frequency division multiple access (OFDMA) technology.', 'title': 'Elementary Properties of Positive Concave Mappings With Applications to Network Planning and Optimization', 'embedding': []}, {'id': 15156, 'abstractText': 'In this paper, we are studying the optimization of the number and positions of access points required to ensure the radio coverage of an indoor environment. In order to estimate the propagation effects in an indoor environment on radio wave propagation, we present existing path loss models and we list the corresponding advantages and drawbacks. In the context of this paper, simplicity and reduced computation time are the main constraints of the choice of an appropriate indoor propagation model. The accuracy of the model is of second order of importance. We choose an empirical model to estimate the indoor radio coverage. In a second step, we propose and develop an algorithm that divides an indoor environment map into sub- maps. Each of these sub-maps requires one access point to ensure its coverage. Finally, in order to test the good coverage of the whole map, each access point is placed at the centroid of the corresponding sub-map and the coverage map is calculated. The results obtained shows an enhancement of the coverage, and a determination of the minimum number of access points required in the indoor environment.', 'title': 'Optimization of indoor radio coverage', 'embedding': []}, {'id': 15157, 'abstractText': 'The entrenched instability of solar power output throttles its further integration into power grids worldwide. Thus the precise solar power forecasting (SPF) is helpful for the improvement of power grid stability and better exploitation of clean solar energy. As an important role of ultra-short-term SPF, sky images always contain volatile clouds, which results in tempestuous variation of the output of PV plants. Therefore, an accurate model that can capture the mapping relationship between sky image data and solar irradiance data is significant for fulfilling the ultra-short-term SPF. To fill the gap in the content of this research field, this paper proposes two end to end irradiance mapping models based on deep learning technologies, namely convolutional neural network (CNN) and long short-term memory (LSTM) neural network. Then the mapping performance of the above two mapping models is compared to that of traditional artificial neural network (ANN) based model. In all the aforementioned models, it should be noted that the solar irradiance data output by mapping models is in one-to-one correspondence with the input sky image data in time. The deterministic and probability methods are applied to statistically evaluate the mapping result of CNN and LSTM models. Our case study shows that deep learning architectures, especially the CNN based model, are good at mapping sky images to corresponding surface solar irradiance.', 'title': 'Deep Learning Based Irradiance Mapping Model for Solar PV Power Forecasting Using Sky Image', 'embedding': []}, {'id': 15158, 'abstractText': 'A striking example of brain organisation is the stereotyped arrangement of cell preferences in the visual cortex for edges of particular orientations in the visual image. These “orientation preference maps” appear to have remarkably consistent statistical properties across many species. However fine scale analysis of these properties requires the accurate reconstruction of maps from imaging data which is highly noisy. A new approach for solving this reconstruction problem is to use Bayesian Gaussian process methods, which produce more accurate results than classical techniques. However, so far this work has not considered the fact that maps for several other features of visual input coexist with the orientation preference map and that these maps have mutually dependent spatial arrangements. Here we extend the Gaussian process framework to the multiple output case, so that we can consider multiple maps simultaneously. We demonstrate that this improves reconstruction of multiple maps compared to both classical techniques and the single output approach, can encode the empirically observed relationships, and is easily extendible. This provides the first principled approach for studying the spatial relationships between feature maps in visual cortex.', 'title': 'Estimating Cortical Feature Maps with Dependent Gaussian Processes', 'embedding': []}, {'id': 15159, 'abstractText': 'Autonomous cars are currently widely studied in the automotive and robotics industries because autonomous driving can satisfy the needs of human drivers regarding safety, efficiency, and comfortable driving. Behavior and motion planning for an autonomous car is one of the main requirements of autonomous driving. For autonomous cars, a local route along with its road geometry and attributes of the area is required. There are two ways to obtain the local route: perception-based local route (PBLR) and precise map-based local route (MBLR) methods. First, this paper analyzes the characteristic of both the PBLR and MBLR inference methods. Then based on this analysis, this paper proposes a hybrid local route generation algorithm that chooses the best local route between the PBLR and MBLR options, according to the perceived performance and the precise map availability. To effectively create an expensive precise map, a mapping region classification algorithm is presented to selectively choose the mapping area, where the precise map must be constructed for the MBLR inference. The hybrid local route generation algorithm with the mapping region classification allows the area used for autonomous driving to be extended while reducing the cost due to the precise map. The advantages of the proposed algorithm were verified with experiments in real traffic conditions.', 'title': 'Hybrid Local Route Generation Combining Perception and a Precise Map for Autonomous Cars', 'embedding': []}, {'id': 15160, 'abstractText': 'This study elaborates on a comprehensive design methodology of fuzzy cognitive maps (FCMs). Here, the maps are regarded as a modeling vehicle of time series. It is apparent that whereas time series are predominantly numeric, FCMs are abstract constructs operating at the level of abstract entities referred to as concepts and represented by the individual nodes of the map. We introduce a mechanism to represent a numeric time series in terms of information granules constructed in the space of amplitude and change of amplitude of the time series, which, in turn, gives rise to a collection of concepts forming the corresponding nodes of the FCMs. Each information granule is mapped onto a node (concept) of the map. We identify two fundamental design phases of FCMs, namely 1) formation of information granules mapping numeric data (time series) into activation levels of information granules (viz., the nodes of the map), and 2) optimization of information granules at the parametric level, viz., learning (estimating) the weights between the nodes of the map. The learning is typically realized in a supervised mode on a basis of some experimental data. A construction of information granules is realized with the aid of fuzzy clustering, namely fuzzy C-means. The optimization is realized with the use of particle swarm optimization. The proposed approach is illustrated in detail by a series of experiments using a collection of publicly available data.', 'title': 'Design of Fuzzy Cognitive Maps for Modeling Time Series', 'embedding': []}, {'id': 15161, 'abstractText': 'Chaotic systems are widely studied in various research areas such as signal processing and secure communication. Existing chaotic systems may have drawbacks such as discontinuous chaotic ranges and incomplete output distributions. These drawbacks may lead to the defects of some chaos-based applications. To accommodate these challenges, this paper proposes a two-dimensional (2D) modular chaotification system (2D-MCS) to improve the chaos complexity of any 2D chaotic map. Because the modular operation is a bounded transform, the improved chaotic maps by 2D-MCS can generate chaotic behaviors in wide parameter ranges while existing chaotic maps cannot. Three improved chaotic maps are presented as typical examples to verify the effectiveness of 2D-MCS. The chaos properties of one example of 2D-MCS are mathematically analyzed using the definition of Lyapunov exponent. Performance evaluations demonstrate that these improved chaotic maps have continuous and large chaotic ranges, and their outputs are distributed more uniformly than the outputs of existing 2D chaotic maps. To show the application of 2D-MCS, we apply the improved chaotic maps of 2D-MCS to secure communication. The simulation results show that these improved chaotic maps exhibit better performance than several existing and newly developed chaotic maps in terms of resisting different channel noise.', 'title': 'Two-Dimensional Modular Chaotification System for Improving Chaos Complexity', 'embedding': []}, {'id': 15162, 'abstractText': \"Real-time interaction has become increasingly important. At the same time, panoramic video has gradually become popular. In this paper, the problem we study is predicting the Field-of-View(FoV) at the future moment when people are enjoying a dynamic panoramic immersive video. Existing methods either estimate the future viewing area based on the previous trajectory, or predict the FoV based on salient region in video frames. Here, we design a new model to predict the viewing points in future moments. Firstly, we predict a point from the viewer's previous viewing trajectory using LSTM(Long Short-Term Memory) network. At the mean time, panoramic video frames are mapped to 6 patches by cube map in advance. The modified VGG-16 network is used for each patch image to perform saliency detection. Then, these 6 salient maps are combined to a single salient map as output. A 3layer convolutional neural network to refined the salient map is utilized. Finally, the salient map of corresponding moment is combined with the predicted point by LSTM input to a two-layer fully connected network to produce the final predicted point. The experiment results show that our model's prediction accuracy is higher than the traditional prediction algorithm and has better performance than the model without using the second CNN network.\", 'title': 'Viewport Prediction for Panoramic Video with Multi-CNN', 'embedding': []}, {'id': 15163, 'abstractText': 'Simultaneous Localization and Mapping (SLAM) is a fundamental problem for autonomous mobile robots (AMRs). AMRs are widely used in automated warehousing, factory material transfer systems, flexible assembly systems, and other intelligent transportation sites. The visual Inertial Odometry (VIO) which consists of the camera and inertial-measurement-unit (IMU), is a popular approach to achieve accurate 6-DOF state estimation. However, such locally accurate VIO is prone to drift and cannot provide a global consistent map. The prerequisite for re-localizing the robot and ensuring precise autonomous navigation is an accurate and global consistent map of its environment. In this study, we propose a stereo visual-inertial mapping system. The front-end is a robust stereo VIO based on a tightly-coupled sliding window optimization. The core of the back-end is the global Bundle-Adjustment (BA) which is a nonlinear optimization, in which IMU is also added as a time-domain constraint. Meanwhile, stereo-camera-IMU extrinsic calibration is performed in BA to improve mapping accuracy. The selection principles of keyframes and map points are also designed according to the AMRs application characteristics. Further, the forward and backward Perspective-n-Point (PNP) method is also adopted to avoid the loop-detection mismatch. The performance of the system was validated and compared against other state-of-the-art algorithms. The findings revealed the effectiveness and robustness of this stereo visual-inertial mapping algorithm.', 'title': 'Stereo Visual Inertial Mapping Algorithm for Autonomous Mobile Robot', 'embedding': []}, {'id': 15164, 'abstractText': 'MapReduce includes three phases of map, shuffle, and reduce. Since the map phase is CPU-intensive and the shuffle phase is I/O-intensive, these phases can be conducted in parallel. This paper studies a joint scheduling optimization of overlapping map and shuffle phases to minimize the average job makespan. Challenges come from the dependency relationship between map and shuffle phases, since the shuffle phase may wait to transfer the data emitted by the map phase. A new concept of the strong pair is introduced. Two jobs are defined as a strong pair, if the shuffle and map workloads of one job equal the map and shuffle workloads of the other job, respectively. We prove that, if the entire set of jobs can be decomposed to strong pairs of jobs, then the optimal schedule is to pairwisely execute jobs that can form a strong pair. Following the above intuition, several offline and online scheduling policies are proposed. They first group jobs according to job workloads, and then, execute jobs within each group through a pairwise manner. Real data-driven experiments validate the efficiency and effectiveness of the proposed policies.', 'title': 'Optimizing MapReduce Framework through Joint Scheduling of Overlapping Phases', 'embedding': []}, {'id': 15165, 'abstractText': 'Mapping the quasi-circular vegetation patches (QVPs) is the most basic and necessary step for studying the mechanisms of vegetation pattern formation, spontaneous plant colonization, and ecosystem maintenance and degradation in the Yellow River Delta (YRD), China. It is well known that the use of multi-seasonal image data may be expected to obtain better results for mapping the vegetation than the use of a single date image because of vegetation phenology and the contrasts between vegetation and background. Although the spring GF-1 images have been used to detect the QVPs, the potential of multi-seasonal fused GF-1 multispectral images for mapping the QVPs has not been well explored. With the objective to fill this gap, we evaluated the potential of the winter, spring, summer, and autumn fused GF-1 multispectral images for mapping the QVPs with object-based example-based feature extraction with support vector machine classification method to understand the seasonal effect on the QVPs mapping accuracy. The results showed that classification based on the spring data acquired on 6 April 2014 (OA=99.8%, kappa=0.988) was more accurate than the classification base on the other tree seasonal images. The lowest classification accuracy was obtained from the autumn data acquired on September 21 2015 (OA=93.1%, kappa=0.885). We recommend the he spring and winter images to map the QVPs in the YRD. In the future, more machine learning techniques should be applied to classify and compare the potential of the different seasonal fused GF-1 multispectral images for mapping the QVPs.', 'title': 'Comparisons of Different Seasonal Fused GF-1 Multispectral Images for Mapping Quasi-circular Vegetation Patches', 'embedding': []}, {'id': 15166, 'abstractText': \"Multibeam echosounder (MBES) technology has been constantly evolving since its commercial introduction in the late 1970s. The early systems were large and designed to efficiently acquire bathymetric data in deep water. As the underlying sonar technologies improved and computing power increased, systems became smaller and capable of operating on a wider range of vessels over a broader range of depths. Modern deep seafloor exploration, and our present understanding of the geomorphological and biophysical processes that shape it, are closely linked to advances in multibeam echosounder technology. Low to mid-frequency (12-30 kHz) acoustic waves generated by MBES sonars can penetrate kilometers of water column and remotely measure the deep seafloor and shallow subsurface. Reflectivity measurements of the seafloor and water column can also be extracted from MBES datasets, but until the last decade of the 20th century, only the bathymetric swath data was being utilized. In the 1990s, scientists began taking advantage of the multibeam acoustic wave's reflected energy, or backscatter, to interpret information on seafloor geometry (slope), physical characteristics (hardness and roughness), and intrinsic properties, such as composition, surficial and volumetric scattering. Analyzing the geophysical signature of reflected acoustic beams has proven an effective quantitative and qualitative tool to remotely characterize the lithologic composition and geologic nature of the seafloor. Analyzing seafloor backscatter and most recently, backscatter intensities in the water column, has been used for a wide range of applications, including fisheries research, marine biomass assessment, benthic habitat mapping, geological classification, subsea engineering and geohazard mitigation, and hydrocarbon seep studies. This presentation will briefly look at the evolution of MBES technology before focusing on how modern MBES surveys, using the latest generation technology, can deliver a comprehensive characterization of the seafloor and the waters above, as opposed to bathymetry data alone. With The Nippon Foundation-GEBCO Seabed 2030 Project now underway, and planning for the United Nations (UN) Decade of Ocean Science for Sustainable Development having recently commenced, modern MBES technology will play a critical role in bridging ocean bathymetry and ocean observation to improve our understanding of the ocean, its seafloor and its processes. One of the key R&amp;D priorities of the Decade is a comprehensive map (digital atlas) of the ocean. Modern multibeam surveys will support not only bathymetric mapping, but also physical, biological, chemical, geologic, ecosystem, cultural and resource mapping of the world's oceans. Such an approach can feed both Seabed 2030 and the Decade to deliver, as the UN has so eloquently stated, “the ocean we want for the future we need.” Keywords- multibeam; sonar; mbes; echosounder; bathymetry; hydrography; marine geology; mapping; ocean mapping; seeps; seabed seeps; hydrocarbon seeps; exploration; ocean exploration; geophysical; survey; geophysical survey; ocean; seafloor; characterization; seafloor characterization; backscatter; multibeam backscatter; water column; observation; ocean observation; Seabed 2030, United Nations Decade of Ocean Science for Sustainable Development; Ocean Decade.\", 'title': 'How Modern Multibeam Surveys Can Dramatically Increase Our Understanding of the Seafloor and Waters Above to Support the United Nations Decade of Ocean Science for Sustainable Development', 'embedding': []}, {'id': 15167, 'abstractText': 'In this paper, we study the three-dimensional (3D) path planning for a cellular-connected unmanned aerial vehicle (UAV) to minimize its flying distance from given initial to final locations, while ensuring a target link quality in terms of the expected signal-to-interference-plus-noise ratio (SINR) at the UAV receiver with each of its associated ground base stations (GBSs) during the flight. To exploit the location-dependent and spatially varying channel as well as interference over the 3D space, we propose a new radio map based path planning framework for the UAV. Specifically, we consider the channel gain map of each GBS that provides its large-scale channel gains with uniformly sampled locations on a 3D grid, which are due to static and large-size obstacles (e.g., buildings) and thus assumed to be time-invariant. Based on the channel gain maps of GBSs as well as their loading factors, we then construct an SINR map that depicts the expected SINR levels over the sampled 3D locations. By leveraging the obtained SINR map, we proceed to derive the optimal UAV path by solving an equivalent shortest path problem (SPP) in graph theory. We further propose a grid quantization approach where the grid points in the SINR map are more coarsely sampled by exploiting the spatial channel/interference correlation over neighboring grids. Then, we solve an approximate SPP over the reduced-size SINR map (graph) with reduced complexity. Numerical results show that the proposed solution can effectively minimize the flying distance/time of the UAV subject to its communication quality constraint, and a flexible trade-off between performance and complexity can be achieved by adjusting the grid quantization ratio in the SINR map. Moreover, the proposed solution significantly outperforms various benchmark schemes without fully exploiting the channel/interference spatial distribution in the network.', 'title': 'Radio Map-Based 3D Path Planning for Cellular-Connected UAV', 'embedding': []}, {'id': 15168, 'abstractText': 'Carry chains on FPGAs have traditionally been only used for fast binary arithmetic operations. In this paper, we propose using the carry chain to implement general logic as a means of reducing the critical path delay and raising performance. To achieve this, we use a Majority-Inverter Graph (MIG) to represent the application during technology mapping, since carry functionality directly maps to the majority logic function. This aligns the subject graph of technology mapping with the capabilities of the carry chain. We first map an application to LUTs, then determine a chain of critical LUTs containing paths of majority “gates” that we deem beneficial for mapping onto the carry chain. We place such paths onto the carry chains, with the remaining logic in LUTs. In an experimental study using a suite of benchmarks, we observe that the proposed approach yields a post-place-and-route critical path delay that is superior to using delay-optimized mapping, yet without the significant area penalty. With carry-chain optimizations, area-delay product is improved by 9% vs. baseline LUT mappings.', 'title': 'Post-LUT-Mapping Implementation of General Logic on Carry Chains Via a MIG-Based Circuit Representation', 'embedding': []}, {'id': 15169, 'abstractText': 'In this paper, we propose a semantic simultaneous localization and mapping (SLAM) framework for rescue robots, and report its use in navigation tasks. Our framework can generate not only geometric maps in the form of dense point-clouds but also corresponding point-wise semantic labels generated by a semantic segmentation convolutional neural network (CNN). The semantic segmentation CNN is trained using our RGB-D dataset of the RoboCup Rescue-Robot-League (RRL) competition environment. With the help of semantic information, the rescue robot can identify different types of terrains in a complex environment, so as to avoid specific obstacles or to choose routes with better traversability. To reduce the segmentation noise, our approach utilizes depth images to perform filtering on the segmentation results of each frame. The overall semantic map is then further improved in the point-cloud voxels. By accumulating results of multiple frames in the voxels, semantic maps with consistent semantic labels are obtained. To show the advantage of having a semantic map of the environment, we report a case study of how the semantic map can be utilized in a navigation task to reduce the arrival time while ensuring safety. The experimental result shows that our semantic SLAM framework is capable of generating a dense semantic map for the complex RRL competition environment, with which the arrival time of the navigation time is effectively reduced.', 'title': 'Semantic RGB-D SLAM for Rescue Robot Navigation', 'embedding': []}, {'id': 15170, 'abstractText': 'Localization of human long bones in ultrasound images has quite complex challenges. This is due to a representation of the reflection of a sound wave emitted by a B-scan sensor. The ultrasound scan does not only display bone specimens, but also contains muscles, soft tissue, and other parts under the skin tissue Therefore we need a system that can automatically recognize bone specimens in ultrasound images. This study implements deep learning based learning systems using the convolutional neural network (CNN) method with YOLOv3. The training results from the network detector with IoU threshold 0.5 can recognize bone objects in mAP<sub>@50</sub>, mAP<sub>@75</sub> and mAP<sub>@50:95</sub> with values of 99.98, 97.68 and 85.67 respectively. And for the results of training the network detector with IoU threshold 0.75 can recognize bone objects in mAP<sub>@50</sub>, mAP<sub>@75</sub> and mAP<sub>@50:95</sub> with values of 99.96, 97.46 and 86.35 respectively.', 'title': 'Human Bone Localization in Ultrasound Image Using YOLOv3 CNN Architecture', 'embedding': []}, {'id': 15171, 'abstractText': 'Commercial maps often offer traffic awareness which is critical for many location based services. On the other hand free and open map services (such as government maps or OSM) are traffic oblivious and hence are of limited value for such services. In this paper we show that coarse information available from a commercial map routing API, can be dissected into fine-grained per-road-segment traffic information which can be reused in any application requiring traffic-awareness. Our system MapReuse queries a commercial map for a (relatively small) number of routes, and uses the returned routes and expected travel times, to infer travel time on each individual edge of the road network. Such fine-grained travel time information can be used not only to infer travel time on any given route but also to compute complex spatial queries (such as traffic-aware isochrone map) for free. We test our system on four representative metropolitan areas: Bogota, Doha, NYC and Rome, and report very encouraging results. Namely, we observe the median and mean percentage errors of MapReuse, measured against the travel times reported by the commercial map, to be in the range of 4% to 8%, implying that MapReuse is capable to accurately reconstruct the traffic conditions in all four studied cities.', 'title': 'MapReuse: Recycling Routing API Queries', 'embedding': []}, {'id': 15172, 'abstractText': \"In this work, we argue that Location-Based Social Media (LBSM) feeds may offer a new layer to improve traffic and transit comprehension. Initially, we showed the significant correlation between Twitter's feed and traditional traffic sensors. Then, we presented the Twitter MAPS (T-MAPS) a low-cost spatiotemporal model to improve the description of traffic conditions through tweets. T-MAPS enhance traditional traffic sensors by carrying the human lens into the transportation system. We conducted a case study by running T-MAPS and Google Maps route recommendation, in which, we showed T-MAPS viability, as an additional traffic descriptor. As a result, we noticed the median of route similarity reached 62%, and for a quarter of the evaluated trajectories, the similarity achieved between 75% and 100%. Also, we presented three route description services, based on natural language analyzes, Route Sentiment (RS), Route Information (RI), and Area' Tags (AT) aiming to enhance the route information.\", 'title': 'Enriching Traffic Information with a Spatiotemporal Model based on Social Media', 'embedding': []}, {'id': 15173, 'abstractText': 'Choropleth maps are among the most common visualization techniques used to present geographical data. These maps require an equal-area projection but there are no clear criteria for selecting one. We collaborated with 20 social scientists researching on the Global South, interested in using choropleth maps, to investigate their design choices according to their research tasks. We asked them to design world choropleth maps through a survey, and analyzed their answers both qualitatively and quantitatively. The results suggest that the design choices of map projection, center, scale, and color scheme, were influenced by their personal research goals and the tasks. The projection was considered the most important choice and the Equal Earth projection was the most common projection used. Our study takes the first substantial step in investigating projection choices for world choropleth maps in applied visualization research.', 'title': 'Mapping the Global South: Equal-Area Projections for Choropleth Maps', 'embedding': []}, {'id': 15174, 'abstractText': 'The accuracy of the naval map representation determines the quality of route construction when solving the problem of automatic control of ships movement in a difficult navigation environment. Using a high-precision raster map requires a large amount of memory to store data. Vector representation of the map is an alternative approach to the problem of storing data about terrain, it allows to store only the data containing the terrain contours. In this paper we implement and research the algorithms which allow to vectorize the raster maps by the set of isoline slices using irregular grid of heights. These algorithms approximate the map with a set of elevation slices, recursively search for nested isolines on the slice, build and optimize contours of the isolines on the slice. We conduct a study that shows a significant reduction in the amount of stored data in vector form compared to raster format. The proposed format for storing map data will be used to represent realistic landscapes in the tasks of building ship routes.', 'title': 'Raster to Vector Map Convertion by Irregular Grid of Heights', 'embedding': []}, {'id': 15175, 'abstractText': \"The use of mobile robots to explore and supervise confined and uneven environments such as pipes, caves, and galleries improves operational safety by removing human operators from these dangerous areas. In many types of inspections, the robot must generate realistic, colored, and geometrically accurate maps, which experts can use to study and assess the environment remotely at a safe location. This paper investigates two approaches for visual reconstruction of confined environments: a point cloud registration combined with visual odometry and the RTAB-Map SLAM method. Real experiments performed inside a closed corridor and within an underground gold mine show that both approaches are suitable for estimating the robot's pose and perform 3D mapping. Preliminary results indicate that the point cloud registration generates denser maps suitable for visual inspection, and RTAB-Map provides less noisy maps proper for navigation.\", 'title': 'Investigation of Visual Reconstruction Techniques Using Mobile Robots in Confined Environments', 'embedding': []}, {'id': 15176, 'abstractText': 'Total Suspended Sediment (TSS) is one of the factors to determine water quality. The large number of TSS indicated the turbid water. Highly turbid water can affect water biophysics. Therefore, mapping TSS distribution is important for managing and preserving coastal areas. Sentinel-2A image uses to create multitemporal TSS from January 2017 to May 2018. This study goals are testing Sentinel-2A image to create TSS map and making TSS temporal map. Image transformations that used were Normalized Difference Suspended Sediment Index (NDSSI), Normalized Suspended Material Index (NSMI) and Band Ratio using green, red, and blue band channels. The best image transformation that analysis with Stepwise Regression is Band Ratio image. The results of laboratory tests showed minimum TSS content of 0.98 mg/L and maximum content of 4.56 mg/L. The accuracy value from TSS mapping with a band ratio image reach 80.51%, indicates that the use of band ratios (b3/b4) is representative to TSS mapping. TSS distribution is dynamic, indicated by TSS temporal map. Dynamic content of TSS influenced by the season. TSS content in wet seasons (October-April) is higher than in dry season (April-October).', 'title': 'Utilization of Sentinel-2A imagery For Mapping The dynamics of Total Suspended Sediment at The River Mouth of The Padang City', 'embedding': []}, {'id': 15177, 'abstractText': 'Automatic identification of weld seam types by welding robot is a key link in intelligent welding as some adjustment scheme (e.g., welding trajectory planning, initial welding position, welding parameters) vary with the weld seam types. However, the variable welding environment and various weld seam profile omnifarious affect the robustness of weld seam types identification. To overcome the challenges derived from the weld seam diversity, in this paper, the silhouette-mapping was selected as the weld seam intermedium and a multi-type weld seam automatic identification system based on vision sensor was introduced. Two different laser sources were adopted to obtain robust silhouette-mapping features in proper gestures. Based on the silhouette-mapping data (stripe-mapping and spot-mapping), the related image processing algorithms were carried out to achieve automatic identification. Specifically, the bidirectional deviation search method was proposed to locate the spot-mapping area based on the stripe-mapping image. Aiming at the characteristics of the spot-mapping image, a carefully designed CNN (convolutional neural network) model was used to classify types. Experimental results prove that the silhouette-mapping and CNN are an effective combination for the multi-type weld seam identification, and a total of 97.6% of weld seam types were correctly predicted. Some weld-related studies include welding features extraction, and welding quality detection may improve its accuracy on the basis of determining weld seam types.', 'title': 'Automatic Identification of Multi-Type Weld Seam Based on Vision Sensor With Silhouette-Mapping', 'embedding': []}, {'id': 15178, 'abstractText': 'Mapping for terrestrial ecosystem service has exponentially soared in recent years, which provides a reliable theoretical foundation for acknowledging functions, valuation and management of various kinds of ecosystem services. This study was conducted by collecting peer reviewed papers from 2000 to 2018, and establishing a relevant database of 113 papers. Forest, grassland, wetland and desert were selected as four basic components of terrestrial ecosystem. As a result, researches on mapping for terrestrial ecosystem services in each continent could be found, and medium geographical scale is the most widely preferred by researchers. Services origined from different ecosystem might share similar qualities, thus it is hard to identify them when researches refer to only one ecosystem or specific services. Models combining with relevant approaches applied in mapping can complement each other. In conclusion, maps for demonstrating hotspots and effects of climate changes are promising to make a significant progress in the future. Moreover, it is essential for districts and countries to select adaptable mapping methods and models according to their own demands. Drivers like needs for management and governmental planning, cognition of ecosystem services and disservices will motivate researches and the application of mapping forward.', 'title': 'Mapping for terrestrial ecosystem services: a review', 'embedding': []}, {'id': 15179, 'abstractText': 'This paper presents a speech encryption model that uses two different chaotic maps (Hennon and Logistic maps) based on the OFDM technique. The purpose of using the encrypted chaotic key in OFDM techniques is to improve the Bit Error Rate (BER) performance in receiver side which has robustness against AWGN effects, thus increasing the security of data transmitted. In the last few years, a significant research effort has been devoted, concerning secret chaotic key as compared with the conventional OFDM modulation schemes. Chaotic maps have a wideband, non-periodic, unpredictable nature, very sensitive to initial conditions; so chaotic map is used for security. This paper uses two chaotic maps Logistic &amp; Hennon each individually to create encrypted undercover keys and compares a model performance between them. The comparison showed that the performance is similar to the two chaotic maps with a minimal difference between them (After SNR=16 dB the BER=10<sup>-3</sup>). As for residual Intelligibility Measurements, they are very close and there is little difference in the limits (0.01-0.1) for R, SSNR and CD test. The suggested model has been carried out by employing MATLAB by utilizing SIMULINK (R2018b). Execution of this program is studied, the outcomes are good.', 'title': 'Performance Comparison of Hybrid Chaotic Maps Based on Speech Scrambling for OFDM Techniques', 'embedding': []}, {'id': 15180, 'abstractText': \"Reflexion Modelling is a popular method used in industry for Software Architectural Consistency Checking (SACC). However, it involves a mapping step that is manual and tedious. There exist techniques and tools that attempt to automate mapping, yet they are either limited in their approach or they require an initial set of manually pre-mapped entities. This study proposes a novel technique, InMap, that improves the mapping process in reflexion modelling by both providing versatility and eliminating the constraint of needing a set of manually pre-mapped entities in order to automate mapping. Using a software's architecture descriptions, InMap applies information retrieval concepts to the software's source code to interactively provide mapping recommendations to an architect. For the six systems InMap was evaluated on, the recommendations it provided achieved an average recall of 0.97, and an average precision of 0.82. InMap also achieved higher, f<sub>1</sub>-scores in comparison to existing techniques that require premapping. This provides a basis for improving industry tools that use reflexion modelling or similar SACC methods.\", 'title': 'InMap: Automated Interactive Code-to-Architecture Mapping Recommendations', 'embedding': []}, {'id': 15181, 'abstractText': 'In this letter, we propose an end-to-end stitching network, which takes two images with a narrow field of view (FOV) as inputs, and produces a single image with a wide FOV. Our method estimates multiple homographies to cover the depth differences in the scene and is therefore robust against parallax distortion. In particular, global warping maps are generated using estimated multiple homographies and adjusted by local displacement maps. The final result is made by warping input images multiple times using the warping maps and then merging warped images with the weight maps. Multiple homographies, local displacement maps, and weight maps are generated simultaneously by our stitching network. To train the stitching network, we construct a dataset using the CARLA simulator. Then, using this dataset, our network is trained by end-to-end supervised learning based on appearance matching loss and depth layer loss. In experiments, we show that our method is superior to existing methods both qualitatively and quantitatively. Also, we provide various empirical studies for in-depth analysis as well as the result of the expansion to 360<sup>°</sup> panoramas.', 'title': 'End-to-End Image Stitching Network via Multi-Homography Estimation', 'embedding': []}, {'id': 15182, 'abstractText': 'Despite the significant progress in the understanding of the phenomenon of lightning and the physics behind it, locating and mapping its occurrence remain a challenge. Such localization and mapping of very high frequency (VHF) lightning radiation sources provide a foundation for the subsequent research on predicting lightning, saving lives, and protecting valuable assets. A major technical challenge in attempting to map the sources of lightning is mapping accuracy. The three common electromagnetic radio frequency-based lightning locating techniques are magnetic direction finder, time of arrival, and interferometer (ITF). Understanding these approaches requires critically reviewing previous attempts. The performance and reliability of each method are evaluated on the basis of the mapping accuracy obtained from lightning data from different sources. In this work, we review various methods for lightning mapping. We study the approaches, describe their techniques, analyze their merits and demerits, classify them, and derive few opportunities for further research. We find that the ITF system is the most effective method and that its performance may be improved further. One approach is to improve how lightning signals are preprocessed and how noise is filtered. Signal processing can also be utilized to improve mapping accuracy by introducing methods such as wavelet transform in place of conventional cross-correlation approaches.', 'title': 'Lightning Mapping: Techniques, Challenges, and Opportunities', 'embedding': []}, {'id': 15183, 'abstractText': 'Convolutional neural networks (CNN) are widely used in various computer vision applications. Recently, there have been many studies on FPGA-based CNN accelerators to achieve high performance and power efficiency. Most of them have been on CNN-based object detection algorithms, but researches on image super-resolution have been rarely conducted. Fast super-resolution CNN (FSRCNN), well known for CNN-based super-resolution algorithm, are a combination of multiple convolutional layers and a single deconvolutional layer. Since the deconvolutional layer generates high-resolution (HR) output feature maps from low-resolution (LR) input feature maps, its execution cycles are larger than those of the convolutional layer. In this paper, we propose a novel architecture of the FPGA-based CNN accelerator with the efficient parallelization. We develop a method of transforming a deconvolutional layer into a convolutional layer (TDC), a new methodology for the deconvolutional neural networks (DCNN). There is a massive parallelization source in the deconvolutional layer where multiple outputs within the same output feature map are created with the same inputs. When this new parallelization technique is applied to the deconvolutional layer, it generates the LR output feature maps the same as the convolutional layer. Thus, the performance of the accelerator increases without any additional hardware resources because the kernel size required to generate the LR output feature maps is smaller. In addition, if there is a DSP underutilization problem in the deconvolutional layer that some of the processors are in an idle state, the proposed method solves this problem by allowing more output feature maps to be processed in parallel. Experimental results show that the proposed TDC method achieves up to 81 times higher throughput than the state-of-the-art DCNN accelerator with the same hardware resources. We also improve the speed by 7.8 times by having all layers in the hourglass-type FSRCNN to be processed in inter-layer parallelism without additional DSP usage.', 'title': 'Optimizing FPGA-based convolutional neural networks accelerator for image super-resolution', 'embedding': []}, {'id': 15184, 'abstractText': 'In semiconductor manufacturing systems, defects on wafer maps tend to cluster and then these spatial patterns provide important process information for helping operators in finding out root-causes of abnormal processes. Promptly recognizing wafer map defects is an effective way to increase manufacturing process stability and then to improve yields. Deep learning has been widely applied and obtained many successes in image and visual analysis. This paper proposes an effective deep learning method, enhanced stacked denoising autoencoder (ESDAE) with manifold regularization for wafer map pattern recognition (WMPR) in manufacturing processes. This study will concentrate on developing a deep learning model to learn effective discriminative features from wafer maps through a deep network architecture for WMPR improvement. An indication based on ESDAE is developed for detecting map defects online. An ESDAE-based classifier is finally developed to implement recognition of wafer map defects. The most motivation for developing deep learning and manifold regularization techniques is to achieve higher accuracy and applicability than that of some regular recognizers. The effectiveness of the proposed method has been demonstrated by experimental results from a real-world wafer map dataset (WM-811K).', 'title': 'Enhanced Stacked Denoising Autoencoder-Based Feature Learning for Recognition of Wafer Map Defects', 'embedding': []}, {'id': 15185, 'abstractText': 'Super-resolution mapping (SRM) aims to generate a fine spatial resolution land cover map from input coarse spatial resolution fraction images. The spatial prior model used to describe the spatial land cover patterns at the fine spatial resolution is crucial to the SRM analysis. At present, the learning-based SRM algorithm has shown its advantage, because more information of spatial land cover patterns can be captured from available training fine spatial resolution land cover maps. In practice, for learning-based SRM, the training fine spatial resolution land cover maps should include various spatial land cover pattern examples as rich as possible. However, gathering training fine spatial resolution land cover maps is always a hard work. In order to overcome this shortcoming, this study proposes an approach to provide additional transformed examples (land cover maps) in the learning-based SRM approach. By transforming the original fine spatial resolution land cover maps with rotation and mirroring operations, the number of available training examples can increase eight times. The proposed SRM algorithm is compared with several popular SRM algorithms using both synthetic and real images. Experimental results indicate that more spatial details can be produced when the tranformed samples are applied in the learning-based SRM algorithm and the result produced by the proposed method has higher accuracies than the SRM results used for comparison.', 'title': 'Learning-Based Super-Resolution Land Cover Mapping with Additional Transformed Examples', 'embedding': []}, {'id': 15186, 'abstractText': 'Visualizing and communicating insights through maps offers an intuitive and familiar way to explore large-scale dynamic relational data. In this paper, we present VideoMap, which is a novel approach for presenting and interacting with relational video content by taking advantage of the map metaphor. VideoMap employs a metaphor to visualize video content by elements of a map with the aim of enabling exploration of video content as if reading a map. Video content is visualized in a hierarchal structure from a very large scale to a small scale of finely detailed representation. VideoMap recognizes a small set of sketch gestures for semantic zooming in and out, annotating the map, and automatically completing path navigation. To achieve this, VideoMap synthesizes map-derived visuals and binds them to the underlying data by operating the map with sketch interaction to facilitate interactive exploration. Extensive user studies were conducted to evaluate VideoMap, and the results demonstrated the effectiveness of VideoMap for facilitating the exploration and understanding of large video content.', 'title': 'Visualizing and Analyzing Video Content With Interactive Scalable Maps', 'embedding': []}, {'id': 15187, 'abstractText': 'This paper details a system for an autonomous navigation of electric wheelchair in outdoor urban area. The goal for this study is to achieve autonomous navigation on paved roads including the slopes in a busy street. We propose the autonomous navigation system using “2D drivable map” that explains steps, obstacles and paved/unpaved areas. 2D drivable map generated from 3D point cloud map by robot itself. We detected smoothly connected planes and obstacles by region growing segmentation based on normal in 3D point cloud map and projected the vertically onto a 2D map. A paved/unpaved area segmentation is based on intensity of 2D map. The experimental results show the 2D drivable map correctly explains steps and obstacles in urban areas and detects changes from roads to lawns. An electric wheelchair achieved autonomous navigation in urban areas without colliding with obstacles or mounting lawns.', 'title': 'Autonomous Navigation of Electric Wheelchairs in Urban Areas on the Basis of Self-Generated 2D Drivable Maps', 'embedding': []}, {'id': 15188, 'abstractText': \"Building efficient embedded deep learning systems requires a tight co-design between DNN algorithms, hardware, and algorithm-to-hardware mapping, a.k.a. dataflow. However, owing to the large joint design space, finding an optimal solution through physical implementation becomes infeasible. To tackle this problem, several design space exploration (DSE) frameworks have emerged recently, yet they either suffer from long runtimes or a limited exploration space. This article introduces ZigZag, a rapid DSE framework for DNN accelerator architecture and mapping. ZigZag extends the common DSE with uneven mapping opportunities and smart mapping search strategies. Uneven mapping decouples operands (W/I/O), memory hierarchy, and mappings (temporal/spatial), opening up a whole new space for DSE, and thus better design points are found by ZigZag compared to other SotAs. For this, ZigZag uses an enhanced nested-for-loop format as a uniform representation to integrate algorithm, accelerator, and algorithm-to-accelerator mapping. ZigZag consists of three key components: 1) an analytical energy-performance-area Hardware Cost Estimator, 2) two Mapping Search Engines that support spatial and temporal even/uneven mapping search, and 3) an Architecture Generator that auto-explores the wide memory hierarchy design space. Benchmarking experiments against published works, in-house accelerator, and existing DSE frameworks, together with three case studies, show the reliability and capability of ZigZag. Up to 64 percent more energy-efficient solutions are found compared to other SotAs, due to ZigZag's uneven mapping capabilities.\", 'title': 'ZigZag: Enlarging Joint Architecture-Mapping Design Space Exploration for DNN Accelerators', 'embedding': []}, {'id': 15189, 'abstractText': 'Spectrum maps based on power spectral density (PSD) of spatial signals are helpful for cognitive users (CUs) to accurately sense and utilize the spectrum holes, achieving interference coordination among network nodes, and enhancing link robustness of mobile CUs. As an image matrix based data, the cost of spectrum map dissemination in wireless networks cannot be ignored. Disseminating spectrum maps of excessively high resolution would consume too much bandwidth resources, which prevents the growth space of data traffics. In this paper, we study the spectrum map dissemination problem in the next generation data-driven cognitive radio network scenario and propose a method to obtain the optimal spectrum map resolution and quantization bit of PSD for dissemination. The experiment results demonstrate that we can obtain the tradeoff between spectrum map size and network throughput, so as to achieve better network throughput with relatively low map dissemination consumption.', 'title': 'Research on Spectrum Map Dissemination for Data-Driven Cognitive Radio Networks', 'embedding': []}, {'id': 15190, 'abstractText': 'Underwater mapping is important for many studies such as underwater cable/pipe platform placement and monitoring, bridge piers placement, dam construction, geological and geophysical studies. The position and orientation information of the sea surface vehicle is measured from the Global Positioning System (GPS) and Inertial Measurement Unit (IMU) sensors are placed on the vehicle, and the height from the seafloor is measured from a single beam sonar. External disturbance effects such as waves and wind cause oscillations in the surface vehicle. In bathymetric measurements, which are of great importance in mapping, errors due to oscillations occur. In underwater mapping, the orientation effect of the sea surface vehicle should be known in order to minimize these errors. In the study, to minimize these error sources, data from 3 different IMUs integrated into the sea surface vehicle are fused with sensor fusion algorithms such as the Integrated Navigation System (INS) and Support Vector Machine (SVM). In this study, a machine learning-based SVM integrated navigation system with minimum error in optimum positioning of the surface vehicle under external disturbance effects is proposed. With the INS, the performance of the machine learning-based SVM sensor fusion algorithm is analyzed comparatively.', 'title': 'Sensor Fusion Based on Integrated Navigation Data of Sea Surface Vehicle with Machine Learning Method', 'embedding': []}, {'id': 15191, 'abstractText': 'Objective: The objective of this study was to define the clinical relevance of in vivo electrophysiologic (EP) studies in a rat model of chronic ischemic heart failure (CHF). Methods: Electrical activation sequences, voltage amplitudes, and monophasic action potentials (MAPs) were recorded from adult male Sprague-Dawley rats six weeks after left coronary artery ligation. Programmed electrical stimulation (PES) sequences were developed to induce sustained ventricular tachycardia (VT). The inducibility of sustained VT was defined by PES and the recorded tissue MAPs. Results: Rats in CHF were defined (p &lt;; 0.05) by elevated left ventricular (LV) end-diastolic pressure (5 ± 1 versus 18 ± 2 mmHg), decreased LV + dP/dt (7496 ± 225 versus 5502 ± 293 mmHg/ s), LV - dP/dt (7723 ± 208 versus 3819 ± 571 mmHg), LV ejection fraction (79 ± 3 versus 30 ± 3%), peak developed pressure (176 ± 4 versus 145 ± 9 mmHg), and prolonged time constant of LV relaxation Tau (18 ± 1 versus 29 ± 2 ms). The EP data showed decreased (p &lt;; 0.05) electrogram amplitude in border and infarct zones (Healthy zone (H): 8.7 ± 2.1 mV, Border zone (B): 5.3 ± 1.6 mV, and Infarct zone (I): 2.3 ± 1.2 mV), decreased MAP amplitude in the border zone (H: 14.0 ± 1.0 mV, B: 9.7 ± 0.5 mV), and increased repolarization heterogeneity in the border zone (H: 8.1 ± 1.5 ms, B: 20.2 ± 3.1 ms). With PES we induced sustained VT (&gt;15 consecutive PVCs) in rats with CHF (10/14) versus Sham (0/8). Conclusions: These EP studies establish a clinically relevant protocol for studying genesis of VT in CHF. Significance: The in vivo rat model of CHF combined with EP analysis could be used to determine the arrhythmogenic potential of new treatments for CHF.', 'title': 'In vivo Electrophysiological Study of Induced Ventricular Tachycardia in Intact Rat Model of Chronic Ischemic Heart Failure', 'embedding': []}, {'id': 15192, 'abstractText': 'Phonation is controlled by complex synergism of muscles over the front neck region. Proper evaluation of the muscular activities in this region would not only help to estimate phonation function, but may also provide characteristics to diagnose dysphonia. While surface electromyography (sEMG) technique has been used to study the physiological aspects of phonation in previous studies, it remains unclear if the phonating function could be dynamically characterized by the sEMG signals of the neck muscles associated with phonation. In this study, almost 80 channels of high-density (HD) sEMG signals were acquired from four healthy subjects when the vowel /a/ was phonated across different pitches by them. The root mean square (RMS) of the HD sEMG signals was computed within a series of segmented analysis windows and used to construct dynamic sEMG topographic maps. And the RMS maps represented the energy distribution of the front neck muscles, which would provide both the temporal and spatial information in accordance with the physiological and biomechanical principles of phonation. Our pilot results from the sEMG topographic maps across different pitch levels showed that the muscular activities consistently increased with the enhancement of the pitch levels. This pilot study suggests that HD sEMG might be a potential tool to visualize the distribution of the muscular activities and observe the coordination of muscular contractions during phonation. Also, it might pave way for proper screening and diagnosis of dysphonia as well as its associated pathologies.', 'title': 'A pilot study on the evaluation of normal phonating function based on high-density sEMG topographic maps', 'embedding': []}, {'id': 15193, 'abstractText': 'This study investigates the interplay between load and power in load coupled interference networks. In more detail, the first objective of this study is to derive a positive concave mapping having as its fixed point the power allocation inducing a desired network load. Knowledge of this mapping is important for many theoretical and practical reasons. First, it opens up the possibility of applying many existing algorithms to compute the power inducing a desired load, which is an estimation problem typically used to obtain energy efficient network configurations. With the results in this study, systems designers can now select an algorithm based on practical considerations such as the computational complexity, memory requirements, and convergence speed. In particular, we show that algorithms based on simple fixed point iterations already have many advantages over the previously only known method for the power estimation problem. Second, knowledge of specific properties of the mapping, such as concavity, enables us to use standard tools in convex analysis to analyze the network, and it may also give rise to novel optimization tools for self-organizing networks. The second main objective of this study is the development of a truly distributed algorithm for power estimation in real networks. This algorithm uses only information that is readily available at base stations, and it does not require any additional signaling overhead. These two characteristics make the proposed algorithm especially useful in ultra-dense wireless networks, one of the main visions for 5G networks.', 'title': 'Low Complexity Iterative Algorithms for Power Estimation in Ultra-Dense Load Coupled Networks', 'embedding': []}, {'id': 15194, 'abstractText': 'Predicting the next foot placement of humans during walking can help improve compliant interactions between humans and walking aid robots. Previous studies have focused on foot placement estimation with wearable inertial sensors after heel-strike, but few have predicted foot placements in advance during the early swing phase. In this study, a Bayesian inference-based foot placement prediction approach was proposed. Possible foot placements were modeled as a probability distribution grid map. With selected foot motion feature events detected sequentially in the early swing phase, the foot placement probability map could be updated iteratively using the feature models we built. The weighted center of the probability distribution was regarded as the predicted foot placement. Prediction errors were evaluated with collected walking data sets. When testing with the data from inertial measurement units, the prediction errors were (5.46 cm ± 10.89 cm, -0.83 cm ± 10.56 cm) for cross-velocity walking data and (-4.99 cm ± 12.31 cm, -11.27 cm ± 7.74 cm) for cross-subject–cross-velocity walking data. The results were comparable to previous works yet the prediction could be made earlier. For the subject who walked with more stable gaits, the prediction error can be further decreased. The proposed foot placement prediction approach can be utilized to help walking aid robots adjust their pose before each heel-strike event during walking, which will make human–robot interactions more compliant. This study is also expected to inspire additional probabilistic gait analysis works.', 'title': 'A Probability Distribution Model-based Approach for Foot Placement Prediction in the Early Swing Phase with a Wearable IMU Sensor', 'embedding': []}, {'id': 15195, 'abstractText': \"Urban commerce and its distribution have always been an important part of urban research. However, most previous studies were based on statistical data and did not reflect real street experience. Thanks to the Street View image and deep learning technology, researchers are able to carry out large scale studies from real human visual experience. In this article, we aim at sensing the commercial spaces in cities. In order to achieve this ultimate goal, deep learning is applied to process the raw data of Street View image. We disassemble the goal into three tasks: firstly, obtaining all the Street View images in a specific area; then classifying the Street View images according to the commercial facilities in it; and finally creating a visualization of the detected data into a map. For the first task, we get the road network coordinate information from the openstreetmap (OSM) website, set the sampling point on the road, and then download the Street View images of the sampling points' coordinate through the API provided by Baidumap. For the second task, we adopt a two-level learning strategy rather than directly using Deep Convolutional Neural Network for classification. For the final task, we choose the heat map as the expression of the results and draw the map by the existing GIS software. Furthermore, the results from this study can be conveniently combined with other data because of the use of street-network-based data structure. An application of this method combines with street-network data, the calculation of a city's 15-minute commercial service circle coverage is also shown in this study.\", 'title': 'Urban Commerce Distribution Analysis Based on Street View and Deep Learning', 'embedding': []}, {'id': 15196, 'abstractText': 'The paper introduces a fundamental technological problem with collecting high-speed eye tracking data while studying software engineering tasks in an integrated development environment. The use of eye trackers is quickly becoming an important means to study software developers and how they comprehend source code and locate bugs. High quality eye trackers can record upwards of 120 to 300 gaze points per second. However, it is not possible to map each of these points to a line and column position in a source code file (in the presence of scrolling and file switching) in real time at data rates over 60 gaze points per second without data loss. Unfortunately, higher data rates are more desirable as they allow for finer granularity and more accurate study analyses. To alleviate this technological problem, a novel method for eye tracking data collection is presented. Instead of performing gaze analysis in real time, all telemetry (keystrokes, mouse movements, and eye tracker output) data during a study is recorded as it happens. Sessions are then replayed at a much slower speed allowing for ample time to map gaze point positions to the appropriate file, line, and column to perform additional analysis. A description of the method and corresponding tool, Déjà Vu, is presented. An evaluation of the method and tool is conducted using three different eye trackers running at four different speeds (60Hz, 120Hz, 150Hz, and 300 Hz). This timing evaluation is performed in Visual Studio and Eclipse IDEs. Results show that Déjà Vu can playback 100% of the data recordings, correctly mapping the gaze to corresponding elements, making it a well-founded and suitable post processing step for future eye tracking studies in software engineering.', 'title': 'Automated Recording and Semantics-Aware Replaying of High-Speed Eye Tracking and Interaction Data to Support Cognitive Studies of Software Engineering Tasks', 'embedding': []}, {'id': 15197, 'abstractText': \"The sustainability of agricultural sector is becoming an increasing concern in Indonesia, with the shock of the palm oil's products export restrictions to several countries. Therefore, it is vital to avoid the same case for all other export-oriented agricultural products, such as cocoa. This study aims to map the challenges and opportunities in developing a sustainable cocoa supply chain. The study commenses with mapping the role of stakeholders along the supply chain, studying the supply chain mapping literature in a sustainable perspective, and identifying the indicators used to measure the supply chain. The results of this study propose a conceptual model to be utilized as a base reference for research of the agricultural supply chain sustainability model development in Indonesia.\", 'title': 'Literature Review of a Multi Actor Analysis for Developing a Sustainable Agriculture Supply Chain (Case Study of Cocoa)', 'embedding': []}, {'id': 15198, 'abstractText': 'Glacial landforms are a significant element of landscape in many regions of Earth. The increasing availability of high-resolution digital elevation models (DEMs) provides an opportunity to develop automated methods of glacial landscape exploration and classification. In this study, we aimed to: 1) identify glacial landforms based on high-resolution DEM datasets; 2) determine relevant geomorphometric and spectral parameters and object-based features for the mapping of glacial landforms; and 3) develop an accurate workflow for glacial landform classification based on DEM. The developed methodology included the extraction of secondary features from DEM, feature selection with the Boruta algorithm, object-based image analysis, and random forest supervised classification. We applied the workflow for three study sites: one in Svalbard and two in Poland. It allowed the identification of six categories of glacial landforms: till plains, end moraines, hummocky moraines, outwash/glaciolacustrine plains, valleys, and kettle holes. The majority of relevant secondary features represented DEM spectral parameters calculated from 2-D Fourier analysis. The supervised classification models with the highest performance exhibited up to 96% overall accuracy with regard to a groundtruth dataset. This study showed that glacial landforms can be identified using novel image-processing methodology and spectral parameters of high-resolution DEM. The complete classification workflow developed herein provides a solution for the transparent generation of thematic maps of glacial landforms that may be reproducible and transferrable to various glacial regions worldwide.', 'title': 'Exploration of Glacial Landforms by Object-Based Image Analysis and Spectral Parameters of Digital Elevation Model', 'embedding': []}, {'id': 15199, 'abstractText': 'The dengue disease has been reported as a major cause of morbidity and mortality for the last 40 years worldwide including in Malaysia. According to the Malaysian Statists Department, total reported number of dengue cases for the year 2008 to 2010 is 136,992 and it is increasing every year. According to Ministry of Health (MoH) of Malaysia, in urban area 77% of people are suffering from dengue virus compared to 23% people suffer in rural areas. Several studies on dengue are based on their serotype, epidemiology, weather forecasting, but this thesis look into the prediction modelling based on clustering algorithm. Since, the motivation to work on this projects comes from old existing studies which only represent the spatial map with undefined number of incidences and no clustering model exist. Since, in our study we have proposed the spatial- temporal mapping along with the Clustering techniques which comes with k-mean as the initial step to generate the clusters of incidences after that to optimize K-means, K-NN techniques is applied to find best fit K values. After that Gaussian Mixture Model is applied to find density of the dengue incidences, since where K-means is used to find the centroid of the incidences. To process the Gaussian Mixture Model, Estimation Maximization (EM) algorithm is used to relate the cluster with their respective clusters. To optimize the EM algorithm Bayesian Information Criteria (BIC) is performed which gives the best fitting model of BIC and optimizes the EM algorithm. In the end, Geographical Information System (GIS) technique is use to visualize vulnerability mapping to locate the accurate prediction location for dengue incidences in state Selangor of Malaysia (area of study). This research work discusses and implements the visualization and prediction modelling based on machine learning concepts, for the vector borne diseases (dengue). The results are tested for a region (Petaling district of Selangor state) in Malaysia and they showed good performance in predicting the dengue incidences. Thus, the proposed method is able to localize the nature of dengue incidence that can further be utilized for vector disease controlled process. The results confirms location of the predicted coordinates based on the previous data for the year 2014.', 'title': 'Spatial-Temporal Visualization of Dengue Incidences Using Gaussian Kernel', 'embedding': []}, {'id': 15200, 'abstractText': 'As a first step of genomics signal processing, alphabetical sequence is mapped to numerical. The choice of mapping techniques depends on the application and affects the result of the study. Since biological function is the result of amino acids interactions, a significant method for alphabetical to numerical conversion of sequence is to use the physico-chemical and biochemical properties of amino acids. AAindex database is a rich collection of such properties that can be used for numerical representation of protein. Each of these properties gives a viewpoint in the study of biological functions. Taking into account all AAindex indices leads to a multi-viewpoint representation and provides more options to observe and study the target biological phenomena. But this advantage increases variables number, space dimension and computation time. Since there is correlation between AAindex databases, to handle the issue of space dimension increasement, compact versions of correlated indices are extracted. This paper aims at the construction of new indices through clustering of AAindex database with correlation distance. The results suggest that due to the correlation of these new maps with groups of AAindex indices (in clusters); they have the potential to be used for numerical representation of protein sequence in different studies.', 'title': 'A multivariate clustering of AAindex database for protein numerical representation', 'embedding': []}, {'id': 15201, 'abstractText': 'For medical imaging tasks, it is a prevalent practice to have a multi-modality image dataset, as experts prefer using multiple medical devices to diagnose a disease. Each device can show different aspects of segmentation, which in our case, is magnetic resonance imaging (MRI) brain tumor segmentation. For such medical imaging tasks, researchers tend to combine all modalities as an input into the network for feature extraction, and neglect the complexity between different modalities. It is no longer novel to use an encoder-decoder-based model and residual connections to transfer information from high-resolution maps to lower-resolution maps in medical segmentation tasks. In this work, we propose a multimodal fusion network with bi-directional feature pyramid network (MM-BiFPN) using an individual encoder to extract the features of each of the four modalities (FLAIR, T1-weighted, T1-c, and T2-weighted) to focus on the exploitation of the complex relationships among the modalities. In addition, by using the bi-directional feature pyramid network (Bi-FPN) layer, we focus on the aggregation of multiple modalities to study the cross-modality relationship and multi-scale features. Our experiment was conducted on the brain segmentation challenge datasets, the MICCAI BraTS2018 and MICCAI BraTS2020 datasets. We also implemented two ablation studies on our model with different cross-scale modalities fusion networks, as well as a study on different modality settings to see the effect each modality brings in detecting tumor content. With missing modalities, our method achieves a comparable result, demonstrating that our method is robust for brain tumor segmentation.', 'title': 'MM-BiFPN: Multi-Modality Fusion Network with Bi-FPN for MRI Brain Tumor Segmentation', 'embedding': []}, {'id': 15202, 'abstractText': 'A quick response to a large-scale natural disaster such as earthquake and tsunami is vital to mitigate further loss. Remote sensing, especially the spaceborne sensors, provides the possibility to monitor a very large scale area in a short time and with regular revisit circle. Damage ranges and damage levels of the destructed urban areas are extremely important information for rescue planning after an event. Rapid mapping of the urban damage levels with synthetic aperture radar (SAR) is still challenging. Compared with single-polarization SAR, fully polarimetric SAR (PolSAR) has a better potential to understand the urban damage from the viewpoint of scattering mechanism investigation. In radar polarimetry, the dominant double-bounce scattering mechanism in an urban area is primarily induced by the ground-wall structures and can reflect the changes of these structures. In this sense, urban damage level in terms of destroyed ground-wall structures can be indicated by the reduction of the dominant double-bounce scattering mechanism, which is the basis of this study. This work first establishes and validates the linear relationship between the urban damage level and the proposed polarimetric damage index using polarimetric model-based decomposition. Then, efforts are focused on the development of a rapid urban damage level mapping technique which mainly includes two steps of urban area extraction and polarimetric damage level estimation. The 3.11 East Japan earthquake and tsunami inducing great-scale destruction are adopted for study using L-band multitemporal spaceborne PolSAR data. Experimental studies demonstrate that the estimated damage levels are closely consistent to the ground-truth. The final urban damage level map for the full scene is generated thereafter. Results achieved in this study further validate the necessity of exploring fully polarimetric technique for damage assessment.', 'title': 'Urban Damage Level Mapping Based on Scattering Mechanism Investigation Using Fully Polarimetric SAR Data for the 3.11 East Japan Earthquake', 'embedding': []}, {'id': 15203, 'abstractText': 'Context: As software applications become more complex and competition between companies demands shorter time-to-market, the methodologies to develop these applications needs to adapt to this new environment of continuous change. New strategies are required to allow effective management of changing requirements, continuous deliveries, testing, integration, etc. Software reuse emerges as a potential solution to these demands. This work presents results from a systematic mapping study aimed at assessing the use of software reuse in the context of continuous software development. Objective: To identify, analyse, and classify the published works in order to provide an overview of current challenges and the trends in the application of systematic software reuse to the continuous software development. Method: We use a systematic mapping study, posing a set of questions to map the research space, defining exclusion/inclusion criteria and developing a classification schema. We limit our study to peer-reviewed works published up to December 2016. Results: Our study includes a set of fifteen works. We classified the works according to reuse processes, CSD activities and type of research. Component-based development is the most frequent reuse process, while the three CSD activities (Deployment, integration and testing) are mentioned equally. Quality, productivity and reduction of costs are the reuse effects more reported. Conclusions: We have not found previous systematic reviews in this field. The main outcome is that we do not find causal relations between reuse processes and CSD activities. The effect of reuse do not relate to the context of the experience reported. There is a lack of empirical evidence and reported data. This is a poorly investigated area offering a promising domain for future research.', 'title': 'Software Reuse and Continuous Software Development: A Systematic Mapping Study', 'embedding': []}, {'id': 15204, 'abstractText': 'Understanding the geometry and kinematics of the broad line region (BLR) of active galactic nuclei (AGN) is important to estimate black hole masses in AGN and study the accretion process. The technique of reverberation mapping (RM) has provided estimates of BLR size for more than 100 AGN now; however, the structure of the BLR has been studied for only a handful number of objects. Towards this, we investigated the geometry of the BLR for a large sample of 57 AGN using archival RM data. We performed systematic modelling of the continuum and emission line light curves using a Markov chain Monte Carlo method based on Bayesian statistics implemented in PBMAP (Parallel Bayesian code for reverberation−MAPping data) code to constrain BLR geometrical parameters and recover velocity integrated transfer function. We found that the recovered transfer functions have various shapes such as single-peaked, double-peaked, and top-hat suggesting that AGN have very different BLR geometries. Our model lags are in general consistent with that estimated using the conventional cross-correlation methods. The BLR sizes obtained from our modelling approach is related to the luminosity with a slope of 0.583\\xa0±\\xa00.026 and 0.471\\xa0±\\xa00.084 based on H\\u2009β and H\\u2009α lines, respectively. We found a non-linear response of emission line fluxes to the ionizing optical continuum for 93 <tex>${{\\\\ \\\\rm per\\\\ cent}}$</tex> objects. The estimated virial factors for the AGN studied in this work range from 0.79 to 4.94 having a mean at 1.78\\xa0±\\xa01.77 consistent with the values found in the literature.', 'title': 'Estimation of the size and structure of the broad line region using Bayesian approach', 'embedding': []}, {'id': 15205, 'abstractText': 'In comparative genomics, one goal is to find similarities between genomes of different organisms. Comparisons using genome features like genes, gene order, and regulatory sequences are carried out with this purpose in mind. Genome rearrangements are mutational events that affect large extensions of the genome. They are responsible for creating extant species with conserved genes in different positions across genomes. Close species — from an evolutionary point of view — tend to have the same set of genes or share most of them. When we consider gene order to compare two genomes, it is possible to use a parsimony criterion to estimate how close the species are. We are interested in the shortest sequence of genome rearrangements capable of transforming one genome into the other, which is named <italic>rearrangement distance</italic>. Reversal is one of the most studied genome rearrangements events. This event acts in a segment of the genome, inverting the position and the orientation of genes in it. Transposition is another widely studied event. This event swaps the position of two consecutive segments of the genome. When the genome has no gene repetition, a common approach is to map it as a permutation such that each element represents a conserved block. When genomes have replicated genes, this mapping is usually performed using strings. The number of replicas depends on the organisms being compared, but in many scenarios, it tends to be small. In this work, we study the rearrangement distance between genomes with replicated genes considering that the orientation of genes is unknown. We present four heuristics for the problem of genome rearrangement distance with replicated genes. We carry out experiments considering the exclusive use of the reversals or transpositions events, as well as the version in which both events are allowed. We developed a database of simulated genomes and compared our results with other algorithms from the literature. The experiments showed that our heuristics with more sophisticated rules presented a better performance than the known algorithms to estimate the evolutionary distance between genomes with replicated genes. In order to validate the application of our algorithms in real data, we construct a phylogenetic tree based on the distance provided by our algorithm and compare it with a know tree from the literature.', 'title': 'Heuristics for Genome Rearrangement Distance With Replicated Genes', 'embedding': []}, {'id': 15206, 'abstractText': 'With the advent of Service-Oriented Architecture (SOA), services can be registered, invoked, and combined by their identical Quality of Services (QoS) attributes to create a new value-added application that fulfils user requirements. Efficient QoS-aware service composition has been a challenging task in cloud computing. This challenge becomes more formidable in emerging resource-constrained computing paradigms such as the Internet of Things and Fog. Service composition has regarded as a multi-objective combinatorial optimization problem that falls in the category of NP-hard. Historically, the proliferation of services added to problem complexity and navigated solutions from exact (none-heuristics) approaches to near-optimal heuristics and metaheuristics. Although metaheuristics have fulfilled some expectations, the quest for finding a high-quality, near-optimal solution has led researchers to devise hybrid methods. As a result, research on service composition shifts towards the hybridization of metaheuristics. Hybrid metaheuristics have been promising efforts to transcend the boundaries of metaheuristics by leveraging the strength of complementary methods to overcome base algorithm shortcomings. Despite the significance and frontier position of hybrid metaheuristics, to the best of our knowledge, there is no systematic research and survey in this field with a particular focus on strategies to hybridize traditional metaheuristics. This study’s core contribution is to infer a framework for hybridization strategies by conducting a mapping study that analyses 71 papers between 2008 and 2020. Moreover, it provides a panoramic view of hybrid methods and their experiment setting in respect to the problem domain as the main outcome of this mapping study. Finally, research trends, directions and challenges are discussed to benefit future endeavours.', 'title': 'Hybrid Metaheuristics for QoS-aware Service Composition: A Systematic Mapping Study', 'embedding': []}, {'id': 15207, 'abstractText': 'Forest is the largest terrestrial ecosystem on the earth. Quantitative evaluation of the impacts of the land surface slope on forest spatial distributions is of great significance for a deeper understanding of functions and stability of forest ecosystem, scientific planning and rational management of forest resources. The superposition analysis of map of vegetation and digital elevation model (DEM) is an effective method to study impacts of the land surface slope on forest spatial distributions. In the past time, the data of land surface slope was mainly obtained by field measurement which had some problems of time-consuming, labor-intensive and high investigation cost. With the development of space technology, DEM data can be used to obtain land surface slope data rapidly and efficiently which has been widely used in digital forestry construction. However, studies on the influence of land surface slope on forest spatial distribution by using DEM data are rare at home and abroad. Dali City of Yunnan Province was selected as the research area in this study. The contour line vector is used to establish DEM of this area, then collect the slope data and divide the slope grades into five groups: flat slopegentle slopemoderate slopesteep slope and sharp slope. Supervised classification and visual interpretation were executed to interpret and classify the map of vegetation of Dali. By putting map of forest distribution and DEM togetherthe relationship between forest distribution and slope was analyzed and the trend of forest spatial distribution was found out. The results showed that Dali city is relatively flat and the terrain is complex and diverse. The woodland has a large area distribution in Dali City, which is related to the monsoon climate of the subtropical plateau in Dali. The shrub forests were mainly distributed on moderate slope and steep slope, and the coniferous forests were mainly distributed on gentle slope and moderate slope. As the slope changes, the distribution of shrubberies increases and decreases sharply, indicating that shrubberies are highly dependent on slope. Coniferous forests have a large area distribution at each grade, indicating that they are less dependent on slope. In general, with the increase of slope, both forest types showed a trend of increasing first and then decreasing, indicating that the surface slope has an impact on the spatial distribution of forests. This study can offer reference to the rational and scientific management of forest resources.', 'title': 'Impacts of the Land Surface Slope on Forest Spatial Distributions', 'embedding': []}, {'id': 15208, 'abstractText': 'Based on the analysis of the current literature, mathematical modelling of the studied phenomenon was carried out. It was conducted according to the graph theory for vehicles transporting dangerous, oversized, and valuable cargo, moving within a transport network. The mapping of the parking areas included organisational, technical, and security aspects. The main algorithm determines the driving routes based on the parkings that meet the requirements specified by the carrier. These routes can ultimately be analysed and evaluated based on the parameters resulting from the formulated criterion functions. To verify the performance of the algorithm, its implementation was carried out in a computer environment using the Neo4j graph database. The transport network, consisting of 462 transport nodes and 602 transport links, was mapped based on the real national road network of the province in Poland. Data from 113 parking locations in the study area were included in the analysis. The research covered a total of three case studies, one for each vehicle type requiring specific parking conditions, for various input data. The results obtained allowed us to assess the validity of selecting particular routes and evaluating the scalability of the solution. The result of this work is a method, which, on the one hand, fits the current transportation requirements. On the other hand, it lends itself to scaling, extension by additional logical constraints, and is compatible with modern parking systems.', 'title': 'Parking lots assignment algorithm for vehicles requiring specific parking conditions in Vehicle Routing Problem', 'embedding': []}, {'id': 15209, 'abstractText': \"This study aims to obtain a future mapping of NO2 and SO2 pollutant levels in the city of Bandung and it's around. Two mathematical models are used in this study namely the Generalized Space-Time Autoregressive (GSTAR) and Simple Kriging models. The GSTAR model is an estimator model based on time and this study is used to predict future pollutant levels at specific location points. While the Simple Kriging model is a spatial model which in this study is used to interpolate or estimate pollutant levels around points previously estimated using the GSTAR model. The pollutant level estimation of Simple Kriging is then used to map the pollutant levels in Bandung in the next few years. Ljung-Box statistical test results show that the GSTAR model is feasible to use. The assumption of stationary data on the simple Kriging model is determined using the Augmented Dickey-Fuller (ADF) test, while the determination of the semivariogram model is determined using the RMSE calculation. The ADF test showed that stationary data was obtained for the first derivative, whereas the most appropriate theoretical semivariogram model is an exponential semivariogram. Pollutant mapping results in 2021-2024 show that the northern and southern Bandung regions have higher levels of pollutant levels than other locations, although significant increases only occurred for NO2 levels.\", 'title': 'Prediction and Mapping of Air Pollution in Bandung Using Generalized Space Time Autoregressive and Simple Kriging', 'embedding': []}, {'id': 15210, 'abstractText': 'We study the task of single person dense pose estimation. Specifically, given a human-centric image, we learn to map all human pixels onto a 3D, surface-based human body model. Existing methods approach this problem by fitting deep convolutional networks on sparse annotated points where the regression on both surface coordinate components for each body part is uncorrelated and optimized separately. In this work, we devise a novel, unified regression loss function that explicitly characterizes the correlation between the two surface coordinate components, achieving significant improvements in both accuracy and efficiency. Furthermore, based on an observation that the image-to-surface correspondence is intrinsically invariant to geometric transformations from input images, we propose to enforce a geometric equivariance consistency on the target mapping, thereby allowing us to enable reliable supervision on large amounts of unlabeled pixels. We conduct comprehensive studies on the effectiveness of our approach using a quite simple network. Extensive experiments on the DensePose-COCO dataset show that our model achieves superior performance against previous state-of-the-art methods with much less computation complexity. We hope that our work would serve as a solid baseline for future study in the field. The code will be available at https://github.com/Johnqczhang/densepose.pytorch.', 'title': 'Single Person Dense Pose Estimation via Geometric Equivariance Consistency', 'embedding': []}, {'id': 15211, 'abstractText': 'The purpose of this study was to develop a consensus-based computed tomographic (CT) atlas that defines lymph node stations in radiotherapy for lung cancer based on the lymph node map of the International Association for the Study of Lung Cancer (IASLC). A project group in the Japanese Radiation Oncology Study Group (JROSG) initially prepared a draft of the atlas in which lymph node Stations 1–11 were illustrated on axial CT images. Subsequently, a joint committee of the Japan Lung Cancer Society (JLCS) and the Japanese Society for Radiation Oncology (JASTRO) was formulated to revise this draft. The committee consisted of four radiation oncologists, four thoracic surgeons and three thoracic radiologists. The draft prepared by the JROSG project group was intensively reviewed and discussed at four meetings of the committee over several months. Finally, we proposed definitions for the regional lymph node stations and the consensus-based CT atlas. This atlas was approved by the Board of Directors of JLCS and JASTRO. This resulted in the first official CT atlas for defining regional lymph node stations in radiotherapy for lung cancer authorized by the JLCS and JASTRO. In conclusion, the JLCS–JASTRO consensus-based CT atlas, which conforms to the IASLC lymph node map, was established.', 'title': 'The Japan Lung Cancer Society–Japanese Society for Radiation Oncology consensus-based computed tomographic atlas for defining regional lymph node stations in radiotherapy for lung cancer', 'embedding': []}, {'id': 15212, 'abstractText': \"With the advancement in the technology, objects can be represented effectively in their 3D digital models which accurately represents their physical counterparts. Navigation services and mapping based on geographical data have become very popular in supporting our everyday lives. Much of these services are currently available mostly for outdoor purposes, however applications for indoor purposes are being explored where most of the human activities takes place. This can help transform cities into “Smart Cities”. The aim of this study is to develop an indoor mapping system for data collection in a building environment by exploring new, efficient and cost effective scanning devices. The conventional devices currently in use are expensive which makes them difficult to implement for large scale applications. The data will be collected using a 3D scanning camera technology which develops depth maps of various locations. Xbox's Kinect Sensor and Stereolab's ZED camera are being used and compared in this study. Comparisons based on resolution, lighting, accuracy, speed and memory are being made in this study. Their pros and cons over conventional scanning devices are also discussed. The study shows the possibility of using this technology in a large scale building environment in an autonomous method for the future. This technology can then be potentially used for commercial purposes especially to track progress at construction sites, security purposes, facility management, retail and augmented reality applications.\", 'title': 'Indoor mapping for smart cities — An affordable approach: Using Kinect Sensor and ZED stereo camera', 'embedding': []}, {'id': 15213, 'abstractText': 'In the current competitive world, producing quality products has become a prominent factor to succeed in business. In this respect, defining and following the software product quality metrics (SPQM) to detect the current quality situation and continuous improvement of systems have gained tremendous importance. Therefore, it is necessary to review the present studies in this area to allow for the analysis of the situation at hand, as well as to enable us to make predictions regarding the future research areas. The present research aims to analyze the active research areas and trends on this topic appearing in the literature during the last decade. A Systematic Mapping (SM) study was carried out on 70 articles and conference papers published between 2009 and 2019 on SPQM as indicated in their titles and abstract. The result is presented through graphics, explanations, and the mind mapping method. The outputs include the trend map between the years 2009 and 2019, knowledge about this area and measurement tools, issues determined to be open to development in this area, and conformity between conference papers, articles and internationally valid quality models. This study may serve as a foundation for future studies that aim to contribute to the development in this crucial field. Future SM studies might focus on this subject for measuring the quality of network performance and new technologies such as Artificial Intelligence (AI), Internet of things (IoT), Cloud of Things (CoT), Machine Learning, and Robotics.', 'title': 'Software Product Quality Metrics: A Systematic Mapping Study', 'embedding': []}, {'id': 15214, 'abstractText': \"Understanding the 3-D geometric structure of the Earth's surface has been an active research topic in photogrammetry and remote sensing community for decades, serving as an essential building block for various applications such as 3-D digital city modeling, change detection, and city management. Previous research studies have extensively studied the problem of height estimation from aerial images based on stereo or multiview image matching. These methods require two or more images from different perspectives to reconstruct 3-D coordinates with camera information provided. In this letter, we deal with the ambiguous and unsolved problem of height estimation from a single aerial image. Driven by the great success of deep learning, especially deep convolutional neural networks (CNNs), some research studies have proposed to estimate height information from a single aerial image by training a deep CNN model with large-scale annotated data sets. These methods treat height estimation as a regression problem and directly use an encoder-decoder network to regress the height values. In this letter, we propose to divide height values into spacing-increasing intervals and transform the regression problem into an ordinal regression problem, using an ordinal loss for network training. To enable multiscale feature extraction, we further incorporate an Atrous Spatial Pyramid Pooling (ASPP) module to extract features from multiple dilated convolution layers. After that, a postprocessing technique is designed to transform the predicted height map of each patch into a seamless height map. Finally, we conduct extensive experiments on International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam data sets. Experimental results demonstrate significantly better performance of our method compared to state-of-the-art methods.\", 'title': 'Height Estimation From Single Aerial Images Using a Deep Ordinal Regression Network', 'embedding': []}, {'id': 15215, 'abstractText': 'Currently, the use of Information and Communication Technology (ICT) is developing to meet the demands of a better life. This enhances studies to try to improve services to meet these demands. is one to shape a better experience. development needs to be done in a structured way so that they are integrated and efficiently implement the latest technology to build a digital economy. For this reason, it is necessary to apply good governance in planning for the development of technology-based services, so that the development of smart cities towards digital prosperity can be realised. The most important thing that needs to map out is prioritising services in smart cities development.In this study, we conducted a literature study and observations of ICT implementation in the development of services in Indonesia. Then we do a mapping between services that can shape into a digital economy by referring to digital welfare introduced by Atkinsons.Our results show that the initial program that supports faster productivity growth is an ICT service that makes up smart cities is a priority that must be considered by the central and local governments.', 'title': 'The Governance Strategies To Build Smart City Towards Digital Prosperity', 'embedding': []}, {'id': 15216, 'abstractText': 'This paper presents the study of mineral and vegetation in explored fields around the San Juan coal mines west of Farmington, New Mexico. The purpose of this research work is to map the mineral rock &amp; vegetation for statistically analyzing the study area. Pre-processing of Hyperspectral imagery (HSI) data is required for conversion from digital value to reflectance. Minimum Noise Fraction (MNF) and Pure Pixel Index (PPI) method is used for extraction of Endmember fraction. Spectral signature matching procedure is done with U.S. Geological Survey (USGS) Spectral library, which contain spectra of individual species that have been acquired at test sites representatives of varied terrain and climatic zones, observed in the field under natural conditions. Spectral Angle Mapper (SAM) technique is used for spectral analysis and mapping of image. Finally study area is mapped in two classes namely Carnallite mineral and Sagebrush vegetation plants. Land covered by Sagebrush plant is 8.31% and Carnallite is 1.41% of study area.', 'title': 'Mapping of the carnallite mineral and sagebrush vegetation plant by using hyperspectral remote sensing and usgs spectral library', 'embedding': []}, {'id': 15217, 'abstractText': 'The use of games in software engineering education is not new. However, recent technologies have provided new opportunities for using games and their elements to enhance learning and student engagement. The goal of this paper is twofold. First, we discuss how game related methods have been used in the context of software engineering education by means of a systematic mapping study. Second, we investigate how these game related methods support specific knowledge areas from software engineering. The systematic mapping study identified 106 primary studies describing the use of serious games, gamification and game development in software engineering education. Based on this mapping, we aimed to track the learning goals of each primary study to the knowledge areas defined in ACM/IEEE curricular recommendations. As a result, we observed that \"Software Process\", \"Software Design\" and \"Profession Practice\" are the most recurring knowledge areas explored by game related approaches in software engineering education. We also uncover possible research opportunities for game-related education methods.', 'title': 'Games for learning: bridging game-related education methods to software engineering knowledge areas', 'embedding': []}, {'id': 15218, 'abstractText': 'Quantitative acoustic microscopy (QAM) at 500 MHz permits measuring acoustic properties, such as speed of sound (SOS), attenuation (A) and acoustic impedance (Z), of tissue microstructure with a spatial resolution of 4 μm. Although high-frequency QAM has been shown to be a suitable tool for measuring the acoustic properties of several soft tissues, very few studies have been performed at 500 MHz, and none in cancerous lymph nodes (LNs). However, data at such fine resolutions are essential to improve our understanding of ultrasound scattering at lower frequencies (25 MHz) from LNs to detect clinically significant micrometastases. In this study, quantitative acoustic microscopy (QAM) at 500 MHz was performed to obtain 2D maps of speed of sound (c), attenuation (A), acoustic impedance (Z), and other derived acoustical parameters of nodal tissue microstructure with a spatial resolution of 4 μm. Thin section (i.e., 6-μm) of lymph nodes were scanned using a custom-built QAM system based on a novel F-1.08, 500-MHz transducer. The system digitized radio-frequency (RF) signals at 2.5 GHz with 12-bit accuracy.2D QAM maps of the acoustic parameters were obtained using custom a signal-processing algorithm. Following QAM scanning, the samples were stained using hematoxylin and eosin and imaged by light microscopy. The study illustrates that fine-resolution maps of acoustic properties of lymph nodes can be obtained and can provide previously unavailable information. Future studies will investigate the use of 2DZMs to improve the model of ultrasound scattering at 26 MHz. The new lymphnode-specific, ultrasound-scattering models could improve sensitivity and specificity of current QUS approaches for detecting metastatic regions in freshly excised lymph nodes from cancer patients.', 'title': '500-MHz quantitative acoustic microscopy imaging of unstained fixed 6-µm thin sections from cancerous human lymph nodes', 'embedding': []}, {'id': 15219, 'abstractText': 'Semi-supervised learning aims to learn prediction models from both labeled and unlabeled samples. There has been extensive research in this area. Among existing work, generative mixture models with Expectation-Maximization (EM) is a popular method due to clear statistical properties. However, existing literature on EM-based semi-supervised learning largely focuses on unstructured prediction, assuming that samples are independent and identically distributed. Studies on EM-based semi-supervised approach in structured prediction is limited. This paper aims to fill the gap through a comparative study between unstructured and structured methods in EM-based semi-supervised learning. Specifically, we compare their theoretical properties and find that both methods can be considered as a generalization of self-training with soft class assignment of unlabeled samples, but the structured method additionally considers structural constraint in soft class assignment. We conducted a case study on real-world flood mapping datasets to compare the two methods. Results show that structured EM is more robust to class confusion caused by noise and obstacles in features in the context of the flood mapping application.', 'title': 'Semi-supervised Learning with the EM Algorithm: A Comparative Study between Unstructured and Structured Prediction', 'embedding': []}, {'id': 15220, 'abstractText': 'This paper presents a Bayesian framework for estimating a Probabilistic Linear Discriminant Analysis (PLDA) model in the presence of noisy labels. True class labels are interpreted as latent random variables, which are transmitted through a noisy channel, and received as observed speaker labels. The labeling process is modeled as a Discrete Memoryless Channel (DMC). PLDA hyperparameters are interpreted as random variables, and their joint posterior distribution is derived using mean-field Variational Bayes, allowing maximum a posteriori (MAP) estimates of the PLDA model parameters to be determined. The proposed solution, referred to as VB-MAP, is presented as a general framework, but is studied in the context of speaker verification, and a variety of use cases are discussed. Specifically, VB-MAP can be used for PLDA estimation with unreliable labels, unsupervised PLDA estimation, and to infer the reliability of a PLDA training set. Experimental results show the proposed approach to provide significant performance improvements on a variety of NIST Speaker Recognition Evaluation (SRE) tasks, both for data sets with simulated mislabels, and for data sets with naturally occurring missing or unreliable labels.', 'title': 'Bayesian Estimation of PLDA in the Presence of Noisy Training Labels, with Applications to Speaker Verification', 'embedding': []}, {'id': 15221, 'abstractText': \"Satellite remote sensing is playing an increasing role in the rapid mapping of damage after natural disasters. In particular, synthetic aperture radar (SAR) can image the Earth's surface and map damage in all weather conditions, day and night. However, current SAR damage mapping methods struggle to separate damage from other changes in the Earth's surface. In this study, we propose a novel approach to damage mapping, combining deep learning with the full time history of SAR observations of an impacted region in order to detect anomalous variations in the Earth's surface properties due to a natural disaster. We quantify Earth surface change using time series of interferometric SAR coherence, then use a recurrent neural network (RNN) as a probabilistic anomaly detector on these coherence time series. The RNN is first trained on pre-event coherence time series, and then forecasts a probability distribution of the coherence between pre- and post-event SAR images. The difference between the forecast and observed co-event coherence provides a measure of confidence in the identification of damage. The method allows the user to choose a damage detection threshold that is customized for each location, based on the local behavior of coherence through time before the event. We apply this method to calculate estimates of damage for three earthquakes using multiyear time series of Sentinel-1 SAR acquisitions. Our approach shows good agreement with observed damage and quantitative improvement compared to using pre- to co-event coherence loss as a damage proxy.\", 'title': 'Deep Learning-Based Damage Mapping With InSAR Coherence Time Series', 'embedding': []}, {'id': 15222, 'abstractText': 'Mapping of dangerous and flammable gas levels in outdoor environments, such as the oil pipeline and the industrial areas, is needed to monitor gas leaks. The use of sensor networks will require a large number of the sensors for a wide area. The mobile robot can be used as a sensor node that moves to map the desired area. In this study, we have developed a gas level mapping system for outdoor environment using solar-powered mobile robot. The proportional-integral-derivative control method is used by the robot to maneuver and follow the direction setpoint of provided Global Position System waypoints. The experimental results show that the use of solar panel can increase the duration of mobile robot operation. The results of gas concentration mapping by the mobile robot can be displayed online at the website integrated with Google Maps in both personal computer and smartphone.', 'title': 'Online Gas Mapping in Outdoor Environment using Solar-Powered Mobile Robot', 'embedding': []}, {'id': 15223, 'abstractText': \"Mapping and modeling the complex ecosystems and their changes over time are key issues in spatial ecology, biogeography, ecosystem ecology and biodiversity researches. This paper attempts to propose a simple, practical and automatic method to produce the ecosystem map in mountainous areas by fusing multi-source data and the related knowledge. The multi-source data included the 30m-resolution land cover map and the vegetation map of China (1:1 000 000). Three fusion strategies were contained in the proposed approach: hard matching, buffer matching and merged categories matching. Meanwhile, the related spatial distribution knowledge and the law of spatial distance decay were used to determine the optimal vegetation type, when more than two vegetation types are matched simultaneously. Taking the Southwestern China as study area, a new 30m-resolution ecosystem map with 144 ecosystem types was generated by the proposed method, which was used to establish the red list of ecosystems and evaluate the condition of biodiversity in the White Paper: China's Biodiversity.\", 'title': 'Ecosystem mapping in mountainous areas by fusing multi-source data and the related knowledge', 'embedding': []}, {'id': 15224, 'abstractText': 'In many robotic applications, especially for the autonomous driving, understanding the semantic information and the geometric structure of surroundings are both essential. Semantic 3D maps, as a carrier of the environmental knowledge, are then intensively studied for their abilities and applications. However, it is still challenging to produce a dense outdoor semantic map from a monocular image stream. Motivated by this target, in this paper, we propose a method for large-scale 3D reconstruction from consecutive monocular images. First, with the correlation of underlying information between depth and semantic prediction, a novel multi-task Convolutional Neural Network (CNN) is designed for joint prediction. Given a single image, the network learns low-level information with a shared encoder and separately predicts with decoders containing additional Atrous Spatial Pyramid Pooling (ASPP) layers and the residual connection which merits disparities and semantic mutually. To overcome the inconsistency of monocular depth prediction for reconstruction, post-processing steps with the superpixelization and the effective 3D representation approach are obtained to give the final semantic map. Experiments are compared with other methods on both semantic labeling and depth prediction. We also qualitatively demonstrate the map reconstructed from large-scale, difficult monocular image sequences to prove the effectiveness and superiority.', 'title': 'Monocular Outdoor Semantic Mapping with a Multi-task Network', 'embedding': []}, {'id': 15225, 'abstractText': 'The evaluation of electroencephalographic (EEG) signals is very crucial for human studies. Mostly there are animal recordings of EEG used for further research as well. One of the often used quantitative EEG methods for EEG analysis is topographical mapping. The barycentric interpolation method for 3D animal brain-mapping was implemented and tested. The 3D spline and spherical spline interpolation methods were used for comparison purpose. The surrogate data and real EEG recording were used for the interpolation methods testing. The Root Mean Square Error (RMSE) and significant probability mapping via Fisher non-parametric permutation test were used for evaluation. The RMSE of surrogate data was 0.070 for 3D spline, 0.594 for spherical spline and 0.321 for 3D barycentric interpolation method. The RMSE of the real EEG recordings nearby the electrodes was 0.1848 for 3D spline, 1.1027 for spherical spline and 0.0001 for 3D barycentric interpolation method. The 3D barycentric method differs from the 3D spline interpolation method in the large area on 0.001 alpha level based on significant probability mapping. The non-significant parts are only in the nearby electrode area. The 3D spline interpolation method gives the best results based on RMSE and significant probability testing.', 'title': '3D Barycentric Interpolation Method for Animal Brainmapping', 'embedding': []}, {'id': 15226, 'abstractText': \"In this study, we evaluated the performance of AR-assisted navigation in a real underground mine with good and limited illumination conditions as well as without the illumination considering possible search and rescue conditions. For this purpose, we utilized the Lumin SDK's embedded spatial mapping algorithm for mapping and navigating. We used the spatial mapping algorithm to create the mesh model of the escape route and to render it with the user input into the Magic Leap One. Then we compared the spatial mapping algorithm in three different scenarios for the evacuation of an underground mine in an emergency situation. The escape route has two junctions and 30 meters (100 feet). The baseline scenarios are (i) evacuation of the mine in a fully illuminated condition, (ii) evacuation with the headlamp and (iii) without any illumination. In the first scenario (fully illuminated route with the rendered meshes) the evacuation took 40 seconds. In the second scenario (illumination with the headlamp), the evacuation took 44 seconds. For the last scenario (no light source and hence in total darkness) the evacuation took 54 seconds. We found that AR-assisted navigation is effective for supporting search and rescue efforts in high attrition conditions of underground space.\", 'title': 'An Evaluation of AR-Assisted Navigation for Search and Rescue in Underground Spaces', 'embedding': []}, {'id': 15227, 'abstractText': 'An accurate global map of current cropland is needed to tackle the agricultural challenges of the next decades. Several remote sensing products have provided with global cropland maps. To create the GEOGLAM best available global crop-specific maps, we integrated the SPAM 2005 with regional maps developed by different countries and international organisms. The comparison with other available global cropland products showed a clear linear agreement between products and a tendency towards overestimation of GEOGLAM best available global crop-specific maps. In addition, a sensitivity analysis was performed using the forecast production model for winter wheat developed in previous studies. It showed how at constant values of adjusted NDVI small variations of cultivated area (5000 ha) produced a 17% rate of variation in the estimated production values.', 'title': 'Geoglam Best Available Crop-Specific Global Maps: Strengths and Limitations', 'embedding': []}, {'id': 15228, 'abstractText': 'PET imaging of small animals is often used for assessing biodistribution of a novel radioligand and pharmacology in small animal models of disease. PET acquisition and processing settings may affect reference region or image-derived input function (IDIF) kinetic modeling estimates. We examined four different factors in comparing quantitative results: 1) effect of reconstruction algorithm, 2) number of MAP iterations, 3) strength of the MAP prior, and 4) Attenuation and scatter. The effect of these parameters has not been explored for small-animal reference region and IDIF kinetic modeling approaches. Dynamic PET/CT scans were performed in 3 species with 3 different tracers: house sparrows with 11Craclopride, rats with 18FAS2471907 (11bHSD1) and mice with 11CUCB-J (SV2A). FBP yielded lower kinetic modeling estimates compared to 3D-OSEM-MAP reconstructions, in sparrow and rat studies. Target resolutions (MAP prior strength) of 1.5 and 3.0mm demonstrated reduced VT in rats but only 3.0mm reduced BPND in sparrows. Therefore, use of the highest target resolution (0.8mm) is warranted. We demonstrated using kinetic modeling that forgoing CT-based attenuation and scatter correction may be appropriate to improve animal throughput when using short-lived radioisotopes in sparrows and mice. This work provides recommendations and a framework for future optimization of kinetic modeling for preclinical PET methodology with novel radioligands.', 'title': 'Optimized Methodology for Reference Region and Image-Derived Input Function Kinetic Modeling in Preclinical PET', 'embedding': []}, {'id': 15229, 'abstractText': 'A data encryption model based on logistic map is studied and multiple chaotic dynamical systems were implemented to improve by addressing the issues related to use of logistic map. Use of different chaotic systems other than logistic map, provides us to vary control parameters to a larger extent, hence enabling to overcome the limitations of logistic map without compromising security of data. Further, a large range of control parameters were provided to make system more key dependent. The measure of degree of complexity in the cipher was examined for different chaotic dynamical systems. The proposed method eliminates the issues related to distribution of randomly generated data by using Lozi, Henon, Tent and Bernoulli maps for given values of control parameters.', 'title': 'Improved cryptographic model for better information security', 'embedding': []}, {'id': 15230, 'abstractText': 'While accuracy requirement for simulation-based efficiency map becomes high for traction motors which have high efficiency over wide drive range and where even a small improvement in efficiency is critical, their accuracy level has not been well studied. To evaluate the accuracy of Finite Element Analysis (FEA) based efficiency map, maps were generated with different loss calculation methods for an IPM machine. The simulation results were compared with a measurement-based efficiency map. The comparison indicated that FEA can reproduce an efficiency map with less than 1% error when losses were calculated taking into account high fidelity factors such as minor hysteresis loops, AC loss, manufacturing degradations, and stray losses. Furthermore, the sensitivities of those factors on the error were evaluated.', 'title': 'An Accuracy Study of Finite Element Analysis-based Efficiency Map for Traction Interior Permanent Magnet Machines', 'embedding': []}, {'id': 15231, 'abstractText': 'Electrocardiographic imaging has been shown to provide useful information for pre-procedure planning of catheter-ablation procedures. The methodology involves reconstruction of unipolar electrograms (EGMs) and isochronal maps on the epicardial surface from non-invasively acquired body-surface potentials. We have developed an algorithm for evaluating global myocardial activation times. First, the cross-correlation method determines the delay in local activation times among pairs of neighboring nodes. Next, a sparse linear system is constructed from known activation delays of neighboring nodes. To solve this system, we use a sparse Bayesian learning method to calculate the global myocardial activation times. The aim of this study was to assess the proposed method in both structurally normal and scarred ventricular myocardium. Isochronal maps of calculated activation times were compared with local activation times (LATs) derived from directly-measured epicardial EGMs obtained by electroanatomic contact mapping, for pacing delivered by an implantable cardioverter defibrillator (ICD) at the endocardial right-ventricular (RV) apex, and for catheter pacing at RV epicardial site. We found that even in the presence of infarct scar, isochronal maps calculated by the proposed method correlated closely with known LATs exported from an electroanatomic mapping system.', 'title': 'An Algorithm for Imaging Isochrones of Ventricular Activation on Patient-Specific Epicardial Surface', 'embedding': []}, {'id': 15232, 'abstractText': 'Localization with laser range finder (LRF) needs an environment map. In some environments, the localization using the environment map prepared in advance often fails. In this study, we achieve localization using both the environment map and the feature of supplemented objects as the estimated map. Moreover, to suppress the influence of the observation noises of LRF, we estimate the feature of supplemented objects using the extended Kalman filter (EKF). In this paper, we verify the influence of the estimated map on estimation accuracy through the simulation and verify the effectiveness of this localization method through the experiment conducted in a gymnasium where is larger than the scanning range of LRF.', 'title': 'Localization with LRF using both environment map and feature of supplemented objects', 'embedding': []}, {'id': 15233, 'abstractText': 'A continuous map <tex>$\\\\mathbb {C}^d\\\\longrightarrow \\\\mathbb {C}^N$</tex> is a complex <tex>$k$</tex>-regular embedding if any <tex>$k$</tex> pairwise distinct points in <tex>$\\\\mathbb {C}^d$</tex> are mapped by <tex>$f$</tex> into <tex>$k$</tex> complex linearly independent vectors in <tex>$\\\\mathbb {C}^N$</tex>. The existence of such maps is closely connected with classical problems of algebraic/differential topology, such as embedding/immersion problems. Our central result on complex <tex>$k$</tex>-regular embeddings extends results of Cohen &amp; Handel (1978), Chisholm (1979) and Blagojević, Lück &amp; Ziegler (2013) on real <tex>$k$</tex>-regular embeddings. We give the following lower bounds for the existence of complex <tex>$k$</tex>-regular embeddings. Let <tex>$p$</tex> be an odd prime, <tex>$k\\\\geq 1$</tex> and <tex>$d=p^t$</tex> for <tex>$t\\\\geq 1$</tex>. If there exists a complex <tex>$k$</tex>-regular embedding <tex>$\\\\mathbb {C}^d\\\\longrightarrow \\\\mathbb {C}^N$</tex>, then <tex>$ d(k-\\\\alpha _p(k))+\\\\alpha _p(k)\\\\leq N$</tex>. Here <tex>$\\\\alpha _p(k)$</tex> denotes the sum of coefficients in the <tex>$p$</tex>-adic expansion of <tex>$k$</tex>. These lower bounds are obtained by modifying the framework of Cohen &amp; Handel (1978) and a study of Chern classes of complex regular representations. As a main technical result we establish for this an extended Vassiliev conjecture, the following upper bound for the height of the cohomology of an unordered configuration space: If <tex>$d\\\\geq 2$</tex> and <tex>$k\\\\geq 2$</tex> are integers, and <tex>$p$</tex> is an odd prime. Then <tex>$$\\\\mathfrak{h}\\\\left(H^*\\\\left(F\\\\left(\\\\mathbb{R}^d,k\\\\right)/\\\\mathfrak{S}_k;\\\\mathbb{F}_p\\\\right)\\\\right)\\\\leq\\\\min\\\\left\\\\{p^t: 2p^t\\\\geq d \\\\right\\\\}.$$</tex> Furthermore, we give similar lower bounds for the existence of complex <tex>$\\\\ell $</tex>-skew embeddings <tex>$\\\\mathbb {C}^d\\\\longrightarrow \\\\mathbb {C}^N$</tex>, for which we require that the images of the tangent spaces at any <tex>$\\\\ell $</tex> distinct points are skew complex affine subspaces of <tex>$\\\\mathbb {C}^N$</tex>. In addition we give improved lower bounds for the Lusternik–Schnirelmann category of <tex>$F(\\\\mathbb {C}^d,k)/\\\\mathfrak {S}_k$</tex> as well as for the sectional category of the covering <tex>$F(\\\\mathbb {C}^d,k)\\\\longrightarrow F(\\\\mathbb {C}^d,k)/\\\\mathfrak {S}_k$</tex>.', 'title': 'On Complex Highly Regular Embeddings and the Extended Vassiliev Conjecture', 'embedding': []}, {'id': 15234, 'abstractText': 'In the present study, we develop a contactless optical characterization tool that quantifies and maps the trapping defects density within a thin film photovoltaic device. This is achieved by probing time-resolved photoluminescence and numerically reconstructing the experimental decays under several excitation conditions. The values of defects density in different Cu(In,Ga)Se<sub>2</sub> solar cells were extracted and linked to photovoltaic performances such as the open-circuit voltage. In the second part of the work, the authors established a micrometric map of the trapping defects density. This revealed areas within the thin film CIGS solar cell with low photovoltaic performance and high trapping defects density. The final part of the work was dedicated to finding the origin of the spatial fluctuations of the thin film transport properties. To do so, we started by establishing a micrometric map of the absolute quasi-Fermi levels splitting within the same CIGS solar cell, using the hyperspectral imager. A correlation is obtained between the map of quasi-Fermi levels splitting of and the map of the trapping defects density. The latter is found to be the origin of the frequently observed spatial fluctuations of thin film materials properties.', 'title': 'Micrometric mapping of absolute trapping defects density using quantitative luminescence imaging', 'embedding': []}, {'id': 15235, 'abstractText': \"Face recognition, although being a popular area of research and study, still has many challenges, and with the appearance of the Microsoft Kinect device, new possibilities of research were uncovered, one of which is face recognition using the Kinect. With the goal of enhancing face recognition, this paper is aiming to prove how depth maps, since not effected by illumination, can improve face recognition with a benchmark algorithm based on the Eigenface. This required some experiments to be carried out, mainly in order to check if algorithms created to recognize faces using normal images can be as effective if not more effective with depth map images. The OpenCV Eigenface algorithm implementation was used for the purpose of training and testing both normal and depth-map images. Finally, results of the experiments are presented to prove the ability of the tested algorithm to function with depth maps, also, proving the capability of depth maps face recognition's task in poor illumination.\", 'title': 'Novel approach to enhance face recognition using depth maps', 'embedding': []}, {'id': 15236, 'abstractText': 'In semiconductor foundries, wafer map defect analysis is crucial to prevent yield excursion. However, traditional manual inspection can hardly meet the high throughput demand. Deep learning based automatic defect detection shows promising efforts to achieve high accuracy and efficiency, yet the current approaches’ performance is limited by the imbalanced dataset and lack of interpretability. In this paper, we propose a Variational Autoencoder Enhanced Deep Learning Model (VAEDLM) for wafer defect imbalanced classification. It is light-weighted and effective in wafer defect pattern recognition on imbalanced dataset. It used variational autoencoder and decoder to generate similar wafer defect maps and a refined deep convolutional neural network for feature learning. We demonstrate the method using an authentic wafer map dataset, WM-811K. The performance is not only significantly improved after data augmentation, but it also beats the state-of-the art methods, reaching 99.19% accuracy, 99.10% recall, 99.23% precision, 99.96% AUC and 99.16% for F1-score. It clearly demonstrates the method’s efficacy to deal with the imbalanced defect pattern. Our study using saliency map and t-SNE further leads to enhanced interpretability.', 'title': 'A Variational Autoencoder Enhanced Deep Learning Model for Wafer Defect Imbalanced Classification', 'embedding': []}, {'id': 15237, 'abstractText': 'ASTER measures the spectral radiation from the Earth in thermal infrared (TIR; 7-14 μm) region at five bands where the major terrestrial minerals exhibit distinct spectral properties. The mineralo-lithological indices for ASTER-TIR proposed by the author have been utilized for regional mapping in detecting silica, carbonate, sulfate and mafic-ultramafic minerals as well as delineating silicate rocks. On the other hand, the global emissivity dataset (GED) are developed and recently supplied to the public. In this study, the global map generated with the indices for the GED is shown. It is compared with the regional map covering a part of Tibetan Plateau produced with the procedures developed by the author. The result indicates the global map revealing the rough trend of lithology and mineralogy on Earth, however, the quality is not sufficient. The combined use of the regional and global maps are suggested for efficient analysis of global mineralo-lithology.', 'title': 'Global Mapping of Mineralo-Lithological Indices Derived With Aster Multispectral Thermal Infrared Data', 'embedding': []}, {'id': 15238, 'abstractText': 'Land cover multiresolution mapping of remote sensing images contributes greatly to land-use management, environmental protection, and city planning. In traditional mapping of this type, the representation of different land-use types depends on the image resolution, and the geometric, topologic, and semantic characteristics are not considered. This approach can cause a loss of useful information and the redundancy of useless information. In this study, we propose a superpixel-based land cover (multiresolution representation SULR) method for remote sensing images that employs multifeature fusion. In this process, we first define three basic superpixel operations, collapse, connection, and cutting, as the basic operators of multiresolution land cover mapping. Then, the topological adjacent land parcels are combined through the amalgamation of polygons with heterogeneous properties and aggregation of polygons with homogeneous properties based on the three proposed superpixel operators. Finally, the geometric boundaries of parcels are simplified by combining the superpixel collapse operator and image thinning technologies. Compared with traditional image scale transformation methods, the proposed method can more effectively achieve multiresolution mapping of land cover from remote sensing images by considering the geometric, topologic, and semantic characteristics of land parcels.', 'title': 'Multiresolution Mapping of Land Cover From Remote Sensing Images by Geometric Generalization', 'embedding': []}, {'id': 15239, 'abstractText': 'Data usually resides on a manifold, and the minimal dimension of such a manifold is called its intrinsic dimension. This fundamental data property is not considered in the generative adversarial network (GAN) model along with its its variants; such that original data and generated data often hold different intrinsic dimensions. The different intrinsic dimensions of both generated and original data may cause generated data distribution to not match original data distribution completely, and it certainly will hurt the quality of generated data. In this study, we first show that GAN is often unable to generate simulation data, holding the same intrinsic dimension as the original data with both theoretical analysis and experimental illustration. Next, we propose a new model, called Hausdorff GAN, which removes the issue of different intrinsic dimensions and introduces the Hausdorff metric into GAN training to generate higher quality data. This provides new insights into the success of Hausdorff GAN. Specifically, we utilize a mapping function to map both original and generated data into the same manifold. We then calculate the Hausdorff distance to measure the difference between the mapped original data and the mapped generated data, toward pushing generated data to the side of original data. Finally, we conduct extensive experiments (using MNIST, CIFAR10, and CelebA datasets) to demonstrate the significant performance improvement of the Hausdorff GAN in achieving the largest Inception Score and the smallest Frechet inception distance (FID) score as well as producing diverse generated data at different resolutions.', 'title': 'Hausdorff GAN: Improving GAN Generation Quality With Hausdorff Metric', 'embedding': []}, {'id': 15240, 'abstractText': 'The Hydrogen Intensity and Real-time Analysis eXperiment (HIRAX [1], Table 1) is an intensity mapping project to be co-located with the Square Kilometer Array in South Africa. By making use of an 1024 element interferometer of low-cost 6m dishes arranged in a compact grid, HIRAX will map the low-frequency southern sky from 400-800 MHz, probing neutral hydrogen emission over the redshift range of 0.8-2.5. The principle goal of this survey is to accurately measure the observed baryon acoustic oscillation feature imprinted on large-scale structure through HI intensity mapping (See e.g. [2]). As this epoch spans the onset of the transition from matter-dominated to dark energy-dominated expansion, HIRAX will provide highly competitive constraints on cosmological parameters, particularly the equation of state of dark energy. Forecasts of these contraints using the methodology of [3] are shown in Figure 1. The final survey will map 15,000 square degrees of the southern sky, overlapping contemporary and forthcoming surveys such as those from the Large Synoptic Survey Telescope and the Dark Energy Survey, as well as ground based Cosmic Microwave Background surveys. HIRAX will therefore enable a wide range of HI cross-correlation studies with external large-scale structure probes. Additionally, HIRAX will be a powerful instrument for detecting and monitoring radio transients such as Fast Radio Bursts and pulsars.', 'title': 'The Hydrogen Intensity and Real-Time Analysis Experiment', 'embedding': []}, {'id': 15241, 'abstractText': 'High spatial resolution maps of relative water level changes in wetlands environment have been successfully generated using spaceborne interferometric synthetic aperture radar (InSAR) techniques. However, the wetland InSAR application has limited hydrological monitoring application, because it estimates water level changes not absolute water levels, which are used by hydrologists. TanDEM-X bistatic observations provide simultaneous phase measurements of water surfaces with a two-satellite constellation without temporal decorrelation. In this study, the TanDEM-X bistatic science phase observations with very large baseline (&gt; 1.3 km) geometric configuration were evaluated to extract absolute water levels of the Everglades wetland in south Florida, U.S.A. Thanks to the large perpendicular baseline, spatial variation of water level surfaces with extremely low slope were estimated. We processed two datasets of TanDEM-X bistatic observations acquired on August 26 and 31, 2015. The perpendicular baselines are 1.43 km and 1.36 km and the ambiguity heights were calculated as 3.61 m and 3.90 m in each interferometric pair. The estimated absolute water level maps with 3.6 m and 7.4 m pixel spacing in range and azimuth directions (multilook factor of 4), respectively, show vast detailed variation of the water surfaces for each acquisition date. Hourly water level measurements obtained by stage stations, which are provided by the Everglades Depth Estimation Network (EDEN), were used for verifying the estimated absolute water levels. Some of stage stations, which are located in low interferometric coherence areas, such as dense vegetated and tree areas, were considered as outliers and were excluded from the comparison. The verification results show very good agreements (code of determination &gt; 0.95) between the TanDEM-X derived absolute water levels and the stage station measurements. The root mean square error (RMSE) between the TanDEM-X results and stage records for the two datasets were 0.77 m and 0.66 m. Although, TanDEM-X bistatic observations have no temporal baseline, there are severe volume decorrelations over various tree types due to the very large perpendicular baseline. The TanDEM-L mission with longer wavelength of radar signal will enable us to generate more coherent interferometric phase observations over wetlands and, consequently, generate improved absolute water level maps.', 'title': 'Bistatic Science Phase Observations with a Large Perpendicular Baseline', 'embedding': []}, {'id': 15242, 'abstractText': \"As the scientific payload of the Chang 'e-4 lunar rover, the panoramic camera undertakes the task of exploring the lunar geology. It can be used for mapping and terrain reconstruction, directly serving the navigation planning of the rover. The lunar remote sensing mapping data, the lunar Digital elevation model (DEM), and Digital orthophoto map (DOM), the mapping of the area near the Chang 'e-4 landing site are studied, and the development direction of the future lunar fine-mapping is discussed.\", 'title': \"Mapping Research of the Area Near the Chang 'e-4 Landing Site\", 'embedding': []}, {'id': 15243, 'abstractText': 'Information security plays an important role in modern technologies. Stream encryption is one of the common tools used for secure communications. The stream encryption algorithms require sequence with pseudo-random properties. The chaotic maps as a source of pseudo-random numbers with desired statistical properties is widely studied in the last decades. One of the known problems of the implementation of the chaos-based pseudo-random number generators implemented with low-precision arithmetic is a short period length. A possible solution of the short-cycle problem is a perturbation of the orbit or the nonlinearity parameter of the chaotic map for breaking out the cycle. In the present article, we propose a new approach for increasing the cycle length by changing the symmetry coefficient of the adaptive Chirikov map. We switch two values of the symmetry coefficient according to the output of the linear feedback shift register. We calculate the estimations of the period length for the perturbed and original Chirikov maps and confirm the efficiency of the proposed approach. Properties verification for the output sequences of generators based on the adaptive Cirikov map is carried out using correlation analysis methods and the NIST statistical test suite. The obtained results can be used in cryptography applications as well as in secure communication systems design.', 'title': 'Adaptive Chirikov Map for Pseudo-random Number Generation in Chaos-based Stream Encryption', 'embedding': []}, {'id': 15244, 'abstractText': 'Tensor-based methods have been widely studied to attack inverse problems in hyperspectral imaging since a hyperspectral image (HSI) cube can be naturally represented as a third-order tensor, which can perfectly retain the spatial information in the image. In this article, we extend the linear tensor method to the nonlinear tensor method and propose a nonlinear low-rank tensor unmixing algorithm to solve the generalized bilinear model (GBM). Specifically, the linear and nonlinear parts of the GBM can both be expressed as tensors. Furthermore, the low-rank structures of abundance maps and nonlinear interaction abundance maps are exploited by minimizing their nuclear norm, thus taking full advantage of the high spatial correlation in HSIs. Synthetic and real-data experiments show that the low rank of abundance maps and nonlinear interaction abundance maps exploited in our method can improve the performance of the nonlinear unmixing. A MATLAB demo of this work will be available at https://github.com/LinaZhuang for the sake of reproducibility.', 'title': 'Using Low-Rank Representation of Abundance Maps and Nonnegative Tensor Factorization for Hyperspectral Nonlinear Unmixing', 'embedding': []}, {'id': 15245, 'abstractText': 'Biometric recognition has been proven to offer a reliable solution to authenticate the identity of the legitimate user. Because of the inherent nature of biometrics, which involves the practice of digitally scanning the physiological or behavioral characteristics of individuals as a means of identification, the biometric trait employed is associated permanently with an individual and cannot be modified. This paper proposes a multimodal biometric template protection algorithm for the security of stored biometric templates in a database. The proposed algorithm is based on watermarking, shuffling process, Hadamard matrix and chaotic map. The security of the algorithm depends on the secrecy of the keys and the chaotic map. Watermarking based on Discrete Wavelet Transform–Singular Value Decomposition has been applied to fuse the features of face and fingerprint in order to improve the security of biometric data. To satisfy the requirements of a biometric template protection algorithm, each genuine user is assigned a unique key for the shuffling process and a unique code is generated using Hadamard matrix. Hadamard matrix is used to provide orthogonality and chaotic map for randomization. The identification process is carried out using Hamming distance. The experimental results give a good performance in terms of achieving a minimum value of FRR and FAR in comparison with previous studies.', 'title': 'Hybrid Multi-Biometric Template Protection Using Watermarking', 'embedding': []}, {'id': 15246, 'abstractText': 'Body surface potential mapping (BSPM) can be an important tool in ablation therapy planning. Results obtained with high resolution (HR) computer models must be translated to realistic numbers of leads. This study aims to evaluate the impact on atrial tachycardia (AT), flutter (AFL) and fibrillation (AF) characterization by reducing the number of BSPM leads. 19 realistic computer simulations with 567 leads (HR) have been used to characterize the arrhythmias concerning the dominant frequencies (DF) and phase singularity point (SP) distributions. DF maps were generated combining Welch periodogram and activation detection with wavelet transform modulus maxima. Phase was calculated with Hilbert transform on signals filtered around the highest DF (±1Hz); dynamics of SPs were analyzed using histograms (heatmaps, HMs) and connecting SPs along time (filaments). The analyses were reproduced for 6 layouts with 252 to 16 leads and results were compared using the structural similarity index (SSIM), sensitivity and precision in SP detection and analyzing features extracted from the maps. SSIM was lower in AF than in AFL or AT for DF maps and HMs, but was in average above 0.6 for layouts with 32 leads or more. In HMs, a loss in spatial resolution with fewer leads is reflected in decreasing values for sensitivity and precision. Features from DF maps, filaments or HMs were statistically equivalent in all layouts.', 'title': 'Effect of Reducing the Number of Leads from Body Surface Potential Mapping in Computer Models of Atrial Arrhythmias', 'embedding': []}, {'id': 15247, 'abstractText': 'Dual respirator!y-cardiac gating (DG) method was shown to produce motion-freeze myocardial perfusion SPECT images. However, appropriate attenuation correction (AC) for DG SPECT is yet to be determined. This study aims to evaluate the performance of various attenuation maps for DG cardiac SPECT. We used the 4D Extended Cardiac Torso (XCAT) phantom to simulate a male patient with respiratory cycle of 5 s and respiratory -motion amplitude of 2 cm along the z-axis. The respiratory cycle was divided into 1152 frames which were grouped to 6 respiratory and 8 cardiac phases, i.e., a total of 48 phases. The corresponding attenuation maps were grouped to represent different AC maps: DG CT (DCT), respiratory gated CT (RCT), average CT (ACT), interpolated CT (ICT), HCTs at endinspiration (HCT-in), end-expiration (HCT-ex) and midrespiration (HCT-mid) respectively. The ICT was obtained by interpolation based on the motion vector generated between HCTin and HCT-ex from affine+b-spline image registration. We used an analytical projector to simulate a LEHR collimator with 120 noise-free projections over 180°, which were later reconstructed by OS-EM method using different AC maps respectively. For each cardiac phase, reconstructed images from different respiratory phases were registered to end-expiration and summed to get a registered cardiac image. Polar plots were generated for 8 registered cardiac images and relative difference (RD) was computed for each segment for the 17-segment analysis. For all cardiac phases, the average RD<sub>max</sub> for AC with ACT, HCT-in, HCT-ex, HCT-mid, RCT and ICT comparing to DCT were 4.09%, 9.24%, 5.04%, 4.02%, 3.22% and 2.85% respectively. Since DCT and RCT are clinically challenging due to the high radiation and implementation complexity, ICT is recommended for AC in DG cardiac SPECT, followed by HCT-mid or HCT-ex, with good accuracy and relative low radiation dose.', 'title': 'Attenuation Correction Methods for Dual Gated Myocardial Perfusion SPECT/CT', 'embedding': []}, {'id': 15248, 'abstractText': 'In this study, we explore a direction-of-arrival (DoA) estimation approach using the steered response power with phase transform (SRP-PHAT) and DeepSphere, a graph-based spherical convolutional neural network (CNN) suitable for spherical topology. The SRP-PHAT maps were adjusted to the Hierarchical Equal Area isoLatitude Pixelization (HEALPix) algorithm. We performed simulations for an Eigenmike spherical microphone array and different resolutions of the SRP-PHAT maps. Results show an improvement for the lower resolution maps, as the mean angular error for the Spherical CNN-derived maps was reduced by about half when compared with the original maps.', 'title': 'DOA Estimation for Spherical Microphone Array using Spherical Convolutional Neural Networks', 'embedding': []}, {'id': 15249, 'abstractText': 'Space mapping technology has been one of the first and most widely used physics-based surrogate-assisted approaches to rapid design optimization of expensive EM-simulation models in microwave engineering. When used with care and experience, it offers computational efficiency that is unmatched by conventional numerical optimization techniques. Numerous variations of space mapping have been proposed over the last two decades and a large number of design case studies have been demonstrated. Yet, limited progress has been observed so far in terms of its full automation. This includes ensuring global convergence, immunity to coarse model inaccuracy, as well as robustness with respect to the surrogate model setup. This paper discusses a few open problems pertaining to space mapping, reviews available theoretical results, provides some generic recommendations for successful usage of space mapping in microwave design, and briefly mentions various surrogate-assisted methodologies that stem from or have been inspired by space mapping.', 'title': 'Space mapping: Performance, reliability, open problems and perspectives', 'embedding': []}, {'id': 15250, 'abstractText': 'In recent years, connected and automated vehicles (CAVs) have attracted considerable attention because they can improve driver convenience and safety using vehicle to everything (V2X) communication. CAV system must satisfy safety and performance requirements, and they are required to go through rigorous evaluation processes. In this study, we design an integrated simulator to evaluate a CAV system, cooperative eco-driving system, which models vehicular ad hoc network (VANET) topology, driver models, vehicle models, and vehicle control algorithms. This is done by integrating three simulators: network simulator, traffic simulator, and driving simulator. Cooperative eco-driving system requires precise and accurate map data including real-time traffic conditions and surrounding environmental information. Therefore, we construct a local map system based on V2X communication to provide a host vehicle (HV) with surrounding traffic and environment information. A local map system generates a local map by utilizing surrounding traffic information of HV acquired from each simulator and shares it with the simulators.', 'title': 'Integrated Simulator for Evaluating Cooperative Eco-driving System', 'embedding': []}, {'id': 15251, 'abstractText': 'We present a volumetric mesh-based algorithm for parameterizing the placenta to a flattened template to enable effective visualization of local anatomy and function. MRI shows potential as a research tool as it provides signals directly related to placental function. However, due to the curved and highly variable in vivo shape of the placenta, interpreting and visualizing these images is difficult. We address interpretation challenges by mapping the placenta so that it resembles the familiar ex vivo shape. We formulate the parameterization as an optimization problem for mapping the placental shape represented by a volumetric mesh to a flattened template. We employ the symmetric Dirichlet energy to control local distortion throughout the volume. Local injectivity in the mapping is enforced by a constrained line search during the gradient descent optimization. We validate our method using a research study of 111 placental shapes extracted from BOLD MRI images. Our mapping achieves sub-voxel accuracy in matching the template while maintaining low distortion throughout the volume. We demonstrate how the resulting flattening of the placenta improves visualization of anatomy and function. Our code is freely available at https://github.com/ mabulnaga/placenta-flattening.', 'title': 'Volumetric Parameterization of the Placenta to a Flattened Template', 'embedding': []}, {'id': 15252, 'abstractText': 'Dynamic object detection, state estimation, and map-building are crucial for autonomous robot systems and intelligent transportation applications in urban scenarios. Most current LiDAR Simultaneous Localization and Mapping (SLAM) systems operate on the assumption that the observed environment is static. However, the overall accuracy and robustness of a SLAM system can be compromised by dynamic objects in the environment. Aiming at the problem of inaccurate odometry estimation and wrong mapping caused by the existing LiDAR SLAM method which cannot detect the dynamic objects, we study the SLAM problem of robots and unmanned vehicles equipped with LiDAR traveling in the dynamic urban scenes. We propose a fast LiDAR-only model-free dynamic objects detection method, which uses the spatial and temporal information of point cloud through a convolutional neural network (CNN), and the detection accuracy is improved by 35 use spatial information. We further integrate it into a state-of-the-art LiDAR SLAM framework to improve the SLAM performance. Firstly, the range image constructed by LiDAR point cloud is used for ground extraction and non-ground point clustering. Then, the motion of objects in the scene is estimated by the difference between adjacent frames, and the segmented objects are further divided into dynamic objects and static objects by their motion features. After that, the stable feature points are extracted from the static objects. Finally, the pose transformation of adjacent frames is solved by matching feature point pairs. We evaluated the accuracy and robustness of our system on datasets with different challenging dynamic environments, and the results show our system has significant improvements in accuracy and robustness of odometry and mapping, while still maintain real-time performance, which is sufficient for autonomous robot systems and intelligent transportation applications in urban scenarios.', 'title': 'DLOAM: Real-time and Robust LiDAR SLAM System Based on CNN in Dynamic Urban Environments', 'embedding': []}, {'id': 15253, 'abstractText': 'Generating and providing the next generation of geological maps for a world needing resources is an ongoing activity. New sensor technologies, improvements in processing algorithms and the means for large scale spatial data distribution are required. The extensive ASTER image archive provides one avenue for providing another geoscience data base source to supplement and help update past published geological mapping. Examples of using ASTER imagery to provide province to continental scale compositional mapping, have been demonstrated in Australia and elsewhere. This study introduces and examines the possibility of applying this approach in East Africa across difficult terrain and boundaries where traditional techniques have been applied by different mapping agencies. The preliminary results show the potential for ASTER mapping but further processing and on site field validation is needed.', 'title': 'Supplementing Geological Mapping with Aster in East Africa', 'embedding': []}, {'id': 15254, 'abstractText': 'In this work, a simple yet effective deep neural network is proposed to generate the dense depth map of the scene by exploiting both LiDAR sparse point cloud and the monocular camera image. Specifically, a feature pyramid network is firstly employed to extract feature maps from images across time. Then the relative pose is calculated by minimizing the feature distance between aligned pixels from inter-frame feature maps. Finally, the feature maps and the relative pose are further applied to compute the feature-metric loss for training the depth completion network. The key novelty of this work lies in that a self-supervised mechanism is presented to train the depth completion network by directly using visual-LiDAR odometry between consecutive frames. Comprehensive experiments and ablation studies on benchmark dataset KITTI demonstrate the superior performance over other state-of-the-art methods in terms of pose estimation and depth completion. The detailed performance of the proposed approach (referred to as SelfCompDVLO) can be found on the KITTI depth completion benchmark. The source code, models, and data have been made available at GitHub.', 'title': 'Self-Supervised Depth Completion From Direct Visual-LiDAR Odometry in Autonomous Driving', 'embedding': []}, {'id': 15255, 'abstractText': 'Onomatopoeias can simply describe sounds or state of things. Therefore they are often used in Japanese daily conversation. On the other hand, one of the problems, which Japanese learners face, is the lack of ways to learn Japanese onomatopoeias. This study proposes a thesaurus map which can describe semantic relationship among onomatopoeias. Our proposed method can transform the onomatopoeia into a 2D-vector and assign it on the map. In our experiment, we examined whether the map can describe the semantic relationship as local distance on the map. The experiment utilized onomatopoeia samples which represent \"human motion\" to evaluate the map.', 'title': 'Visualized Onomatopoeia Thesaurus Maps Based on Deep Autoencoder', 'embedding': []}, {'id': 15256, 'abstractText': 'To deploy a transmitter to correct place is so important for more qualified wireless communication systems. In order deploy the transmitter to correct place, it is required to calculate the electric field strength emanates from base station and generate coverage map. Electric field strength is calculated by Uniform Theory of Diffraction (UTD) model. mxn number coloured coverage maps (red-blue) are generated by changing of the base station place. The best coverage map is selected among all maps by signal processing techniques. In this study, optimum place having the best coverage map is determined for 3x3 scenario.', 'title': 'Determination of Optimum Base Station Location with Signal Processing Techniques', 'embedding': []}, {'id': 15257, 'abstractText': 'The automatic digitizing of paper maps is a significant and challenging task for both academia and industry. As an important procedure of map digitizing, the semantic segmentation section is mainly relied on manual visual interpretation with low efficiency. In this study, we select urban planning maps as a representative sample and investigate the feasibility of utilizing U-shape fully convolutional based architecture to perform end-to-end map semantic segmentation. The experimental results obtained from the test area in Shibuya district, Tokyo, demonstrate that our proposed method could achieve a very high Jaccard similarity coefficient of 93.63% and an overall accuracy of 99.36%. For implementation on GPGPU and cuDNN, the required processing time for the whole Shibuya district can be less than three minutes. The results indicate the proposed method can serve as a viable tool for urban planning map semantic segmentation task with high accuracy and efficiency.', 'title': 'Semantic Segmentation for Urban Planning Maps Based on U-Net', 'embedding': []}, {'id': 15258, 'abstractText': 'Vision-based intelligent systems such as driver assistance systems and transportation systems should take into account weather conditions. The presence of haze in images can be a critical threat to driving scenarios. Haze density measures the visibility and usability of hazy images captured in real-world conditions. The prediction of haze density can be valuable in various vision-based intelligent systems, especially in those systems deployed in outdoor environments. Haze density prediction is a challenging task since the haze and many scene contents have a lot in common in appearance. Existing methods generally utilize different priors and design complex handcrafted features to predict the visibility or haze density of the image. In this article, we propose a novel end-to-end convolutional neural network (CNN) based method to predict haze density, named as HazDesNet. Our HazDesNet takes a hazy image as input and predicts a pixel-level haze density map. The density map is then refined and smoothed, and the average of the refined map is calculated as the global haze density of the image. To verify the performance of HazDesNet, a subjective human study is performed to build a Human Perceptual Haze Density (HPHD) database, which includes 500 real-world hazy images and 100 synthetic hazy images, and the corresponding human-rated perceptual haze density scores. Experimental results show that our method achieves the best haze density prediction performance on our built HPHD database and existing databases. Besides the global quantitative results, our HazDesNet is capable of predicting a continuous, stable, fine, and high-resolution haze density map. We will make the database and code publicly available at https://github.com/JiaheZhang/HazDesNet.', 'title': 'HazDesNet: An End-to-End Network for Haze Density Prediction', 'embedding': []}, {'id': 15259, 'abstractText': 'Because <sup>222</sup>Rn is a progeny of <sup>238</sup>U, the relative abundance of uranium may be used to predict the areas that have the potential for high indoor radon concentration and therefore determine the best areas to conduct future surveys. Geographic Information System (GIS) mapping software was used to construct maps of South Dakota that included levels of uranium concentrations in soil and stream water and uranium deposits. Maps of existing populations and the types of land were also generated. Existing data about average indoor radon levels by county taken from a databank were included for consideration. Although the soil and stream data and existing recorded average indoor radon levels were sparse, it was determined that the most likely locations of elevated indoor radon would be in the northwest and southwest corners of the state. Indoor radon levels were only available for 9 out of 66 counties in South Dakota. This sparcity of data precluded a study of correlation of radon to geological features, but further motivates the need for more testing in the state. Only actual measurements should be used to determine levels of indoor radon because of the strong roles home construction and localized geology play in radon concentration. However, the data visualization method demonstrated here is potentially useful for directing resources relating to radon screening campaigns.', 'title': 'Use of a geographic information system (GIS) for targeting radon screening programs in South Dakota', 'embedding': []}, {'id': 15260, 'abstractText': 'Compared with continuous memristor, discrete memristor has not been deeply studied. In this work, a new discrete memristor model and its pinched hysteresis loops are explored. Based on this model, a simple 2D hyperchaotic map and its dynamics are exhibited. Reservoir computing is an extension of neural networks, which has been received adequate attention. So far, the application of discrete memristor on reservoir has not been reported. To this end, we consider the discrete memristor-based map as a reservoir and verify its performance by a nonlinear regression task. The results indicate that the memristor-based map can be used effectively as a reservoir and enable reservoir computing systems go further on the way of efficient processing temporal signals in the future.', 'title': 'A 2D Hyperchaotic Discrete Memristive Map and Application in Reservoir Computing', 'embedding': []}, {'id': 15261, 'abstractText': 'Cost-effective permafrost characterization and monitoring should be possible due to advances in the technology of earth observation satellites. In particular, the long-penetration capabilities of L-band ALOS2-PALSAR2 should permit large scale mapping of discontinuous permafrost in peatland areas. Recently, it has been shown that the long penetrating polarimetric L-band ALOS is very promising for boreal and subractic peatland mapping and monitoring [1], [2]. The unique information provided by the Touzi decomposition [3], [4], and the Touzi scattering phase in particular, on peatland subsurface water flow permits enhanced discrimination of bogs from fens; two peat- land classes that can hardly be discriminated using conventional optical remote sensing. In this study, the Touzi scattering phase is investigated for mapping discontinuous permafrost in peatland regions Northern Alberta. Polarimetric ALOS-2 (FP6-4) and field data were collected in August 2014 over discontinuous distributed within wooded palsa bogs and peat plateaus near the Namur Lake (Northern Alberta). The ALOS2 image is re-calibrated to reduce the residual error from -33 dB down to -43 dB. This permits full exploiting the excellent ALOS2 performance in term of low noise floor (NESZ about -38 dB) to increase the sensitivity of the Touzi phase to deep permafrost. It is shown that the information provided by the scattering type phase permits enhanced mapping of discontinuous permafrost. The results obtained with the long penetrating L-band polarimetric PALSAR2 are much better than the ones obtained with conventional discontinuous permafrost mapping methods based on Lidar and optical (Landsat and Spot) images.', 'title': 'Polarimetric L-band PALSAR2 for Discontinuous Permafrost Mapping In Peatland Regions', 'embedding': []}, {'id': 15262, 'abstractText': 'Despite the relevance of historical land cover maps for scientists and policy makers, an accurate high resolution record is currently lacking over the Sudano-Sahel. In this study, 30m resolution historically consistent land cover and cover fraction maps are provided over the Sudano-Sahel for the period 1986–2015. These land cover/cover fraction maps are achieved based on the Landsat archive preprocessed on Google Earth Engine and a random forest classification/regression model, while historical consistency is achieved using the hidden Markov model. Using these historical maps, a multitude of variability in the dynamic Sudano-Sahel region over the past 30 years is revealed. These include cropland expansion and the re-greening of the Sahel, forest degradation &amp; the detection of fine-scale changes, such as smallholder or subsistence farming. The historical land cover / cover fraction maps are made available via an open-access platform.', 'title': 'Thirty Years of Land Cover and Fraction Cover Changes Over the Sudano-Sahel Using Landsat Time Series', 'embedding': []}, {'id': 15263, 'abstractText': 'Hyperspectral sensors have high spectral resolution by capturing images in hundreds of bands. Despite the high spectral resolution, low spatial resolution of these sensors restricts the performance of the hyperspectral imaging applications such as target tracking and image classification. Fusing the hyper-spectral image (HSI) with higher spatial resolution RGB or multispectral image (MSI) data is a commonly used method in the resolution enhancement of the HSIs. In this paper, we propose a new fusion technique for the HSI super-resolution. The main contribution of this study is formulating the fusion problem in a quadratic manner and also regularizing the solution quadratically using smoothness prior. Moreover, another contribution of the proposed method is converting the fusion problem from spectral domain to the abundance map domain which gives more robust and spectrally consistent results. In the proposed method, first, abundance maps are obtained using linear spectral unmixing and then a quadratic energy function is obtained using these maps and high resolution (HR) RGB image. In addition, quadratic function is regularized using additional constraints. Solving the regularized quadratic function gives the HR abundance maps and these maps are used to reconstruct HR HSI. Experiments show that proposed method yields better performance as compared to state of the art methods in different performance metrics.', 'title': 'Image Fusion for Hyperspectral Image Super-Resolution', 'embedding': []}, {'id': 15264, 'abstractText': 'This paper describes the application of mobile robots in the current coronavirus epidemic. Localization is a frequently discussed topic in mobile robotics research. Before a robot can start a task, it must know its current location on a map. The system proposed in this paper scans the obstacles and terrain around the robot by LiDAR to obtain a map of the environment and then uses the image recognition algorithm proposed in this paper to achieve the robot’s location. This system can be applied to frontline medical robots, which can disinfect the environment or deliver medication, especially in the case of the COVID-19 epidemic, to help healthcare workers. The proposed localization algorithm is different from the traditional Adaptive Monte Carlo Localization (AMCL), which uses a 2D LiDAR sensor with image recognition to complete the localization. By using a modified template matching technique, the local map is compared with the known global map to deduce the robot’s position, which is more accurate than AMCL. In this study, an indoor environment is created using Gazebo 3D environment simulation software, and a robot with a 2D LiDAR sensor is used in this environment to conduct the experiment. We designed three scenarios to validate the proposed algorithm, one with simple terrain, the second scene will appear throughout the map with other scenes of similar terrain, and the third with long straight lines. The results show that this method is feasible.', 'title': 'Pose Detection of a Mobile Robot Based on LiDAR Data', 'embedding': []}, {'id': 15265, 'abstractText': 'Increasing need for fresh water resources, particularly in irrigation, makes it imperative to develop tools to monitor and improve water use efficiency at field, regional and national levels. Satellite observations are highly suitable for this task and for this reason FAO, the custodian agency of Sustainable Development Goals Indicators 6.4.1 and 6.4.2, developed the WaPOR portal through which it distributes satellite-based (Terra and Aqua, PROBA-V and Landsat) evapotranspiration (ET) maps. The aim of this study is to evaluate the suitability of using Copernicus data (Sentinel-2 and Sentinel-3 observations and ERA5 meteorological model) to produce high-resolution, national-scale ET maps during the evolution of WaPOR portal. Results indicate that Copernicus-based maps show generally similar ET patterns to WaPOR maps across climatic and land-use gradients while providing more accurate and detailed field-level estimates compared to MODIS-based WaPOR maps.', 'title': 'Assessing Utility of Copernicus-Based Evapotranspiration Maps for National Monitoring of Field-Scale Water Use', 'embedding': []}, {'id': 15266, 'abstractText': 'With the near completion of WUDAPT (World Urban Database and Access Portal Tools) Level 0 data, one of the next goals is to generate more accurate and detailed local climate zone (LCZ) maps. An important issue is how to integrate building height information into LCZ maps. We here present a multi-label classification method using very high resolution (VHR) imagery to implicitly integrate building height information. Since we humans can tell whether a place is high-rise or not based on the shading of buildings and the surrounding context, it is possible to extract such information using deep learning methods. We use Hong Kong as a case study and show the potential of LCZ mapping with VHR imagery in distinguishing small-scale landscape features like city parks. The multi-label LCZ maps also provide a solution to generate fine-grained subclass LCZ mapping, in which a place can be classified as a combination of multiple LCZs, e.g., compact low-rise with open high-rise.', 'title': 'Multi-Label Local Climate Zone Mapping as Scene Classification Using Very High Resolution Imagery: Preliminary Result of Hong Kong', 'embedding': []}, {'id': 15267, 'abstractText': 'An effective treatment for scar-related ventricular tachycardia (VT) is to interrupt the circuit by catheter ablation. If activation sequence and entrainment mapping can be performed during sustained VT, the exit and isthmus of the circuit can often be identified. However, with invasive catheter mapping, only monomorphic VT that is hemody namically stable can be mapped in this manner. A non-invaive approach to fast mapping of unstable VTs can potentially allow an improved identification of critical ablation sites. In this pilot study, noninvasive ECG-imaging were carried out on patients with unstable scar-related VT. The reconstructed reentry circuits correctly revealed both epicardial and endocardial origins of activation, consistent with locations of exit sites found during ablation procedures. The results also indicated that some reentry circuits involve both epicardial and endocardial layers, and can only be properly interpreted by mapping both layers.', 'title': 'Noninvasive epicardial and endocardial electrocardiographic imaging of scar-related ventricular tachycardia', 'embedding': []}, {'id': 15268, 'abstractText': 'This study focus on the evaluation of scientific and industry-grade hyperspectral airborne sensors for the mapping of methane (CH4) emissions in the SWIR range. An imaging dataset from areas with known CH4 emissions was processed using the classic matched filter technique, and a new CH4 index. The airborne sensors were evaluated based on sensor design (spectral sampling and band centers), effectiveness of image processing, and impact of the signal-to-noise ratio (SNR) on CH4 mapping. The gas plume was mapped only in the images acquired with scientific-grade sensors. Results demonstrated that superior performance could be achieved when the position of band centers are closely located to the center of diagnostic CH4 absorption features. The impact of SNR was examined using a noise simulation, adding white noise to simulate images with varying SNR levels. Results indicate that the noisier signal of the industry-grade sensor is probably what prevented mapping the CH4 plume in this dataset. Simulations also demonstrated that as densest the plumes lower is the impact of SNR. Combined, results indicate that an imaging spectrometer with a scientific-grade SNR and band centers properly positioned to match the main CH4 features would improve the mapping of CH4 plumes with airborne sensors operating in the SWIR range.', 'title': 'Assessing Scientific and Industry Grade SWIR Airborne Imaging Spectrometers for CH<inf>4</inf> Mapping', 'embedding': []}, {'id': 15269, 'abstractText': 'This study investigated the potential for using principal component analysis (PCA) and feature map fusion to improve real-time prostate cpasula detection algorithms. Some objective detection algorithms can realize real-time object detection, but the mean average precision (mAP) is not ideal. PCA is processed firstly to make dimensionality reduction. After that, the processed images are sent to the new network called Almost Fully Feature Fusion Single Shot Multibox Detector (AFFSSD) to train. Feature map size at 150×150, 75×75, 38×38, 19×19, 10×10, 5×5, 3×3, 1×1 are all utilized for feature collection by feature map fusion and feature pyramid therein. Results showed that PCA and AFFSSD has the highest mAP for the prostate capsula images detection, by 76.23%, compared with Faster R-CNN, R-FCN, YOLOv3, SSD, FSSD applied independently. The proposed architecture fused many low-level features, so the confidence score are very high, almost 100 percent.', 'title': 'Application of Primary Component Analysis and Feature Map fusion to Improve Real-time Prostate Capsula Detection', 'embedding': []}, {'id': 15270, 'abstractText': 'Most researchers working on electrical capacitance tomography (ECT) use a set of generic sensitivity maps, based on empty background. The aim of this study is to investigate whether or not different sensitivity maps should be used for imaging different dielectric materials. For this purpose, different sensitivity maps are generated by dot multiplication of electric fields with different permittivity backgrounds. To see the effect of different sensitivity maps on image reconstruction, stratified and annular distributions are chosen for simulation with a 12-electrode ECT sensor. The reconstructed images are evaluated by the image error. To verify the simulation results, experiments are carried out. Preliminary results show that the generic sensitivity maps based on empty background are suitable for image reconstruction in most cases.', 'title': 'Image reconstruction with different sensitivity maps generated with different background', 'embedding': []}, {'id': 15271, 'abstractText': 'This paper introduces an anti-aliasing algorithm based on saliency map for virtual reality applications. In order to do it, we first render the whole scene into a single texture image and feed it into saliency map construction. The result saliency map is then input to the second rendering step with the original texture image. The second rendering step performs the anti-aliasing algorithm selectively based on the value of saliency map. Through the user study, it turns out that participants do not distinguish between full anti-aliasing and selective anti-aliasing based on saliency map. The rendering time has a 5-10% performance increase if we use the selective anti-aliasing that we propose.', 'title': 'Selective Anti-Aliasing for Virtual Reality Based on Saliency Map', 'embedding': []}, {'id': 15272, 'abstractText': 'Model-Driven Software Engineering (MDSE) is a development method in which models are used to generate software. Despite documented advantages, projects employing MDSE may fail due to development challenges. In this paper, we study and document these challenges by conducting an up-to-date systematic mapping that goes beyond reviewing MDSE studies: we also include two derived paradigms (Model-Oriented Programming and Models at Run-time). Therefore, we present a systematic mapping with two objectives: The first objective was to identify specific domains in which MDSE is successful, while the second objective was to identify what are the challenges to apply this methodology to general purpose development processes. Following the review of 4859 studies (3727 are unique), we have identified the application and technological domains in which MDSE projects are more likely to succeed. We also discuss challenges presented by 17 primary studies. The analysis of the results indicate that MDSE application is consolidated in specific domains. A common feature identified among studies related to general purpose MDSE processes is that, initially, authors reported lack of proper methods and training. After new techniques have risen, it has been pointed that MDSE projects still face maintenance problems that can discourage their usage in other domains.', 'title': 'Understanding the Successes and Challenges of Model-Driven Software Engineering - A Comprehensive Systematic Mapping', 'embedding': []}, {'id': 15273, 'abstractText': 'Spreadsheets provide a very flexible programming environment and are used by almost every organization, company, institution or business for data processing and data storage tasks. The main objectives of this research are: (1) to have an overview of the research that is being done related to spreadsheet smells; (2) to classify the spreadsheet smells research according to ten criteria: techniques, year of publication, publication venues, publication channels, datasets, countries, contribution type, research approaches, tools used, tools/techniques proposed; and (3) analyze studies from different perspectives like study objectives, methods, method accuracy and limitations. We performed a systematic mapping on the spreadsheet smells studies published in the time span of 2010-2019, and adopted proper methods to classify, review and analyze them. In total, we were able to identify 28 studies and map the results of these studies.', 'title': 'Spreadsheet Smells: A Systematic Mapping Study', 'embedding': []}, {'id': 15274, 'abstractText': 'Publication on digital education continues to develop but is limited to one country and/or one field. From a bibliometric overview, this study aims to visually study mapping and publication trends in the field of digital education on an international scale. This study used bibliometric techniques with secondary data from Scopus. Analyze and visualize data using the VOSViewer program and the analyze search results function on Scopus. This study analyzed 696 scientific documents published from 1983 to 2020. According to the research, the Nanyang Technological University, Singapore and Josip Car had the most active individual scientists in digital education research. Social science was the most studied outlet of digital education. There were three category maps of collaborative researchers from around the world. Based on the identification of a collection of knowledge created from the past thirty-seven years of publication, this research proposes a grouping of digital education themes: Computer education, Humanities, Information technology, Knowledge, and E-learning, abbreviated as CHIKE publication themes.', 'title': 'A Bibliometric Overview and Visualization of The Digital Education Publication', 'embedding': []}, {'id': 15275, 'abstractText': \"Thinking in and understanding of three-dimensional structures is omnipresent in many sciences from chemistry to geosciences. Current visualizations, however, are still using two-dimensional media such as maps or three-dimensional representations accessible through two-dimensional interfaces (e.g., desktop computers). The emergence of immersive virtual reality environments, both accessible and of high-quality, allows for creating embodied and interactive experiences that permit for rethinking learning environments and provide access to three-dimensional information through three-dimensional interfaces. However, there is a shortage of empirical studies on immersive learning environments. In response to this shortcoming, this study examines the role of immersive VR (iVR) in improving students' learning experience and performance in terms of penetrative thinking in a critical 3D task in geosciences education: drawing cross-sections. We developed a pilot study where students were asked to draw cross-sections of the depth and geometry of earthquakes at two subduction zones after visualizing the earthquake locations either in iVR or 2D maps on a computer. The results of our study show that iVR creates a better learning experience; students reported significantly higher scores in terms of Spatial Situation Model and there is anecdotal evidence in favor of higher reflective thinking in iVR. In terms of learning performance, we did not find a significant difference in the graded exercise of drawing cross-sections. However, iVR seems to have a positive effect on understanding the geometry of earthquake locations in a complex tectonic environment such as Japan. Our results, therefore, add to the growing body of literature that draws a more nuanced picture of the benefits of immersive learning environments calling for larger scale and in-depth studies.\", 'title': 'Fostering Penetrative Thinking in Geosciences Through Immersive Experiences: A Case Study in Visualizing Earthquake Locations in 3D', 'embedding': []}, {'id': 15276, 'abstractText': 'Context: The Software Engineering (SE) research continues to gain strength and interest for researchers considering the need to apply rigor and scientific validity to research results. Objective: Establishing an overview of the topic through a classification scheme of publications and structure the field of interest. Method: We conducted a Systematic Mapping Study, including articles published until 2019, that report at least one study of empirical strategies in SE. Results: 80 initial sets of studies were selected and analyzed, identifying: i) empirical strategy type used and ii) Software Engineering hypotheses types used. Also, 20 papers of the set of studies for mapping were selected and analyzed, identifying 17 empirical strategies and 11 main characteristics to address the empirical research inception in SE. Conclusions: We corroborate that the selection of an empirical strategy in Software Engineering research depends on the nature and scope of the research and on the resources that the researcher has at that moment, in addition to the degree of scientific and methodological knowledge that he has to carry out an empirical study. It is necessary to continue studying in-depth the behavior and nature of the empirical strategies in Software Engineering research that allows strengthening the scientific taxonomy in SE, besides walking towards the automation of the experimental process.', 'title': 'Empirical Strategies in Software Engineering Research: A Literature Survey', 'embedding': []}, {'id': 15277, 'abstractText': \"The gamification in ride-sharing applications is actualized in a point system for incentives and ratings for feedback. By growth of driver's number, the current gamification is facing problems. Drivers should spend more time and serve more orders to achieve targeted points. As the impact, their performance got worse, and their customers' satisfaction was reduced. To analyze the gamification effect on driver motivation, this study synthesizes Self Determination Theory (SDT) and Motivational Affordance Perspective (MAP). This study is a case study based on empirical data using a quantitative approach. By involving 103 participants, this study examines seven variables: Identified Regulation, External Regulation, Need for Autonomy, Self-Efficacy, Playfulness, Extrinsic Motivation, and Intrinsic Motivation. After mapping them into nine hypotheses, there were five accepted ones. The results unveiled that gamification can influence extrinsic motivation, but it cannot influence intrinsic motivation. As general interpretation, gamification motivates drivers to take more orders since they are forced to reach targeted points. This study provides recommendations for ride-sharing operators to improve gamification by adding new features to cover self-efficacy, need for autonomy, and playfulness in order to influence drivers' intrinsic motivation.\", 'title': 'Does Gamification Motivate Gig Workers? A Critical Issue in Ride-Sharing Industries', 'embedding': []}, {'id': 15278, 'abstractText': \"Brain-wide and genome-wide association (BW-GWA) study is presented in this paper to identify the associations between the brain imaging phenotypes (i.e., regional volumetric measures) and the genetic variants [i.e., single nucleotide polymorphism (SNP)] in Alzheimer's disease (AD). The main challenges of this study include the data heterogeneity, complex phenotype-genotype associations, high-dimensional data (e.g., thousands of SNPs), and the existence of phenotype outliers. Previous BW-GWA studies, while addressing some of these challenges, did not consider the diagnostic label information in their formulations, thus limiting their clinical applicability. To address these issues, we present a novel joint projection and sparse regression model to discover the associations between the phenotypes and genotypes. Specifically, to alleviate the negative influence of data heterogeneity, we first map the genotypes into an intermediate imaging-phenotype-like space. Then, to better reveal the complex phenotype-genotype associations, we project both the mapped genotypes and the original imaging phenotypes into a diagnostic-label-guided joint feature space, where the intraclass projected points are constrained to be close to each other. In addition, we use ℓ<sub>2,1</sub>-norm minimization on both the regression loss function and the transformation coefficient matrices, to reduce the effect of phenotype outliers and also to encourage sparse feature selections of both the genotypes and phenotypes. We evaluate our method using AD neuroimaging initiative dataset, and the results show that our proposed method outperforms several state-of-the-art methods in term of the average root-mean-square error of genome-to-phenotype predictions. Besides, the associated SNPs and brain regions identified in this study have also been shown in the previous AD-related studies, thus verifying the effectiveness and potential of our proposed method in AD pathogenesis study.\", 'title': \"Brain-Wide Genome-Wide Association Study for Alzheimer's Disease via Joint Projection Learning and Sparse Regression Model\", 'embedding': []}, {'id': 15279, 'abstractText': 'In this study, a systematic mapping study was conducted to systematically evaluate publications on Intrusion Detection Systems with Deep Learning. 6088 papers have been examined by using systematic mapping method to evaluate the publications related to this paper, which have been used increasingly in the Intrusion Detection Systems. The goal of our study is to determine which deep learning algorithms were used mostly in the algortihms, which criteria were taken into account for selecting the preferred deep learning algorithm, and the most searched topics of intrusion detection with deep learning algorithm model. Scientific studies published in the last 10 years have been studied in the IEEE Explorer, ACM Digital Library, Science Direct, Scopus and Wiley databases.', 'title': 'Intrusion Detection Systems with Deep Learning: A Systematic Mapping Study', 'embedding': []}, {'id': 15280, 'abstractText': 'Despite the importance of software qualities, they are not well understood, especially in the context of the interrelationships between qualities. A number of systematic mapping studies have been conducted prior to 2015 to summarize the literature on the topic and to identify research gaps. To provide a better understanding of the current state of the art, we conducted a systematic mapping study on relevant studies from 2015 to 2019 through a database search and a subsequent snowballing approach. In total, 18 studies were selected as the study subjects wherein we evaluated the types of software quality interrelationships and the qualities that comprise them. Based on our findings, we report on the progress made to address previously identified research gaps.', 'title': 'Recent Trends in Software Quality Interrelationships: A Systematic Mapping Study', 'embedding': []}, {'id': 15281, 'abstractText': 'Dengue, zika and chikungunya are examples of serious diseases affecting mainly developing countries, such as Brazil. Data from the World Health Organization (WHO) about these diseases transmitted by Aedes aegypti mosquito are worrisome, especially in relation to their possible effects. Considering this scenario severity, Internet of Things based applications (IoT applications) are being used to support it. So, this paper aims to select and organize, through information collection, difficulties that can be found in the medical use of IoT applications to combat this mosquito. The objective was to find possible gaps for future research. A Systematic Literature Mapping was performed to find possible deficiencies in the methodologies addressed by the authors of the returned studies. This systematic mapping intends to list these difficulties, as well methods proposed to improve the use of IoT in Health (especially combat to Aedes aegypti). In total, 570 studies were found. When applying the inclusion and exclusion criteria, 22 studies were selected for a full reading, which led to rejection of 15, therefore 07 studies were finally accepted. By analyzing these selected studies, it was possible to observe monitoring of information is essential in dengue combat systems. It was also possible to notice the lack of software reuse initiatives that could make it easier to develop new applications in this domain.', 'title': 'IoT Applications to Combat Aedes Aegypti: A Systematic Literature Mapping', 'embedding': []}, {'id': 15282, 'abstractText': 'Research on the smart economy as a part of a smart city continues to develop but is limited to one country and/or one field. From a bibliometric review, this study aims to visually study mapping and research trends in the field of the smart economy on an international scale. This study used bibliometric techniques with secondary data from Scopus. Analyze and visualize data using the VOSViewer program and the analyze search results function on Scopus. This study analyzed 125 scientific documents published from 2011 to 2020. According to the research, the Peter the Great St. Petersburg Polytechnic University, Russia, and Mboup, G. had the most active individual scientists and in smart economy research. Computer science and Advances in 21st Century Human Settlements were the most studied and disseminated outlet of smart economy research. There was one category map of collaborative researchers from around the world. Based on the identification of a collection of knowledge generated from nine years of publication, this research proposes a grouping of smart economy research themes: Economy, Application of smart cities, Smart technology, Smart cities character, Economy of urban, Education and Smart environment, abbreviated as EASSEES research themes.', 'title': 'Mapping of Smart Economy Research Themes: A Nine-Year Review', 'embedding': []}, {'id': 15283, 'abstractText': 'The study was carried out in order to enable systems with weak processing power and motion to detect objects using cloud services. In addition, the dataset is expanded by continuous labeling to create big data. In the study, it is aimed to detect objects using cloud-based deep learning methods with an unmanned aerial vehicle (UAV). In the study, training processes were carried out with Google Colaboratory, a cloud service provider. The training processes are a YOLO-based system, and a convolutional neural network was created by revising the parameters in line with the needs. The convolutional neural network model provides communication between neurons in the convolutional layers by bringing the image data to the desired pixel ranges. Unlabeled pictures are included in the training by being tagged. In this way, it is possible to continuously enlarge the data pool. Since the microcomputers used in UAVs are insufficient for these processes, a cloud-based training model has been created. As a result of the study, cloud-based deep learning models work as desired. It is possible to show the accuracy of the model with the low losses seen in the loss functions and the mAP value. Graphic cards with high processing power are needed to provide training. It is essential to use powerful graphics cards when working on image data. Cost reduced by using cloud services. The training was accelerated and high-rate object detections were made. YOLOv5x was used in the study. It is preferred because of its fast training and high frame rate. Recall 80% Precision 93% mAP 82.6% values were taken.', 'title': 'Real-Time Puddle Detection Using Convolutional Neural Networks with Unmanned Aerial Vehicles', 'embedding': []}, {'id': 15284, 'abstractText': 'Permanent magnet synchronous machines (PMSMs) are widely used in many applications. The performance of the PMSM is highly dependent on the motor parameters. Many research studies have been done to evaluate the PMSM performances in terms of maximum torque capability, power capability, and field-weakening capability. This paper proposes a new normalized PMSM model and uses two parameters, characteristic current and saliency ratio, to uniquely define the motor characteristic. Based on this normalized model, the full map motor behaviors, including torque capability, power capability, torque/power-speed characteristics, and power factor behaviors, are studied parametrically. A new unitless metric, i.e., copper loss factor, is introduced to evaluate the copper loss variation of PMSMs. The saturation effect on the motor behavior is studied based on the 2004 Prius traction motor, which confirms that the field-weakening characteristic can be well predicted using the linearized model. A new design flow for traction PMSMs, which requires wide-speed operation, is proposed based on the full map motor behavior study, and a prototype machine is designed accordingly. The behavior study and the effectiveness of the traction motor design flow are validated experimentally by the prototype.', 'title': 'Behavior Study of Permanent Magnet Synchronous Machines Based on a New Normalized Model', 'embedding': []}, {'id': 15285, 'abstractText': \"Sustainability is a topic of increasing interest. The United Nations has released a list of 17 goals for sustainable development for the global community. Blockchain is a recent technological innovation that shows great promise in changing industries. In this paper, we look specifically at smart grids and supply chain management systems as areas where sustainable technological innovation can happen. To identify software engineering aspects of blockchain in smart grids and supply chain management, we start upon online libraries focusing on engineering and information technology, and we opted for the methodology of systematic mapping studies in software engineering. The search strategy identified 535 papers, of which 60 were identified as main studies for our mapping. To the best of the authors' knowledge, no previous similar studies exist. Results of the study show that the research connecting blockchain technology to smart grids and supply chain management systems is still young. None of the techniques or systems have yet been implemented in a real life setting. As such, more work has to be done before we can look at the actual implications of putting such technologies into use. Software engineering practices could prove to be very useful in the process of development. We propose that future studies can focus on bringing the technologies closer to real life implementations, as well as how to involve the end users in the development of the blockchain-based systems.\", 'title': 'Blockchain and Sustainability: A Systematic Mapping Study', 'embedding': []}, {'id': 15286, 'abstractText': \"The concept of Local Climate Zone (LCZ) has been developed to quantify the correlation between urban morphology and urban heat island (UHI). Each LCZ is supposed to have homogenous air temperature. However, traditional air temperature observation methods have limited spatial coverage and poor spatial resolution. Land surface temperature (LST) acquired from satellite images can be used to study the temperature characteristics of LCZ classes by providing continuous data on surface temperature. This study aims to study the relationship between LST and LCZ classes with Shanghai selected as a case study because of its high urbanization rate and serious UHI effect. This study has three major steps: Firstly, Shanghai local climate zone map was generated using the World Urban Database and Portal Tool (WUDAPT) method. Secondly, a remote sensing approach was taken to acquire Shanghai's LST from night-time Aster thermal data in different seasons. Thirdly, the LST was associated with the Shanghai LCZ map and the correlation between LCZ and LST in Shanghai was discussed. The results show that there are large variations in LST across LCZ classes in different months in Shanghai. These results will be able to offer integrated information under urban climate principles for urban planners and urban climate researchers.\", 'title': 'Investigating the relationship between Local Climate Zone and land surface temperature', 'embedding': []}, {'id': 15287, 'abstractText': 'We study the anomalous microwave emission (AME) in the Lynds Dark Nebula (LDN)\\u20091780 on two angular scales. With publicly available data at an angular resolution of 1°, we studied the spectral energy distribution of the cloud in the 0.408–2997\\u2009GHz frequency range. The cloud presents a significant (&gt;20σ) amount of AME, making it one of the clearest examples of AME on 1\\u2009° scales, and its spectrum can be well fitted with a spinning dust (SD) model. We also find at these angular scales that the location of the peak of the emission at lower frequencies (23–70\\u2009GHz) differs from the location at the higher frequencies (90–3000\\u2009GHz) maps. In addition to the analysis on 1° angular scales, we present data from the Combined Array for Research in Millimeter-wave Astronomy (CARMA) at 31\\u2009GHz with an angular resolution of 2\\u2009arcmin, in order to study the origin of the AME in LDN\\u20091780. We studied morphological correlations between the CARMA map and different infrared tracers of dust emission. We found that the best correlation is with the 70-\\u2009μm template, which traces warm dust (T ∼ 50\\u2009K). Finally, we study the difference in radio emissivity between two locations within the cloud. We measured a factor of ≈6 difference in 31-GHz emissivity. We show that this variation can be explained, using the SD model, by a variation on the dust grain size distribution across the cloud, particularly changing the fraction of polycyclic aromatic hydrocarbon for a fixed total amount of carbon.', 'title': 'Modelling the spinning dust emission from LDN\\u20091780', 'embedding': []}, {'id': 15288, 'abstractText': 'Three-Dimensional Magnetic Resonance Imaging (3D-MRI) and Computer-Aided Detection (CAD) have been widely studied in the detection of bipolar disorder (BD). In this study, the structural alterations at the grey matter (GM) and white matter (WM) of BD subjects versus healthy controls (HCs) have been compared using Voxel-Based Morphometry (VBM). In order to obtain 3D GM and WM masks, the two sample ttest method and total intracranial volumes of BD and HC as a covariate have been utilized. In addition to analyzing effects of GM and WM tissue maps separately in the detection of BD, impacts of both GM and WM ones are studied by concatenating them ın a matrix. The correlation-based feature selection (CFS) feature ranking method is applied to the obtained 3D masks to rank the features, the number of selected top-ranked features are determined using a Fisher criterion (FC) approach, and different classification algorithms are used to classify BD apart from HCs. In this study, 26 BDs and 38 HCs data are used. The experimental results indicate that the classification accuracy of Naive Bayes outperforms the other four classification algorithms used in this study. Additionally, concatenation of GM and WM tissue maps enhances the classification performances of using GM-only and WM-only ones. The classification accuracies obtained for GM, WM, and their concatenation are 72.92%, 78.33%, and 80.00% respectively.', 'title': 'Diagnosis of Bipolar Disease Using Correlation-Based Feature Selection with Different Classification Methods', 'embedding': []}, {'id': 15289, 'abstractText': 'Disruptive technology, blockchain is propelling a technological intervention in healthcare due to its unique features and advantages. The healthcare industry is migrating to Health 4.0. Therefore, peer-to-peer (P2P) transactions in a decentralized and distributed manner makes blockchain more lucrative to serve the needs of the healthcare industry of today. The revolutionary system, blockchain has been discussed in the field of healthcare over the past five years. Hence, a systematic investigation of the existing body of knowledge concerning blockchain research in healthcare is essential. The motivation of this study is to support further studies based on the current research trend analysis via graphical visualization and bibliographic material analysis. Therefore, this study maps the expansion of scientific and academic research conducted concerning blockchain that is relevant to healthcare by utilizing a bibliometric analytic method to understand the state of the art. Bibliometric statistics were utilized to analyze current scientific articles published in the Scopus database from 2016 to 2019. In addition, an overview of the publication trends over the first three months of 2020 was undertaken to understand the research trend for the current year so far. The study serves the purpose of mapping research development trends in healthcare. The outcome discovered some beneficial insights such as the yearly trend of publications, top listed authors, institutes, countries, and publishers from around the world. Moreover, this article assists scholars in developing a theoretical framework to provide a primary source of reference for further studies regarding blockchain technology in the healthcare domain.', 'title': 'Mapping Research Trends of Blockchain Technology in Healthcare', 'embedding': []}, {'id': 15290, 'abstractText': \"For decades, Software Process Improvement (SPI) programs have been implemented, inter alia, to improve quality and speed of software development. To set up, guide, and carry out SPI projects, and to measure SPI state, impact, and success, a multitude of different SPI approaches and considerable experience are available. SPI addresses many aspects ranging from individual developer skills to entire organizations. It comprises for instance the optimization of specific activities in the software lifecycle as well as the creation of organization awareness and project culture. In the course of conducting a systematic mapping study on the state-of-the-art in SPI from a general perspective, we observed Global Software Engineering (GSE) becoming a topic of interest in recent years. Therefore, in this paper, we provide a detailed investigation of those papers from the overall systematic mapping study that were classified as addressing SPI in the context of GSE. From the main study's result set, a set of 30 papers dealing with GSE was selected for an in-depth analysis using the systematic review instrument to study the contributions and to develop an initial picture of how GSE is considered from the perspective of SPI. Our findings show the analyzed papers delivering a substantial discussion of cultural models and how such models can be used to better address and align SPI programs with multi-national environments. Furthermore, experience is shared discussing how agile approaches can be implemented in companies working at the global scale. Finally, success factors and barriers are studied to help companies implementing SPI in a GSE context.\", 'title': 'How Does Software Process Improvement Address Global Software Engineering?', 'embedding': []}, {'id': 15291, 'abstractText': 'Of recent, Building Information Modelling (BIM) has become an influential paradigm for the development of better project delivery practices to improve construction and operational efficiencies. In the last 6 years, a significant number of studies have been published on the integration of BIM in Internet of Things (IoT). This paper aims to examine the general research productivity, demographics, and trends shaping the research domain. Hence, the paper will also help to identify, categorize, and synthesize important studies in the research domain. In doing so, we adopt an evidence-based systematic mapping methodology to ensure the coverage of key studies through a systematic and unbiased selection and evaluation process which results in the final selection of 55 relevant studies. The results of the mapping study show that the research on the integration of BIM in IoT is gaining more attention in last 6 years with stable and consistent publication output. Prominent application domains, validation methods, contribution facets, research types, and simulation tools in the field of study were identified and presented. Five research types were also identified, i.e. solution proposal, experience paper, evaluation research, validation research, and opinion paper, with solution proposals getting more research attention. In general, the overall demographics of the research domain were presented and discussed.', 'title': 'Building Information Modelling (BIM) and the Internet-of-Things (IoT): A Systematic Mapping Study', 'embedding': []}], 'calculateClusters': True}\n",
      "(\"Abstract\":mapping study)\n",
      "[{'id': 14692, 'abstractText': 'The field of human information behavior runs the gamut of processes from the realization of a need or gap in understanding, to the search for information from one or more sources to fill that gap, to the use of that information to complete a task at hand or to satisfy a curiosity, as well as other behaviors such as avoiding information or finding information serendipitously. Designers of mechanisms, tools, and computer-based systems to facilitate this seeking and search process often lack a full knowledge of the context surrounding the search. This context may vary depending on the job or role of the person; individual characteristics such as personality, domain knowledge, age, gender, perception of self, etc.; the task at hand; the source and the channel and their degree of accessibility and usability; and the relationship that the seeker shares with the source. Yet researchers have yet to agree on what context really means. While there have been various research studies incorporating context, and biennial conferences on context in information behavior, there lacks a clear definition of what context is, what its boundaries are, and what elements and variables comprise context. In this book, we look at the many definitions of and the theoretical and empirical studies on context, and I attempt to map the conceptual space of context in information behavior. I propose theoretical frameworks to map the boundaries, elements, and variables of context. I then discuss how to incorporate these frameworks and variables in the design of research studies on context. We then arrive at a unified definition of context. This book should provide designers of search systems a better understanding of context as they seek to meet the needs and demands of information seekers. It will be an important resource for researchers in Library and Information Science, especially doctoral students looking for one resource that covers an exhaustive range of the most current literature related to context, the best selection of classics, and a synthesis of these into theoretical frameworks and a unified definition. The book should help to move forward research in the field by clarifying the elements, variables, and views that are pertinent. In particular, the list of elements to be considered, and the variables associated with each element will be extremely useful to researchers wanting to include the influences of context in their studies.', 'title': 'Exploring Context in Information Behavior: Seeker, Situation, Surroundings, and Shared Identities', 'embedding': []}, {'id': 14693, 'abstractText': 'A representation is a thing that can be interpreted as providing information about something: a map, or a graph, for example. This book is about the expanding world of computational representations, representations that use the power of computation to provide information in new forms, and in new ways. Unlike printed maps or graphs, computational representations can be dynamic, and even interactive, so that what is represented, and how, can be shaped by user actions. Exploring these new possibilities can be guided by an emerging theory of representation, that clarifies what characteristics representations must have to express the meaning being represented, and to enable users to discern that meaning easily and accurately. The theory also shows the way to inclusive design, for example using sounds to represent information commonly presented visually, so that people who cannot see can understand what is being presented. Because representations must be shaped by the abilities of their users, and by the nature of the meanings they convey, creating them requires perspectives from multiple disciplines, including psychology, as well as computer science, and the sciences appropriate to the content being expressed. The book presents a series of explorations of this large and complicated space, as invitations to further study, and to innovation.', 'title': 'Representation, Inclusion, and Innovation: Multidisciplinary Explorations', 'embedding': []}, {'id': 14694, 'abstractText': 'In this study, phenomena observed in scale-free coupled circle map are investigated. The circle map is a one-dimensional discrete-time dynamical system which exhibits various kinds of behavior as the parameters change. As the high-dimensional coupled circle map, the coupled map lattice and the globally coupled map have been studied. However, the scale-free coupled circle map has not been well investigated so far. We study the model in which one of the circle maps corresponding to a hub node of the scale-free network has the parameter which leads the single circle map to generate high-order periodic points, and the parameter values of the other maps are set to converge to a fixed point. Changing the coupling strength between each map, we investigated the synchronization in the scale-free coupled circle map by calculating the value of the order parameter. The result of this study elucidated that when the coupling strength between each map was negative, all circle maps in the network behaved chaotic whatever the parameter value of the hub node was set to. That means the phenomena was generated because of the scale-free network structure itself but not the state of the hub node. On the other hand, the coupling strength was positive, the behavior observed in the network was based on the state of the hub node. In addition, the result showed the periodic point observed in the hub node could change to the fixed point after the scale-free network generated. The result suggests that the phenomena such as chaos and periodic oscillation could change to be converged into stable fixed point by forming the scale-free network and appropriately controlling the parameters.', 'title': 'An investigation of phenomena observed in scale-free coupled circle map', 'embedding': []}, {'id': 14695, 'abstractText': \"The increasing number of studies in the knowledge map shows attention from researchers in academic and professional areas. However, the knowledge map implementation has not effectively implemented in an organization whose business in the digital business industry, especially startup organization. The main reason is the lack of stakeholder's understanding of the knowledge map concept. Thus, this study gives a comprehensive understanding of knowledge map implementation in the digital business industry within the last five years period. The study will answer what problems knowledge map tackled, tools, and techniques used currently, the obstacles and benefits of using a knowledge map. The review was conducted through the structured systematic literature review procedure. It started with a review protocol declaration and ended with an analysis of the prior researches obtained from five credible sources. Only 25 of 775 studies remain after several filtering stages. It is found that a knowledge map is mostly used for decision-making purposes. Most studies adopted a visual knowledge map and concept map, even though it is difficult to align the knowledge depth. In the end, this study's result will help stakeholders to reflect on their existing knowledge relationship structure. This study also offers directions for future research and professional practices in digital business industry firms to perfect their existing organizational intellectual capital through a knowledge map.\", 'title': 'An Overview of Knowledge Mapping in Digital Business Industry: A Systematic Literature Review', 'embedding': []}, {'id': 14696, 'abstractText': \"MAP algorithm has outperformed in medical image reconstruction with noise suppression and edge preservation. However MAP-OSL algorithm could hardly apply a strong prior by using a large regularization parameter. In this study we proposed a MAP-Newton reconstruction framework to enable a strong prior to be applied in MAP optimization. EM iterative framework was used in MAP-Newton. In the step of maximizing expectation of the complete data log-likelihood function, MAP-Newton solves the non-linear equation accurately with Newton iterative algorithm, which is potentially better than approximate linearization method used in MAP-OSL. MAP-Newton reconstruction algorithm was implemented with both Bowsher's prior and joint total variation (JTV) prior based on anatomical image. <sup>18</sup>F PET and CT data of an image-quality phantom were acquired on the small animal PET/SPECT/ CT Iniview 3000 system for performance evaluation. The priors with three different strength (regularization parameter was from 0.01 to 1.0) were applied in the list mode reconstruction studies with three noise levels (100 M, 12 M and 2 M LORs separately). The normalized standard derivation (NSTD) of region of interest was calculated. The results with Bowsher's prior for all noise level cases showed no significant difference between MAP-Newton and MAP-OSL when the regularization parameter was smaller than 1.0. When the parameter was set to be 1.0, the proposed MAP-Newton can greatly reduce NSTD with reasonable image quality although no reasonable images could be obtained with MAP-OSL. The results with JTV prior showed the same trend, except that increased regularization parameter did not reduce NSTD. In conclusion, with a strong prior, MAP-Newton can result in better image quality than MAP-OSL in terms of noise suppression, while no significant difference when applying a light prior. It indicated that the MAP-Newton reconstruction with a strong prior could be applied, when we have enough confidence on the prior.\", 'title': 'Regularized MLEM reconstruction with a strong anatomical prior using newton iterative algorithm', 'embedding': []}, {'id': 14697, 'abstractText': 'One of the topics that have successful applications in engineering technologies and computer science is chaos theory. The remarkable area among these successful applications has been especially the subject of chaos-based cryptology. Many practical applications have been proposed in a wide spectrum from image encryption algorithms to random number generators, from block encryption algorithms to hash functions based on chaotic systems. Logistics map is one of the chaotic systems that has been the focus of attention of researchers in these applications. Since, Logistic map can be shown as the most widely used chaotic system in chaos-based cryptology studies due to its simple mathematical structure and its characterization as a strong entropy source. However, in some studies, researchers stated that the behavior displayed in relation to the dynamics of the Logistic map may pose a problem for cryptology applications. For this reason, alternative studies have been carried out using different chaotic systems. In this study, it has been investigated which one is more suitable for cryptographic applications for five different derivatives of the Logistic map. In the study, a substitution box generator program has been implemented using the Logistic map and its five different derivatives. The generated outputs have been tested for five basic substitution box design criteria. Analysis results showed that the proposals for maps derived from Logistic map have a more robust structure than many studies in the literature.', 'title': 'Eligibility Analysis of Different Chaotic Systems Derived from Logistic Map for Design of Cryptographic Components', 'embedding': []}, {'id': 14698, 'abstractText': 'Modern geophysical data acquisition technology makes it possible to measure multiple geophysical properties with high spatial density over large areas with great efficiency. Instead of presenting these co-located multigeophysical data sets in separate maps, we take advantage of cluster analysis and its pattern exploration power to generate a cluster map with objectively integrated information. Each cluster in the resulting cluster map is characterized by multigeophysical properties and can be associated with certain geological attributes or rock types based on existing geological maps, field data and rock sample analysis. Such a cluster map is usually high in resolution and proven to be more helpful than single-attribute maps in terms of assisting geological mapping and interpretation. In this paper, we present the workflow and technical details of applying cluster analysis to multigeophysical data of a study area in the Tr⊘ndelag region in Mid-Norway. We address the importance of carefully designed pre-processing procedures regarding the input data sets to ensure an unbiased data integration using cluster analysis. Random forest as a supervised machine learning method for classification/regression is strategically employed post-clustering for quality evaluation of the results. The multigeophysical data used for this study include airborne magnetic, frequency electromagnetic and radiometric measurements, together with ground gravity measurements. Due to the nature of these input data, the resulting cluster map carries multidepth information. When associated with available geological information, the cluster map can help interpret not only bedrock outcrops but also rocks underneath the sediment cover.', 'title': 'Multigeophysical data integration using cluster analysis: assisting geological mapping in Tr⊘ndelag, Mid-Norway', 'embedding': []}, {'id': 14699, 'abstractText': 'We apply random matrix and free probability techniques to the study of linear maps of interest in quantum information theory. Random quantum channels have already been widely investigated with spectacular success. Here, we are interested in more general maps, asking only for <tex>$k$</tex>-positivity instead of the complete positivity required of quantum channels. Unlike the theory of completely positive maps, the theory of <tex>$k$</tex>-positive maps is far from being completely understood, and our techniques give many new parametrized families of such maps. We also establish a conceptual link with free probability theory and show that our constructions can be obtained to some extent without random techniques in the setup of free products of von Neumann algebras. Finally, we study the properties of our examples and show that for some parameters, they are indecomposable. In particular, they can be used to detect the presence of entanglement missed by the partial transposition test, that is, positive partial transposition entanglement. As an application, we considerably refine our understanding of positive partial transposition states in the case where one of the spaces is large, whereas the other one remains small.', 'title': 'Random and Free Positive Maps with Applications to Entanglement Detection', 'embedding': []}, {'id': 14700, 'abstractText': 'Objective: The ability to monitor catheter contact force (CF) plays a major role in assessing radiofrequency ablation, impacting lesion size and arrhythmia recurrence, and dictating ablation duration and/or overall patient safety. Our study sought to determine the relative CFs required to elicit reproducible monophasic action potential (MAP) recordings. Methods: The study utilized four swine in which: first, median sternotomies were performed and MAPs were collected from seven ventricular locations on the epicardial surface of each heart; and second, a subset of endocardial signals was recorded from a reanimated heart. In these studies, the initial elicitation and then loss of stable MAP waveforms were recorded, as were their associated catheter CFs (n = 371). Results: Mean CF at the onset of stable MAP recordings was 14.2 ± 2.9 g for epicardial and 16.6 ± 2.5 g for endocardial locations. Across epicardial locations, no significant differences in CF were required to elicit MAPs. Additionally, endocardial and epicardial CFs for MAPs did not significantly differ for respective locations, i.e., right ventricular septum endocardial versus epicardial. In our study, the catheter CFs required to elicit MAPs were within optimal ranges previously reported for eliciting clinically viable radiofrequency ablations. Conclusion: We believe that MAP recordings could complement CF measurements with electrical data, providing additional clinical feedback for physicians performing cardiac ablation. Significance: If applied clinically, MAP recordings could potentially improve ablation outcomes in patients with cardiac arrhythmias.', 'title': 'Contact Forces Required to Record Monophasic Action Potentials: A Complement to Catheter Contact Force Measurement', 'embedding': []}, {'id': 14701, 'abstractText': 'In order to monitoring the soil heavy metal pollution, and evaluate the effect of mine ecological environment restoration. This paper use Manganese Mine in Xiangtan as study area, and the image is divided into study area, whole map and control area, statistical and analysis the three regions farmland Vegetation index, get some parameters like Mean Value Difference ratio, STD, STD/MEAM. Based on those data, growth status and heavy metal stress condition of farmland vegetation in Manganese Mining from 1996 to 2016 were analyzed, and the ecological restoration effect in Manganese Mining was analyzed and evaluated by using different years data. The study shows; (1)The Mean of NDVI in the whore map, the study area and the contrast area Mean of NDVI decreased from 0.7906, 0.7978 and 0.7900 to 0.6322, 0.6443 and 0.6181, the difference ratio between the study area and the whole map increased from 0.902% to 1.878%, the study area and the contrast area increased from 0.902% to 1.878%, the STD/MEAN between study area, the whole map and contrast area were increased from 6.543%, 6.906% and 6.582%to 13.503%, 15.755% and 15.839%; (2)The growth of three areas was decreased year by year; the growth of vegetation in study area is better than contrast area and whole area, and the gap was increased year by year, but the increase trend slowed down, and the vegetation growth status in study area is more stable more balance than the contrast area and whole area; (3) Through above study, the effect of ecological restoration in the study area is better than the effect of natural restoration in other area, the ecological restoration project of the Manganese Mine got a good result. The study shows analyst Vegetation index background can monitor the mine ecological restoration effect quickly, and provide reference for mine ecological restoration project.', 'title': 'The study on the method of monitoring the restoration effect of heavy metals in mine based on the background value of vegetation', 'embedding': []}, {'id': 14702, 'abstractText': 'The purpose of this study is to build a learning-based attenuation map estimation scheme for brain attenuation correction of the positron emission tomography/magnetic resonance (PET/MR) imaging using an artificial neural network (ANN). The attenuation map estimation is cast as a regression problem that models a nonlinear mapping between the MR image patches and the corresponding patches of the attenuation map. An ANN is used to solve the regression problem through learning from examples. Using the BrainWeb phantoms, we simulated brain PET data with corresponding MR image and attenuation map pairs of 20 subjects. The ANN was trained with the image patches from the MR image and attenuation map of 1 subject. The trained ANN was then applied to estimate the attenuation maps from the MR images for the other 19 subjects. The estimated attenuation maps were compared with their simulated counterparts using the mean absolute error (MAE) on regions of soft tissue and bone. The impact of estimation accuracy on the reconstructed PET images was evaluated by calculating the relative error with respect to the PET images reconstructed using the AC with the simulated attenuation maps. The estimated attenuation maps obtain the MAE of 0.0021 for soft tissue and 0.0096 for bone, revealing strong agreement with the corresponding simulated ones. The images reconstructed with the estimated attenuation maps have average relative errors of -0.24%± 0.8% on gray matter and of -0.28%± 0.28% on white matter. We demonstrate that the ANN model trained using the MR image and attenuation map of one subject applies well to other subjects and the estimated attenuation map has potential to produce accurate AC for brain PET/MR imaging.', 'title': 'Learning-Based Attenuation Correction for Brain PET/MRI Using Artificial Neural Networks', 'embedding': []}, {'id': 14703, 'abstractText': \"Accurate vehicle localization with map is an important task in urban environment. Currently, vehicle positioning with various types of maps has studied. In this paper, we propose a method which matches 2D-NDT map with road marking image for fast and accurate vehicle positioning. This method extracts features from occupancy grid map by Maximally Stable Extremal Region (MSER) detector. The extracted features on the map is modelled to centroid and covariance and these are stored on the map. This map contains little data but also key information. Camera images also require feature extraction as well as map to match with 2D-NDT map. Images of forward looking camera are converted to top view images via Inverse Perspective Mapping(IPM). Road markings can be accurately extracted by MSER in a bird's eye view image. Vehicle Localization is performed by NDT map matching between the map and road markings in a bird's eye view road image. The evaluation of this method was conducted on a straight road in urban environment. Vehicle positioning results revealed accurate lateral, longitudinal position estimation and orientation estimation. Since NDT map matching was done with only significant lanes on the road, matching speed was fast.\", 'title': 'Vehicle Localization Using Road Marking Image Matching (ICCAS 2018)', 'embedding': []}, {'id': 14704, 'abstractText': \"Various coordinate systems of cadastral map are not very easy to be unified in Taiwan area due to temporal-spatial limitation of these maps. Therefore, the urban planning map, topographic map and other base map cannot integrate into geographic information system (GIS) for additional value-added applications. Cadastral map is directly related to the people's property, it is essential to develop “three map-in-one” project to precisely overlap three types of base maps (cadastral map, topographic map, and urban planning map) through rigorous operation procedures and innovative technology. For urban planning and development, “three map in one” project not only enhances the accuracy of three types of urban base maps, but also fulfills people's need in geospatial information. However, it is a very time and labor consuming task to integrate such huge capacity of maps. It is critical for local government to promote operation efficiency and to reduce the required human resources for this project. Recently, Unmanned Aerial Vehicle (UAV) equips with Global Position System (GPS), Inertial Measurement Unit (IMU), and high resolution camera which draws great research attention in this field. The purpose of this study is to access the feasibility of utilizing UAV for “three map-in-one” project operation by area analysis and man hour comparison. We conclude that the UAV-derived true orthophoto, can provide precise topography and building geographic position for “three map-in-one” project to reduce the required human resources and increase the operation efficiency.\", 'title': 'Assessment on the feasibility of three map in one work using unmanned aerial vehicle', 'embedding': []}, {'id': 14705, 'abstractText': 'Maximum a posteriori probability (MAP) decoding minimizes the symbol or bit error probability, however, few studies have performed an exact error performance evaluation, although the optimality does not require explanation. The MAP algorithm is much more complex than maximum likelihood decoding methods, therefore, suboptimal MAP algorithms are considered for practical systems. The Max-Log-MAP decoding algorithm is one of several near optimum algorithms that reduce decoding complexity. However, it is shown that turbo decoding with Max-Log-MAP has an error-performance degradation compared with MAP decoding. Log-MAP decoding can be realized using Max-Log-MAP decoding with a correction term, which corrects the error induced by maximum approximation. Constant Log-MAP decoding employs the constant correction term instead of the log-domain correction term. In this paper, analytical results of bit error probability of convolutional codes with constant Log-MAP decoding are shown. Furthermore, the analytical results are compared with the result of Max-Log-MAP decoding, and the improvement by the correction term which correct error induced by maximum approximation is presented, theoretically. The results show that the error performance of constant Log-MAP decoding is slightly better than Max-Log-MAP decoding.', 'title': 'On the bit error probability for constant log-MAP decoding of convolutional codes', 'embedding': []}, {'id': 14706, 'abstractText': 'Considering the success of generative adversarial networks (GANs) for image-to-image translation, researchers have attempted to translate satellite images to maps (si2map) through GAN for cartography. However, these studies involved limited scales, which hinders multi-scale map creation. By extending their method, high-resolution satellite images can be trivially translated to multi-scale maps through scale-wise si2map generators trained for certain scales. However, this strategy has two theoretical limitations. First, inconsistency between high-resolutions satellite images and object generalization on multi-scale maps (SI-M inconsistency) increasingly complicates the extraction of geographical information from satellite images for generators with decreasing scale. Second, as si2map translation is cross-domain, generators incur high computation costs to transform the pixel distribution on satellite images to that on maps. Thus, we designed a series strategy of generators for multi-scale si2map translation to address these limitations. In this strategy, high-resolution satellite images are inputted to an si2map generator to output large-scale maps, which are translated to multi-scale maps through series multi-scale map generators. The series strategy avoids SI-M inconsistency as high-resolution satellite images are only translated to large-scale maps, and transforms cross-domain translation to approximately intradomain translation when generates multi-scale maps. Our experimental results showed better quality multi-scale map generation with the series strategy, as shown by average increases of 11.69%, 53.78%, 55.42%, and 72.34% in the structural similarity index, edge structural similarity index, intersection over union (road), and intersection over union (water) for data from Mexico City and Tokyo at zoom level 17–13.', 'title': 'Generating Multi-scale Maps from Satellite Images via Series Generative Adversarial Networks', 'embedding': []}, {'id': 14707, 'abstractText': 'Empirical Research in Information Systems: 2001-2015 provides a first step in providing empirical evidence and knowledge about the practical relevance of IS research. The monograph first develops a broad yet sufficiently fine-grained framework of IS research by integrating earlier frameworks. It then identifies all empirical IS research published from 2001 to 2015 in four top IS journals (Journal of the Association for Information Systems, Journal of Management Information Systems, Information Systems Research, and MIS Quarterly), and maps onto this framework all the constructs and relationships that were examined by the 1,361 empirical papers published in this 15-year period. Next, based on this mapping and by drawing on criteria proposed by organizational and IS researchers, it provides a preliminary assessment of the relevance of empirical IS research to practice, and discusses the study’s findings and their implications.', 'title': 'Empirical Research in Information Systems: 2001-2015', 'embedding': []}, {'id': 14708, 'abstractText': 'For robots to navigate and interact more richly with the world around them, they will likely require a deeper understanding of the world in which they operate. In robotics and related research fields, the study of understanding is often referred to as semantics, which dictates what does the world ‘mean’ to a robot, and is strongly tied to the question of how to represent that meaning. With humans and robots increasingly operating in the same world, the prospects of human-robot interaction also bring semantics and ontology of natural language into the picture. Driven by need, as well as by enablers like increasing availability of training data and computational resources, semantics is a rapidly growing research area in robotics. The field has received significant attention in the research literature to date, but most reviews and surveys have focused on particular aspects of the topic: the technical research issues regarding its use in specific robotic topics like mapping or segmentation, or its relevance to one particular application domain like autonomous driving. A new treatment is therefore required, and is also timely because so much relevant research has occurred since many of the key surveys were published. This survey provides an overarching snapshot of where semantics in robotics stands today. We establish a taxonomy for semantics research in or relevant to robotics, split into four broad categories of activity in which semantics are extracted, used, or both. Within these broad categories, we survey dozens of major topics including fundamentals from the computer vision field and key robotics research areas utilizing semantics such as mapping, navigation and interaction with the world. The survey also covers key practical considerations, including enablers like increased data availability and improved computational hardware, and major application areas where semantics is or is likely to play a key role. In creating this survey, we hope to provide researchers across academia and industry with a comprehensive reference that helps facilitate future research in this exciting field.', 'title': 'Semantics for Robotic Mapping, Perception and Interaction: A Survey', 'embedding': []}, {'id': 14709, 'abstractText': 'Rayleigh wave amplitudes are the primary data set used for imaging shear attenuation in the upper mantle on a global scale. In addition to attenuation, surface-wave amplitudes are influenced by excitation at the earthquake source, focusing and scattering by elastic heterogeneity, and local structure at the receiver and the instrument response. The challenge of isolating the signal of attenuation from these other effects limits both the resolution of global attenuation models and the level of consistency between different global attenuation studies. While the source and receiver terms can be estimated using relatively simple approaches, focusing effects on amplitude are a large component of the amplitude signal and are sensitive to multiscale velocity anomalies. In this study we investigate how different theoretical treatments for focusing effects on Rayleigh wave amplitude influence the retrieved attenuation models. A new data set of fundamental-mode Rayleigh wave phase and amplitude at periods of 50 and 100 sis analysed. The amplitudes due to focusing effects are predicted using the great-circle ray approximation (GCRA), exact ray theory (ERT), and finite-frequency theory (FFT). Phase-velocity maps expanded to spherical-harmonic degree 20 and degree 40 are used for the predictions. After correction for focusing effects, the amplitude data are inverted for global attenuation maps and frequency-dependent source and receiver correction factors. The degree-12 attenuation maps, based on different corrections for focusing effects, all contain the same large-scale features, though the magnitude of the attenuation variations depends on the focusing correction. The variance reduction of the amplitudes strongly depends on the predicted focusing amplitudes, with the highest variance reduction for the ray-based approaches at 50\\ue251s and for FFT at 100\\ue251s. Although failure to account for focusing effects introduces artefacts into the attenuation models at higher spherical-harmonic degrees, the low-degree structure can be robustly retrieved. The new attenuation maps compare favourably with previous attenuation studies derived using independent amplitude data sets.', 'title': 'Effects of elastic focusing on global models of Rayleigh wave attenuation', 'embedding': []}, {'id': 14710, 'abstractText': 'Rock-magnetic and geochemical characteristics of three Vertisol profiles with different degree of textural differentiation have been studied. Thermomagnetic analyses, thermal demagnetization of laboratory remanences and acquisition of isothermal remanence curves are applied for identification of iron oxide mineralogy. The main magnetic minerals in Vertisols are ferrihydrite, single-domain magnetite, maghemite and hematite. Variations in magnetic susceptibility, anhysteretic remanent magnetization, isothermal remanent magnetization, as well as different ratios (Xarm/X, ARM/SIRM, S-ratio) along depth are studied. Concentration of magnetic minerals in Vertisols is low, influenced by the intense reductomorphic processes. The lowest magnetic susceptibility is found in the most texturally differentiated soil. However, rock-magnetic data suggest the presence of small, but well defined fraction of single domain-like magnetite with relatively wide grain-size distribution found in those parts of the profiles, which are subjected to most intense and frequent seasonal changes in oxidation-reduction conditions. It is suggested that this fraction is formed as a result of transformations of ferrihydrite under repeated cycles of anaerobic/aerobic conditions. Based on geochemical data, CALMAG weathering index was calculated for the three Vertisols. Using the established relation between CALMAG and mean annual precipitation (MAP), palaeo-MAP was evaluated for the studied profiles. The obtained MAP estimations fall in the range 1000–1200\\ue251mm and are much higher compared to contemporary precipitation in the area (MAP in the interval 540–770\\ue251mm). This finding confirms the relict character of Vertisols on Bulgarian territory and gives more information about the palaeoclimate during the initial stages of Vertisol formation.', 'title': 'Rock-magnetic and geochemical characteristics of relict Vertisols—signs of past climate and recent pedogenic development', 'embedding': []}, {'id': 14711, 'abstractText': 'Land Use and Land Cover (LULC) are the basic units of human activities and also serve as the significant factors to assess the climate change studies and environmental protection. Therefore, it is of significance to accurately and timely obtain the LULC maps on the earth’s surface, in particular for those regions where dramatic LULC changes are undergoing. Since 2017 April, a new state-level new area, Xiong’an New Area, was established in China, which would inevitably lead to some LULC changes in this region. In order to better understand what kinds of LULC change in this region with more details and further evaluate the potential impacts that the LULC changes will bring, this study makes full use of the 10-m multi-temporal Sentinel-2 images on the cloud-computing Google Earth Engine (GEE) and the powerful classification capability of Random Forest (RF) models, to generate the continuous LULC maps in Xiong’an New Area from 2017 to 2020. The derived LULC map for each year in Xiong’an New Area achieves high accuracy, with OA and Kappa values less than 0.9. Based on the obtained LULC maps, this study analyzed the spatial and temporal changes of LULC types in the last four years. It revealed that the estimated dry farmland has decreased from 2017 to 2018 and then kept almost stable, and the majority of it has converted to non-cropland, especially to the tree seedlings and landscape dual-use regions. The estimated impervious areas that were mainly composed of buildings and transport infrastructures first increased due to the newly constructed Xiong’an Station with some high-speed roads and then decreased from 2019 to 2020, mainly due to the relocation of some small villages. Accordingly, some building groups and aggregations were going to show in this region, and are expected to further developed in the future. Other LULC types such as lakes and forests have not changed dramatically, indicating that the environmental protection in Xiong’an New Area was, to date, satisfactory. The obtained 10-m and 4-year LULC maps in this study can provide some valuable information on the monitoring and understanding of what kinds of changes were and will be undergoing in Xiong’an New Area, and can also be used for evaluating the potential impacts and challenges such as environmental protection. Additionally, the utilization of GEE and the multi-temporal 10-m Sentinel-2 images can help to achieve LULC mapping in this region in an accurate and timely manner, which can be used in future studies.', 'title': 'The rapid Land Use and Land Cover change analysis using the Sentinel-2 images in Google Earth Engine: A Case Study of Xiong’an New Area from 2017 to 2020', 'embedding': []}, {'id': 14712, 'abstractText': \"In recent years, many support systems intended for elderly and disabled people have been developed. Among them, the study on the interface utilizing human gaze has been attracting attention. Human vision contains a lot of important information. Some studies that estimate a next action of operator intended to perform by using the visual information have been reported. Also we performed the research that recognizes the human instruction based on the operator's gaze point and controls the locomotion of wheelchair by paying attention to the motion and the characteristics of human eyes. As an example, the visual saliency map model is one of the means using to estimate the intention of the human gaze. This model enables to estimate the place directed human attention from the image. Also, this model has an advantage that uses only the image information. As the research dealing with the visual saliency map model, the study suppressing the changing value of saliency map by the movement of operator's head and the study controlling the weight of saliency map used a human knowledge as a value have been reported. However, there are few applications used the visual saliency map model by the movement of human eyes in real time because it is difficult to adjust it in the conventional studies. In this research, we focus on the combination of visual saliency map and gaze information, and extract a behavioral intention estimation map from the landscape and the movement of human eye. We propose a gaze instruction system using panoramic expansion image for controlling the locomotion of omni-directional wheelchair. In this system, the human intention by various input information is estimated by using fuzzy reasoning. To evaluate the effectiveness of this method, we carried out the comparison experiments for gaze instruction system by using only movement of human eyes.\", 'title': 'Gaze Instruction System Used Panoramic Expansion Image of Omnidirectional Camera', 'embedding': []}, {'id': 14713, 'abstractText': 'Synthetic aperture radar (SAR) can be used to obtain remote sensing images of different growth stages of crops under all weather conditions. Such time-series SAR images can provide an abundance of temporal and spatial features for use in large-scale crop mapping and analysis. In this study, we propose a temporal feature-based segmentation (TFBS) model for accurate crop mapping using time-series SAR images. This model first extracts deep-seated temporal features and then learns the spatial context of the extracted temporal features for crop mapping. The results indicate that the TFBS model significantly outperforms traditional long short-term memory (LSTM), U-network, and convolutional LSTM models in crop mapping based on time-series SAR images. TFBS demonstrates better generalizability than other models in the study area, which makes it more transferable, and the results show that data augmentation can significantly improve this generalizability. The visualization of the temporal features extracted by the TFBS shows that there is a high degree of intraclass homogeneity among rice fields and interclass heterogeneity between rice fields and other features. TFBS also achieved the highest accuracy of the four deep learning models for multicrop classification in the study area. This study presents a feasible way of producing high-accuracy large-scale crop maps based on the proposed model.', 'title': 'Semantic Segmentation Based on Temporal Features: Learning of Temporal-Spatial Information From Time-Series SAR Images for Paddy Rice Mapping', 'embedding': []}, {'id': 14714, 'abstractText': 'Map retrieval, the problem of similarity search among a large collection of 2D pointset maps previously built by mobile robots, is crucial for autonomous navigation in both indoor and outdoor environments. Unlike previous Bag-of-words scene model which lacks spatial layout information, we exploit a holistic local map descriptor which is view-independent and highly discriminative. The current study is inspired by our recent papers on “local map descriptor”, in which the viewpoint or the origin of local map coordinate is planned by scene parsing and the spatial layout of a local map is represented with respect to the planned viewpoint. The main contribution of this study is to build on the framework of local map descriptor and experimentally evaluate the effectiveness of the proposed method. The key observation is that accuracy of the spatial layout should be improved by introducing an accurate local mapping technique. To this end, we implemented the state-of-the-art particle filter-based mapping algorithm of Grid Mapping. The next contribution is that we conducted map retrieval experiments using a real mobile robot in feature-less office-like environments. We experimentally demonstrate the effectiveness of the proposed approach.', 'title': 'Combining Grid Mapping with local map descriptor for fast succinct map retrieval', 'embedding': []}, {'id': 14715, 'abstractText': \"The mapping of vegetation and Land Cover (LC) is important for research and for public policy planning but, in Brazil, although diverse maps exist there are few studies comparing them. The semiarid region of the Caatinga, in northeastern Brazil is an area long neglected by scientific research and its vegetation is diverse and relatively rich despite years of human occupation and very little preservation effort. In this study we make a comparison between the main maps made for the Caatinga from four different sources: IBGE (Brazilian Institute of Geography and Statistics), TCN (Third National Communication), ProBio (Project for Conservation and Sustainable Use of Biological Biodiversity) and MapBiomas. We also test these maps against well-known Land Cover maps from ESA and NASA: ESA's GlobCover and Climate Change Initiative (CCI) Land Cover, and NASA's MODIS MCD12Q1. This was done on a sample area where many of the Caatinga's vegetation physiognomies can be found, using well-established Difference metrics and the new SPAtial EFficiency (SPAEF) algorithm as they present complementary viewpoints to test the correspondence of mapped classes as well as that of their spatial patterns. Our results show considerable disagreement between the maps tested and their class semantics, with IBGE's and ProBio's being the most similar among all national maps and MapBiomas' the most closely related to global LC maps. The nature of the observed disagreement between these maps shows they diverge not only in the application of their classification systems, but also in their mapped spatial pattern, signaling the need for a better classification system and a better map of vegetation and land cover for the region.\", 'title': 'Classification System Drives Disagreement Among Brazilian Vegetation Maps at a Sample Area of the Semiarid Caatinga', 'embedding': []}, {'id': 14716, 'abstractText': \"The computational capabilities of today's smartphones make it possible to take advantage of mobile three-dimensional (3-D) maps to support navigation in the physical world. In particular, 3-D maps might be useful to facilitate indoor wayfinding in large and complex buildings, where the typical orientation cues (e.g., street names) and location tracking technologies that can be used outdoors are unavailable. The use of mobile 3-D maps for indoor wayfinding is still largely unexplored and research on how to best design such tools has been scarce to date. One overlooked but important design decision for 3-D maps concerns the perspective from which the map content should be displayed, with first-person and third-person perspectives being the two major options. This paper presents a user study involving wayfinding tasks in a large and complex building, comparing a mobile 3-D map with first-person perspective, a mobile 3-D map with third-person perspective, and a traditional mobile 2-D map. The first-person perspective shows the mobile 3-D map of the building from a floor-level egocentric point of view, whereas the third-person perspective shows the surroundings of the user from a fixed distance behind and above her position. Results of the study reveal that the mobile 3-D map with third-person perspective leads to shorter orientation time before walking, better clarity ratings, lower workload, mental demand and effort scores, and higher preference score compared to the mobile 3-D map with first-person perspective. Moreover, it leads to shorter orientation time before walking, better pleasantness ratings, lower mental demand scores, and higher preference score compared to the mobile 2-D map.\", 'title': 'Mobile Three-Dimensional Maps for Wayfinding in Large and Complex Buildings: Empirical Comparison of First-Person Versus Third-Person Perspective', 'embedding': []}, {'id': 14717, 'abstractText': 'This chapter provides a summary of the current research on urban impervious surface estimation and mapping. It focuses on the examination of sub‐pixel estimation techniques, including linear spectral mixture analysis (LSMA), artificial neural networks, and fuzzy classifiers. The chapter presents a case study to demonstrate the capability of two conventional methods (LSMA and multilayer perceptron) for impervious surface estimation using Hyperion imagery. Satellite remote sensing provides a cost‐effective and time‐efficient way for impervious surface mapping. Medium spatial resolution imagery has been utilized for large‐area mapping, and high spatial resolution imagery, air photos, and Light Detection and Ranging data for extracting urban features. Numerous methods have been developed and applied in previous studies based on per‐pixel, sub‐pixel, and object‐based algorithms. However, fewer studies have examined the spectral diversity of impervious surfaces. Hyperspectral imagery with rich spectral information is suitable for spectral analysis and should be extensively employed in future studies.', 'title': 'Estimation and Mapping of Impervious Surfaces', 'embedding': []}, {'id': 14718, 'abstractText': 'Simultaneous Localization and Mapping (SLAM) has seen a tremendous interest amongst research community in recent years due to its ability to make the robot truly independent in navigation. The capability of an autonomous robot to locate itself within the environment and construct a map at the same time, it is known as Simultaneous Localization and Mapping (SLAM). Visual Simultaneous Localization and Mapping (VSLAM) is when autonomous robot employs a vision sensor such a camera to explore the environment. Various researchers have embarked on the study of Visual Simultaneous Localization and Mapping (VSLAM) with excellent results, however the challenge of environmental noise such as light intensity still persist. In this study we propose a framework for Visual Simultaneous Localization and Mapping (VSLAM) that will address the challenge of light intensity in an environment in order to improve the performance of Visual Simultaneous Localization and Mapping (VSLAM) system. In executing of Visual Simultaneous Localization and Mapping (VSLAM) system, we have introduced a filtering algorithm to reduce or limit the effects of noise on images taken from the environment. The outcome of our study is a framework that will enable an autonomous robot to successfully navigate, localize itself and map the environment.', 'title': 'Framework for Visual Simultaneous Localization and Mapping in a Noisy Static Environment', 'embedding': []}, {'id': 14719, 'abstractText': 'Recently, Unmanned Aerial Vehicle (UAV), equipped with different high accurate remote sensing sensors acquiring various physical or geometric spatial information has many mature applications in different fields, such as environmental patrol, pollution monitoring, disaster prevention and relief, and mapping. High resolution imagery acquisition and spatial geometric information extraction is still the current major application for UAV. The visual representation of traditional cadastral maps is still limited in 2D drawings. It seems insufficient to provide intuitive understanding of the variety of problems in high density residential areas. Even though the 2D cadastral maps can be overlapped on traditional semi orthophoto, it still represents in two dimensional map. It is very hard to represent land legal boundary and physical information on imagery preciously. Because 2D cadastral maps and 3D image-based model are produced separately, it is also a very challenge issue for the land boundary line to follow the variation of terrain surface. This study propose a 3D cadastral map production and update technology using UAV-derived imagery. This study overlaps the cadastral map with true orthophoto using 3D visualization technique which change the way for visual inspection of traditional cadastral map. In this study, digitized land corner coordinates from UAV-derived true orthophoto are verified by field survey data. The accuracy of these land corner points is proved to fulfill the requirement of cadastral map accuracy.', 'title': 'Research on the production of 3D image cadastral map', 'embedding': []}, {'id': 14720, 'abstractText': 'Simultaneous Localization and Mapping (SLAM) has seen a tremendous interest amongst research community in recent years due to its ability to make the robot truly independent in navigation. The capability of an autonomous robot to locate itself within the environment and construct a map at the same time, it is known as Simultaneous Localization and Mapping (SLAM). They are various sensors that are employed in a Simultaneous Localization and Mapping (SLAM) which characterized either as a laser, sonar and vision sensor. Visual Simultaneous Localization and mapping (VSLAM) is when autonomous robot embedded with a vision sensor such as monocular, stereo vision, omnidirectional or Red Green Blue Depth (RGBD) camera to localize and map the environment. Numerous researchers have embarked on the study of Visual Simultaneous Localization and Mapping (VSLAM) with incredible results, however many challenges stills exist. The purpose of this paper is to review the work done by some of the researchers in Visual Simultaneous Localization and Mapping (VSLAM). We conducted a literature survey on several studies and outlined the frameworks, challenges and limitation of these studies. Open issues, challenges and future research in Visual Simultaneous Localization and Mapping (VSLAM) are also discussed.', 'title': 'A Review on Vision Simultaneous Localization and Mapping (VSLAM)', 'embedding': []}, {'id': 14721, 'abstractText': 'Ionospheric scintillation occurs mainly at high and low latitude regions of the Earth and may impose serious degradation on GNSS (Global Navigation Satellite System) functionality. The Brazilian territory sits on one of the most affected areas of the globe, where the ionosphere behaves very unpredictably, with strong scintillation frequently occurring in the local postsunset hours. The correlation between scintillation occurrence and sharp variations in the ionospheric total electron content (TEC) in Brazil is demonstrated in Spogli et al. (2013). The compounded effect of these associated ionospheric disturbances on long baseline GNSS kinematic positioning is studied in this paper, in particular when ionospheric maps are used to aid the positioning solution. The experiments have been conducted using data from GNSS reference stations in Brazil. The use of a regional TEC map generated under the CALIBRA (Countering GNSS high-Accuracy applications Limitations due to Ionospheric disturbances in BRAzil) project, referred to as CALIBRA TEC map (CTM), was compared to the use of the Global Ionosphere Map (GIM), provided by the International GNSS Service (IGS). Results show that the use of the CTM greatly improves the kinematic positioning solution as compared with that using the GIM, especially under disturbed ionospheric conditions. Additionally, different hypotheses were tested regarding the precision of the TEC values obtained from ionospheric maps, and its effect on the long baseline kinematic solution evaluated. Finally, this study compares two interpolation methods for ionospheric maps, namely, the Inverse Distance Weight and the Natural Neighbor.', 'title': 'Performance of ionospheric maps in support of long baseline GNSS kinematic positioning at low latitudes', 'embedding': []}, {'id': 14722, 'abstractText': 'We study the effect of baryonic processes on weak lensing (WL) observables with a suite of mock WL maps, the κTNG, based on the cosmological hydrodynamic simulations IllustrisTNG. We quantify the baryonic effects on the WL angular power spectrum, one-point probability distribution function (PDF), and number counts of peaks and minima. We also show the redshift evolution of the effects, which is a key to distinguish the effect of baryons from fundamental physics such as dark energy, dark matter, and massive neutrinos. We find that baryonic processes reduce the small-scale power, suppress the tails of the PDF, peak and minimum counts, and change the total number of peaks and minima. We compare our results to existing semi-analytical models and hydrodynamic simulations, and discuss the source of discrepancies. The κTNG suite includes 10\\u2009000 realizations of <tex>$5 \\\\times 5 \\\\, \\\\mathrm{deg}^2$</tex> maps for 40 source redshifts up to z<inf>s</inf> = 2.6, well covering the range of interest for existing and upcoming WL surveys. We also produce the κTNG-Dark suite of maps, generated based on the corresponding dark matter-only IllustrisTNG simulations. Our mock maps are not only suitable for developing analytical models that incorporate the effect of baryons, but also particularly useful for studies that rely on mass maps, such as non-Gaussian statistics and machine learning with convolutional neural networks. The suite of mock maps is publicly available at Columbia Lensing (http://columbialensing.org).', 'title': 'κTNG: effect of baryonic processes on weak lensing with IllustrisTNG simulations', 'embedding': []}, {'id': 14723, 'abstractText': 'Lithological mapping is important parameters for interpretation, identification and mapping of minerals. Lithological mapping in the study area defines the characteristics of nature of rock types and their association and formation. In this research study Landsat 8 OLI remote sensing data used for lithological mapping of Jahajpur region of Bhilwara super group. The task of lithological mapping completed by analogical and numerical analysis of digital image processing method which involves several digital image processing techniques such as natural band combination, false color composite, principal component analysis, band ratio and minimum noise fraction. The observed result is verified and validated by field survey and published geological survey of India geological map. The observe result have shown complete correlation and similarity with established map.', 'title': 'Lithological Mapping using Digital Image Processing Techniques on Landsat 8 OLI Remote Sensing Data in Jahajpur, Bhilwara, Rajasthan', 'embedding': []}, {'id': 14724, 'abstractText': \"Precision agriculture has been proposed to improve the sustainability of agriculture and solve the environmental pollution of soil. In precision agriculture process, the management of water and fertilizer is carried out on agricultural operation units. Therefore, acquisition of accurate soil nutrient distribution information is a key step for precision agriculture application and digital soil mapping is an effective technology. Significant progress has been made in digital soil mapping over the past 20 years. However, the current digital soil mapping framework was implemented based on grids, which was not consistent with the operation units of precision agriculture. This paper proposed a geo-parcel based digital soil mapping framework on the support of artificial intelligence technology for precision agriculture application. Two key technologies were studied for the implementation of this framework. Geo-parcels automatic extraction was the basis of this method, and a modified VGG 16 network was used for geo-parcels' accurate boundary extraction from high resolution images. Different machine learning methods were attempted to construct the relationship between soil available phosphorus and environment on geo-parcels. We chose an agricultural region in Zhongning County, Ningxia Province as the study area, and the new digital soil mapping framework was applied for soil available phosphorus mapping. This research showed that geo-parcel based digital mapping method could reduce the number of prediction units more than 50% for fine soil mapping, and effectively improve the prediction and application efficiency. This study was an attempt to realize soil mapping based on agricultural operation units for precision agriculture application. The high resolution remote sensing images provide basic data for the realization of this idea and the development of AI technology provides technical support for it. In the future, we will carry out experiments in larger areas to further optimize this method and key technologies for the applications in more complex environments.\", 'title': 'Digital Mapping of Soil Available Phosphorus Supported by AI Technology for Precision Agriculture', 'embedding': []}, {'id': 14725, 'abstractText': 'A timely and reliable inventory is essential for landslide hazard assessment and risk management. In this study, we use images from PlanetScope, which provides global 3 m daily Earth observations, for rapid mapping of landslide inventory. We propose a semi-automated method that combines change detection and region-based level set evolution (RLSE) to improve the landslide mapping efficiency. In particular, our approach uses the change detection methods of independent component analysis (ICA), principal component analysis (PCA) and change vector analysis (CVA) for automated generation of landslide zero-level curves (ZLCs), and then incorporates the RLSE method to refine the landslide mapping results. To corroborate the applicability of the proposed method, we test the landslide mapping performance on the Kodagu event (India, 2018) using ICA-, PCA- and CVA-based RLSE. The results show that ICA-based RLSE can achieve better landslide mapping accuracy in terms of completeness, correctness, and the Kappa coefficient. This study demonstrates the suitability and potential of low-orbit miniature satellites such as PlanetScope for landslide mapping. To the best of our knowledge, it is the first attempt to incorporate PlanetScope images and the change detection-based RLSE method for landslide mapping.', 'title': 'Landslide Mapping from PlanetScope Images Using Improved Region-based Level Set Evolution', 'embedding': []}, {'id': 14726, 'abstractText': 'The mining of software repositories has provided significant advances in a multitude of software engineering fields, including defect prediction. Several studies show that the performance of a software engineering technology (e.g., prediction model) differs across different project repositories. Thus, it is important that the project selection is replicable. The aim of this paper is to present STRESS, a semi-automated and fully replicable approach that allows researchers to select projects by configuring the desired level of diversity, fit, and quality. STRESS records the rationale behind the researcher decisions and allows different users to re-run or modify such decisions. STRESS is open-source and it can be used used locally or even online (www.falessi.com/STRESS/). We perform a systematic mapping study that considers studies that analyzed projects managed with JIRA and Git to asses the project selection replicability of past studies. We validate the feasible application of STRESS in realistic research scenarios by applying STRESS to select projects among the 211 Apache Software Foundation projects. Our systematic mapping study results show that none of the 68 analyzed studies is completely replicable. Regarding STRESS, it successfully supported the project selection among all 211 ASF projects. It also supported the measurement of 100 projects characteristics, including the 32 criteria of the studies analyzed in our mapping study. The mapping study and STRESS are, to our best knowledge, the first attempt to investigate and support the replicability of project selection. We plan to extend them to other technologies such as GitHub.', 'title': 'STRESS: A Semi-Automated, Fully Replicable Approach for Project Selection', 'embedding': []}, {'id': 14727, 'abstractText': 'The concept of eServices originated in the early 2000s in the field of business and commerce. However, in recent years, eServices are being applied in many domains. Therefore, a thorough study on eServices is required to identify the areas in which eServices have been applied till date and to what extent. The main objective of this research is to perform a mapping study to provide an extensive review, gather trends, and identify the state of the art in the research on eServices to answer the research questions designed to conduct this research. A mapping study has been conducted employing an automatic search in digital repositories by developing a mapping protocol. Mapping studies are useful for categorizing and classifying the existing information concerning a particular research question in an unbiased manner. The search procedure identified 806 studies of which 318 were selected for full analysis during the years 2000 and 2016 in the field of computer science. No study was published before this time period. Research on eServices were recorded and classified into tabulated spread sheets, and finally analyzed. According to the study, the range of eService service and application domains is quite wide. Most studies conducted have focused on eService composition and eService Adoption. However, the most common application domains identified were eGovernment, eBusiness, eHealth, and eLearning. The study findings show that the research on eService composition, design, provision, and adoption is increasing with the passage of time. The literature not only discusses various domains of eServices but also provides the in-depth classification, review and trend of eService studies over time.', 'title': 'eServices Classification, Trends, and Analysis: A Systematic Mapping Study', 'embedding': []}, {'id': 14728, 'abstractText': 'This paper addresses on an autonomous exploration algorithm of unknown environment for mapping. An autonomous indoor carrying robot requires a kind of map as knowledge of the work space in order to efficiently execute the navigation task. SLAM (Simultaneously Localization and Mapping) is widely used as an generating a map. However, the map is based on the premise that humans operate robots equipped with sensors to generate the map. This is a burdensome task for the operator. Thus, an exploration method in an unknown environment for autonomously generating a map has been studied for decades. The main method is frontier-based exploration [1] which performs autonomous exploration using local map information. The algorithm is useful approach for the mapping unknown environments, on the other hand, the method has the following problem for applying to mapping method in a wide environment like a factory; since the map information which is being gradually extended is used, the calculation cost increases in proportion to the time [2]. From this problem, when the calculation time of the map information becomes long, the exploring efficiency will be deteriorated. Thus, it takes the time to generate the map. For more efficient exploration, an autonomous exploration algorithm using only infrared sensor and odometry information of a robot is proposed as a sensor-based exploration approach without using map information. The proposed method requires only a depth sensor and camera for which the Kinect is used as the equipment and one mobile robot. Turtlebot2 with Kinect is used as a wheeled mobile robot, and the effectiveness of the proposed method will be confirmed by comparing with frontier-based exploration and proposed method by indoor experiments.', 'title': 'A sensor-based exploration algorithm for autonomous map generation on mobile robot using kinect', 'embedding': []}, {'id': 14729, 'abstractText': 'Pathfinding is a fundamental problem for many areas, e.g., robotics, automation, computer-aided design, and computer graphics. Although outdoor pathfinding is fledged, indoor pathfinding remains a challenge due to the lack of indoor maps. Currently, some efforts have utilized building information modeling (BIM) to generate either the grid-based map or the topological map. However, either the grid-based map or the topological map is not sufficient to provide accurate and efficient pathfinding service. This article proposes a novel grid-topological map and develops an accurate and efficient indoor pathfinding scheme based on BIM. The grid-topological map is modeled jointly adopting the advantages of both the grid-based map and the topological map. First, the grid-based map is generated using the BIM data by extracting and mapping geometric and semantic data into planar grids. Second, a grid thinning algorithm is proposed to produce the topological map directly using the grid-based map. Third, a grid-topological map is presented by combining both the grid-based map and topological map. On top of the grid-topological map, an accurate and efficient pathfinding algorithm is developed. Empirical studies proved the effectiveness of the grid-topological map, as well as the accuracy and efficiency of the proposed indoor pathfinding algorithm.', 'title': 'Accurate and Efficient Indoor Pathfinding Based on Building Information Modeling Data', 'embedding': []}, {'id': 14730, 'abstractText': 'The rapid development of industrialized agriculture has leads to the problems of soil pollution and water pollution. In order to solve these problems, precision agriculture (PA) has been applied to achieve precise management of agricultural water and fertilizer. In PA process, fine mapping of soil nutrient is an effective technology to acquire accurate water and fertilizer distribution information and make agricultural decision. A significant progress has been made in digital soil mapping (DSM) of soil nutrient content over the past 20 years. However, the accuracy of grid-based DSM cannot meet the practical application needs of PA. This paper proposed a fine DSM method of soil nutrient content using high resolution remote sensing images and multi-scale auxiliary data for PA application. Three key technologies were studied for the implementation of this method. The automatic extraction of fine mapping units was the basis of this method. We designed different automatic extraction methods based on high resolution remote sensing images for agricultural production units in plains and mountainous areas. The auxiliary variables in different scales were chosen and converted to construct fine-scale soil nutrient-environment relationship model. Finally, machine learning methods were used to map the spatial distribution of soil nutrients. We chose Zhongning County, Ningxia Province as the study area, which includes typical plain and mountainous agriculture. The proposed method and technologies were applied for typical soil nutrients mapping. A common grid-based spatial interpolation method was implemented with the same soil sample dataset to evaluate the effect of the proposed method. The result showed that this method could reduce the number of prediction units and effectively improve the prediction efficiency in both plain and mountainous areas for fine soil mapping and precision agriculture application. This study was an attempt to realize fine soil mapping based on PA application unit in different environments. The high-resolution remote sensing images provide basic data for the realization of this idea, and the conversion technology of multi-scale data provides better support for the spatial inference of fine soil attribute information. In the future, we will carry out experiments in larger areas to further improve the efficiency of application, and plan to expand this study to consider three-dimensional soil property prediction.', 'title': 'Fine mapping of key soil nutrient content using high resolution remote sensing image to support precision agriculture in Northwest China', 'embedding': []}, {'id': 14731, 'abstractText': 'Due to change in global mean temperature, most Himalayan glaciers have shown retreat, resulting in an increase in the number and size of glacial lakes, which may give rise to glacial lake outburst flood event. These glacial lakes typically grow from small supraglacial lakes (SGLs). Therefore, it is important to map and monitor the SGLs on a regular basis. Most of the studies have utilized medium to coarse resolution images for the extraction of glacial lakes. With the availability of high spatial resolution data, it has also become possible to map small glacial lakes. The literature suggests some studies on the use of high resolution data to map glacial lakes. However, the extraction is majorly based on simple to apply pixel-based classification methods, which results into high misclassification thereby increasing the task of manual post processing. In this study, we used a novel approach, known as object-based image analysis (OBIA), for the mapping of SGLs. A new index has also been proposed for classification of SGLs. As a case study, the SGLs of Gangotri glacier (Uttarakhand Himalayas) have been mapped from the high spatial resolution data of LISS-IV using the proposed OBIA approach. The results have been compared with those obtained from object-based normalized difference water index (NDWI) and pixel-based mapping methods. The validation of the SGLs boundaries has been carried out with respect to the manually digitized database of SGLs. A significant increase in accuracy of the mapping has been observed over the benchmarked traditional methods.', 'title': 'Extraction of Glacial Lakes in Gangotri Glacier Using Object-Based Image Analysis', 'embedding': []}, {'id': 14732, 'abstractText': 'Mangrove forests constitute an important coastal ecosystem, which provides valuable ecosystem services such as coastal erosion protection, water filtration and shelters for a wide variety of plants and animals. Remote sensing satellite imagery provides valuable information for mangrove mapping and monitoring. In this study, we use high resolution images from SPOT-5, Landsat-7 and Landsat-8 satellites to perform change detection on an area of interest that covers the mangrove forests at the Ramsar sites of Pulau Kukup, Tanjung Piai and Sungai Pulai in the southwestern part of Johor State of Malaysia. Land use/land cover maps and the mangrove change map are generated to identify the mangrove distribution and temporal variation between 2000 and 2016. A hierarchical object-based classification method was used to produce the land use/land cover map in 2000, 2005, 2013 and 2016. The WorldView2 data were also used for validation of the classification results. This study presented an approach for mangrove mapping and change detection analysis. The map provides an up to date information for the study area and can be used for future comparative study.', 'title': 'Mangrove mapping and change detection using satellite imagery', 'embedding': []}, {'id': 14733, 'abstractText': \"This Research Full Paper presents a study using brain monitoring technology to measure and compare engineering students' performance on complex tasks that require systems thinking and connecting knowledge from different domains. The study used an electroencephalograph (EEG) and self-report data to investigate students' cognitive load and performance when completing concept mapping and listing tasks related to complex issues like food security and water availability. The study was designed to test two hypotheses: first, concept maps allow individuals to organize their thoughts using a networked or systems thinking framework, and thus will result in a more complete and holistic response than listing tasks; and second, creating a concept map is a more complex cognitive process and thus students will experience greater cognitive load during concept mapping tasks than listing tasks. Twenty-seven students at a mid-size public university participated in the study, which is an adequate size for EEG data analysis. For each participant, over forty pieces of data were recorded, including: demographic data, responses to the Revised Systems Thinking Scale, order effects, EEG performance variables, NASA-TLX scores, listing task metrics, and concept map scores. The paper presents and discusses quantitative results related to three questions: (1) do students perform better on listing or concept mapping tasks? (2) do students exert more mental effort (cognitive load) for listing or concept mapping? (3) how did performance compare across different direct and self-report measures? The ultimate goal of this research is to create learning approaches that enhance students' cognitive resources to meet and exceed the requirements of working within the sustainable design paradigm. More broadly, we expect that using neuroeducation measures to triangulate results with other qualitative and quantitative assessments could provide powerful evidence for the effectiveness of different learning interventions aimed at improving applications of engineering knowledge.\", 'title': 'Measuring connections: Engineering students’ cognitive activities and performance on complex tasks', 'embedding': []}, {'id': 14734, 'abstractText': 'More than 3800 landslide locations have been reported in the area near the Three Gorges Reservoir (TGR) along the Yangtze River in China, which poses a serious threat to the socioeconomic stability of the region. An efficient and accurate method of generating landslide susceptibility maps is very important to mitigate the loss of lives and properties caused by these landslides. In this paper, a deep belief network (DBN) with datasets developed via a geographic information system (GIS) and remotely sensed data was used to create a landslide spatial susceptibility map for the TGR region on the Yangtze River in Zigui County. The landslide inventory map was initially constructed using field surveys, aerial photographs, and a literature search of historical landslide records. The twelve causative factors were evaluated in different ways: elevation, topographic slope, topographic aspect, curvature, distance from drainage, distance from road, schedule performance index and topographic wetness index were derived from a digital topographical map at 1:10,000 scale; engineering petrofabric, slope structure and distance from faults were obtained from a geological map at 1: 50,000 scale; normalized difference vegetation index were generated from CBERS (China-Brazil Earth Resources Satellite) data. All the causative factors were resampled to a grid cell size of 10 * 10 m by using the grid analysis function of ArcGIS software. Initially, 30% of the landslides pixels and equal number of non-landslides pixels were randomly selected as training data. Then these twelve factors were used as the input to DBN. By integrating the twelve factor maps in the GIS via pixel-based computing, the landslide spatial susceptibility map was obtained. Then the study area was reclassified into four categories of landslide susceptibility: high, moderate, low, and very low by using natural breaks classification methods. Approximately 13.6% of the study area was identified as severe susceptibility, and moderate, low, very low susceptibility zones covered 1.3%, 1.4%, and 83.7% of the study area, respectively. To show the performance of the DBN, the results were then compared with a logistic regression model (LRM) by using the receiver operating characteristics (ROC). The area under the ROC curve was 0.949 for the DBN and 0.859 for the LRM. The results showed that, the DBN proposed in this study outperforms the LSM. Based on the efficiency and accuracy of DBN, the proposed approach can be employed for rapid response to natural hazards in the Three Gorges area.', 'title': 'Landslide spatial susceptibility mapping by using deep belief network', 'embedding': []}, {'id': 14735, 'abstractText': \"Deep Learning (DL), a subset of Artificial Intelligence (AI), is growing rapidly with possible applications in different domains such as speech recognition, computer vision etc. Deep Neural Network (DNN), the backbone of DL algorithms is a directed graph containing multiple layers with different number of neurons residing in each layer. The use of these networks has been increased in the last few years due to availability of large data sets and huge computation power. As the size of DNN is growing over the years, researchers have developed specialized hardware accelerators to reduce the inference compute time. An example of such domain specific architecture designed for Neural Network acceleration is Tensor Processing Unit (TPU) which outperforms GPU in the inference stage of DNN execution. The heart of this inference engine is a Matrix Multiplication unit which is based on systolic array architecture. The TPU's systolic array is a grid-like structure made of individual processing elements that can be extended along rows and columns. Due to external environmental factors or internal scaling of semiconductor, these systems are often prone to faults which leads to improper calculations and thereby resulting in inaccurate decisions by the DNN. Although a lot of work has been done in the past on the computing array implementation and it's reliability concerns, their fault tolerance behavior for DNN application is not very well understood. It is not even clear what would be the impact of various different faults on the accuracy. We in this work, first study possible mapping strategies to implement a convolution and dense layer weights on TPU systolic array. Next we consider various faults scenarios that may occur in the array. We divide these fault scenarios into low, high row and column faults (Fig. 1(a) pictorially represents column faults) modes with respect to the multiplication unit. Next, we study the impact of these fault models on the overall accuracy of the DNN performance on a faculty TPU unit. The goal is to study the resiliency and overcome the limitations of earlier work. The previous work was very effective in masking the random faults which used pruning of weights (removing weights or connections in the DNN) plus retraining to mask the faults on the array. However, it failed in the case of column faults which is clearly shown in Fig. 1(b). We also propose techniques to mitigate or bypass the row and column faults. Our mapping strategy follows physical_x(i) = i%N and physical_y(j) = j%N where (i,j) represents the index of dense (FC) weight matrix and (physical x(i), physical y(j)) indicates the actual physical location on the array of size N. The convolution filters are linearized with respect to every channel so as to convert them into proper weight matrix and mapped according to the previous mentioned policy. It was shown that DNNs can up to certain faults in the array while retaining the original accuracy (low row faults). The accuracy of the network decreases even with one column faults if it (column) is in the use. As per the results, it is proved that for the same number of row and column faults, the latter has most impact on the network accuracy because pruning input neuron has very little effect than pruning an output neuron. We experimented with three different networks and found the influence of these different faults to be the same. These faults can be mitigated using techniques like Matrix Transpose and Array Reduction which does not require retraining of weights. For low row faults, the original mapping policy can be retained such that weights can be mapped at their exact locations which does not affect the accuracy. Low column faults can be converted into low row faults by transposing the matrix. In the case of high row (column) faults, the entire row (column) has to be avoided to completely bypass the faulty locations. Static mapping of weights along with retraining the network on the array can be effective in the case of random faults. Adapting to change in the case of structured faults can reduce the burden of retraining which happens outside the TPU.\", 'title': 'Impact of Structural Faults on Neural Network Performance', 'embedding': []}, {'id': 14736, 'abstractText': 'Context: A tertiary study can be performed to identify related reviews on a topic of interest. However, the elaboration of an appropriate and effective search string to detect secondary studies is challenging for Software Engineering (SE) researchers. Objective: The main goal of this study is to propose a suitable search string to detect secondary studies in SE, addressing issues such as the quantity of applied terms, relevance, recall and precision. Method: We analyzed seven tertiary studies under two perspectives: (1) structure – strings’ terms to detect secondary studies; and (2) field: where searching – titles alone or abstracts alone or titles and abstracts together, among others. We validate our string by performing a twostep validation process. Firstly, we evaluated the capability to retrieve secondary studies over a set of 1537 secondary studies included in 24 tertiary studies in SE. Secondly, we evaluated the general capacity of retrieving secondary studies over an automated search using the Scopus digital library. Results: Our string was capable to retrieve an optimum value of over 90% of the included secondary studies (recall) with a high general precision of almost 60%. Conclusion: The suitable search string for finding secondary studies in SE contains the terms “systematic review”, “literature review”, “systematic mapping”, “mapping study” and “systematic map”.', 'title': 'Establishing a Search String to Detect Secondary Studies in Software Engineering', 'embedding': []}, {'id': 14737, 'abstractText': 'Maps of the large-scale structure of the Universe at redshifts 2–4 can be made with the Lyman α forest which are complementary to low-redshift galaxy surveys. We apply the Wiener interpolation method of Caucci et\\xa0al. to construct three-dimensional maps from sets of Lyman α forest spectra taken from cosmological hydrodynamic simulations. We mimic some current and future quasar redshift surveys [Baryon Oscillation Spectroscopic Survey (BOSS), extended BOSS (eBOSS) and Mid-Scale Dark Energy Spectroscopic Instrument (MS-DESI)] by choosing similar sightline densities. We use these appropriate subsets of the Lyman α absorption sightlines to reconstruct the full three-dimensional Lyman α flux field and perform comparisons between the true and the reconstructed fields. We study global statistical properties of the intergalactic medium (IGM) maps with autocorrelation and cross-correlation analysis, slice plots, local peaks and point-by-point scatter. We find that both the density field and the statistical properties of the IGM are recovered well enough that the resulting IGM maps can be meaningfully considered to represent large-scale maps of the Universe in agreement with Caucci et\\xa0al., on larger scales and for sparser sightlines than had been tested previously. Quantitatively, for sightline parameters comparable to current and near future surveys the correlation coefficient between true and reconstructed fields is r &gt; 0.9 on scales &gt;30\\u2009h<sup>−1</sup>\\u2009Mpc. The properties of the maps are relatively insensitive to the precise form of the covariance matrix used. The final BOSS quasar Lyman α forest sample will allow maps to be made with a resolution of ∼30\\u2009h<sup>−1</sup>\\u2009Mpc over a volume of ∼15\\u2009h<sup>−3</sup>\\u2009Gpc<sup>3</sup> between redshifts 1.9 and 2.3.', 'title': 'Large-scale 3D mapping of the intergalactic medium using the Lyman α forest', 'embedding': []}, {'id': 14738, 'abstractText': 'Aiming at the problem that the navigation map operation time occupies more resources in the embedded system. In this paper, by studying the digital map of MIF format, the corresponding map is drawn and the map is optimized. Using the GPS module to locate and use the Dijiestra algorithm to achieve the shortest path of the navigation module design, the use of the map database to remove the corresponding area of the data mapping map information, in a translation to a certain distance when the scene clear the data, Draw a map. So there will be no data left in the view. Since the Dijiestra algorithm is based on the graph, it is necessary to link the roads in the area to create an undirected graph to find the shortest path to the end point. If the point of the line as an undirected node, the composition of the undirected map will be very large. And the use of the line as an undirected node, you can reduce the number of nodes without the map, to improve the efficiency of Dijie Stella algorithm traversal. Based on the open system platform and data standard, this paper designs and develops a secure, stable and low cost embedded GPS navigation and navigation system. The experimental results are optimized.', 'title': 'QT-based car navigation map realization and optimization', 'embedding': []}, {'id': 14739, 'abstractText': 'With the dawn of 5G network, a new set of requirements for site spectrum monitoring, cellular planning are emerging, all of which are relying on fine-grained signal map. Although with significant importance, the traditional signal map construction could be time-consuming and labor-intensive. The state-of-the arts usually employ crowdsourcing scheme and matrix completion algorithm to solve the dilemma. However, the crowdsourcing scheme usually suffers from uneven distributed and inadequate participants. To this end, in this paper, we study how to effectively reconstruct and update the signal map in the case of partially measured signal maps with smaller cost and propose a GAN-based active signal map reconstruction method (GSMAC). Our method is mainly innovative in two parts: GSMC, GAN-based signal map construction, and ACS, an active crowdsourcing scheme. Specifically, GSMC can effectively update the signal map with only a small number of observations while updating the signal map online. ACS consists of a reinforce learning-based active query mechanism which quantitatively evaluates the most valuable measurement site for reconstruction. The simulation results and real implemented data-driven experiments demonstrate the advantages and effectiveness of our approach in both accuracy and cost.', 'title': 'GSMAC: GAN-based Signal Map Construction with Active Crowdsourcing', 'embedding': []}, {'id': 14740, 'abstractText': 'With the use of deep learning, object detection has achieved great breakthroughs. However, existing object detection methods still can not cope with challenging environments, such as dense objects, small objects, and object scale variations. To address these issues, this paper proposes a novel keypoint-based detection framework, called CrossNet, which significantly improves detection performance with minimal costs. In our approach, an object is modeled as a cross that consists of a center keypoint and a specific size, which eliminates the need of hand-craft anchor design. The proposed CrossNet outputs three types of maps: the center map, size map, and offset map, where both center map and offset map are to predict the center keypoints of objects and the size map is to estimate the sizes (width and height) of objects. Specifically, we first design a cascaded center prediction method that introduces a coarse-to-fine idea to improve center prediction. Furthermore, since center prediction considered as a classification task is easier than size regression relatively, we design a center-attention size regression module that uses the detection results of centers to assist the size prediction. In addition, a slightly modified hourglass network is designed to enhance the quality of feature maps for center and size prediction. Extensive experiments are conducted to demonstrate the effectiveness of CrossNet on the challenging PASCAL VOC, COCO, KITTI, and WiderFace datasets. Empirical studies show that CrossNet achieves competitive results with top-ranked one-stage and two-stage detectors while being time-efficient.', 'title': 'CrossNet: Detecting Objects as Crosses', 'embedding': []}, {'id': 14741, 'abstractText': 'Ontology Matching is a process to find correspondences between semantically related entities of two ontologies. Most matching systems do evaluation by comparing the correspondences with reference alignment. Since 2010 another method has been used to measure a logic-based of correspondence or mapping, called incoherent mapping measurement. The more incoherent of the mapping the lower quality of mapping. Incoherent mapping repair will restore the incoherent to coherent condition in mapping, by removing unwanted mapping. The process of removing unwanted mapping to restore the coherent condition is called diagnosis process. Since mappings are very important sources to support data integration and exchange, then diagnosis should be done as minimal as possible. We propose two focuses minimal using global new technique to repair the incoherent mapping. This approach should (1) ensure minimal impact on the input alignment by minimizing the number of mapping removed; and (2) minimize the average of confidence values of the mapping removed. The next study about minimal diagnosis is finding the right method to implement the two focuses minimal with global new techniques in the real world.', 'title': 'Diagnosis process with two focuses minimal in incoherent mapping repair', 'embedding': []}, {'id': 14742, 'abstractText': 'Super-resolution Reconstruction (SRR) is technique to increase the spatial resolution of images. It is especially useful for hyperspectral images (HSI), which have good spectral resolution but low spatial resolution. In this study, we propose an improvement to our previous work and present a novel MAP-MRF (maximum a posteriori-Markov random Fields) based approach for the SRR of HSI. The key point of our approach is to find the abundance maps of an HSI and perform SRR on the abundance maps using MRF based energy minimization, without needing any other additional source of information. In order to do so, first, PCA is used to determine the endmembers. Second, SISAL and fully constraint least squares (FCLS) are used to estimate the abundance maps. Third, in order to find the high resolution abundance maps, the ill-posed inverse SRR problem for abundances is regularized with a MAP-MRF based approach. The MAP-MRF formulation is restricted with the constraints which are specific to the abundances. Using the non-linear programming (NLP) techniques, the convex MAP formulation is minimized and High Resolution (HR) abundance maps are obtained. Then, these maps are used to construct the HR HSI. This improved SRR method is verified on real data sets, and quantitative performance comparison is achieved using PSNR, SSIM and PSNR metrics. Our results indicate that this improved method gives very close results to the original high resolution images, keeps the spectral consistency, and performs better than the compared algorithms.', 'title': 'Super-resolution Reconstruction of hyperspectral images via an improved MAP-based approach', 'embedding': []}, {'id': 14743, 'abstractText': \"At the Fukushima Daiichi Nuclear Power Station (NPS), robots are used to propel the decommissioning work. Creating a 3D map of the internal environment of the decommissioning work is a necessary technology for improving the working efficiency of a decommissioning robot. The purpose of this research is to realize a 3D map creating a system using a camera by operating the humanoid general-purpose robot Pepper using the Robot Operating System (ROS) and implementing and executing Visual SLAM. The system aims to be applied to a decommissioning robot in the future. In this study, we implemented Visual SLAM methods R-tab Map, which is a method for constructing 3D maps in real-time, and Large-Scale Direct SLAM, which is method that generates a map using luminance with a large gradient between frames without using features for map generation. We also compared and evaluated the effectiveness of the generated maps. Besides, the robot was manually operated using nao_teleop, which can operate Pepper from the ROS library with the PS3 controller, the visualization software rviz, and the point cloud visualization library PointCloudViewer. Using two Visual SLAM's methods, we implemented the experiment. During the experiment, a chair was placed in front of Pepper as an obstacle. In this environment, Pepper was rotated 360 [deg] by manual operation. However, nao_teleop and Visual SLAM methods caused a conflict between the manual operation of Pepper and the 3D map generation process, and both processes stopped. Therefore, the 3D map wasn't generated. To resolve the conflict with operation, Pepper was moved by human and the experiment was again. As a result, the camera image obtained from Pepper was distorted, the map could not be optimized. Therefore, the part of 3D map was only generated. For this reason, to realize a 3D map generation system, it was important to properly calibrate the camera that could acquire a flat image without distortion.\", 'title': '3D Map Generation for Decommissioning Work', 'embedding': []}, {'id': 14744, 'abstractText': \"Recently, autonomous vehicle of self-localization based on the high definition (HD) map become more popular due to the availability of HD map and the dropping prices of LIDAR. Many types of studies have improved the local and global accuracy of HD map for accurate localization. However, the global accuracy of a map does not guarantee the accurate self-localization within the map. In this paper, by investigating the scene errors in the map and comparing their characteristics, we introduced four factors that affect the self-localization ability of the map. These factors are highly related to the environment, namely feature sufficiency, layout, local similarity, and representation quality of the map. Then, in order to overcome the limitations brought by the map itself, we proposed a multi-sensor information fusion self-localization system. The system uses GNSS, LIDAR, IMU, and local map to fuse multi-source information. Global mapping is not required for autonomous vehicle's accurate self-localization. To achieve a high precision of global positioning, the system uses Kalman filter to integrate GNSS positioning, slam and inertial navigation solution to improve the robustness of the system, and the method of local map matching is used to eliminate the accumulated error and modify the positioning system. By conducting the experiments in a typical testing environment, we have evaluated the performance of this system by comparing the ground truth with the self-localization error.\", 'title': 'Robust self-localization system based on multi-sensor information fusion in city environments', 'embedding': []}, {'id': 14745, 'abstractText': 'Mapping human hand motion to robotic hands has great significance in a wide range of applications, such as teleoperation and imitation learning. The ultimate goal is to develop a device-independent control solution based on human hand synergies. Over the past twenty years, a considerable number of mapping methods have been proposed, but most of the mapping methods use intrusive devices, such as the CyberGlove data gloves, to capture human hand motion. Until recently, a very small number of mapping methods have been proposed based on vision-based human hand pose estimation. Traditionally, mapping methods and vision-based human hand pose estimation have been studied independently. To the best of our knowledge, no review has been conducted to summarize the achievements on haptic mapping methods or explore the feasibility of applying off-the-shelf human hand pose estimation algorithms to teleoperation. To address this literature gap, we present the first survey on mapping human hand motion to robotic hands from a kinematic and algorithmic perspective. We discuss the realistic challenges, intuitively summarize recent mapping methods, analyze the theoretical solutions, and provide a teleoperation-oriented human hand pose estimation overview. As a preliminary exploration, a vision-based human hand pose estimation algorithm is introduced for robotic hand teleoperation.', 'title': 'Survey on Mapping Human Hand Motion to Robotic Hands for Teleoperation', 'embedding': []}, {'id': 14746, 'abstractText': 'Before the digital measurement technology has been applied, various maps measurement results were use graphical method. Among them, the number of cadastral maps and topographic maps is mostly. Cadastral maps related to peoples rights, topographic map is used by engineering design. Both must be kept as the basis for data traceability. At present, most of these maps are digitally kept by scanning. However, there are different sizes of maps that need different sizes scanners. Image distortion may occur during scanning, causing misjudgment of subsequent map data.In recent years, processing paper data into true orthophoto through multi-view image processing technology, the result of obtaining consistency with paper datas and maps has been verified and feasible. However, the verification method is too simple. It is quite inconvenient for the map datas that need to be processed in large quantities. This study will design a simple and convenient photography platform, through Smooth moving slide rail system and height-adjustable shooting base, can quickly capture fixed-position results for image processing. Expand the number of experiments and the type of maps in the verification process. Research results show that this new photography platform can effectively speed up processing and expand the multi-size maps. There is considerable help in the subsequent preservation of cadastral data.', 'title': 'Innovative Design on Photography Platform for the Digitized Data of Original Cadastral Maps', 'embedding': []}, {'id': 14747, 'abstractText': \"The Simultaneous Localization and Mapping (SLAM) is the process of building a map of an environment with an unknown topography by a mobile robot. The purpose of this paper is to build a mapping of an unknown environment by the mobile robot which we designed through the help of sensor fusion algorithms we have established. The mobile robot performs its mapping process by using the combination of ultrasonic, optical encoder and IMU sensors. Determining the position of the obstacles and its own location, for the mobile robot, is the core of this study. Inertial and rotational sensors are utilized to calculate the distance and position of the mobile robot. Due to low cost, the ultrasonic sensor is used instead of a Lidar laser, and the real-like results were provided. In this study, the robot's direction and movement is performed by an algorithm developed on the Raspberry Pi processor. This algorithm controls the movement of the wheels with the information received from the optical encoder and protractor. The data received from the gyroscope and the accelerometer is very affected from many external factors such as vibrational motion and the noise, eventhough, we used moving average filter and complementary filter to reduce the effect of the noise and measurement error problems. However, they still produce faulty results when calculating distance values. Therefore, the distance computation is carried out by using optical encoder instead of the accelerometer. The algorithm of the distance computation is written in Python programming language. In this study, it is established that the comparative usage of several detectors provide more accurate results. At the same time, the system is quite efficiently developed by using open structure software (Raspberry Pi, Linux etc.) and writing authentic libraries. The robot's coordinate information are combined under simulation medium by using Pygame library and by computing the coordinates of its location and the coordinates of the objects' locations it detects during its navigation. The mobile robot executes its mapping process according to these data derived. Also, the effects of margin of error in the information obtained during the comparable usage of detectors are studied within the scope of this study.\", 'title': 'Effective sensor fusion of a mobile robot for SLAM implementation', 'embedding': []}, {'id': 14748, 'abstractText': 'Minkowski functionals (MF) are excellent tools to investigate the statistical properties of the cosmic background radiation (CMB) maps. Between their notorious advantages is the possibility to use them efficiently in patches of the CMB sphere, which allow studies in masked skies, inclusive analyses of small sky regions. Then, possible deviations from Gaussianity are investigated by comparison with MF obtained from a set of Gaussian isotropic simulated CMB maps to which are applied the same cut-sky masks. These analyses are sensitive enough to detect contaminations of small intensity like primary and secondary CMB anisotropies. Our methodology uses the MF, widely employed to study non-Gaussianities in CMB data, and asserts Gaussian deviations only when all of them points out an exceptional χ<sup>2</sup> value, at more than 2.2σ confidence level, in a given sky patch. Following this rigorous procedure, we find 13 regions in the foreground-cleaned Planck maps that evince such high levels of non-Gaussian deviations. According to our results, these non-Gaussian contributions show signatures that can be associated to the presence of hot or cold spots in such regions. Moreover, some of these non-Gaussian deviations signals suggest the presence of foreground residuals in those regions located near the Galactic plane. Additionally, we confirm that most of the regions revealed in our analyses, but not all, have been recently reported in studies done by the Planck collaboration. Furthermore, we also investigate whether these non-Gaussian deviations can be possibly sourced by systematics, like inhomogeneous noise and beam effect in the released Planck data, or perhaps due to residual Galactic foregrounds.', 'title': 'Local analyses of Planck maps with Minkowski functionals', 'embedding': []}, {'id': 14749, 'abstractText': \"This Research to Practice full paper reviews two student-made systematic mapping studies and has two goals. The first goal is to report the students' and the supervisor's experiences with this research method, and thereby contribute to practice and knowledge on student research. A mapping study was experienced to fit a situation where the student initially had no personal research interests. Moreover, the method was argued to be appropriate for technology-related disciplines where student identities readily match with the pedantry required. Such student reflections suggest that rewarding research experiences can arise from an initially open starting point, and by considering student background in relation to the nature of work with a particular research method. Supervisors should hence learn about their students' identities when proposing research methods-method matters. The second goal is to disseminate the results of the two mapping studies. The other study mapped intellectual property as an educational research topic, while the other focused on creative coding, similarly as an educational topic. The main results of these studies are summarized and explanation for how the results demonstrate acknowledged benefits of professional mapping studies is provided.\", 'title': 'Method Matters: Reflections from Student-Made Mapping Studies', 'embedding': []}, {'id': 14750, 'abstractText': 'We study the morphology of convergence maps by perturbatively reconstructing their Minkowski functionals (MFs). We present a systematic study using a set of three generalized skew spectra as a function of source redshift and smoothing angular scale. These spectra denote the leading-order corrections to the Gaussian MFs in the quasi-linear regime. They can also be used as independent statistics to probe the bispectrum. Using an approach based on pseudo-S<inf>ℓ</inf>s, we show how these spectra will allow the reconstruction of MFs in the presence of an arbitrary mask and inhomogeneous noise in an unbiased way. Our theoretical predictions are based on a recently introduced fitting function to the bispectrum. We compare our results against state-of-the-art numerical simulations and find an excellent agreement. The reconstruction can be carried out in a controlled manner as a function of angular harmonics ℓ and source redshift z<inf>s</inf>, which allows for a greater handle on any possible sources of non-Gaussianity. Our method has the advantage of estimating the topology of convergence maps directly using shear data. We also study weak lensing convergence maps inferred from cosmic microwave background observations, and we find that, though less significant at low redshift, the post-Born corrections play an important role in any modelling of the non-Gaussianity of convergence maps at higher redshift. We also study the cross-correlations of estimates from different tomographic bins.', 'title': 'Morphology of weak lensing convergence maps', 'embedding': []}, {'id': 14751, 'abstractText': 'Automotive players have recently shown an increasing interest in high-precision mapping, with the aim of enhancing vehicles safety and autonomy. Nevertheless, the acquisition, processing, and updates of accurate maps remains an economic challenge. Collaborative mapping through vehicles crowdsourcing represents a promising solution to tackle this problem. However, the potential scalability and accuracy provided by such an approach have yet to be studied and assessed. In this paper, we study the use of graph optimization in the scope of collaborative mapping. We build a map of geo-localized landmarks by crowdsourcing observations from multiple vehicles, and applying several successive map updates. We present different strategies to adapt graph optimization to the crowdsourced approach, and compare their performances in terms of map quality and scalability on simulation data. We show the critical requirement, in a long-term context, to ensure consistency of the map updates, and we propose a scalable solution which is able to build an accurate map of geolocalized landmarks.', 'title': 'Graph Optimization Methods for Large-Scale Crowdsourced Mapping', 'embedding': []}, {'id': 14752, 'abstractText': 'Virtual network mapping (VNM) is to build a network on demand by deploying virtual machines in a substrate network, subject to constraints on capacity, bandwidth and latency. It is critical to data centers for coping with dynamic cloud workloads. This paper shows that VNM can be approached by graph pattern matching, a well-studied database topic. (i) We propose to model a virtual network request as a graph pattern carrying various constraints, and treat a substrate network as a graph in which nodes and edges bear attributes specifying their capacity. (ii) We show that a variety of mapping requirements can be expressed in this model, such as virtual machine placement, network embedding and priority mapping. (iii) In this model, we formulate VNM and its optimization problem with a mapping cost function. We establish complexity bounds of these problems for various mapping constraints, ranging from polynomial time to NP-complete. For intractable problems, we show that their optimization problems are approximation-hard, i.e. NPO-complete in general and APX-hard even for special cases. (iv) We also develop heuristic algorithms for priority mapping, an intractable problem. (v) We experimentally verify that our algorithms are efficient and are able to find high-quality mappings, using real-life and synthetic data.', 'title': 'Virtual Network Mapping in Cloud Computing: A Graph Pattern Matching Approach', 'embedding': []}, {'id': 14753, 'abstractText': 'The maximum a posteriori (MAP) estimation model-based sub-pixel mapping (SPM) method is an alternative way to solve the ill-posed SPM problem. The MAP estimation model has been proven to be an effective SPM approach and has been extensively developed over the past few years, as a result of its effective regularization capability that comes from the spatial regularization model. However, various spatial regularization models do not always truly reflect the detailed spatial distribution in a real situation, and the over-smoothing effect of the spatial regularization model always tends to efface the detailed structural information. In this article, under the scenario of time-series observation by remote sensing imagery, the joint spectral-spatial-temporal MAP-based (SST_MAP) model for SPM is proposed. In SST_MAP, a newly developed temporal regularization model is added to the MAP model, based on the prerequisite for a temporally close fine image covering the same study region. This available fine image can provide the specific spatial structures most closely conforming to the ground truth for a more precise constraint, thereby reducing the over-smoothing effect. Furthermore, the three dimensions are mutually balanced and mutually constrained, to reach an equilibrium point and achieve restoration of both smooth areas for the homogeneous land-cover classes and a detailed structure for the heterogeneous land-cover classes. Four experiments were designed to validate the proposed SST_MAP: three synthetic-image experiments and one real-image experiment. The restoration results confirm the superiority of the proposed SST_MAP model. Notably, under the background of time-series observation, SST_MAP provides an alternative way of land-cover change detection (LCCD), achieving both detailed spatial-scale and high-frequency temporal LCCD observation for the study case of urbanization analysis within the city of Wuhan in China.', 'title': 'Spectral–Spatial–Temporal MAP-Based Sub-Pixel Mapping for Land-Cover Change Detection', 'embedding': []}, {'id': 14754, 'abstractText': 'In many areas of Taiwan, there are still used graphically digitized cadastral maps. Due to poor precision and environmental changes, there are often inconsistencies in the application. There are two ways to update cadastral maps: cadastral map resurveying and cadastral rearranging. The purpose is to redraw the cadastral map and calculate the area so that the location, shape, place number, and area of each land are consistent with the contents of the register. Currently, the cadastral resurveying method is adopted. However, due to financial and manpower problems, there are still many areas where cadastral maps cannot be updated. In this study, the Miaoli county cadastral resurveying area was used as the experimental area. Using the UAS aerial survey to obtain the true orthophoto image overlay cadastral map, it was explored whether UAS aerial survey could improve the current cadastral map resurveying process. All experimental data in this study were compared with the data of the current cadastral retesting procedure, confirming that UAS aerial survey can improve the following, 1. Assist in cadastral surveys and results announcements to reduce public concerns about resurvey results. 2. Improve image efficiency and correctness by image-assisted area analysis. After comparing the image measurement area and the registered area of each piece of land, the maximum difference is 0.8276 square meters still within the regulation margin. 3. The ground measurement work can be replaced by the image point coordinate measurement. 4. The cadastral boundary line after the area analysis can be converted into a new cadastral map.', 'title': 'Improvement on the Process of Cadastral Map Resurveying Using UAS Photogrammetry', 'embedding': []}, {'id': 14755, 'abstractText': \"Measurement of precise three-dimensional surface displacements is really helpful to understand Earth's internal processes. In this point of view, many researches have been performed to retrieve three-dimensional surface displacements using three or more surface displacements maps from SAR, which represent different geometries respectively [1-6]. SAR interferometry, Multiple-aperture SAR interferometry or Offset tracking have been selectively employed to restore three-dimensional surface displacements occurring by earthquakes, volcanisms, glaciers or land subsidence [7-14]. However the limitations are still remain. SAR interferometry is difficult to measure large surface displacements. Although offset tracking is appropriate to measure large surface displacements, generally produce the surface displacements map of worse measurement precision than SAR interferometry [1]. Meanwhile, recent developments in SAR sensor and improvements in offset tracking have made it possible to complement each of these limitations [9]. In this study, we precisely measured the large surface displacements by employing the integration of SAR Interferometry and Offset tracking methods. Figure 1 show the shaded relief map for Kumamoto, Japan. Three interferometric pairs, which cover surface displacements occurring by the 2016 Kumamoto Earthquakes, were collected from ascending and descending paths of L-band ALOS PALSAR-2 datasets (asc1: 20151115_20160616; asc2: 20160211_20160602; dsc: 20160307_20160419) (Fig. 1). When the offset tracking map and integrated LOS displacements map are compared, integrated LOS map could observe surface displacements without unwrapping error, even in the decorrelated region of SAR interferometry because of the large surface displacements [15]. It enable us to extract the three-dimensional displacements without unmeasured area. Figure 2 shows three-dimensional surface-deformation field for the 2016 Kumamoto Earthquakes. The maximal surface displacements for east-west, north-south and up-down direction were -1.91, -1.57 and -2.49 m respectively. The root mean square errors of 1.76, 1.41 and 1.85 cm in the east, north and up directions, respectively, were estimated by comparing with the GPS measurements [16]. These results suggest that the integration method can be help to map the three-dimensional surface displacements even near the fault lines, which was impossible to observe by conventional SAR interferometry. Since it can be applied not only to earthquakes but also to large surface displacements of various factors, the observation method of this study will be produce more beneficial data to understand the Earth's internal processes.\", 'title': 'Precise three-dimensional mapping of the 2016 Kumamoto earthquake through the integration of SAR interferometry and offset tracking', 'embedding': []}, {'id': 14756, 'abstractText': 'In this paper, we present a method of effectively creating environment maps on an auto-transport system in logistics and industrial site management applications, e.g., an automobile assembly plant. The key objective of the study is creating a map effectively. Simultaneous Localization and Mapping (SLAM) is established as a general map-generating method. The map is, however, created with ad hoc and manual. Thus, an exploration method in an unknown environment for autonomously generating a map has been studied for decades. The main method is frontier-based exploration. This method presents a problem for an efficient mapping method in a wide environment, and for accuracy of the map depending on the local area. In the backgrounds, an autonomous exploration algorithm using only infrared sensor and odometer information from a robot is proposed as a sensor-based exploration approach without using map information. The proposed method requires only a depth sensor and camera on a robot. Next, we propose a hybrid exploration to decrease unavailable areas in frontier-based exploration. To perform our proposed method, an environment map is created by a mobile robot, and the effectiveness of the hybrid exploration method is demonstrated.', 'title': 'Hybrid Sensor-Based and Frontier-Based Exploration Algorithm for Autonomous Transport Vehicle Map Generation', 'embedding': []}, {'id': 14757, 'abstractText': 'In recent electroencephalograph (EEG)-based emotion recognition, the differential entropy (DE) features extracted from multiple electrodes are organized as a 2D feature map for convolutional neural network (CNN) in order to utilize the information hidden in the electrodes. In this study, we attempt to investigate the influence of different feature maps on the recognition performance. Six different 2D feature maps (M1-M4: baseline feature maps without sparsity and location relationship, M5-M6: pre-defined feature maps with sparsity and location relationship) are used to organize the DE features for the traditional CNN model. Evaluation study on the DEAP dataset finds that the 2D feature map configuration exhibits statistically significant effect on the classification performance of the traditional CNN model in classifying the high/low arousal and high/low valence, respectively. However, the differences are rather limited, e.g., only 1% improvement can be resulted from selecting the optimal 2D feature map among 6 feature maps. This implies that the feature map may not be a critical issue when applying the DE features to classifying the emotion states in a CNN.', 'title': 'EEG-based Emotion Recognition Under Convolutional Neural Network with Differential Entropy Feature Maps', 'embedding': []}, {'id': 14758, 'abstractText': 'We present an experimental study for the generation of large 3D maps using our CoMapping framework. This framework considers a collaborative approach to efficiently manage, share, and merge maps between vehicles. The main objective of this work is to perform a cooperative mapping for urban and rural environments denied of continuous-GPS service. The study is split in to 2 stages: Pre-Local and Local. In the first stage, each vehicle builds a Pre-Local map of its surroundings in real-time using laser-based measurements, then relocates the map in a global coordinate system using just the low cost GPS data from the first instant of the map construction. In the second stage, vehicles share their pre-local maps, align and merge them in a decentralized way in order to generate more consistent and larger maps, named Local maps. To evaluate performance of all the cooperative system in terms of map alignments, tests are conducted using 3 cars equipped with LiDARs and GPS receiver devices in urban outdoor scenarios of the Ecole Centrale Nantes campus and rural environments.', 'title': 'CoMapping: Multi-robot Sharing and Generation of 3D-Maps applied to rural and urban scenarios', 'embedding': []}, {'id': 14759, 'abstractText': \"In this study, the effect of practical courses (especially robotics) on students' success is evaluated using fuzzy cognitive maps. This study particularly focuses on how practical courses increase the learning achievements in vocational schools. For this purpose, students and academicians were chosen as stakeholders for this study. Student were selected from departments of machine, electronic and computer. In total views of 30 students were taken and evaluated using fuzzy cognitive maps before the study. Students were studied on three projects within this study. These projects are autonomous cleaning robot, chameleon robot and sumo wrestler robot. Each robot was developed by a group consisting of ten students. Three main outputs determined to be gained by students in the robot development process as sensor calibration, improvement of mechanical systems and software optimization. These outputs are also among the output concepts of the fuzzy cognitive map. Sensor calibration, improvement of the mechanical systems and optimization of the software are the critical tasks respectively for the student groups of electronics, machine and computer. Each student group shared students of their departments homogeneously. Academicians were selected as one from each department. Fuzzy cognitive maps' survey applied to 15 academicians. After the design and development processes of robots, students and academicians views re-evaluated. It was observed clearly from the comparison of the results that, learning achievement, desire to learn, desire to interdisciplinary collaboration, self-confidence, interest in robotic technologies, satisfaction and desire to give priority to practical training in other subjects are increased by %45, %63, %42, %74, %85, %88 and %76 respectively.\", 'title': 'Performance evaluation of practice courses using fuzzy cognitive maps', 'embedding': []}, {'id': 14760, 'abstractText': 'Since elderly users lack intellectual ability, which becomes less due to age factor their interactions with computer interfaces exhibit limitations essentially arising from problems inherent to their age. This is due to the minimal cognitive flow, physical impairments, and lack of knowledge about computers and technologies. All these factors are often analyzed by employing qualitative and quantitative research methods. The empathy map tool was used as the qualitative analysis method in this research study. To address this study objective, the study target group was elderly people 60 years and above, and the group consists of 15 volunteers, with seven females and eight males. Age is the only factor in this study. The test used in this study is the usability test to do seven search tasks by using Google engine and seven different search tasks with Bing engine individually by each volunteer. The observation method and note-taking are used in this study to collect data. Also, a SUS questionnaire was used to be a validation method to prove the results obtained by the empathy map instrument. The result of the empathy map tool enables to explore, understand, and empathize, from the perspective of the elderly user and their interaction with search engines; more specifically, with the Google and Bing interface. The results of the empathy mapping method show that the elderly are dissatisfied with the usability of search interfaces, which is consistent with the findings of the SUS questionnaire.', 'title': 'Empathy Map Instrument for Analyzing Human-Computer Interaction in Using Web Search UI by Elderly Users', 'embedding': []}, {'id': 14761, 'abstractText': \"The phenomenon of deforestation is a reality in Côte d'Ivoire, which dates back to the colonial period [1]. This study aims to determine the spatial and temporal evolution of dense rainforests using remote sensing data. Many previous studies have attempted to determine the area of forest cover in Côte d'Ivoire ([2], [3], [4], [5], [6] and [7]). Most of these studies were conducted from either aerial photographs or satellite images of low spatial resolutions such as NOAA / AVHRR [8]. However, through these studies, methods of estimates of the area of forest cover generally vary from one author to another, thus causing an inconsistency in the results [9]. Remote sensing allows a better assessment of the scope and scale of the problem of deforestation. Using multi-temporal data, one can better detect and analyze changes in the forest cover. By comparing images of previous years with recent scenes, it becomes possible to effectively measure the differences in the extent of deforestation and loss of rainforests in Côte d'Ivoire ([10] and [11]). In a first phase, the country's dense forest cover of the 1960s was determined from the digitization of existing vegetation map. In a second phase, the dense forest cover of the 1980s and 2000s were obtained by processing of Landsat TM and ETM+ satellite images. First the geometric correction of the satellite images was performed. Then, series of supervised classifications based on the maximum likelihood method were done to generate maps of land cover, with overall accuracies of 90%. From these land cover maps, the class of dense forests was extracted and converted into a map layer in vector format. By crossing dense forests covers of the 1980s and 2000 with those of the 1960s, the specific characterizations of those forests were derived (Figure 1, 2, 3). The results show a dense humid forest cover of about 8.4 million ha in 1960, with the presence of large blocks. The 1980s and 2000s have dense forest cover of 2.6 million ha and 1.35 million ha, respectively (Tables 1, 2, 3). These latter periods are characterized by high loss and fragmentation of rainforests in Côte d'Ivoire. Thus, since its independence, the country has lost more than 80% of its forest cover. The results of this study show that clearing associated with industrial agriculture mainly cocoa, coffee and rubber farming as well as demographic pressure and abusive exploitation of valuable wood species for export are the main causes of deforestation in Cote d'Ivoire. This study advocates the restoration of forests through the implementation of an awareness policy and reforestation one hand, and an efficient monitoring and protection of national protected areas and parks.\", 'title': \"Multitemporal monitoring of the forest cover in Côte d'Ivoire from the 1960s to the 2000s, using Landsat satellite images\", 'embedding': []}, {'id': 14762, 'abstractText': 'Radio map, serving as an efficient indicator of wireless environments, has been widely used in smart-city applications, including network monitoring/planning, anomaly signal detection, and indoor/outdoor localization. It is hard to maintain an update-to-date fine-grained radio map within a large area, since the radio map changes rapidly due to the internal and external factors. Previous studies usually relied on time-consuming site surveys at densely predefined reference points, leading to either coarse-grained or out-of-date radio maps. In this paper, we propose a fine-grained radio map reconstruction framework, called Supreme, based on crowd-sourced data in an image super-resolution manner. Specifically, Supreme explores spatial-temporal relationships in historical coarse-grained radio maps and builds a real-time fine-grained radio map using deep spatial-temporal reconstruction networks. Furthermore, a heterogeneous data fusion module is devised to make full use of external information. To evaluate the performance of Supreme, we conduct extensive experiments and ablation studies on a large-scale dataset with a total of six-month data collected from two university campuses. Besides, we investigate the transferability of Supreme in different locations and service networks, showing that the fine-tuned model can largely reduce the training time and achieve better performance. Experimental results demonstrate that our model outperforms state-of-the-art baselines and a case study on the localization is enhanced with marginal improvements on accuracy.', 'title': 'Supreme: Fine-grained Radio Map Reconstruction via Spatial-Temporal Fusion Network', 'embedding': []}, {'id': 14763, 'abstractText': 'The study of molecular gas is crucial for understanding star formation, feedback and the broader ecosystem of a galaxy as a whole. However, we have limited understanding of its physics and distribution in all but the nearest galaxies. We present a new technique for studying the composition and distribution of molecular gas in high-redshift galaxies inaccessible to existing methods. Our proposed approach is an extension of carbon monoxide intensity mapping methods, which have garnered significant experimental interest in recent years. These intensity mapping surveys target the 115\\xa0GHz <sup>12</sup>CO (1–0) line, but also contain emission from the substantially fainter 110\\xa0GHz <sup>13</sup>CO (1–0) transition. The method leverages the information contained in the <sup>13</sup>CO line by cross-correlating pairs of frequency channels in an intensity mapping survey. Since <sup>13</sup>CO is emitted from the same medium as the <sup>12</sup>CO, but saturates at a much higher column density, this cross-correlation provides valuable information about both the gas density distribution and isotopologue ratio, inaccessible from the <sup>12</sup>CO alone. Using a simple model of these molecular emission lines, we show that a future intensity mapping survey can constrain the abundance ratio of these two species and the fraction of emission from optically thick regions to order ∼30\\u2009per\\u2009cent. These measurements cannot be made by traditional CO observations, and consequently the proposed method will provide unique insight into the physics of star formation, feedback and galactic ecology at high redshifts.', 'title': 'Feeding cosmic star formation: exploring high-redshift molecular gas with CO intensity mapping', 'embedding': []}, {'id': 14764, 'abstractText': 'Showing flows of people and resources between multiple geographic locations is a challenging visualisation problem. We conducted two quantitative user studies to evaluate different visual representations for such dense many-to-many flows. In our first study we compared a bundled node-link flow map representation and OD Maps [37] with a new visualisation we call MapTrix. Like OD Maps, MapTrix overcomes the clutter associated with a traditional flow map while providing geographic embedding that is missing in standard OD matrix representations. We found that OD Maps and MapTrix had similar performance while bundled node-link flow map representations did not scale at all well. Our second study compared participant performance with OD Maps and MapTrix on larger data sets. Again performance was remarkably similar.', 'title': 'Many-to-Many Geographically-Embedded Flow Visualisation: An Evaluation', 'embedding': []}, {'id': 14765, 'abstractText': \"Non-linear learning technique called as mind mapping has recently emerged in higher education. With the help of information computer and technology, this study proposes the use of mind mapping technique in the development of e-Integral Map to assist students to think critically and link information in the teaching and learning of integration. This study aims to evaluate the students' perceptions on e-Integral Map web-based instruction by using a survey, to the selected Diploma of Chemical Engineering and Electrical Engineering respondents from UiTM Sarawak Campus. All respondents had used e-Integral Map in their study and had applied it through Integral Calculus courses. All the respondents completed the online survey form and submitted their feedback online. From the survey results, it was shown that all three aspects in web-based contents: the content, the instructional and the technical of e-Integral Map, were satisfactory. Majority of the respondents were positive on the use of e-Integral Map in assisting them to learn and solve Integral Calculus problems. In addition, the findings showed that the students' first experience on learning Integral Calculus with this technique provides the reason for such technique to be used in other topics of calculus education.\", 'title': \"Students' perceptions on teaching and learning of integral calculus through e-Integral Map\", 'embedding': []}, {'id': 14766, 'abstractText': 'We introduce <italic>Tilt Map</italic>, a novel interaction technique for intuitively transitioning between 2D and 3D map visualisations in immersive environments. Our focus is visualising data associated with areal features on maps, for example, population density by state. <italic>Tilt Map</italic> transitions from 2D choropleth maps to 3D prism maps to 2D bar charts to overcome the limitations of each. Our article includes two user studies. The first study compares subjects’ task performance interpreting population density data using 2D choropleth maps and 3D prism maps in virtual reality (VR). We observed greater task accuracy with prism maps, but faster response times with choropleth maps. The complementarity of these views inspired our hybrid <italic>Tilt Map</italic> design. Our second study compares <italic>Tilt Map</italic> to: a side-by-side arrangement of the various views; and interactive toggling between views. The results indicate benefits for <italic>Tilt Map</italic> in user preference; and accuracy (versus side-by-side) and time (versus toggle).', 'title': 'Tilt Map: Interactive Transitions Between Choropleth Map, Prism Map and Bar Chart in Immersive Environments', 'embedding': []}, {'id': 14767, 'abstractText': 'This paper studies the judgement problem of full-period maps on Z(p<sup>n</sup>) and proposes a novel congruential map with double modulus on Z(p<sup>n</sup>). The full-period properties of the sequences generated by the novel map are studied completely. We prove some theorems including full-period judgement theorem on Z(p<sup>n</sup>) and validate them by some numerical experiments. In the experiments, full-period sequences are generated by a full-period map on Z(p<sup>n</sup>). By the binarization, full-period sequences are transformed into binary sequences. Then, we test the binary sequences with the NIST SP 800-22 software package and make the poker test. The passing rates of the statistical tests are high in NIST test and the sequences pass the poker test. So the randomness and statistic characteristics of the binary sequences are good. The analysis and experiments show that these full-period maps can be applied in the pseudo-random number generation (PRNG), cryptography, spread spectrum communications and so on.', 'title': 'On the judgement of full-period sequences and a novel congruential map with double modulus on Z(p<sup>n</sup>)', 'embedding': []}, {'id': 14768, 'abstractText': \"Accurate and high-resolution maps of vegetation are critical for projects seeking to understand the terrestrial ecosystem processes and land-atmosphere interactions in Arctic ecosystems, such as U.S. Department of Energy's Next Generation Ecosystem Experiment (NGEE) Arctic. However, most existing Arctic vegetation maps are at a coarse resolution and with a varying degree of detail and accuracy. Remote sensing-based approaches for mapping vegetation, while promising, are challenging in high latitude environments due to frequent cloud cover, polar darkness, and limited availability of high-resolution remote sensing datasets (e.g., ~ 5 m). This study proposes a new remote sensing based multi-sensor data fusion approach for developing high-resolution maps of vegetation in the Seward Peninsula, Alaska. We focus detailed analysis and validation study around the Kougarok river, located in the central Seward Peninsula of Alaska. We seek to evaluate the integration of hyper-spectral, multi-spectral, radar, and terrain datasets using unsupervised and supervised classification techniques over a ~343.72 km<sup>2</sup> area for generating vegetation classifications at a variety of resolutions (5 m and 12.5 m). We fist applied a quantitative goodness-of-fit method, called Mapcurves, that shows the degree of spatial concordance between the public coarse resolution maps and k-means clustering values and relabels the k values based on the best overlap. We develop a convolutional neural network (CNN) approach for developing high resolution vegetation maps for our study region in Arctic. We compare two CNN approaches: (1) breaking up the images into small patches (e.g., 6 × 6) and predict the vegetation class for entire patch and (2) semantic segmentation and predict the vegetation class for every pixel. We also perform accuracy assessments of the developed data products and evaluate varying CNN architectures. The fusion of hyperspectral and optical datasets performed the best, with accuracy values increased from 0.64 to 0.96-0.97 when using a training map produced by unsupervised clustering and Mapcurves labeling for both CNN models.\", 'title': 'Convolutional Neural Network Approach for Mapping Arctic Vegetation Using Multi-Sensor Remote Sensing Fusion', 'embedding': []}, {'id': 14769, 'abstractText': 'Antarctica plays a significant role in global change studies. As a typical land cover in Antarctica, exposed rock is considered indispensable in many studies, and the mapping of exposed rocks is seen as a key basis of work to meet the demand for more accurate and updated datasets with the consecutive development of satellite technology. Although the normalized difference snow index has been commonly used for differentiating exposed rocks and snow, it often misidentifies clouds as rocks. The British Antarctic Survey has used Landsat 8 data to create a new rock outcrop map for Antarctica, overcoming the limitations of previous techniques and generating Antarctic Digital Database (ADD) New Rock Outcrop with higher accuracy than previously achieved. However, there are still some omission and commission errors apparent in the shaded areas, which affect the accuracy. Widespread shaded areas in Antarctica due to low solar elevation angles and extreme topography cause difficulty in accurately mapping exposed rock. In addition, major differences are present between existing products. Addressing the existing issues about extraction of exposed rock, this study used the near infrared band and shortwave infrared 2 band of Landsat 8 reflectance data to build a specific exposed rock index for the extraction of exposed rocks. A shadow detection method combined with a blue reflectance threshold is used for the shadowed rock identification. Accuracy assessment of these extraction results showed that the accuracy of the new product is higher than all existing exposed rock products. The conversion from DN values to top of atmosphere reflectance and the solar elevation correction for each pixel individually eliminate a variety of errors associated with the different acquisition times of each image. From the statistics of the reflectance related to the training samples, this paper established the threshold of exposed rock extraction so as to ensure the applicability of the same threshold for exposed rock extraction in all images. This method is applied to a total of 1100 high-quality images that were collected for covering the Antarctic continent from November 2013 to February 2014. The results show that 253 of the images contain exposed rocks, and these images were used for mapping work. The map showed that the main exposed rock areas are mainly distributed in four coastal regions: The Antarctic Peninsula, Queen Maud Land, Lambert Glacier basin, and Victoria Land regions. We also compared our results with ADD New Rock Outcrop and Bedrock Mapping Project 2 (Bedmap2) data in the four main regions. Our results were close to the ADD rock outcrops and exhibited remarkable differences with Bedmap2. We explored the possibility of analyzing and explaining these differences. Especially, as using the same data source Landsat 8 but with a different method, the comparison between our results and ADD New Rock Outcrop is discussed and concludes in shadowed rocks extraction, mixed-pixels, and omission disagreement. The results also show that shadowed rocks accounts for nearly 12% of the total exposed rocks and cannot be neglected. The method we developed can be quickly applied for extraction and mapping of large areas of exposed rocks using Landsat 8.', 'title': 'An Accurate and Automated Method for Identifying and Mapping Exposed Rock Outcrop in Antarctica Using Landsat 8 Images', 'embedding': []}, {'id': 14770, 'abstractText': \"Students attending object-oriented analysis and design (OOAD) courses typically encounter difficulties transitioning from requirements analysis to logical design and then to physical design. Concept maps have been widely used in studies of user learning. The study reported here, based on the relationship of concept maps to learning theory and semantic memory, suggests that integrating OOAD instruction with concept maps might help learners understand these transitions more clearly. An empirical experiment, involving two treatments based on concept maps and conventional instruction, was conducted to examine the relevance of concept map-based instruction for an effective understanding of phase transitions. The results indicate that compared with conventional instruction, concept map-based instruction is more efficient in improving learner understanding because of greater learner engagement and because of concept maps' properties when they are used as an information medium. This study can aid instructors in realizing the difficulties in learning phase transitions and motivate researchers to develop more effective learning instructions.\", 'title': 'Concept Maps as Instructional Tools for Improving Learning of Phase Transitions in Object-Oriented Analysis and Design', 'embedding': []}, {'id': 14771, 'abstractText': 'Many-core architectures are more promising hardware to design real-time systems than multi-core systems as they should enable an easier mastered integration of an higher number of applications, potentially of different level of criticalities. However, the worst-case behavior of the Network-on-Chip (NoC) for both inter-core and core-to-Input/Output (I/O) communications of critical applications must be established. We use the term core-to-I/O for both core communications from or to I/O interfaces. The mapping over the NoC of both critical and non-critical applications has an impact on the network contention these critical communications exhibit. So far, all existing mapping strategies have focused on inter-core communications. However, we claim that many-cores in embedded real-time systems will be integrated within backbone ethernet networks, as they mostly provide ethernet controllers as I/O interfaces. In this work, we first show that ethernet packets can be dropped due to an internal congestion in the NoC, if these core-to-I/O communications are not taken into account while mapping applications. To this end, we rely on a case study from the avionic domain. It is made of a critical Full Authority Digital Engine (FADEC) application and a non-critical Health Monitoring (HM) application of the engine, used for recognizing incipient failure conditions. Based on this analysis, we introduce our approach to map critical and non critical real-time applications over many-cores that reduces the WCTT of core-to-I/O communications. We show for two variants of our case study that our algorithm successfully find a mapping that avoids ethernet packets, whose payload are making the core-to-I/O communications, to be dropped. This demonstrates the benefits of our proposal compared to a state of the art mapping strategy that fails to do so.', 'title': 'Poster Abstract: I/O Contention Aware Mapping of Multi-Criticalities Real-Time Applications over Many-Core Architectures', 'embedding': []}, {'id': 14772, 'abstractText': 'Based on new data from permanent and temporary networks, we present fundamental mode Rayleigh wave group velocity maps at periods of 10–150 s related to the lithosphere beneath South America. We analyse waveform data from 1043 earthquakes, from 2002 to 2019, which were recorded by 282 stations. To isolate fundamental mode Rayleigh waves, a phase-matched filter is applied, and the measurements of group velocity are obtained from multiple filter analysis techniques. Thus, we obtain 17\\u2009838 paths, covering most of the South American continent, which reach their maximum at the period of 30 s and decrease for both shorter and longer periods. We calculate average dispersion curves and probability density distribution of all measured curves to check the consistency of our data set. Then, regionalized group velocity maps are obtained by iteratively combining the fast marching method and the subspace inversion method. The resolution of our models is assessed by checkerboard tests, which show that the synthetic group velocities are well recovered, despite some amplitude and smearing effects in some portions of the model, probably owing to regularization and uneven ray path coverage. Compared to previous group velocity studies for South America, our models present better resolution, mainly for shorter periods. Our maps of 10 and 20 s, for example, show an excellent correlation with the sedimentary thickness (CRUST1.0) and topography density (UNB<tex>$\\\\_$</tex>TopoDens). Regions of exposed basement and high-density are related to fast group velocities, while sedimentary basins and low-densities are observed as areas of slow group velocities. We identify small-scale fast group velocity heterogeneities that may be linked to the Rio Apa and Rio Tebicuary cratons as well as to the geochronological provinces of the Amazonian Craton. The most striking feature of our map at 40 s is a fast group velocity structure with the same NE trend of the Transbrasiliano lineament, a Neoproterozoic megashear fault that crosses a large part of the South American continent. Our long-period maps sample lithospheric depths, revealing that cratonic areas of South America, such as the Amazonian and São Francisco cratons, correlate well with fast group velocities. Another interesting feature is the presence of a strong group velocity gradient between the Paraná and Chaco-Paraná basins, which nearly coincides with the location of the Western Paraná Suture, a continental-scale gravity discontinuity. From our group velocity maps, we estimate 1-D S-wave velocity depth profiles at 10 locations in South America: Chaco-Tarija Basin, Borborema Province (BP), Amazonian Craton, Paraná Basin, Tocantins Province, Acre Basin (AcB), Altiplano-Puna Volcanic Complex, Mantiqueira Province (MP), Parnaíba Basin and São Francisco Craton. Most of our inverted S-wave velocity profiles show good agreement with the SL2013sv model at lithospheric depths, except the BP, AcB and MP profiles. Particularly for the BP, a low shear wave velocity, from about 75 to 150 km depth, is a feature that is not present in the SL2013sv model and was probably resolved in our model because of our denser ray path coverage. This decreased S-wave velocity may be due to a lithospheric thinning beneath the BP, as already pointed out by previous studies.', 'title': 'Rayleigh wave group velocity maps at periods of 10–150 s beneath South America', 'embedding': []}, {'id': 14773, 'abstractText': 'We study the problem of continuous maintenance of range sum heat maps over dynamically updating data objects. The range sum (RS) here refers to the sum of the weights of the data objects enclosed by a given range (rectangle) R. Range sum problems are useful in spatio-temporal data analytics and decision making processes. Recent studies on range sum problems focus on computing the MaxRS query, which finds a location to place a rectangle R such that its RS is maximized. In real applications, knowing only the location with the maximum RS may be insufficient, because decision making is a multi-factor process where maximizing the RS may just be one of the factors. It is also important to gain an overview of the RS distribution at different locations, so that decisions can be made based on global knowledge. We therefore propose to compute a range-sum heat map that visualizes the RS value for every location in a data space. Considering that data objects may be inserted into or removed from the data space dynamically, we further study the continuous maintenance of range-sum heat maps over dynamically updating data objects. We adapt algorithms to compute range-sum heat maps and to perform heat map updates. We build a demo system to showcase the usefulness of range sum heat maps and the effectiveness of the adapted algorithms.', 'title': 'Continuous Maintenance of Range Sum Heat Maps', 'embedding': []}, {'id': 14774, 'abstractText': 'Fuzzy cognitive maps are a soft computational technique with artificial neural network nature expressed by graphs used to model complex systems. Fuzzy cognitive maps are used in areas like health, business, energy, computer science, etc. in the literature. It is used to solve problems in many areas. Fuzzy cognitive maps have a non-dynamic method of weight determination and updating in their classical applications. This is usually done by subjecting the expert opinions to fuzzy membership functions. In some studies, the weights obtained by using expert data are improved by utilizing historical data of the system modeled. In some studies, weight determination and updating process is realized by using intuitive optimization methods without benefiting from expert opinions. Here, fuzzy cognitive maps using a different weight matrix for different input values are considered dynamic. In this study, we propose a dynamic fuzzy cognitive map structure that performs the weight update process with deep learning. Here, the cognitive map weights determined in the data set used for deep neural network training are determined by a genetic optimization-based method using past system data. The proposed new DFCM structure has been tested on two different scenarios. In addition, the performance comparison with deep artificial neural network models dealing with the same scenarios was performed. When the experimental and comparative results are examined, the performance of the proposed method is quite satisfactory.', 'title': 'A New Deep Neural Network Based Dynamic Fuzzy Cognitive Map Weight Updating Approach', 'embedding': []}, {'id': 14775, 'abstractText': 'Objective: For cardiac arrhythmia mapping and ablation procedures, the ability to record focal cardiac action potentials could aid in precisely identifying lesions, scarred tissue, and/or arrhythmic foci. Our study objective was to validate the electrophysiologic properties of a routinely employed large mammalian in vitro working heart model. Methods: Monophasic action potentials (MAPs) were recorded from 18 swine hearts during viable hemodynamic function both in situ (postmedian sternotomy) and in vitro (using Visible Heart methodologies). We placed specially designed mapping catheters in epicardial and endocardial locations. High-quality MAP signals were recorded for up to 2 h, and MATLAB was utilized to evaluate relative duration and temporal/regional changes in waveform morphology. Results: MAPs were reproducibly recorded from both epicardial and endocardial locations in situ and in vitro. No significant differences were noted in right atrial endocardial, right ventricular endocardial, right ventricular epicardial, or left atrial epicardial waveforms, when baseline recordings were compared to all other in situ and in vitro time points. Furthermore, MAP duration between right ventricular endocardial and epicardial waveforms was not significantly different, in situ or in vitro. Conclusion: The use of in vitro models like the Visible Heart is considered invaluable for the study of cardiac arrhythmias, the development of novel therapies, and/or preclinical testing of future cardiac mapping catheters and systems. Significance: Preclinical studies assessing in situ and/or in vitro recorded cardiac monophasic action potentials could be critical for the future development and validation of cardiac devices.', 'title': 'The Ability to Reproducibly Record Cardiac Action Potentials From Multiple Anatomic Locations: Endocardially and Epicardially, <italic>In Situ</italic> and <italic>In Vitro</italic>', 'embedding': []}, {'id': 14776, 'abstractText': 'Objective: Although HIFU has been successfully applied in various clinical applications in the past two decades for the ablation of many types of tumors, one bottleneck in its wider applications is the lack of a reliable and affordable strategy to guide the therapy. This study aims at estimating the therapeutic beam path at the pre-treatment stage to guide the therapeutic procedure. Methods: An incident beam mapping technique using passive beamforming was proposed based on a clinical HIFU system and an ultrasound imaging research system. An optimization model was created to map the cross-like beam pattern by maximizing the total energy within the mapped area. This beam mapping technique was validated by comparing the estimated focal region with the HIFU-induced actual focal region (damaged region) through simulation, in-vitro, ex-vivo and in-vivo experiments. Results: The results of this study showed that the proposed technique was, to a large extent, tolerant of sound speed inhomogeneities, being able to estimate the focal location with errors of 0.15 mm and 0.93 mm under in-vitro and ex-vivo situations respectively, and slightly over 1 mm under the in-vivo situation. It should be noted that the corresponding errors were 6.8 mm, 3.2 mm, and 9.9 mm respectively when the conventional geometrical method was used. Conclusion: This beam mapping technique can be very helpful in guiding the HIFU therapy and can be easily applied in clinical environments with an ultrasound-guided HIFU system. Significance: The technique is non-invasive and can potentially be adapted to other ultrasound-related beam manipulating applications.', 'title': 'Acoustic beam mapping for guiding HIFU therapy in vivo using sub-therapeutic sound pulse and passive beamforming', 'embedding': []}, {'id': 14777, 'abstractText': 'Quantitative PET image reconstruction requires an accurate map of photon attenuation coefficients (μ-map) in order to correct the PET emission data. Current PET/MR imaging systems use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients. In this study we examine the differences in the quantification of <sup>18</sup>F-FDG standardized uptake values (SUV) in head and neck cancer, using two different MR imaging sequences for MR-based attenuation correction (MRAC): a zero echo time (ZTE) sequence which can image bone directly (ZTE-MRAC), and a vendor-provided 2-point Dixon sequence that neglects bone (Dixon-MRAC). The μ-maps from each MRAC techniques were compared to CT-based attenuation correction (CTAC) maps. Percent SUV-mean and SUV-max differences in relevant regions of interest (ROIs) were calculated for three patients. Relative to Dixon-MRAC, we observed 15±7% and 14±8% increase of SUV-mean and SUV-max, respectively, when ZTE-based bone information was incorporated in the attenuation map and using Dixon-based attenuation map, respectively. We also observed that use of Dixon-MRAC led to 7±7% and 8±8% underestimation of SUV-mean and SUV-max, respectively, whereas with ZTE-MRAC led to 6±8% and 5±8% higher SUV-mean and SUV-max, respectively, compared to CTAC. This study is the first demonstration of ZTE-based attenuation correction in the head and neck region and compared with CTAC as a gold standard with the goal of improving PET quantitation. The study shows that incorporation of bone information on μ-maps has a significant impact on SUV quantitation in head and neck cancer lesions.', 'title': 'Evaluation of Zero-TE-based attenuation correction methods on PET quantification of PET/MRI head and neck lesions', 'embedding': []}, {'id': 14778, 'abstractText': \"Various sensors can be attached and added to autonomous vehicles, included visual cameras, radar, LiDAR (Light Detection And Ranging), and GNSS (Global Navigation Satellite System). These sensors have been studied in many research areas, in particular studies on building precise 3D maps. It is essential for autonomous driving to create an accurate 3D map of the surrounding scene. However, creating an accurate static 3D map is difficult due to changes in moving objects or dynamic environments. Spurious objects on the 3D map can be handled by removing or ignoring them for 3D mapping. Following this idea, we propose an object segmentation and inpainting network. The proposed network called SAM-Net, addresses the object duplication issue by segmenting the objects and inpainting them with the segmentation results. Conventional inpainting research has dealt with RGB images. No matter how well such approaches reconstruct holes or corrupted images, they do not establish 3D points' relationship with the point cloud frame. Therefore, we suggest a depth inpainting method for outdoor object segmentation and inpainting tasks that utilizes a high-precision depth range sensor (Velodyne HDL-64E), which is not suggested before. Unfortunately, no dataset exists for the outdoor depth inpainting task. Thus, to train our model, we generate a new dataset by locating objects on a clean static background. Moreover, our proposed method shows outstanding depth performance compared to the previous visual inpainting method. Our dataset will be available at: ``https://github.com/JunhyeopLee/lidar_inpainting''.\", 'title': 'SAM-Net: LiDAR Depth Inpainting for 3D Static Map Generation', 'embedding': []}, {'id': 14779, 'abstractText': 'Accurately monitoring forest dynamics in the tropical regions is essential for ecological studies and forest management. In this study, images from phase-array L-band synthetic aperture radar (PALSAR), PALSAR-2, and Landsat in 2006-2010 and 2015 were combined to identify tropical forest dynamics on Hainan Island, China. Annual forest maps were first mapped from PALSAR and PALSAR-2 images using structural metrics. Those pixels with a high biomass of sugarcane or banana, which are widely distributed in the tropics and subtropics and have similar structural metrics as forests, were excluded from the SAR-based forest maps by using phenological metrics from time series Landsat imagery. The optical-SAR-based forest maps in 2010 and 2015 had high overall accuracies (OA) of 92-97% when validated with ground reference data. The resultant forest map in 2010 shows good spatial agreement with public optical-based forest maps (OA = 88-90%), and the annual forest maps (2007-2010) were spatiotemporally consistent and more accurate than the PALSAR-based forest map from the Japan Aerospace Exploration Agency (OA = 82% in 2010). The areas of forest gain, loss, and net change on Hainan Island from 2007 to 2015 were 415 000 ha (+2.17% yr<sup>-1</sup>), 179 000 ha (-0.94% yr <sup>-1</sup>), and 236 000 ha (+1.23% yr<sup>-1</sup>), respectively. About 95% of forest gain and loss occurred in those areas with an elevation less than 400 m, where deciduous rubber, eucalyptus plantations, and urbanization expanded rapidly. This study demonstrates the potential of PALSAR/PALSAR-2/Landsat image fusion for monitoring annual forest dynamics in the tropical regions.', 'title': 'Mapping Forest and Their Spatial–Temporal Changes From 2007 to 2015 in Tropical Hainan Island by Integrating ALOS/ALOS-2 L-Band SAR and Landsat Optical Images', 'embedding': []}, {'id': 14780, 'abstractText': 'In search and rescue missions, time is an important factor; fast navigation and quickly acquiring situation awareness might be matters of life and death. Hence, the use of robots in such scenarios has been restricted by the time needed to explore and build a map. One way to speed up exploration and mapping is to reason about unknown parts of the environment using prior information. While previous research on using external priors for robot mapping mainly focused on accurate maps or aerial images, such data are not always possible to get, especially indoor. We focus on emergency maps as priors for robot mapping since they are easy to get and already extensively used by firemen in rescue missions. However, those maps can be outdated, information might be missing, and the scales of rooms are typically not consistent. We have developed a formulation of graph-based SLAM that incorporates information from an emergency map. The graph-SLAM is optimized using a combination of robust kernels, fusing the emergency map and the robot map into one map, even when faced with scale inaccuracies and inexact start poses. We typically have more than 50% of wrong correspondences in the settings studied in this paper, and the method we propose correctly handles them. Experiments in an office environment show that we can handle up to 70% of wrong correspondences and still get the expected result. The robot can navigate and explore while taking into account places it has not yet seen. We demonstrate this in a test scenario and also show that the emergency map is enhanced by adding information not represented such as closed doors or new walls.', 'title': 'SLAM auto-complete: Completing a robot map using an emergency map', 'embedding': []}, {'id': 14781, 'abstractText': 'This paper presents a weather map prediction method using RGB metaphorical feature extraction for atmospheric pressure patterns. In the field of meteorological science, predicting weather based on the analysis of observational data and the knowledge of weather experts is crucial. Weather experts draw weather maps based on air pressure distribution; hence, we believe that weather maps entail the interpretations of weather experts. In this study, we improved the prediction accuracy by using machine learning to recognize patterns of qualitative expert interpretations that cannot be predicted by analyzing observed data alone. The proposed method can be realized via two steps. The first is developing a module for extracting pressure pattern features from a weather map. Certain features, such as tropical cyclones or atmospheric high/low pressure distributions, are emphasized in weather maps to facilitate better understanding of the weather features. Therefore, we can predict weather features based on the knowledge of weather experts using data that contain their interpretations, particularly weather maps. The developed module extracts the atmospheric pressure features from the current weather map as an RGB metaphorical gradation map. The second step is developing a module to design a predicted weather map using the extracted features. The weather map of the following day is predicted using pix2pix. To the best of our knowledge, our method for extracting features from weather maps is the first to create a predicted weather map automatically.', 'title': 'Weather Map Prediction Using RGB Metaphorical Feature Extraction for Atmospheric Pressure Patterns', 'embedding': []}, {'id': 14782, 'abstractText': 'A radio map is a collection of signal fingerprints labeled with their collected locations. It is known that the performance of a fingerprint-based positioning systems is closely related to the precision and accuracy of the underlying radio maps. However, little has been studied on the performance of radio maps in relation to the fingerprint collection methods and the radio map models, which determine the accuracy and precision of radio maps, respectively. This paper evaluates the performance of various radio map construction methods in both indoor and outdoor environments. Four radio map construction methods, i.e., a point-by-point manual calibration, a walking survey, a semisupervised learning-based method, and an unsupervised learning-based method, have been compared. We also evaluate the performance of various types of radio map models that represent the characteristics of collected fingerprints. To demonstrate the importance of the radio map model, a new model named signal fluctuation matrix (SFM) was developed, and its performance was compared with that of the three conventional radio map models, respectively. The evaluation revealed that the performance of the radio maps was very sensitive to the design of radio map models and the number of fingerprints collected at each location. The performance achieved by SFM-based positioning was comparable with that of the other models despite using a small number of fingerprints.', 'title': 'Performance Evaluation of Radio Map Construction Methods for Wi-Fi Positioning Systems', 'embedding': []}, {'id': 14783, 'abstractText': 'Upscaling techniques have been extensively used to produce upscaled maps to fill data gaps serving various Earth observation models by providing area and landscape pattern information. Base maps as input for upscaling techniques inevitably have mapping errors that greatly impact the upscaling performance. However, the influence of mapping error on the representation of landscape pattern of upscaled maps has rarely been explored. To address this issue, the Crop Data Layer (CDL) data for two study sites were first used to generate agricultural maps as the base maps. A probability-based Monte Carlo algorithm was then used to simulate different error levels for the base maps. Two upscaling techniques, Fusing class Membership probability and Confidence level probability (FMC) and a conventional upscaling method (i.e., Majority Rule Based, MRB), were conducted. The results highlight that higher mapping error results in higher change of landscape pattern for upscaled maps. Overall, this work extends our understanding of the influence of mapping error on the upscaling performance. Also, it suggests that next generation upscaling techniques should greatly consider the mapping error and how to accurately present landscape pattern.', 'title': 'Comparing the impact of mapping error on the representation of landscape pattern on upscaled agricultural maps', 'embedding': []}, {'id': 14784, 'abstractText': 'Optimizing a network of maps among a collection of objects/domains (or map synchronization) is a central problem across computer vision and many other relevant fields. Compared to optimizing pairwise maps in isolation, the benefit of map synchronization is that there are natural constraints among a map network that can improve the quality of individual maps. While such self-supervision constraints are well-understood for undirected map networks (e.g., the cycle-consistency constraint), they are under-explored for directed map networks, which naturally arise when maps are given by parametric maps (e.g., a feed-forward neural network). In this paper, we study a natural self-supervision constraint for directed map networks called path-invariance, which enforces that composite maps along different paths between a fixed pair of source and target domains are identical. We introduce path-invariance bases for efficient encoding of the path-invariance constraint and present an algorithm that outputs a path-variance basis with polynomial time and space complexities. We demonstrate the effectiveness of our formulation on optimizing object correspondences, estimating dense image maps via neural networks, and 3D scene segmentation via map networks of diverse 3D representations. In particular, our approach only requires 8% labeled data from ScanNet to achieve the same performance as training a single 3D semantic segmentation network with 30% to 100% labeled data.', 'title': 'Path-Invariant Map Networks', 'embedding': []}, {'id': 14785, 'abstractText': 'In this work, we continue our study on analyzing student created mind maps automatically by providing a new methodology to select the technical vocabulary that students use in their mind maps. The basis of our previous experiments is an instructor chooses a set of twenty words used within the course that will be the set of words to test in mind maps. The instructor then creates their own mind map with this set of words, which is called the criterion map. Next, students create their mind maps using the same twenty words, and the criterion map and student map are analyzed with each other using various algorithms to produce metrics that quantify how similar the two maps are. When this activity is repeated longitudinally over a semester we can show that students are learning if their metrics of similarity are improving over time. One challenge, however, is which twenty words should be selected by the teacher. Similarly, will the set of twenty words impact the quality of observed learning. In 2011 and 2012, we collected our mind map data based on twenty words selected with no methodology (random). In 2013 and 2014, we created a methodology where approximately forty words are initially chosen, and these forty words are reduced down to 20 by creating a larger mind map and picking the words that have low connectivity. The hypothesis here is that less connectivity in the mind map will make it easier for the student to create their own quality maps. Our results show that this new methodology improves the arithmetic average of one of our best comparison metrics for all data points by a worse case of 2.4% better and best case 75% better.', 'title': 'Improved method for creating criterion maps for automatic mind map analysis', 'embedding': []}, {'id': 14786, 'abstractText': 'This paper presents preliminary simulations and analyses done to assess the feasibility of performing Map Relative Localization (MRL) with the Europa Lander LiDAR being developed for the Europa Lander Pre-Phase A concept. Map Relative Localization is the process of determining the horizontal position of a lander with respect to an onboard, a-priori map, by comparing the map to sensor observations of the terrain during deorbit, descent, and landing (DDL). Although kilometer-scale position knowledge is commonly available during DDL, landing in hazard-rich environments requires position errors of 100 m or less. Prior knowledge in the case of Europa Lander will be visual and topographic maps collected by the upcoming Europa Clipper mission. The Mars 2020 Lander Vision System (LVS) uses images from a camera to localize with respect to visual maps. This technology, as well as a 3D imaging LiDAR in development for hazard detection, is currently baselined for the Europa Lander Pre-Phase A concept. This paper investigates the potential use of the hazard detection LiDAR to perform MRL with respect to a 3D digital elevation model (DEM) provided by the Europa Clipper mission, as an alternative or backup solution to passive optical MRL. Compared to passive optical MRL, one advantage of LiDAR-based localization is that it is insensitive to lighting conditions, potentially relaxing requirements on synchronizing map acquisition and landing time of day. To analyze LiDAR based MRL performance, six representative terrains are synthetically up-sampled from Galileo-derived maps of Europa to a resolution of 0.5 m/px and covering an area of 4 km by 4 km. These maps are used as ground-truth to generate simulated noisy a-priori onboard topographic maps expected from Europa Clipper as well as simulated LiDAR DEMs generated at an altitude of 5 km during Europa Lander DDL. The simulated LiDAR DEM is matched against the simulated map via 2D normalized cross-correlation, exploiting the accurately known spacecraft attitude to avoid the need for more computationally intensive algorithms such as Iterative Closest Point (ICP). Two sources of measurement error are identified for analysis: 1) additive Gaussian noise in the range measurements from the Europa Lander LiDAR and the Europa Clipper derived maps and 2) errors in the LiDAR DEM induced by errors in the Europa Lander state estimate which is used to de-warp the LiDAR scan data into a DEM format. We assess the effect of each of these types of errors independently on matching performance as well as the overall performance when all types of error are introduced. Additionally, we present the result of a sensitivity study to terrain frequency content.', 'title': 'LiDAR-Based Map Relative Localization Performance Analysis for Landing on Europa', 'embedding': []}, {'id': 14787, 'abstractText': 'Disaster area mapping is critical to guiding evacuees to safety and aiding responders in decision-making. During disasters however, Cloud-based mapping services cannot be relied upon, because network infrastructures may have been damaged. In this study, we propose a disaster area mapping system that functions under challenged-network environments in a disaster area. The system infers a pedestrian map with walking speed information from data gathered by civilians and responders with mobile devices. To generate the map, the system addresses the following challenges: how to collect disaster area data, how to share data without continuous end-to-end networks, and how to generate maps without Cloud-based mapping services. First, the system leverages human mobility to collect disaster area data. Civilians and responders with mobile devices function as sensor nodes and log their GPS and velocity traces while moving based on the Post-Disaster Mobility Model. Second, the system uses mobile devices to establish a Delay-Tolerant Network, through which nodes opportunistically share data. Finally to generate the map, the collected data are routed to Computing Nodes: devices with more computational resources than mobile devices that are spatially-distributed across the disaster area. The Computing Nodes infer the map from the data and share it with evacuees. Through experimental evaluations and computer simulations, we found that the system significantly decreases the time required to generate and deliver a map to an evacuee, compared to a case without the system. Furthermore, the overall reduction in time increases as the size of the data required to generate the map and the number of DTN nodes increase.', 'title': 'Disaster area mapping using spatially-distributed computing nodes across a DTN', 'embedding': []}, {'id': 14788, 'abstractText': 'For the complex semantic features of objects in a geological map, maintaining a hierarchy between different geological objects is a big challenge when automatically generalizing the geological map. The typical methods focus on automation of geological map generalization or pay more attention to the hierarchical relation between geological objects, which reduces the accuracy of the geological map generalization result. Therefore, a conceptual framework that focuses on both the automated process and the geological objects is particularly important in developing an efficient software designed for automated generalization of geological maps. In this paper, we design a compound conceptual framework for automated generalization of geological maps based on multiple agents and workflow. In this framework, the process is divided into three stages: structure analysis, map generalization, and style standardization. The map objects in the source geological map are abstracted as diverse agents with different properties and behaviors, and the agents can communicate with each other when they are activated. Thus, the relationship of the map objects is coordinated in geological map generalization, avoiding the conflict between the different operation levels. The workflow technology is used to manage the automated process. We discuss the task, modeling method, and specific operation in every stage based on the current conceptual framework and the characteristics of a geological map. Finally, we use a simple geological map for experimental studies that verify the proposed conceptual framework. The result shows that it is advantageous to design the software for automated generalization of geological maps based on the proposed compound conceptual framework.', 'title': 'A Conceptual Framework for the Automated Generalization of Geological Maps Based on Multiple Agents and Workflow', 'embedding': []}, {'id': 14789, 'abstractText': 'Land use and land cover change (LULCC) can influence regional climate by altering the surface roughness, soil moisture, and heat flux partition. LULCC information therefore is important for providing a better understanding of land-atmosphere interaction. A key requirement for such data for use in climate studies is the generation of land use maps on at least an annual basis. However, current continuous annual land use and land cover maps are only available from 2001 to present, limiting the span of the period that can be studied. This study applied a random forests classifier based on nineteen phenological metrics to produce land use and land cover maps of China from 1982 to 2012 using Advanced Very High Resolution Radiometer (AVHRR) third generation NDVI (NDVI3g) dataset, and Moderate-resolution Imaging Spectroradiometer (MODIS) land cover type product (MCD12Q1). The overall accuracy is 86.3%, which indicates the reliability of the maps.', 'title': 'Continuous annual land use and land cover mapping using AVHRR GIMMS NDVI3g and MODIS MCD12Q1 datasets over China from 1982 to 2012', 'embedding': []}, {'id': 14790, 'abstractText': 'This paper presents a case study of flood mapping in Panamaram village, Wayanad, Kerala, India using SAR dataset. Rapid flood mapping can prove to be critical in decision making during flood due to high socio-economic loss associated. In this study, ground range detected (GRD) images of Sentinel-l mission, which are substantially less utilized, are exploited. Pre-event and post-event GRD images are analyzed in Sentinel Application Platform (SNAP) and extent of flooded area is mapped in GIS framework. The generated results are further validated by comparison of cumulative rainfall data during the flood period provided by Indian Meteorological Department, Thiruvananthapuram. Overall, study looks promising in developing flood maps subject to availability of real-time images.', 'title': 'RAPID FLOOD MAPPING USING SENTINEL-1A IMAGES: A CASE STUDY OF FLOOD IN PANAMARAM, KERALA', 'embedding': []}, {'id': 14791, 'abstractText': \"First name to gender mappings have been widely recognized as a critical tool to complete, study and validate data records in a range of different areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 6 million people, provided by a car insurance. We then study how naming conventions have changed over time and how they differ by nationality. Second, we build a probabilistic first name to gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in a two label and three label setting and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies' outcomes and find that our mapping produces high precision results. We validate that the additional information of nationality and year of birth improve the recall scores of name to gender mappings. Therefore, it constitutes an efficient process to improve data quality of organizations' records, whenever the attribute gender is missing or unreliable.\", 'title': 'Improving data quality through high precision gender categorization', 'embedding': []}, {'id': 14792, 'abstractText': 'The main task in navigation of an autonomous vehicle is to have accurate and robust localization. There are variety of localization techniques in study using particle filter; for localization with or without using map information in the measurements. There is a lack of study in localization methods employing map data in particle filter. This paper summarizes the localization techniques that use map information in particle filter for estimation. The study of various literature in the particle filter for localization using map data shows that, accurate and robust localization can be achieved for an autonomous vehicle provided the map to be used in the measurements is constructed precisely with the necessary features like road curbs, edges, trajectories etc.', 'title': 'Map-Based Particle Filter for Localization: Autonomous Vehicle', 'embedding': []}, {'id': 14793, 'abstractText': \"Accurate bathymetric mapping for shallow-water areas is essential for coastal and maritime engineering applications. However, traditional multibeam or light detection and ranging (LiDAR) survey techniques used to produce high-quality bathymetric maps are expensive. Satellite-derived bathymetry provides a fast and inexpensive method for the large-scale mapping of shallow-water areas and can overcome the complexities of traditional bathymetric mapping methods in these areas. Traditionally, linear regression models, most commonly the Stumpf model, are used for satellite-based bathymetric modeling. However, nonlinear artificial neural network (ANN) models have been recently developed and implemented for satellite-based bathymetric modeling and are under significant investigation to develop the most accurate and optimal model. This article proposes two new hybrid ANN-based models for bathymetric modeling and investigates their performance using satellite imagery data and ``truth'' depth data for a coastal shallow-water study area. Two-hybrid ANN algorithms are developed, namely, particle swarm optimization (PSO)-ANN and optimally pruned extreme learning machine (OPELM), and their results are compared with the traditional Stumpf method and current state-of-the-art ANN model. The study area dataset comprises the ``truth'' depth data from a nautical chart of the Alqumriyah Island study area in Saudi Arabia and the corresponding spectral reflection values of green, blue, and near-infrared bands from the free-of-charge Level-1C product of Sentinel-2A images used to train and validate the two newly developed models and the traditional models. The results show that the developed OPELM method can accurately derive the bathymetry and is superior to the developed PSO-ANN model, the current state-of-the-art ANN model, and the traditional Stumpf model by 12.10%, 18.76%, and 32.46%, respectively. The OPELM model can also be used for bathymetric modeling of shallow-water areas with depths up to 30 m with a high level of accuracy compared with the current state-of-the-art ANN and traditional methods. The significant contribution of this research is that it is the first investigation of the artificial intelligence-based hybrid OPELM method for accurate bathymetric modeling and will certainly encourage further investigations of hybrid models. Moreover, this research explores whether these developed hybrid models can meet the International Hydrographic Organization standards for hydrographic survey applications.\", 'title': 'Hybrid Artificial Neural Networks for Modeling Shallow-Water Bathymetry via Satellite Imagery', 'embedding': []}, {'id': 14794, 'abstractText': 'Context: Mobile computing has emerged as a disruptive technology that has empowered its users with portable, connected and context-aware computation. However, issues such as resource poverty, energy efficiency and specifically data security and privacy represents the critical challenges for mobile computing. Objective: The objective of this work is to systematically identify, taxonomically classify and map the state-of-research on adaptive security (a.k.a. self-protection) for mobile computing. Methodology: We followed evidence based software engineering method to conduct a systematic mapping study of 43 qualitatively selected studies-published from 2003 to 2017-on adaptive security for mobile computing. Results and Conclusions: Classification and mapping of the research highlights three prominent themes that support adaptive security for (i) Mobile Device Data and Resources, (ii) Mobile to Mobile Communication, and (iii) Mobile to Server Communication. Mapping analysis suggests that security of mobile device data and resources is the most researched theme. The mapping study highlights that active and futuristic research trends are primarily focused on security as a service, whereas; the frequent research challenges relate to self-protecting mobile devices, user-driven privacy decisions and context-aware security. The results of the mapping study facilitate knowledge transfer that can benefit researchers and practitioners to understand the role of adaptive and context-aware security in mobile computing environments.', 'title': 'Classification and Mapping of Adaptive Security for Mobile Computing', 'embedding': []}, {'id': 14795, 'abstractText': 'This paper investigates spaceborne multiple multispectral data-fusion and blending to generate an integrated data with higher spatio-spectral resolution and spectral coverage in order to obtain improved geological mapping. A hybrid approach using Gram-Schmidt pan-sharpening and Inverse Distance Weighting (IDW) based downsampling technique is developed to generate integrated data from multiple multispectral data. In this study, Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER), Landsat 8, and Sentinel-2 data have been used to evaluate the developed approach for lithological mapping. Liikavaara to Puoltikasvaara including Nautanen and nearby-mining area, in the Gällivare district of Norrbotten county, Sweden, is chosen as a case study. Lithological map of the study area is produced using Support Vector Machine (SVM) classifier. Bedrock geological map from the Geological Survey of Sweden (SGU) is used for classification accuracy assessment. The results show that integrated data produced better accuracy than original individual spaceborne multispectral data for lithological mapping of the study area.', 'title': 'Multiple Multi-Spectral Remote Sensing Data Fusion and Integration for Geological Mapping', 'embedding': []}, {'id': 14796, 'abstractText': 'This paper is about an algorithm that displays the elevation and temperature of a surveillance area in the form of a map using a mobile robot platform equipped with a Lidar and a thermal imaging camera, and determines whether it is abnormal. Localization is performed using Lidar, and the point cloud and thermal distribution at the corresponding location are expressed as a top projection type map. The collected elevation and temperature distribution maps are checked for anomalies using the first-stage auto-encoder and second-stage CNN algorithm. We collected data on normal situations and abnormal situation DB produced by various size boxes and gas burners to study. As a result of the study, it was possible to detect 74.4% for the first stage, 94.5% for the second stage, and 80.2% for the first stage and 99.3% for the second stage for the thermal map. The results of this study will be used in areas such as security robots and guide robots.', 'title': 'Anomaly Detection using Elevation and Thermal Map for Security Robot', 'embedding': []}, {'id': 14797, 'abstractText': 'The evolution of coherent waves in two-dimensional systems with a random potential can give rise to an interesting phenomenon known as Branched Flow. In this process, a wave scatters from a weak random potential with correlation length longer than the wavelength, and forms focused channels that keep dividing as the wave propagates, creating a pattern resembling the branches of a tree. This phenomenon was observed for electrons [1-3], for a specific example of microwaves [4] and for ocean waves [5], but thus far never for light at optical frequencies. Furthermore, the statistical features of branched flow were predicted theoretically but were never observed in any experimental system. Here, we present the first observation of branched flow in optics, prove that the experiments represent branched flow, and study the statistical features. In our experiments, we couple an optical beam to a thin liquid soap film and observe its evolution within this thin membrane. The light experiences scattering from thickness variations in the soap film, which acts as a two-dimensional medium with a random potential. The beam propagates and scatters from the random thickness variations, forming focused branches that keep dividing, ending up in a pattern that resembles the branches of a tree, as shown in Fig.1a. To view the thickness variations directly, we construct a white light microscope illuminating the thin soap film from above, and observe the colorful map shown in Fig. 1b. The colors in Fig. 1b are true colors, and they emerge due to the reflection of white light from the thin soap film, indicating the local thickness (Fig.1c). The colors are mapped to the thickness map shown in Fig. Id, and from that to a two-dimensional map of effective refractive index landscape. The formation of the branches is the result of the appearance of caustics in the random field, following the variations in the refractive index. The theory of Branched Flow [6] gave rise to several predictions, most of which have never been studied in experiments. One example for such a prediction is the distance to the first caustic, d<sub>0</sub>, which was predicted by never observed. From the experiments, we extract d<sub>0</sub> using the Scintillation Index - the variance of the intensity of the fluctuations. It is a convenient notion, because caustics give rise to the highest intensity fluctuations, hence the scintillation index is a measure of the steepness of the caustics. The first caustic is the steepest one; hence, it corresponds to the peak in the scintillation index shown in Fig. 1e. The distance to the first caustic is ~1 in units of ζε<sup>-2/3</sup>, correlation length and potential strength, as shown in that figure. In this regime, the phenomenon of branching of caustics perfectly matches the model of branched flow. To support our experiments, we also carry out simulations (Fig. 1e), of a coherent beam launched into a random 2D potential constructed from the actual experimental image. The beam splits and divides by the potential variations, displaying branched flow as in experiments. From the experiments, we extract additional statistical features of branched flow that were predicted but have never been observed, such as the statistics of the extreme events. In addition, we also study branched flow in curved space (where the soap membrane is curved) and the nonlinear behavior occurring when the light exerts forces on the soap bubble.', 'title': 'Observation of Branched Flow of Light', 'embedding': []}, {'id': 14798, 'abstractText': 'This paper presents the dynamic analysis of two discrete logistic chaotic maps versus the conventional map. The first map is the fractional logistic map with the extra degrees of freedom provided by the added number of variables. It has two more variables over the conventional one. The second map is the double-humped logistic map. It is a fourth-order map which increases the non-linearity over the conventional one. The dynamics of the three maps are discussed in details, including mathematical derivations of fixed points, stability analysis, bifurcation diagrams and the study of their chaotic regions. The chaotic behavior of the three maps, is investigated using the Maximum Lyapunov exponent (MLE).', 'title': 'Dynamics of fractional and double-humped logistic maps versus the conventional one', 'embedding': []}, {'id': 14799, 'abstractText': 'Time-of-Flight (TOF) PET data determines the attenuation map up to a constant. MLAA (Maximum Likelihood Activity and Attenuation Estimation) was proposed for this purpose. However, in real systems the estimated attenuation map usually results in bias and artifacts, due to various factors such as non-uniform timing resolution, detector timing drift, and biased scatter estimation. Moreover, MLAA has much higher computational cost than conventional PET reconstruction. Improving the practical performance of MLAA is important. We proposed an efficient and robust emission-attenuation joint estimation framework, based on the condition that regions with almost uniform attenuation coefficients are segmented. Following the derivation, the update of attenuation map only requires a few weighted additions in sinogram space, which reduces overall computational cost significantly compared with conventional MLAA, as the latter demands at least two backward projections in each iteration. Furthermore, as the parameter space for the attenuation map is reduced and that the math model encourages an averaging effect in reach region, the bias and artifacts due to various factors above can be reduced. We used clinical TOF PET data to evaluate the performance of our proposed method. In each iteration, the computation time for updating the attenuation map was less than 20% of that for conventional MLAA, leading to significant improvement in overall computation efficiency (twice as fast as conventional MLAA). More importantly, the method resulted in high quantitative accuracy. In population study, SUV computed with the estimated attenuation map and the CT based attenuation map had maximum relative error less than 5.7% in multiple VOIs including the spine, liver, kidney, and heart. Our method can be used as a robust and efficient solution to estimate the attenuation map for quantitative PET image reconstruction, based on a single assumption that the attenuation coefficients are similar in each segmented regions. The numerical error caused by treating attenuation coefficients as uniform within each segmented region is acceptable.', 'title': 'Ultra efficientand robust estimation of the attenuation map in PET imaging', 'embedding': []}, {'id': 14800, 'abstractText': \"This study aimed to assess the potential of GLAS (Geoscience Laser Altimeter System) LiDAR data to overcome the saturation at high AGB values of existing AGB map on Madagascar (Vieilledent's AGB map [1]). First, spatially distributed estimations of AGB were obtained from GLAS data. Second, the difference between the Vieilledent's AGB map and GLAS derived AGB at each GLAS footprints location was calculated and a spatially distributed additional correction factors were obtained. Thanks to the spatial structure of these additional correction factors, an ordinary kriging interpolation was thus performed to provide a continuous correction factor map. Finally, the existing and the correction factor map were summed to improve the Vieilledent's AGB map. Results showed that the integration of GLAS data overcome the saturation at high AGB of Vieilledent's AGB map and allow AGB estimation until 650 t/ha (maximum AGB values from Vieilledent AGB map was 550 t/ha).\", 'title': 'Integration of spaceborne lidar data to improve the forest biomass map in madagascar', 'embedding': []}, {'id': 14801, 'abstractText': 'Temporary of preservation regional planning map need to evaluate and renewable execute so deviate possibilities can be prevented. This study is conducted to evaluate the regional planning map in East Lombok based on potential land-use using directive and existing land-use map. Directive land-use map is made by overlaying of some parameters, such as the slope, soils, and rainfall using GIS. Existing land-use map produced by visual interpretation is utilizing Landsat 8 OLI satellite imagery. The field survey did use stratified random sampling to validate parameters, accuracy field survey is about 68% of the existing land-use map. Result based on spatial and regional planning map evaluation of East Lombok using directive land-use and the existing land-use map of suitability is good enough reach 80.15% and not suitable reach 19.85%.', 'title': 'Evaluation of spatial and regional planning map using remote sensing and GIS in East Lombok Indonesia', 'embedding': []}, {'id': 14802, 'abstractText': 'This study is about the disastrous flooding of an Indian metropolitan area of Chennai when the rain had nearly broken the record of 100-years with 374 mm rain falling on December 1, 2015, virtually breaking the November monthly average of 407.4 mm in a day. This city with a population of approximately 6.7 million people came to a standstill. Astonishingly, one of the biggest software development hubs in India was struggling for data and tools to identify which parts of the city were most affected and vulnerable to such climate phenomena. A group of software engineers quickly came up with an idea of using a Flood Map tool managed through crowdsourcing to help the citizens of Chennai and prevent further casualties. They developed a map-based tool called Flooded Streets to report flooded streets using OpenStreetMap (OSM) data. Using this Flood Map tool, anyone in the crowd could click on a street if they knew it was flooded and update the map information. Within the following 24 hours, over 2,500 streets had been reported as flooded by the citizens of Chennai using the Flood Map tool. An ordinary citizen could zoom into a locality, visualize which streets are reported as flooded and decide their next course of action. This map was also a great aid tool for relief and aid workers to track the flooded paths and provide appropriate aids in that area. The map consists of a base layer of low-lying areas created using elevation models from ISRO and NASA, and flooded areas from UNITAR. The map interactivity was built using Mapbox GL and hosted on GitHub. This crowdsourced sensing system is an extraordinary example of disaster response using the crowdsourcing concept, which potentially helped millions of people with the minimum time and resources but with great crowd contributions from both experts and non-experts.', 'title': 'Flooded streets — A crowdsourced sensing system for disaster response: A case study', 'embedding': []}, {'id': 14803, 'abstractText': \"Chaotic dynamics is an important source for generating pseudorandom binary sequences (PRBS). Much efforts have been devoted to obtaining period distribution of the generalized discrete Arnold's Cat map in various domains using all kinds of theoretical methods, including Hensel's lifting approach. Diagonalizing the transform matrix of the map, this paper gives the explicit formulation of any iteration of the generalized Cat map. Then, its real graph (cycle) structure in any binary arithmetic domain is disclosed. The subtle rules on how the cycles (itself and its distribution) change with the arithmetic precision e are elaborately investigated and proved. The regular and beautiful patterns of Cat map demonstrated in a computer adopting fixed-point arithmetics are rigorously proved and experimentally verified. The results can serve as a benchmark for studying the dynamics of the variants of the Cat map in any domain. In addition, the used methodology can be used to evaluate randomness of PRBS generated by iterating any other maps.\", 'title': \"The Graph Structure of the Generalized Discrete Arnold's Cat Map\", 'embedding': []}, {'id': 14804, 'abstractText': 'The significant capability of synoptic coverage of satellite remote sensing data at the time of advent providing accurate and immediate valuable information on various aspects. The progressive development of remote sensing increases its capability to identify and map the precious and valuable materials and minerals. Spectral Angle Mapper (SAM) is classifiers of supervised classification for mapping and classification. The characteristic of SAM is based on similarity between image spectra and reference spectra on the behalf of tolerance level of specified maximum angle of threshold. In this research work SAM algorithms applied for mapping of Chhabadiya talc mineral. In this research work SAM algorithms defines the applicability of similarity of angle and value of threshold parameters of spectral angle which show capability to interpret or map the maximum and minimum abundance of talc minerals. For this research work Hyperion hyperspectral remote sensing data used for study of applicability and efficiency of SAM algorithms for mineral mapping. The quality and abundance of minerals completely depend on the minimum degree of SAM threshold parameter. Maximum spectral angle shows maximum mapping area with minimum similarity, but low spectral angle shows small and more abundant mapping area with maximum similarity in the hyperspectral image. Conclusion of this research work verify the identification of minerals are depend on the spectral and spectral characteristics of hyperspectral remote sensing data and mapping with qualitative abundance of minerals depend on the lower value of spectral angle and threshold of SAM algorithms or maximum similarity of spectra.', 'title': 'Comparative Evaluation Threshold Parameters of Spectral Angle Mapper (SAM) for Mapping of Chhabadiya Talc Minerals, Jahajpur, Bhilwara, India using Hyperion hyperspectral Remote Sensing Data', 'embedding': []}, {'id': 14805, 'abstractText': \"This paper shares an innovative approach of using mind map to support active leaning for electrical &amp; electronics engineering students. It is also aimed at exploring students' experiences in using mind maps. In this study, 80 students were asked to respond to 2 online surveys; one, prior to the mind map activity and the other after. The findings for the survey prior to the mind map activity showed that, majority of the students are familiar with mind map and are aware that mind map does help them to learn and that most are willing to put in some effort to use mind map for learning purposes. The second survey's findings showed that even though some respondents have pessimistic attitude, generally the response have been positive and well received and some students ask for more guidance and training to be able to master the mind map learning technique.\", 'title': 'An Innovative Approach of Using Mind Map to Support Active Learning for Engineering Students', 'embedding': []}, {'id': 14806, 'abstractText': 'Generally robots need to use maps while navigating from one place to another. Metric (grid-based) and topological (node-based) paradigms are utilized to produce maps. In this work, topological map of a large-scale indoor environment is constructed from its metric map. To do this, we follow three stages: 1) Construction of metric map; 2) Determination of nodes; 3) Connecting the nodes to produce the topological map. Apart from the studies in literature, only the new cells in the metric map are considered to determine the nodes. As a result, the topological map grows in an online manner and computational cost of spectral clustering and extended Voronoi graph is reduced. The methods are tested in a simulated model of ESOGU Electrical Engineering Laboratory building in Gazebo simulation environment by using ROS.', 'title': 'A comparative study for topological map construction methods from metric map (In English)', 'embedding': []}, {'id': 14807, 'abstractText': 'With the rapid development of solar distribution, solar panel mapping is becoming increasingly valuable to decision-makers. Weakly supervised methods have been developed to reduce the cost in training sample collection, and the most successful ones follow the alternative training scheme, which first generates coarse object localizations as pseudo labels (PLs) and then utilizes these PLs to train an end-to-end network for object extraction. As remote sensing images are typically characterized by multiple occurrences of objects and complicated background, the alternative training scheme suffers from low mapping accuracy and deficient boundary maintenance due to the varying quality of PLs. In this paper, we focus on addressing these problems by adaptively adjusting the contributions of quality-varying PLs and propose a novel self-paced residual aggregated network (SP-RAN) for solar panel mapping. Specifically, with the initial PLs generated by gradient-weighted class activation mapping, a residual aggregated network is designed for target mapping with a special consideration for the capability in producing complete and well-shaped mapping results. Considering the inconsistent quality of PLs, an effective confidence-aware (CA) loss is developed to emphasize the contribution of high-quality PLs and alleviate the negative impacts brought by the bad-quality ones in the training phase. Moreover, to concentrate on boundary maintenance, a novel self-paced label correction (SP-LC) strategy is proposed to selectively update PLs by considering their reliability. Extensive experimental comparison with state-of-the-art methods and ablation study on two aerial data sets and a remote sensing data set demonstrate the superiority of the proposed method.', 'title': 'SP-RAN: Self-paced Residual Aggregated Network for Solar Panel Mapping in Weakly Labelled Aerial Images', 'embedding': []}, {'id': 14808, 'abstractText': 'In vehicle and robot navigation low-level tasks such as path planning, obstacle avoidance and autonomous operation are extensively studied nowadays. Most of these task require map building. In this paper a map representation is discussed with the focus for the singular domain of our Neobotix MP500 mobile robot. Among others the state of the art map building techniques will be introduced such as topological map, line map, landmark-based map and of course in more detail the occupancy grid based map. The probabilistic representation of the occupancy grid will be examined as a map building problem for the given mobile robot.', 'title': 'Probabilistic occupancy grid map building for Neobotix MP500 robot', 'embedding': []}, {'id': 14809, 'abstractText': 'Most navigation aids for blind people require users to equip various equipment and multi-sensor subsystems for environmental detection. This study integrates the visual simultaneous localization and mapping system with pre-established maps to develop a navigation aid system for blind people. The system operates similarly to how Google Map uses pre-established environmental maps and landmarks. While the user walks, the system reads the map in the database and synchronizes feature points from the visual simultaneous localization and mapping system onto the environment map. In addition, the aid employs audio cues to notify users of signpost information and enables users to make inquiries on specific destinations via human-machine interaction.', 'title': 'A Navigation Aid for Blind People Based on Visual Simultaneous Localization and Mapping', 'embedding': []}, {'id': 14810, 'abstractText': \"The elastic map, or generalized Hooke's Law, associates stress with strain in an elastic material. A symmetry of the elastic map is a reorientation of the material that does not change the map. We treat the topic of elastic symmetry conceptually and pictorially. The elastic map is assumed to be linear, and we study it using standard notions from linear algebra—not tensor algebra. We depict strain and stress using the ‘beachballs’ familiar to seismologists. The elastic map, whose inputs and outputs are strains and stresses, is in turn depicted using beachballs. We are able to infer the symmetries for most elastic maps, sometimes just by inspection of their beachball depictions. Many of our results will be familiar, but our versions are simpler and more transparent than their counterparts in the literature.\", 'title': 'Elastic symmetry with beachball pictures', 'embedding': []}, {'id': 14811, 'abstractText': 'Most autonomous mobile systems require pre- generated detailed maps that are expensive to prepare. In this study, we proposed an autonomous mobile system that does not rely on a detailed map; instead, it uses a simple map for robot navigation. The robot has two maps; one is a detailed map created by sensor observation during movement, and the other is a simple map provided as pre-information. The robot performs the matching between detailed and simple maps and converts waypoints on the simple map to waypoints on the detailed map. Matching is performed based on straight line matching and is optimized by genetic algorithm. Experiments were conducted in buildings. Our method is compared with linear transformations and conditions that our method works effectively or not are confirmed.', 'title': 'Transformation Between Simple and Detailed Maps Based on Line Matching for Robot Navigation', 'embedding': []}, {'id': 14812, 'abstractText': 'Recent work in data visualization has demonstrated that small, perceptually-distinct color palettes such as those used in categorical mapping can connote significant affective qualities. Data that are mapped or otherwise visualized are also often emotive in nature, either inherently (e.g., climate change, disease mortality rates), or by design, such as can be found in visual storytelling. However, little is known about how the affective qualities of color interact with those of data context in visualization design. This paper describes the results of a crowdsourced study on the influence of affectively congruent versus incongruent color schemes on categorical map-reading response. We report both objective (pattern detection; area comparison) and subjective (affective quality; appropriateness; preference) measures of map-reader response. Our results suggest that affectively congruent colors amplify perceptions of the affective qualities of maps with emotive topics, affective incongruence may cause confusion, and that affective congruence is particularly influential in maps of positive-leaning data topics. Finally, we offer preliminary design recommendations for balancing color congruence with other design factors, and for synthesizing color and affective context in thematic map design.', 'title': 'Affective Congruence in Visualization Design: Influences on Reading Categorical Maps', 'embedding': []}, {'id': 14813, 'abstractText': 'The shadowing property has an important significance in terms of theory and application. In this paper, we study the shadowing property of product map in the two-dimensional product space. By means of properties of zero density sets and product map, we give the following conclusions. (1)The product map f × g has the pseudo orbit tracing property if and only if the map f and g has the pseudo orbit tracing property. (2)The product map f × g has the periodic pseudo orbit tracing property if and only if the map f and g has the periodic pseudo orbit tracing property. (3)The product map × g has the asymptotic average shadowing property if and only if the map f and g has the asymptotic average shadowing property. These results enrich the theory of the shadowing property of the product map in two-dimensional product spaces.', 'title': 'Various Shadowing Property of Product Map', 'embedding': []}, {'id': 14814, 'abstractText': 'Identification of targets for catheter ablation of arrhythmias remains a significant challenge. Traditional mapping techniques often neglect the tissue repolarization, hampering the detection of pro-arrhythmic regions. We have recently developed a novel mapping procedure, termed the Reentry Vulnerability Index (RVI), which incorporates both activation (AT) and repolarization (RT) times to identify ablation targets. Despite showing promise in a series of experiments, the RVI requires further development to enable its incorporation into a clinical protocol. The goal of this study was to use computer simulations to optimize the RVI procedure for its future usage within the clinic. A 2D sheet model was employed to investigate the behavior of the RVI algorithm under mapping catheter recordings resembling clinical conditions. Conduction block following premature stimulation was induced and mapped in a cardiac tissue model including repolarization heterogeneity. RVI maps were computed based on the difference between RTs and ATs between successive pairs of electrodes within a given search radius. Within 2D sheet models we show that RVI maps computed on irregular sparse recording sites were in good agreement with high resolution maps. We concude that the RVI algorithm performed well under clinically-relevant mapping conditions and may be used to guide ablation.', 'title': 'Optimization of a Novel Activation-Repolarization Metric to Identify Targets for Catheter Ablation', 'embedding': []}, {'id': 14815, 'abstractText': 'Due to the similarity of the radar backscatter in flooded and unflooded conditions over particular areas, it is not possible to carry out a comprehensive SAR-based flood mapping at large scale. In this paper, an additional information layer derived from Sentinel-l time series data, called Exclusion map (EX-map), is introduced. Its aim is to enhance and complement the results of automatic change detection-based flood mapping methods. The EX-map aims at delineating areas where observed variations of SAR backscatter do not allow detecting the appearance of floodwater. The EX-map is mainly composed of the following land cover classes: topographic shadow/layover, double bounce and smooth tarmac in urban areas, arid areas, dense vegetation and permanent water bodies. The method is evaluated over six study sites across the globe and tested for different flood events. The EX-map not only increases the classification accuracy of change detection-based flood maps derived from Sentinel-l data from 95.92% to 97.02%, but also enables a better interpretation of any SAR-based floodwater map.', 'title': 'Deriving an Exclusion Map (Ex-Map) from Sentinel-l Time Series for Supporting Floodwater Mapping', 'embedding': []}, {'id': 14816, 'abstractText': 'Recently, various types of robots have developed for the exploration and monitoring in unknown and dynamic environment. Especially, the expectation of robots used in disasters is increasing to prevent the second disaster. Especially, it is very important to extract the environmental information related for remote control and monitoring of mobile robots. Simultaneous localization and mapping (SLAM) is an important methodology to deal with environmental information. Various types of methods for SLAM have been proposed such as Extended Kalman Filter (EKF) SLAM, Graph SLAM, visual SLAM, and cooperative SLAM. In general, there are two main approaches of grid mapping and topological mapping in the study on SLAM. In this talk, we focus on topological mapping methods to extract environmental features from a 3D point cloud. Various types of unsupervised learning methods based on topological mapping have been proposed to deal with environmental features in unknown environments. One of them is Growing Neural Gas (GNG) that can dynamically change the topological structure composed of nodes and edges. The advantage of GNG is in the incremental learning capability of nodes and edges according to target data distribution. We have proposed several different topological mapping methods based on GNG to extract the environmental features from a 3D point cloud until now. In this talk, we explain the research background of SLAM, the learning algorithm of GNG, and experimental results of GNG for SLAM in various environments. Next, we explain multi-layer GNG to extract hierarchical features in environmental maps as a multi-scale approach, and batch learning algorithm for GNG (GL-GNG) to improve the convergence property. Furthermore, we explain the modified method of GNG-utility (GNG-U), that we called GNG-U2. GNG-U2 can improve the real-time adaptability of extracting topological structure in non-stationary data distribution. Next, show experimental results of SLAM based on GNG-U2 in dynamic environments. Finally, we show several other application examples of topological approaches, and discuss the applicability and future direction of topological approaches in robotics.', 'title': 'Topological approaches for simultaneous localization and mapping', 'embedding': []}, {'id': 14817, 'abstractText': \"Landsat imagery can be used to establish spatial habitat suitability modeling for prediction habitat quality and evaluation potential habitat. However, the imagery has to limit information for specific habitat characteristics such as turtles' habitat. Chelodina mccordi is an endemic turtle from Roti Island but many previous studies have been done either biological or non-spatial studies. This study uses Landsat-8 OLI and TIRS as actual data and support with Landsat-5 TM as historical data, through a Geographic Information System (GIS). This study aims to create a spatial habitat suitability map model of C. mccordi and its mapping accuracy. The method that is used for modeling is overlaying indicative parameter maps use of logistic regression to presents the habitat suitability index (HSI). There are nine parameters used for modeling in this study, i.e. normalized difference water index (NDWI), land surface temperature (LST), slope, the topographic wetness index (TWI), distance from high canopy density, distance from settlement and agriculture, distance from freshwater, distance from the sea or salty, and distance from the street. We found three parameters that have strong contribution i.e. the NDWI, the LST, and the TWI. The result of spatial habitat suitability model of C. mccordi based on Landsat-8 OLI/TIRS and GIS analysis presents the value of mapping accuracy is about 75%.\", 'title': 'Spatial Habitat Suitability Modeling of the Roti Snake-Necked Turtle (Chelodina Mccordi) Based on Landsat-8 Imagery and GIS', 'embedding': []}, {'id': 14818, 'abstractText': \"The influence of rocks on the microwave thermal emission (MTE) of the lunar regolith has not been fully studied with the four-channel microwave radiometer (MRM) data onboard Chang'e-1/2 satellites. To highlight the influence of the rocks on the MTE of the regolith, the Hertzsprung basin located near the lunar equator in highland regions is selected as the study area. The comparison between the brightness temperature (TB) maps derived from the Chang'e-2 MRM data and rock abundance (RA) map derived from the Diviner data postulates three special issues about the correlation between the MTE features and the regolith with rocks. Then, aimed to interpret the issues, two new layered regolith models and the corresponding radiative transfer models are constructed. The main results are as follows. First, the observation and the simulation both verify that the regolith with rocks will provide a cold TB anomaly at night and at low frequencies at daytime, but result in a hot anomaly at high-frequency at daytime. Moreover, the temperature profiles of the regolith with surface and hidden rocks are evaluated with the theoretical model. Second, the simulation results verify the existence of the hidden rocks in the lunar regolith assumed when studying the TB performances of the Hertzsprung basin. Third, the rock distribution revealed by the TB maps shows a different view compared to that estimated by the Diviner data in space and values, and the change of the TB with frequencies postulates a new view about the variation of the RA with depth. This study hints that the MRM data probably provide a new way to quantitatively estimate the RA values of the lunar regolith, and the results will be meaningful to improve understanding of the evolution of the impact craters.\", 'title': 'Re-Evaluating Influence of Rocks on Microwave Thermal Emission of Lunar Regolith Using CE-2 MRM Data', 'embedding': []}, {'id': 14819, 'abstractText': 'Background: Mining Software Process (MSP) helps distill important information about software process enactment from software data repositories. An increasing amount of research effort is being dedicated to MSP. These studies differ in various aspects (e.g., topics, data, and techniques) of MSP. Objective: We aim to study the state of the art on MSP from following aspects, i.e., research topics, data sources, data types, mining techniques, and mining tools. Method: We conducted a systematic mapping study on the research relevant to MSP at both microprocess and macroprocess levels. Results: Our mapping study identified 40 relevant studies that can be grouped into microprocess and macroprocess levels. The identified mining techniques have been mapped onto the associated mining tools that fall into four types. Driven by the three research questions which represented in a meta-model, the findings revealed the correlations among the research topics, data sources, data types, mining techniques, and mining tools. Conclusion: It is observed that in order to discover the software process model or map, the main data source is from industrial project. Current mining techniques for microprocess research are mostly business process mining or sequence mining techniques used to recover descriptive software process. In addition, various machine learning algorithms and novel proposed methods are used to improve the accuracy of macroprocess level factors (e.g., software effort estimation).', 'title': 'A Mapping Study on Mining Software Process', 'embedding': []}, {'id': 14820, 'abstractText': 'Summary form only given. Quick response to a large-scale natural disaster such as earthquake and tsunami is vital to mitigate the further loss. Remote sensing especially the spaceborne sensors provide the solution to monitor a huge area in a short time and with regular revisit circle. Damage ranges and damage levels of the destructed urban areas are extremely important information for rescue planning after an event. Rapid mapping of the urban damage levels with synthetic aperture radar (SAR) is still challenging. Compared with single-polarization SAR, fully polarimetric SAR (PolSAR) has the better potential to understand the urban damage from the viewpoint of scattering mechanism investigation. In radar polarimetry, dominant double-bounce scattering mechanism in urban area is primarily induced by the ground-wall structures and can reflect the changes of these structures. In this sense, urban damage level in terms of destroyed ground-wall structures can be indicated by the reduction of the dominant double-bounce scattering mechanism, which is the basis of this study. This work first establishes and validates the linear relationship between the urban damage level and the established polarimetric damage index using polarimetric model-based decomposition. Then, efforts are focused on the development of a rapid urban damage level mapping technique which mainly includes two steps of urban area extraction and polarimetric damage level estimation. The 3.11 East Japan Earthquake and Tsunami inducing great-scale destructions selected for study using multi-temporal spaceborne PolSAR data. Experimental studies demonstrate that the estimated damage levels are closely consistent to the ground-truth. The final urban damage level map for the full scene is generated thereafter. Results achieved in this study further validate the necessity of exploring fully polarimetric technique for damage investigation.', 'title': 'Urban damage mapping using fully polarimetric SAR data with scattering mechanism modeling and interpretation technique', 'embedding': []}, {'id': 14821, 'abstractText': \"The main aim of this study was to compare and discuss the effective brain connectivity maps of dyslexic and control groups in terms of differences and similarities. The differences and similarities that may be found in the study, could provide information for the related future studies. There were total of 58 subjects which were 27 control and 31 dyslexics data in this study. All the data according to the groups were averaged with respect to time and group data were generated. Then, effective connectivity maps for both dyslexic and control groups were constructed using the Dynamic Bayesian Networks algorithms. The symmetric connectivities which we call strong connections between electrode pairs were extracted and mainly used for discussion. It was observed that there are differences in effective connectivity maps of both groups in terms of connections' density and directions. The results should also be checked by brain anatomy experts anatomically and by comparing with literature of medicine, their validity and usabilities could be tested.\", 'title': 'The interpretation of the effective connectivity maps obtained by using Dynamic Bayesian Networks on EEG data', 'embedding': []}, {'id': 14822, 'abstractText': 'Landslide is usually happening anywhere in Malaysia without any warning. Most of landslide occurs at manmade slope and natural slopes according to their slope gradient. Recently, Unmanned Aerial Vehicle (UAV) has been widely used for mapping purpose. The first objective of this study is to assess capabilities of UAV in the production of digital photogrammetric products based on landslide risk area along Jeli-Gerik highway. The study area covered is along Jeli-Gerik highway and its surrounding area. Primary data of ground control points (GCP) and check points (CP) were established using real time kinematic technique of Global Positioning System (GPS). All the aerial photographs were processed using digital photogrammetric software and the output in the form of digital elevation model (DEM) and orthophoto were produced. The second objective is to produce landslide risk area map using UAV technology. The risk area map separates the landslide area into three groups which are Low Risk, Medium Risk and High Risk. The parameters that were used to generate the landslide risk area map are slope, aspect and elevation. GIS based Landslide Hazard Zonation models helps not only to map and monitor landslides but also to predict future slope failures. In addition, data collected by UAV can be used to produce accurate ground surface surveys. As conclusion, UAV system has potential use for large scale mapping and could be used by public work department in order to monitoring of landslide area with low cost, less manpower and faster. The ability to detect landslide scarps will lead to a better understanding of landslide mechanisms for the propose area, thus leading to an enhanced identification of the most likely failure sites within a landslide-prone area.', 'title': 'UAV Based Multi-spectral Imaging System for Mapping Landslide Risk Area Along Jeli-Gerik Highway, Jeli, Kelantan', 'embedding': []}, {'id': 14823, 'abstractText': \"This paper explains the details about the development and implementation of various map presentation techniques during a development of virtual reality (VR) application. Maps presentation refers to the method of how the development is being visualized to the users including its orientation. In VR, map presentation is often considered as a secondary 2-Dimensional display which shows the location and destination points. In addition, this paper explains the technical experience as well as the findings that have been discovered. These map presentation techniques are tested using a head-mounted display to visualize a virtual world. In this study, a number of participants are invited to try and give feedback upon completing the completion of the experiment. Findings collected during the experiment shows that a proper map presentation technique increases end-users' VR experience. In addition to a map presentation, this study also addresses issues related to map activation and orientation techniques towards improving the effectiveness during a navigation process.\", 'title': 'Virtual Reality Mini Map Presentation Techniques: Lessons and experience learned', 'embedding': []}, {'id': 14824, 'abstractText': 'Georeferenced social media data are gaining increased application in creating near real-time flood maps needed to improve situational awareness in data-starved regions. However, there is growing concern that the georeferenced locations of flood-related social media contents do not always correspond to the actual locations of the flooding event. But to what extent is this true? Without this knowledge, it is difficult to ascertain the accuracy of flood maps created using georeferenced social media contents. This study aims to improve understanding of the extent to which georeferenced locations of social media flood reports deviate from the actual locations of floods. The study analyses flood-related tweets acquired as part of the PetaJakarta.org project implemented in the coastal mega-city of Jakarta and provides insight into the level of accuracy expected with using georeferenced social media data for flood mapping. Importantly, the results reveal that the accuracy of flood maps generated with georeferenced social media data reduces with increase in the size of the minimum mapping unit of the flood map. Finally, an approach is recommended for creating more accurate real time flood maps from crowdsourced social media data.', 'title': 'Investigating the accuracy of georeferenced social media data for flood mapping: The PetaJakarta.org case study', 'embedding': []}, {'id': 14825, 'abstractText': 'The feasibility of using normalized cumulative difference attenuation (NCDA) map for tracking the spatial and temporal evolution of temperature during microwave hyperthermia experiment on in-vitro phantoms is explored in this study. The NCDA maps were estimated from the beamformed ultrasound radio frequency (RF) data using a regularized log spectral difference (RLSD) technique. The NCDA maps were estimated at different time instants for the entire period of the experiment. The contour maps of the NCDA and the ground truth temperature map, obtained using an infra-red(IR) thermal camera corresponding to the ultrasound imaging plane, showed that NCDA was able to locate the axial and lateral co-ordinates of the hotspot with the error of &lt;; 1.5 mm axially and &lt;; 0.1 mm laterally. The error in the estimated hotspot area was less than 8 %. This preliminary in-vitro study suggests that NCDA maps estimated using RLSD may have potential in evaluating the spatio-temporal evolution of temperature and may help in the development of ultrasound-based image-guided temperature monitoring system for microwave hyperthermia.', 'title': 'Ultrasound-Based Regularized Log Spectral Difference Method For Monitoring Microwave Hyperthermia', 'embedding': []}, {'id': 14826, 'abstractText': 'The abnormal point has an important significance in terms of theory and application. In order to study dynamical properties of the abnormal point in the high dimensional space, we will give a new concept of strongly descendible map according to the definition of the descendible map. We will study the abnormal point of strongly descendible map by means of properties of interval map and product map and it is respectively given that equivalence conclusion of strongly descendible map with abnormal points and without abnormal points in the n -dimensional monomer. These results further generalize the research of Professor of Xiong and Zhou.', 'title': 'Dynamical Properties of Strongly Descendible Map on N Dimensional Monomer', 'embedding': []}, {'id': 14827, 'abstractText': 'Converting probability maps derived from indicator cokriging (ICK) to a specific land cover classification map is the second step of super-resolution mapping (SRM) under the geostatistical framework. In this study, two image segmentation strategies, namely mathematical morphology and region growing, were applied on the ICK-derived probability maps in order to take into account spatial characteristics such as shape and connectivity. A case study in South Carolina (USA) showed that the thematic map created by the proposed method had an overall accuracy improved by 2% and Kappa improved by 6% compared to the map derived from the existing sequential generation process. This indicates our methodology as a promising alternative that can be embedded into SRM tasks.', 'title': 'Integration of region growing and morphological analysis with super-resolution land cover mapping', 'embedding': []}, {'id': 14828, 'abstractText': 'Active learning strategies such as mind mapping have been proven to be effective in helping retain memory. Given that mind mapping has seen notable success in assessment and exam preparation, this study investigated the effectiveness of mind mapping as an active learning tool for university students. In addition, this study also looked into areas of active learning that could possibly require further improvement. Data collected from 22 Strategic management students was analysed using quantitative and qualitative methods. Results from the one-sample t-test suggest that the mind map technique enhances students learning ability. Results from the qualitative phase revealed areas that students feel could assist them to better utilize the mind map technique. Future improvements for the use of mind map technique for students are also discussed.', 'title': 'A Comparative Study of Active Learning with and Without Using Mind Mapping Approach', 'embedding': []}, {'id': 14829, 'abstractText': 'Over the last decade, the horticulture sector has become one of the important driving forces for the rapid development of agriculture in India. Geospatial technology is being operationally used in India for mapping and monitoring horticultural plantations. In general, per pixel or OBIA techniques have been used for mapping while use of deep learning techniques for analysis of high-resolution data for these plantation crops is limited. In the present study, seven deep learning models have been attempted for mapping of coconut plantations to overcome some of the challenges faced by conventional classifiers. U-Net and Siamese architectures are used to develop hybrid models for improved classification. These hybrid models have been implemented with batch normalization and the results indicated that the developed hybrid models showed reasonable IoU score in comparison to other DL models with better than 90.0 per cent classification accuracy for mapping coconut plantations. The trained DL model has been tested in different geographic regions for achieving reasonable accuracy for mapping of coconut plantations. The present study demonstrated the use of DL methods for mapping of coconut plantations using Cartosat-2 MX data within acceptable accuracy and trained DL model could be developed with larger no. of labelled training samples for operational applications.', 'title': 'Performance of Different U-Net Architectures for Inventory of Coconut Plantations Using Cartosat-2 Multispectral Data', 'embedding': []}, {'id': 14830, 'abstractText': \"Structured query language (SQL) is difficult to master because the execution process of SQL statements is invisible. When learning to construct an SQL query, learners must visualise the evolution process of the intermediate datasets of the SQL statement in working memory, which may burden learners' cognitive load and consequently jeopardise learning outcomes. This study describes the execution process of SQL statements by using concept maps to improve learners' understanding of SQL. An empirical experiment was conducted using two database courses, namely concept map-based and conventional instruction, to examine the relationship between concept maps and the understanding of SQL from a cognitive load theory perspective. The experimental results demonstrated the superiority of concept map-based instruction over conventional instruction because concept map-based instruction reduces extraneous load but increases germane load. Concept map construction facilitated learner engagement and promoted meaningful learning. Studying the instructors' concept maps helped learners follow the cognitive structures used by instructors to perform SQL queries, and enabled them to perceive the execution process of SQL queries relatively easily. These results potentially help educators understand the learning difficulties caused by the declarative nature of SQL and motivating researchers to resolve the inherent problem by considering learners' cognitive processes.\", 'title': 'Structured Query Language Learning: Concept Map-Based Instruction Based on Cognitive Load Theory', 'embedding': []}, {'id': 14831, 'abstractText': 'This paper examines the accuracy of the limitations of using drone to generate contour maps. Contour mapping involves the location of elevation datum referenced in the x, y and z planes to accurately depict the contours of an area and for generating topographic maps. Traditionally, this was conducted by using Total Station and other surveying tools which is based on manually locating points on the site using the angle and distance method often referred to as trigonometric leveling. This method is usually labor intensive and relatively time consuming. Several “Drone surveying systems” were introduced to the market. These systems were advertised to replace the traditional surveying method. This case study examines practicality and accuracy of creating topographic maps using unmanned aerial systems (drones). As a case study a 5.522 acres site was surveyed by traditional methods and by Kespry© 2 drone to collect the surface data required to generate a topographic map. The results showed the drone readings are within an average accuracy of 0.0075 ft and 0.0175 for the northing and easting distances respectively with a standard deviation of 0.022ft. and 0.029 ft. and a range of 0.02 ft. The Elevation accuracy average was 0.332 ft with a standard deviation of 0.278 ft. and a range of 0.77 ft. The difference in the calculated cut and fill volume between the drone system and the traditional system was 8.2% The results indicated that topographic mapping using drones can generate topographic maps with acceptable accuracy for general site grading. The Drone system also saves time and reduce human error. However, it usually requires subcontracting the work to a company with special training personnel who have access to special analytical software and relatively expensive equipment. Special Drone flight permissions is also required for some sites.', 'title': 'Examining the practicality and accuracy of Unmanned Aerial System Topographic Mapping (Drones) Compared to Traditional Topographic Mapping', 'embedding': []}, {'id': 14832, 'abstractText': 'Breast cancer is a high incidence of malignancy in women, with a higher mortality rate. Accurate screening is helpful to early detection and improve the treatment success rate and patient survival rate. This study is based on low-cost ultrasound, using ultrasound multifeature maps based on the original radiofrequency (RF) signals and radiomics analysis method to evaluate the benign and malignant of breast tumors. The three ultrasound multifeature maps of breast tumor are composed of direct energy attenuation coefficient (AC), standard deviation of image intensity (SD) and Rician distribution parameters (RD). From the above multifeature maps, high-throughput radiomics features were extracted, then sparse representation method was used for feature selection, and then support vector machine was used to predict the benign and malignant of breast tumors. Eight groups of comparative experiments were established by using ultrasound gray-scale image, single ultrasound feature map and two ultrasound feature maps. The results from 164 patients with breast tumor showed that the AUC, accuracy and sensitivity of the radiomics classification model with feature maps of AC, SD and RD can reach 93.61%, 93.94% and 100%, respectively. The use of RF based ultrasound multifeature maps combined with radiomics could effectively predict the benign and malignant of breast tumors in this study.', 'title': 'Breast tumor diagnosis using radiofrequency signals based ultrasound multifeature maps combined with radiomics analysis', 'embedding': []}, {'id': 14833, 'abstractText': 'Regions within the atria with sustained rapid reentrant or focal activity have been defined as a mechanism of persistent atrial fibrillation (AF). However, the mechanism behind the anchoring of these sites and their stability over time is unknown. We tested the hypothesis that fibrosis anchors sites of high frequency activation during AF and that these sites can be non-invasively determined using cardiac T1 Mapping with MRI.A canine rapid atrial paced model of persistent AF was used (n=12, including 6 controls) for the study. Whole heart T1 Mapping was performed prior to an electrical mapping study. Spatial maps of high dominant frequency (DF) probability were constructed to determine stability of the highest DF sites. These sites were then correlated with fibrotic regions determined by T1 Mapping.The chronic AF animals had at least one site of stable, high DF for at least 22.5 (75%) of 30 minutes of AF. Regions of stable high DF bordered regions offibrosis as determined by T1 Mapping MRI 82% of the time (p&lt;; 0.05).Heterogeneous atrial remodeling, specifically fibrosis, arising from chronic AF may provide a substrate that anchors sites of high DF. Cardiac T1 Mapping with MRI may determine such sites non-invasively.', 'title': 'Regions of High Dominant Frequency in Chronic Atrial Fibrillation Anchored to Areas of Atrial Fibrosis', 'embedding': []}, {'id': 14834, 'abstractText': 'Wetlands are important natural resources which provide many benefits to the environment. Consequently, mapping and monitoring wetlands has gained a considerable attention in recent years among remote sensing experts. Wetlands undergo a considerable change within a year. Thus, it is important to study how much various wetland types are distinguishable at different dates. This will help in choosing an appropriate image for wetland classification. On the other hands, combining various satellite images acquired on different dates is a promising approach to obtain a more accurate classified map compared to the map obtained by single-date satellite imagery. In this study, wetlands within a pilot sites, located in Newfoundland were first classified using each of the several available Landsat 8 data, captured in the three seasons of Spring, Summer, and Fall. By doing this, the separability of the wetland classes in each season was analyzed. Then, these multi-temporal data were integrated to obtain a more accurate map of wetlands. The overall classification accuracy of the final map was 88%, proving that using multi-temporal remote sensing data was necessary to obtain a more reliable and accurate map of the dynamic wetlands in the province.', 'title': 'Evaluation of multi-temporal landsat 8 data for wetland classification in newfoundland, Canada', 'embedding': []}, {'id': 14835, 'abstractText': 'Forest fires occur throughout the year in rainforests and deserts of Australia. The disastrous bush fire event occurred during November 2019, and lasted until February 2020, destroying more than 46 million acres of land. Burn area mapping is a major parameter in carrying out mitigation measures and regrowth activities by forest officials or fire managers post fire event. In this study, multi-temporal satellite datasets such as images acquired from Sentinel-2 (S2) and Landsat-8 (L8) missions are used to map the burn areas. Two thematic indices such as Differenced Normalized Burn Ratio (dNBR) and Relativized Burn Ratio (RBR) are implemented on the study area. The entire analysis, i.e., accessing the datasets, preprocessing, and calculation of indices for brunt area mapping is carried out on Google Earth Engine cloud platform. Rather than ground survey, the active fire product VIIRS product (VNP14IMGTDL) is used as a proxy for the actual fire indices in accuracy assessment. Results revealed that RBR showed better accuracy than dNBR for both the datasets (S2 and L8). S2 burn severity maps of dNBR and RBR showed better accuracy than L8 burn severity maps because of S2 having a higher spatial resolution. Thus, S2 datasets can be useful for rapid mapping of burn areas with improved spatial as well as temporal resolution.', 'title': 'Burn area mapping in Google Earth Engine (GEE) cloud platform: 2019 forest fires in eastern Australia', 'embedding': []}, {'id': 14836, 'abstractText': 'In 2011 A.I. EL Maghrabi and A.M. Mubaraki introduced and studied the notions of Y - open and Y - closed sets in general topology as well as presented some characterizations of these notions. We introduce and investigate several properties and characterizations of a new class of maps between topological spaces called Y - open maps, Y - closed maps, Y - continuous maps and Y - irresolute maps. We also introduce slightly Y - continuous, totally Y - continuous and almost Y - continuous maps between topological spaces and establish several characterizations of these new forms of maps. Furthermore, we introduce and study the notions of Y - separated sets and Y - connectedness in topological spaces.', 'title': 'Y – Continuity and Y – Connectedness in Topological Space', 'embedding': []}, {'id': 14837, 'abstractText': \"In this paper, we address the problem of quantifying the reliability of computational saliency for videos, which can be used to improve saliency-based video processing algorithms and enable more reliable performance and objective risk assessment of saliency-based video processing applications. Our approach to quantify such reliability is twofold. First, we explore spatial correlations in both the saliency map and the eye-fixation map. Then, we learn the spatiotemporal correlations that define a reliable saliency map. We first study spatiotemporal eye-fixation data from the public CRCNS data set and investigate a common feature in human visual attention, which dictates a correlation in saliency between a pixel and its direct neighbors. Based on the study, we then develop an algorithm that estimates a pixel-wise uncertainty map that reflects our supposed confidence in the associated computational saliency map by relating a pixel's saliency to the saliency of its direct neighbors. To estimate such uncertainties, we measure the divergence of a pixel, in a saliency map, from its local neighborhood. In addition, we propose a systematic procedure to evaluate uncertainty estimation performance by explicitly computing uncertainty ground truth as a function of a given saliency map and eye fixations of human subjects. In our experiments, we explore multiple definitions of locality and neighborhoods in spatiotemporal video signals. In addition, we examine the relationship between the parameters of our proposed algorithm and the content of the videos. The proposed algorithm is unsupervised, making it more suitable for generalization to most natural videos. Also, it is computationally efficient and flexible for customization to specific video content. Experiments using three publicly available video data sets show that the proposed algorithm outperforms state-of-the-art uncertainty estimation methods with improvement in accuracy up to 63% and offers efficiency and flexibility that make it more useful in practical situations.\", 'title': 'Unsupervised Uncertainty Estimation Using Spatiotemporal Cues in Video Saliency Detection', 'embedding': []}, {'id': 14838, 'abstractText': 'Land degradation by salinity is one of the main environmental hazards threatening soil sustainability especially in arid and semi-arid regions of the world characterized by low precipitation and high evaporation. Geo-statistical approaches and remote sensing (RS) techniques have provided fast, accurate and economic prediction and mapping of soil salinity within the last two decades. Obtaining multi-temporal data via satellite images in different spatial domains with various scales is one of the key developments of monitoring spatial variability of soil salinity. In addition, geo-statistical methods have the capability of producing prediction surfaces from limited sample data. This study aims to map spatial distribution of soil salinity in the selected pilot area which is located in the western part of Urmia Lake Basin, Iran, by applying geo-statistical methods. A kriging based map and three different co-kriging based maps were produced using electrical conductivity (EC) measurements as primary variable and three different soil salinity index values as secondary variable. Three soil salinity indices were created by using Sentinel-2A image that were acquired in the same date of field measurements to generate 3 various soil salinity prediction maps. Salinity maps obtained from geo-statistical methods were compared and validated to understand the performance of these approaches for soil salinity prediction. The results of this study demonstrated that co-kriging can provide promising estimation of spatial variability of soil salinity especially when there is relevant and abundant set of secondary data derived from satellite images.', 'title': 'Characterizing the spatial variability of soil salinity in Lake Urmia Basin by applying geo-statistical methods', 'embedding': []}, {'id': 14839, 'abstractText': 'This research presents a novel topology preserving map (TPM) called Weighted Voting Supervision -Beta-Scale Invariant Map (WeVoS-Beta-SIM), based on the application of the Weighted Voting Supervision (WeVoS) meta-algorithm to a novel family of learning rules called Beta-Scale Invariant Map (Beta-SIM). The aim of the novel TPM presented is to improve the original models (SIM and Beta-SIM) in terms of stability and topology preservation and at the same time to preserve their original features, especially in the case of radial datasets, where they all are designed to perform their best. These scale invariant TPM have been proved with very satisfactory results in previous researches. This is done by generating accurate topology maps in an effectively and efficiently way. WeVoS meta-algorithm is based on the training of an ensemble of networks and the combination of them to obtain a single one that includes the best features of each one of the networks in the ensemble. WeVoS-Beta-SIM is thoroughly analyzed and successfully demonstrated in this study over 14 diverse real benchmark datasets with diverse number of samples and features, using three different well-known quality measures. In order to present a complete study of its capabilities, results are compared with other topology preserving models such as Self Organizing Maps, Scale Invariant Map, Maximum Likelihood Hebbian Learning-SIM, Visualization Induced SOM, Growing Neural Gas and Beta- Scale Invariant Map. The results obtained confirm that the novel algorithm improves the quality of the single Beta-SIM algorithm in terms of topology preservation and stability without losing performance (where this algorithm has proved to overcome other well-known algorithms). This improvement is more remarkable when complexity of the datasets increases, in terms of number of features and samples and especially in the case of radial datasets improving the Topographic Error.', 'title': 'A Novel Ensemble Beta-Scale Invariant Map Algorithm', 'embedding': []}, {'id': 14840, 'abstractText': 'Mangrove forest (MF) extents and distributions are fundamental for conservation and restoration efforts. According to previous studies, both the commercial Gaofen-2 (GF-2) imagery (0.8 m spatial resolution and 4 spectral bands) and freely accessed Sentinel-2 (S2) imagery (10 m spatial resolution and 13 spectral bands) have been successfully used to map MFs. However, the efficiency and accuracy of MF mapping based on these two data is not clear, especially for large-scale applications. To address this issue, first, we developed a robust classification approach by integrating object-based image analysis (OBIA) and random forest (RF) algorithm; and then, applied this approach to GF-2 and S2 images to map the extents of MF along the entire coasts of Guangxi, China, respectively; at last, compared the efficiency and accuracy of GF-2 and S2 imagery in MF mapping. Results showed that: first, based on OBIA and RF integrated classification approach both MF maps derived from GF-2 and S2 obtained high mapping accuracies (the overall accuracy was 96% and 94%, respectively); second, areal extent of MFs in Guangxi extracted from GF-2 and S2 images was 8182 and 8040 ha, respectively; third, GF-2 imagery has extraordinary abilities in detecting fragmented MF patches located along landward and seaward edges; and finally, S2 imagery performed better in detecting seaward submerged MFs and separating MF from terrestrial vegetation. Results and conclusions of this study can provide basic considerations for selecting appropriate data source in MF or wetland vegetation mapping tasks.', 'title': 'A Comparison of Gaofen-2 and Sentinel-2 Imagery for Mapping Mangrove Forests Using Object-Oriented Analysis and Random Forest', 'embedding': []}, {'id': 14841, 'abstractText': 'Given the explosive growth of Internet of Things (IoT) devices ranging from the 2D ground to the 3D space, it is a necessity to establish a 3D spectrum map to comprehensively present and effectively manage the 3D spatial spectrum resources in smart city infrastructures. By leveraging the popularity and location flexibility of unmanned aerial vehicles (UAVs), we are able to execute spatial sampling with these emerging flying spectrum-monitoring devices (SMDs) at will. In this article, we first present a brief survey to show the state-of-the-art studies on spectrum mapping. Then we introduce the 3D spectrum mapping model. Next, we propose a 3D spectrum mapping framework that is composed of pre-sampling, spectrum situation estimation, UAV deployment, and spectrum recovery. Therein we develop a region-of-interest-driven UAV deployment scheme, which selects new sampling points of the highest estimated interest and the lowest energy cost iteratively. Meanwhile, we slice the entire 3D spectrum map into a series of \"images\" and \"repair\" those unsampled locations. Furthermore, we provide an exemplary case study on 3D spectrum mapping, where, for example, an important event is being held, and the entire spectrum situation needs to be monitored in real time to deal with malicious interference sources. Lastly, the challenges and open issues are discussed.', 'title': '3D Spectrum Mapping Based on ROI-Driven UAV Deployment', 'embedding': []}, {'id': 14842, 'abstractText': 'Online maps play an important role in providing traffic information services for users. The complexity of urban road network and the increase of traffic information have encouraged diversified demands for traffic maps, and provoked great conflict between growing visualization requirements of complex information and the limited map display space. Based on a comparison of road classification and attribute integration of typical online map platforms in different scales, this study provided an optimized design at the aim of exploring better visual integration methods of multiple traffic information for online maps. A user test was conducted, which proved the validity of the integrating design methods in showing that the design strategy of this study can represent more traffic information in different dimensions in a user-accepted manner. Both the design scheme and other results can provide reference for the elements configuration and visual integration of future online map design.', 'title': 'Visual Integration of Multiple Traffic Information for Online Map Design', 'embedding': []}, {'id': 14843, 'abstractText': 'SAR or Microwave remote sensing today has proved itself to be a paradigm shift in the field of remote sensing. With its all-weather availability, it has found variety of applications in several fields today. Till now, and mainly, floods have been mapped using optical remote sensing data since a long time. Floods have been a common and yearly catastrophe which the people of areas situated in lesser elevations and on the foothills of mountains in the Terai area must face. Proper mapping of floods can help to delineate the areas with higher loss of life and property thus aiding the relief measures. This study aims to analyze and exploit the potential of SAR Interferometry (InSAR) for flood inundation mapping using a pair of C-Band dual PolSAR datasets from Sentinel-1. The datasets were preprocessed for speckle removal and radiometric corrections. Thereafter, subset of study area was taken, and the datasets were co- registered followed by interferometric processing. The results showed low phase shift in and around the river and to some extent in the urban areas which generally does not happen unless there is an increased single bounce scatter. The topographic phase removed interferogram showed high phase variations in the core of urban areas while coherence map showed an acute loss of coherence from permanent features as well which showed high degree of flood water accumulation. This flood was mapped by masking out the low values of high coherence areas from the image using a proper threshold value. High degree of flooding was observed in Urban areas and much of agricultural land was seen to have been submerged under water once the resulting inundation map was overlaid on Google earth.', 'title': 'C-band SAR Interferometry based flood inundation mapping for Gorakhpur and adjoining areas', 'embedding': []}, {'id': 14844, 'abstractText': 'Geothermal energy is one of the renewable energy sources that can be utilized to replace fossil fuels. The conventional geophysical methods have been applied to determine geothermal potential areas in the globe. These existing methods are costly and time-consuming for explorations in a wide range and unreachable volcano area. Thus, an alternative and challenging approach for identifying a geothermal prospecting area is remotely sensed thermal infrared imagery. This study aims to explore and evaluate the use of a sensor platform of Thermal Infrared Scanner (TIRS) and Operational Land Imager (OLI), Landsat 8 for mapping geothermal resource in Peut Sagou volcano. The mountain is an active volcano in Aceh Province, Indonesia that is possible to develop a geothermal power plant by an energy estimation of 100 MWe. The several processing techniques have been applied such as composite bands for lithological mapping that related to the geological aspect of the volcano, Normalize Deferential Vegetation Index (NDVI) threshold methods of spectral emissivity used for mapping the land cover area, and the Land Surface Temperature (LST) by split windows algorithm method to study the temperature distribution related to the activity of geothermal resources in the sub-surface. The composite band shows clearly the lineament of the faults, and alluvium deposits in the manifestation area, which is specifically located in the top of the volcano. NDVI data shows a low vegetation index (-0.4 - 0.5) in the manifestation area, while the Land Surface Emissivity (LSE) also allows mapping of a weak zones area that is characterized by low radiation index &lt;; 0.98. The high-temperature value (21 - 30 C) from the LST map is obtained at the top of the volcano in response to the conduit zone area, which functions as a route for fluid discharge from the reservoir to the surface. The LST data also shows some areas with relatively high temperatures (19 - 20 C) in response to the weak zone areas such as faults and the distribution of surface manifestations. We conclude that TIRS dan OLI sensors from Landsat 8 demonstrate high efficiency for potential mapping of geothermal resources in the Peut Sagou volcano.', 'title': 'OLI and TIRS Sensor Platforms for Detection the Geothermal Prospecting in Peut Sagoe Volcano, Aceh Province, Indonesia', 'embedding': []}, {'id': 14845, 'abstractText': 'Automatic generation of level maps is a popular form of automatic content generation. In this study, a recently developed technique employing the do what’s possible representation is used to create open-ended level maps. Generation of the map can continue indefinitely, yielding a highly scalable representation. A parameter study is performed to find good parameters for the evolutionary algorithm used to locate high quality map generators. Variations on the technique are presented, demonstrating its versatility, and an algorithmic variant is given that both improves performance and changes the character of maps located. The ability of the map to adapt to different regions where the map is permitted to occupy space are also tested.', 'title': 'Automatic Generation of Level Maps with the Do What’s Possible Representation', 'embedding': []}, {'id': 14846, 'abstractText': 'Due to recent artificial intelligence (AI) technology progress, more and more applications present all-to-all, irregular or unpredictable communication patterns among compute nodes in high-performance computing (HPC) systems. Traditional communication infrastructures, e.g., torus or fat-tree interconnection networks, may not handle well their matchmaking problems with these newly emerging applications. For these typical non-random network topologies, there are already many communication-efficient application mapping algorithms. However, for the above unpredictable communication patterns, it is difficult to efficiently map their applications onto the non-random network topologies. In this case, a simple optimization is to map their applications with small diameter or average shortest path length (ASPL) among the assigned compute nodes. In this context, we recommend to use random network topologies as the communication infrastructures, which have drawn increasing attention for the use of HPC interconnects. In this study, we make a comparative study to analyze the performance impact of application mapping on non-random and random network topologies. We list several application mapping policies, and compare their job scheduling performances assuming that the communication patterns are unpredictable to the computing system. Evaluations with a large compound application workload show that, when compared to non-random topologies, random topologies can reduce the average turnaround time up to 39.3% by a random connected mapping method and up to 72.1% by a diameter/ASPL-based mapping method.', 'title': 'The Impact of Application Mapping on Non-Random and Random Network Topologies', 'embedding': []}, {'id': 14847, 'abstractText': 'Accurate mapping of urban land cover is still a fundamental challenge in remote sensing communities due to the great spectral variability of urban environments. This study presents an application of multiple criteria spectral mixture analysis (MCSMA) approach to map vegetation, impervious surfaces, and soil (V-I-S) components in a highly urbanized city of Chengdu, China, using the Landsat-8 Operational Land Imager (OLI) surface reflectance product. Unlike its counterparts which rely on single indicator in the mapping process, MCSMA uses multiple indicators to better address the problem of spectral variability. Our results showed that MCSMA produced accurate V-I-S maps that well matched the actual distributions. The vegetation map presented higher accuracies than impervious surfaces and soil maps in root mean square error, mean absolute error and systematic error. Results of this study demonstrate the potential of MCSMA in accurate urban land cover mapping.', 'title': 'Mapping Urban Land Cover Using Multiple Criteria Spectral Mixture Analysis: A Case Study in Chengdu, China', 'embedding': []}, {'id': 14848, 'abstractText': 'Visual search can be time-consuming, especially if the scene contains a large number of possibly relevant objects. An instance of this problem is present when using geographic or schematic maps with many different elements representing cities, streets, sights, and the like. Unless the map is well-known to the reader, the full map or at least large parts of it must be scanned to find the elements of interest. In this paper, we present a controlled eye-tracking study (30 participants) to compare four variants of map annotation with labels: within-image annotations, grid reference annotation, directional annotation, and miniature annotation. Within-image annotation places labels directly within the map without any further search support. Grid reference annotation corresponds to the traditional approach known from atlases. Directional annotation utilizes a label in combination with an arrow pointing in the direction of the label within the map. Miniature annotation shows a miniature grid to guide the reader to the area of the map in which the label is located. The study results show that within-image annotation is outperformed by all other annotation approaches. Best task completion times are achieved with miniature annotation. The analysis of eye-movement data reveals that participants applied significantly different visual task solution strategies for the different visual annotations.', 'title': 'An Evaluation of Visual Search Support in Maps', 'embedding': []}, {'id': 14849, 'abstractText': \"As of today, SAR imagery represents the most commonly used data source for remote sensing-based flood mapping. The data are characterized by a good sensitivity to water and are available day and night, regardless of cloud cover. Many studies have demonstrated that SAR systems are suitable tools for flood mapping on bare soils and scarcely vegetated areas. In spite of the progress in the development of Near Real Time SAR based flood mapping algorithms, the detection of inundation in urban areas still represents a critical issue. Here we propose a methodology for identifying floods that heavily affected the city of Houston (Texas) during the 2017 hurricane season. Our approach takes advantage of the Interferometric SAR coherence feature to detect the presence of floodwater in urbanized areas. In particular, data provided by the Sentinel-1 mission in both, Strip Map and Interferometric Wide Swath modes, have been used, with a geometric resolution of 5m and 20m, respectively. The algorithm takes fully advantage of the Sentinel-1 mission's repeat cycle of six days, thereby providing an unprecedented possibility to develop an automatic, high frequency flood mapping application that is suitable for complex environments. The test of the algorithm for the Houston case study showed promising results for mapping flood in urban areas.\", 'title': 'Monitoring Urban Floods Using SAR Interferometric Observations', 'embedding': []}, {'id': 14850, 'abstractText': 'Several works have focused on Simultaneous Localization and Mapping (SLAM), which is a topic that has been studied for more than a decade to meet the needs of robots to navigate in an unknown environment. SLAM is an essential perception functionality in several applications, especially in robotics and autonomous vehicles. RGB-D cameras are among the sensors commonly used with recent SLAM algorithms. They provide an RGB image and the associated depth map, making it possible to solve scale drift with less complexity and create a dense 3D environment representation. Many RGB-D SLAM algorithms have been studied and evaluated on publicly available datasets without considering sensor specifications or image acquisition modes that could improve or decrease localization accuracy. In this work, we deal with indoor localization, taking into account the sensor specifications. In this context, our contribution is a deep experimental study to highlight the impact of the sensor acquisition modes on the localization accuracy, and a parametric optimization protocol for a precise localization in a given environment. Furthermore, we apply the proposed protocol to optimize a depth-related parameter of the SLAM algorithm. The study is based on a publicly available dataset in an indoor environment with a depth sensor. The reconstruction results’ analysis is founded on the study of different metrics involving translational and rotational errors. These metrics errors are compared with those obtained with a state-of-the-art stereo vision-based SLAM algorithm.', 'title': 'Enhancing RGB-D SLAM Performances Considering Sensor Specifications for Indoor Localization', 'embedding': []}, {'id': 14851, 'abstractText': 'In the last decade, as an emerging technique for business processes management, process mining (PM) has been applied in many domains, including manufacturing, supply-chain, government, healthcare, and software engineering. Particularly in healthcare, where most processes are complex, variable, dynamic, and multi-disciplinary in nature, the application of this technique is growing yet challenging. Several literature reviews, as secondary studies, reveal the state of PM applications in healthcare from different perspectives, such as clinical pathways, oncology processes, and hospital management. In this article, we present the results of a systematic mapping (SM) study which we conducted to structure the information available in the primary studies. SM is a well-accepted method to identify and categorize research literature, in which the number of primary studies is rapidly growing. We searched for studies between 2005 and 2017 in the electronic digital libraries of scientific literature, and identified 172 studies out of the 2428 initially found on the topic of PM in healthcare. We created a concept map based on the information provided by the primary studies and classified these studies according to a number of attributes including the types of research and contribution, application context, healthcare specialty, mining activity, process modeling type and notation/language, and mining algorithm. We also reported the demographics and bibliometrics trends in this domain; namely, publication volume, top-cited papers, most contributing researchers and countries, and top venues. The results of mapping showed that, despite the healthcare data and technique related challenges, the field is rapidly growing and open for further research and practice. The researchers who are interested in the field could use the results to elicit opportunities for further research. The practitioners who are considering applications of PM, on the other hand, could observe the most common aims and specialties that PM techniques are applied.', 'title': 'Systematic Mapping of Process Mining Studies in Healthcare', 'embedding': []}, {'id': 14852, 'abstractText': 'Deep convolutional neural networks have demonstrated superior performance in natural image denoising. Trainable network weights have been typically optimized by minimizing a loss function that computes pixel-wise discrepancies between the noisy image and the clean target image. In this study, we investigate an alternative solution that utilizes more prior knowledge to highlight the features of interest by modifying their contributions to the global loss function. We propose a feature-oriented deep convolutional neural network (FeaOri-DCNN) for PET image denoising that uses weight maps in order to steer the training toward contrast preservation for small features. To obtain the weight maps, we first manually segment the lesions in the target images to create lesion masks. Lesion voxels are assigned stronger weights than the background voxels followed by a Gaussian smoothing. This weight map is then incorporated into the loss function optimization. We first trained the proposed FeaOri-DCNN and a conventional DCNN built on a five-layer residual network architecture with simulated and phantom images containing hot spheres with various size and contrast. We then trained an eight-layer network with 8 patient studies and 1 phantom study. We evaluated the five-layer network on phantom studies and the eight-layer network on 2 patient studies inserted with GATE simulated lesions. The results of the phantom studies show that FeaOri-DCNN improved contrast recovery on small and low contrast spheres by up to 36% while performing similarly in terms of noise reduction in the background. Similar results were also observed in the patient studies.', 'title': 'Feature Oriented Deep Convolutional Neural Network for PET Image Denoising', 'embedding': []}, {'id': 14853, 'abstractText': 'The decision tree (DT) represents a nonparametric estimation method that has been mostly used for both classification and regression problems. DTs were adopted for software development effort estimation (SDEE) generally for their simplicity of use and interpretation contrary to other learning methods. Nevertheless, to our self-knowledge, no systematic mapping has been devoted especially to decision trees. The aim of this study is to elaborate a systematic mapping study that classifies DTs papers in conformity with the succeeding criteria: research approach, contribution type, techniques employed in combination with DT methods besides identifying publication channels and trends. An automated search of five digital libraries was made to carry out a systematic mapping of DT studies mainly devoted to SDEE that were published in the period 1985-2017. We identify 46 relevant studies. Basically, the results revealed that most researchers focus on technique contribution type. In addition, the majority of papers deal with improving the existing DT models while few studies have proposed novel models to improve the reliability of SDEE. Furthermore, solution proposal and case study are the most frequently used approaches.', 'title': 'Decision Trees Based Software Development Effort Estimation: A Systematic Mapping Study', 'embedding': []}, {'id': 14854, 'abstractText': 'The rapid development of the field of geoinformation has paved the way for great hope in the framework of economic development and infrastructure in the world. This study deals with the publication of the geo-information to document the historical monuments of Turkish tourism in Khartoum State. The motives of the study are the geomorphic possibilities in the evaluation of spatial features, the lack of studies and research in the field, Descriptive and spatial features of Turkish monuments as one of the important historical landmarks in the state of Khartoum. The objective of the study is to construct the historical spatial data model GeoHistorical Data Model, the design of the historical spatial database of the geocapital in Gezpatial Database, the implementation of a historical spatial information system application for the Turkish features in Khartoum state, the design of a basic map of the Turkish features in Khartoum State Historical Thematic Map, documenting the Turkish monuments in the state of Khartoum. The importance of the study is to highlight the geomorphic possibilities in documenting the spatial parameters in Khartoum.', 'title': 'GeoSpatial Technology Documental Historical Tourism Site: Turkey in Khartoum', 'embedding': []}, {'id': 14855, 'abstractText': 'Clear-cutting and logging operations are the most drastic and wide-spread changes that affects the hydrological and carbon-balance properties of forested areas. A long time series of Sentinel-1 images are used to study the potential for mapping logged areas in areas in boreal zone region and in tropical forest. In the first case study in southern Finland, the time series covered a full year starting in October 2014, in 200km-by-200-km study site. The Sentinel-1 images were acquired in Interferometric Wide-swath (IW), dualpolarized mode (VV+VH). All scenes were acquired in the same orbit configuration. In the second case study the potential of Sentinel-1 time series for mapping logged areas was studied over tropical forest in Mexico. Acquisitions were made in the time frame between November 2014 and September 2015. The temporal behavior of the C-band backscatter was studied for areas representing: 1) areas clear-cut during the acquisition of the Sentinel-1 time-series, 2) areas remaining forest during the acquisition of the Sentinel-1 time-series, and 3) areas that had been clear-cut before the acquisition of the Sentinel-1 time-series. Algorithms for mapping the spatial extent of logged areas were developed and tested, showing potential of long time series of Sentinel-1 data for successful delineation of clear-cuts despite high sensitivity to seasonal and weather conditions.', 'title': 'Mapping forest disturbance using long time series of Sentinel-1 data: Case studies over boreal and tropical forests', 'embedding': []}, {'id': 14856, 'abstractText': 'Information Security has still the aim of many agencies and organisations. They have great attention to cryptosystems to ensure the security of their information. So, scientists and researchers had proposed many cryptosystems and improve their performance via new methods and techniques. Most recent studies depended on the chaotic map with cipher systems which had gained most researchers to improve its security and robustness. The continued studies had mentioned several methods to strength these chaotic based ciphers and the randomness of its keystream generation. Some of the studies had proposed successful cipher techniques while others had not or advised further improvement. Another side, this paper tests chaotic maps which had used in the cipher algorithms to evaluate its randomness. In general, this paper gives a review of recent stream cipher based on chaotic maps. It also shows the randomness evaluation of chaotic maps. The survey recommended that Chen map is the most random method.', 'title': 'Stream Cipher Based on Chaotic Maps', 'embedding': []}, {'id': 14857, 'abstractText': 'In the present study, we develop a new characterization methodology allowing to directly convert standard electroluminescence (EL) images of a given PV module into absolute quantitative performance maps. The experiments and results were obtained based on both multi-crystalline Al-BSF and mono PERC bifacial silicon modules. After acquiring two EL images, we show that by applying the generalized reciprocity relations, it is possible to optically extract key performance indicator maps. We demonstrate the ability to extract an optical I-V characteristics whereby we optically map the current collection efficiency, the current density, the series resistance across the studied multi-crystalline silicon modules. In addition, we are able to provide a first luminescence-based experimental map of the bifaciality coefficient of the studied mono PERC silicon modules. The results were finally validated by comparing the mean central values extracted from the optically determined maps with the values extracted from electrical I(V) measurements. Such agreement opens the potential of using the presented work for a better control of module reliability, testing accuracies and optimization of manufacturing processes.', 'title': 'On the use of electroluminescence-based reciprocity relations for quantitative mapping of PV modules performance', 'embedding': []}, {'id': 14858, 'abstractText': \"The microwave radiometer (MRM) on-board the Chinese Chang'e-2 (CE-2) lunar probe measures the lunar brightness temperature (also referred to as TB) data that are large-scale scientific data. In order to construct lunar TB map, the optimized hierarchical MK splines method is proposed, which uses a hierarchy of coarse-to-fine control lattices to generate a fine control lattice. The computation of the TB construction function is limited to the small number of control points in the merged control lattice, and then the desired high-resolution TB maps are constructed. At the same time, some basis relations between the lunar TB and frequencies are also analyzed based on the constructed TB maps. It can be found that the high-frequency TB map shows lunar topographic features with close similarity. Furthermore, to express the TB distribution features quantitatively, the lunar TB distribution models, including the global TB model of the Moon, the TB model of the lunar far side, and the TB model of the lunar near side, are established based on the constructed TB maps, and the obtained TB distribution models are log-normal distributions. The establishment of the lunar TB distribution model is important to reasonably select the color layer and intensity of color for the lunar TB maps, and is helpful for studying the lunar TB distribution law. In addition, the topographic data measured by the lunar orbiter laser altimeter are selected to discuss the influence of elevation on the lunar TB, and the CE-2 TB data combined with the FeO and TiO<sub>2</sub> abundances are used to study the microwave thermal emission features of the lunar regolith. The research results have important implications for studying the thermal radiation of the Moon.\", 'title': 'Lunar Brightness Temperature Map and TB Distribution Model', 'embedding': []}, {'id': 14859, 'abstractText': \"The use of the internet today cannot be separated from people's lives. The available information is getting bigger and easier to obtain. Such information can be found in blog articles, news sites, and even statuses on social media. However, the available information sometimes cannot be utilized properly due to lack of a better understanding of the obtained information itself. The purpose of this research is to develop an automatic mind map generator application that can create a mind map from the input of news articles automatically. This is expected to help users understand the contents of the article. The study will take a case study of natural disaster news articles. In its application, researchers used the Support Vector Machine (SVM) method of multi-label classifier one vs rest with linear kernel to classify sentences in the news into the 5W+1H class (what, when, where, who, why, how). Beside classification, this research also includes summarization task as a preliminary task. To get the accurate model, we conducted some experimental study by examining combination of filtering features and candidate features. Our classification model raises F1-scores of 75%. The classification maps the word or phrase into each class, each class is determined as each node in mind map visualization with the root node is the image which shows the title of news article. The usability of our application was evaluated using The System Usability Scale and got the score of 78.5. This mind map generator also provides model evaluation by users, each user can review the classification result and if they agree with the result, they can update the model. By this scheme, the accuracy of our model is getting more accurate and lets us grab new data set automatically.\", 'title': 'Building automatic mind map generator for natural disaster news in Bahasa Indonesia', 'embedding': []}, {'id': 14860, 'abstractText': 'In language learning contexts, reading comprehension is an important learning activity. In EFL reading comprehension learning, one of the frequent styles of reading is the sentence-by-sentence style, in which learners can understand the text as separate sentences only, not as a whole structure. This study focus on the structural understanding of test and map making process from the viewpoints of paragraph. The assumption in this study is that map making in KB mapping does not follow sentence order but focus sets of meanings formed by paragraphs. This study investigates the relation between map making process in KB and SB mapping and paragraph structure of text.', 'title': 'Analysis of the concept mapping style in EFL reading comprehension comparison between kit-build and scratch-build concept mapping from the viewpoint of paragraph structure of text', 'embedding': []}, {'id': 14861, 'abstractText': 'In this work, we study the problem of assimilating high resolution Precipitable Water Vapor (PWV) maps using the Weather Research and Forecast 3D Variational Data assimilation system (WRF-3DVar). The PWV maps are obtained using the Sentinel-1 Synthetic Aperture Radar (SAR) images and the SAR interferometry (InSAR) technique. The influence of the high resolution PWV data on the initial condition of WRF and during the next 12 hours is studied. We demonstrate that the assimilation of InSAR PWV maps increases both the water vapor concentration and temperature over areas affected by extreme weather events so correctly generating localized convection cells. The PWV forecast, after the assimilation of InSAR maps, are compared with the PWV estimates provided by a dense GNSS network. The precipitation pattern and amount are compared to meteorological radar measurements. The case study of the extreme weather event that affected the city of Adra, Spain, on 6<sup>th</sup> September 2015, is used to demonstrate how the assimilation of high resolution PWV maps.', 'title': 'Assimilation of Insar-Derived PWV Maps Exhibit Potential for Atmosphere Convective Storm Characterization', 'embedding': []}, {'id': 14862, 'abstractText': 'High definition (HD) map is a critical part of highly automated driving (HAD) technology and shows potential for high precision vehicle localization when GNSS signals are not available. The current study of using HD map for localiz ation is mostly based on Simultaneous Localization and Mapping (SLAM) technique, which requires high computing power, huge storage space, and quick data transmission ability. Therefore, a study of a new HD map based vehicle localization method which requires less computation is necessary. Geometry is one key component that affects the quality of localization, including accuracy, reliability, and separability. Analysing the geometry can provide reference for designing a localization system to meet the quality requirement of HAD, but is rarely studied. This paper aims to design a high precision and reliable localization system using HD map as a sensor, and the influence of geometry is also explored. Geometric strength is evaluated under different scenarios considering three factors, including feature distribution type, feature number, and distance between vehicle and feature. The results show Minimum Detectable Bias (MDB) and Minimal Separable Bias (MSB) are mostly affected by feature number and distance between vehicle and feature. Randomly distribution, more detected features and close distance between the host vehicle and the features may all contribute to good quality of vehicle position estimation.', 'title': 'High definition map-based vehicle localization for highly automated driving: Geometric analysis', 'embedding': []}, {'id': 14863, 'abstractText': 'Urban buildings are essential components of cities and an indispensable source of urban geographic information. While there are many research efforts focused on urban buildings extraction, there are few studies on large-scale urban building mapping based on satellite images. In this research, a large-scale urban building mapping scheme based on Gaofen-2 satellite (GF-2) images is proposed based on a hierarchical approach. In this hierarchical approach, urban buildings are regarded as a mixture of dense low-rise buildings (DLBs) and sparse independent buildings (SIBs) stacked in space, which are extracted by a semantic segmentation model and an instance segmentation model, respectively. In this study, GF-2 images and OpenStreetMap data were used to extract DLB using <inline-formula><tex-math notation=\"LaTeX\">$U^2$</tex-math></inline-formula>-Net with focal loss. GF-2 images were used to extract SIB using an improved CenterMask model with a deformable convolution network and a spatial coordinate attention module. The main urban area within the 5th ring road of Beijing was selected as the study area. With the trained model, the GF-2 image tiles of Beijing input into the models to first derive coarse maps of DLB and SIB. Postprocessing optimization was performed after combining the maps. The accuracy assessment shows that the overall accuracy of large-scale urban building mapping using the hierarchical approach proposed in this article reaches 91.5%, which is 4.8% higher than that with a traditional method. Overall, the hierarchical approach proposed in this article is effective in large-scale urban building mapping and provides new application opportunities.', 'title': 'A Large-Scale Mapping Scheme for Urban Building From Gaofen-2 Images Using Deep Learning and Hierarchical Approach', 'embedding': []}, {'id': 14864, 'abstractText': 'Summary form only given. Traditional immunostaining techniques utilize antibodies to probe the expression of specific proteins in the brain, but this only allows for two-dimensional mapping that cannot recapitulate the complex circuit and regional interactions that occur in the brain. To achieve a more advanced level of circuitry mapping, we combine classic immunofluorescence staining with a tissue clearing technique, immunolabelling-enabled three-dimensional imaging of solvent-cleared organs (iDISCO), to map neurocircuits in the intact brain. Coupling these technologies permits analysis of neuronal circuits under a variety of conditions, including responses to visual and auditory stimuli, as well as in response to pharmacological agents or drugs of abuse. We apply these technologies to study the neuronal activity changes in the basal ganglia driven by selective dopamine-D1 receptor agonist 2-chloro-APB hydrobromide in wild type and Slc35d3 heterozygous mice and correlated behavioral changes with differences in circuit response. We also map tyrosine hydroxylase positive projection neurons in the whole brains of methamphetamine-injected rats. Based on these studies, we can target specific neuronal populations stimulated in response to these compounds to modulate circuit activation as a potential intervention in basal ganglia function and in drug abuse.', 'title': 'Integrating Immunostaining with Tissue Clearing Techniques for Whole Brain Mapping in Basal Ganglia and Drug Addiction', 'embedding': []}, {'id': 14865, 'abstractText': \"Small-sized unmanned aerial vehicles (UAVs) have been widely investigated for use in a variety of applications such as remote sensing and aerial surveying. Direct three-dimensional (3D) mapping using a small-sized UAV equipped with a laser scanner is required for numerous remote sensing applications. In direct 3D mapping, the precise information about the position and attitude of the UAV is necessary for constructing 3D maps. In this study, we propose a novel and robust technique for estimating the position and attitude of small-sized UAVs by employing multiple low-cost and light-weight global navigation satellite system (GNSS) antennas/receivers. Using the “redundancy'' of multiple GNSS receivers, we enhance the performance of real-time kinematic (RTK)-GNSS by employing single-frequency GNSS receivers. This method consists of two approaches: hybrid GNSS fix solutions and consistency examination of the GNSS signal strength. The fix rate of RTK-GNSS using single-frequency GNSS receivers can be highly enhanced to combine multiple RTK-GNSS to fix solutions in the multiple antennas. In addition, positioning accuracy and fix rate can be further enhanced to detect multipath signals by using multiple GNSS antennas. In this study, we developed a prototype UAV that is equipped with six GNSS antennas /receivers. From the static test results, we conclude that the proposed technique can enhance the accuracy of the position and attitude estimation in multipath environments. From the flight test, the proposed system could generate a 3D map with an accuracy of 5 cm.\", 'title': 'Robust UAV Position and Attitude Estimation using Multiple GNSS Receivers for Laser-based 3D Mapping', 'embedding': []}, {'id': 14866, 'abstractText': 'Fuzzy cognitive maps are studied from the quantitative human-scientific standpoint. The concepts and weights of these maps are thus examined as statistical random variables based on probability distributions in order to make more feasible interpretations on these entities. First, the available fuzzy cognitive maps are considered. Second, our statistical approach is introduced which assumes that the concepts and weights are uniformly or normally distributed random variables. Third, the dynamics and application possibilities of these fuzzy cognitive maps are studied.', 'title': 'A Statistical Random Variable Approach to Fuzzy Cognitive Map Modeling', 'embedding': []}, {'id': 14867, 'abstractText': 'The knowledge of the dynamical state of galaxy clusters allows to alleviate systematics when observational data from these objects are applied in cosmological studies. Evidence of correlation between the state and the morphology of the clusters is well studied. The morphology can be inferred by images of the surface brightness in the X-ray band and of the thermal component of the Sunyaev–Zel’dovich (tSZ) effect in the millimetre range. For this purpose, we apply, for the first time, the Zernike polynomial decomposition, a common analytical approach mostly used in adaptive optics to recover aberrated radiation wavefronts at the telescopes pupil plane. With this novel way, we expect to correctly infer the morphology of clusters and so possibly their dynamical state. To verify the reliability of this new approach, we use more than 300 synthetic clusters selected in the three hundred project at different redshifts ranging from 0 up to 1.03. Mock maps of the tSZ, quantified with the Compton parameter, y-maps, are modelled with Zernike polynomials inside R<inf>500</inf>, the cluster reference radius. We verify that it is possible to discriminate the morphology of each cluster by estimating the contribution of the different polynomials to the fit of the map. The results of this new method are correlated with those of a previous analysis made on the same catalogue, using two parameters that combine either morphological or dynamical-state probes. We underline that instrumental angular resolution of the maps has an impact mainly when we extend this approach to high-redshift clusters.', 'title': 'The Three Hundred project: quest of clusters of galaxies morphology and dynamical state through Zernike polynomials', 'embedding': []}, {'id': 14868, 'abstractText': \"Indoor localization has been recognized as a promising research around the world, and fingerprint-based localization method which leverages WIFI Received Signal Strength (RSS) has been extensively studied since widespread deployment of Access Points (APs) makes WIFI signals omnipresent and easily be obtained. A primary weakness of WIFI-based fingerprinting localization approach lies in its vulnerability under environmental changes and alteration of AP deployment. Despite some studies focus on dealing with effects of AP alterations and low-dynamic environmental factors, such as humidity, temperature, etc., influences of high-dynamic factors, such as changes of crowds' density and position, on WIFI radio map have not been sufficiently studied. In this work, we propose OWUH, an Online Learning-based WIFI Radio Map Updating service considering influences of high-dynamic factors. OWUH utilizes sensors in smart phones as the source of RSS datasets, and it combines historical and newly collected RSS data and purposeful probe data as dataset to incrementally update radio map, which means, compared with traditional methods, the OWUH approach requires a smaller number of RSS data for frequent updating of radio map. Moreover, in order to further enlarge our dataset, we take static data and low-dynamic data into account. An improved online learning method is proposed to recognize periodic pattern and update current radio map. Extensive experiments with 15 volunteers across 10 days indicate that OWUH effectively accommodates RSS variations over time and derives accurate prediction of fresh radio map with mean errors of less than 5dB, outperforming existing approaches.\", 'title': 'Online Learning-Based WIFI Radio Map Updating Considering High-Dynamic Environmental Factors', 'embedding': []}, {'id': 14869, 'abstractText': 'Abstract A matrix convex set is a set of the form <tex>${{\\\\cal S}_n}$</tex> (where each <tex>$d$</tex> is a set of <tex>$n \\\\times n$</tex>-tuples of <tex>${M_n}$</tex> matrices) that is invariant under unital completely positive maps from <tex>${M_k}$</tex> to <tex>${\\\\cal S} = \\\\mathop \\\\cup \\\\limits_{n \\\\ge 1} {{\\\\cal S}_n},$</tex> and under formation of direct sums. We study the geometry of matrix convex sets and their relationship to completely positive maps and dilation theory. Key ingredients in our approach are polar duality in the sense of Effros and Winkler, matrix ranges in the sense of Arveson, and concrete constructions of scaled commuting normal dilation for tuples of self-adjoint operators, in the sense of Helton, Klep, McCullough, and Schweighofer. Given two matrix convex sets <tex>${\\\\cal T} = \\\\mathop \\\\cup \\\\limits_{n \\\\ge 1} {{\\\\cal T}_n}$</tex> and <tex>${\\\\cal S}$</tex>, we find geometric conditions on <tex>${\\\\cal T}$</tex> or on <tex>${{\\\\cal S}_1} \\\\subseteq {{\\\\cal T}_1}$</tex>, such that <tex>${\\\\cal S} \\\\subseteq C{\\\\cal T}$</tex> implies that <tex>$C$</tex> for some constant <tex>${\\\\cal S}$</tex>. For instance, under various symmetry conditions on <tex>$C$</tex>, we can show that <tex>$d$</tex> above can be chosen to equal <tex>$C = d$</tex>, the number of variables. We also show that <tex>${{\\\\cal W}^{\\\\max }}(\\\\overline {{\\\\BBB}{_d}} )$</tex> is sharp for a specific matrix convex set <tex>${\\\\BBB}{_d}$</tex> constructed from the unit ball <tex>${\\\\cal D}$</tex>. This led us to find an essentially unique self-dual matrix convex set <tex>$C = \\\\sqrt d $</tex>, the self-dual matrix ball, for which corresponding inclusion and dilation results hold with constant <tex>${\\\\cal T}$</tex>. For a certain class of polytopes, we obtain a considerable sharpening of such inclusion results involving polar duals. An illustrative example is that a sufficient condition for <tex>${^{(d)}} = \\\\mathop \\\\cup \\\\limits_n \\\\left\\\\{ {({T_1}, \\\\ldots ,{T_d}) \\\\in M_n^d:{T_i} \\\\le 1} \\\\right\\\\}$</tex> to contain the free matrix cube <tex>$\\\\{ x \\\\in {\\\\BBR}{^d}:\\\\sum |{x_j}| \\\\le 1\\\\} \\\\subseteq {1 \\\\over d}{{\\\\cal T}_1}$</tex>, is that <tex>${1 \\\\over d}{{\\\\cal T}_1}$</tex>, that is, that <tex>${[ - 1,1]^d} = _1^{(d)}$</tex> contains the polar dual of the cube <tex>$d$</tex>. Our results have immediate implications to spectrahedral inclusion problems studied recently by Helton, Klep, McCullough and Schweighofer. Our constants do not depend on the ranks of the pencils determining the free spectrahedra in question, but rather on the “number of variables” <tex>$$\\\\matrix{ {{1 \\\\over M}\\\\mathop \\\\sum \\\\limits_{M \\\\le m \\\\lt 2M} {1 \\\\over H}\\\\left| {\\\\mathop \\\\sum \\\\limits_{m \\\\le n \\\\lt m + H} {e^{2\\\\pi iP(n)}}\\\\nu (n)} \\\\right|0} \\\\cr } $$</tex>. There are also implications to the problem of existence of (unital) completely positive maps with prescribed values on a set of operators.', 'title': 'Dilations, Inclusions of Matrix Convex Sets, and Completely Positive Maps', 'embedding': []}, {'id': 14870, 'abstractText': \"An attempt has been made to study the effectiveness of concept maps in physics education through quasi-experimental design. The post-test is used as a measuring instrument and the post-test scores are statistically analysed. The results of Levene's test and t-test reveal that mean of post-test scores of the experimental and control groups are statistically significant. This confirms the Alternate Hypothesis that students who studied using concept maps (experimental group) attained higher learning compared to the students who did not use the concept maps (control group). The learner achievement score reveals that 17.7 % higher learning has been attained by experimental group compared with control group. Questionnaires were used to know the students perceptions on the use of concept maps. The results of questionnaire analysis showed that students find concept maps are useful in teaching and learning. Further details of this study are discussed in the article.\", 'title': 'Concept maps in teaching physics concepts applied to engineering education: An explorative study at the Middle East College, Sultanate of Oman', 'embedding': []}, {'id': 14871, 'abstractText': 'Global forest aboveground biomass (AGB) is very important in quantifying carbon stock, and, therefore, it is necessary to estimate forest AGB accurately. Many studies have obtained reliable AGB estimates by using light detection and ranging (LiDAR) data. However, it is difficult to obtain LiDAR data continuously at regional or global scale. Although many studies have integrated multisource data to estimate biomass to compensate for these deficiencies, few methods can be applied to produce global time series of high-resolution AGB due to the complexity of the method, data source limitations, and large uncertainty. This study developed a new method to produce a global forest AGB map using multiple data sources-including LiDAR-derived biomass products, a suite of high-level satellite products, forest inventory data, and other auxiliary datasets-to train estimated models for five different forest types. We explored three machine learning methods [artificial neural network, multivariate adaptive regression splines, and gradient boosting regression tree (GBRT)] to build the estimated models. The GBRT method was the optimal algorithm for generating a global forest AGB map at a spatial resolution of 1 km. The independent validation result showed good accuracy with an R<sup>2</sup> value of 0.90 and a root mean square error value of 35.87 Mg/ha. Moreover, we compared the generated global forest AGB map with several other forest AGB maps and found the results to be highly consistent. An important feature of this new method is its ability to produce time series of high-resolution global forest AGB maps because it heavily relies on high-level satellite products.', 'title': 'A New Method for Generating a Global Forest Aboveground Biomass Map From Multiple High-Level Satellite Products and Ancillary Information', 'embedding': []}, {'id': 14872, 'abstractText': 'Paddy rice serves as one of the most important crop food globally. The spatial distribution of paddy rice fields plays a fundamental role in describing the rural landscapes, and the precise location and extent of paddy rice fields are of key importance to analyze the subsequent resource allocation, rice yield prediction, and food security. Paddy rice has a distinct character relative to other crops in that the paddy fields need flooding during the initial period of rice seeds preparing and rice transplanting. In order to map paddy rice, remote sensing techniques have long been used to extract and monitor rice crops. Throughout many approaches, phenology-based paddy rice mapping algorithms have been introduced and tested in coarser remote sensing images such as MODIS (Moderate Resolution Imaging Spectroradiorneter), AVHRR (Advanced Very High Resolution Radiometer) and Landsat images. However, the average size of paddy rice fields is commonly smaller than 0.09 ha (e.g., the area of a Landsat pixel). Therefore, a large number of mixed pixels exist, which leads to misclassification. Meanwhile, the phenological indicators used in the previous studies such as LSWI (Land Surface Water Index), and MNDWI (Modified Normalized Difference Water Index) are not feasible to detect the land surface water content at the initial stage of paddy rice seeds preparation and transplanting. Therefore, this study first proposes a new index PMI (Perpendicular Moisture Index) to identify the irrigation in paddy rice fields, and then combines other Vegetation Indices to map paddy rice in Jianghan Plain using time stacks of Sentinel-2 imagery. The results indicates that the proposed PMI can be effectively used as phenological indicator for paddy rice mapping and the Sentinel-2 images provide more detailed spatial distributions. The accuracy assessment suggests that our method has a high accuracy with Overall Accuracy (&gt;97%) and Kappa (&gt;93%). This study suggests that the Sentinel-based automated paddy rice mapping algorithm could potentially and effectively be applied at large spatial scales to monitor paddy rice agriculture.', 'title': 'Automated Paddy Rice Extent Extraction with Time Stacks of Sentinel Data: A Case Study in Jianghan Plain, Hubei, China', 'embedding': []}, {'id': 14873, 'abstractText': \"Concept map can explicitly represent complex ideas and the relationships among them. However, there are few empirical studies on deploying it as a supporting tool for inquiry learning. This pilot study explored the effects of concept mapping on students' group task performance, attitudes towards inquiry learning, and motivational and emotional experiences, in an online learning module. A class of 49 eleventh-grade students participated in the study. They learned in small groups to explore a fish death problem, and constructed concept maps to support their inquiry. The group inquiry task performance was satisfying, with a total mean score of 3.60 out of 5. Survey results showed that students had positive attitude towards inquiry learning (M = 4.31, SD = 0.60), and consensus building (M = 4.40, SD = 0.55). Interviews also revealed that concept mapping enabled students to learn how to think in a logical way, and thus improved their inquiry abilities. The findings demonstrate the promising effects of concept mapping on supporting inquiry learning.\", 'title': 'A Pilot Study of Concept Mapping Mediated Inquiry Learning in an Online Environment', 'embedding': []}, {'id': 14874, 'abstractText': 'Trend Assessment and Scenario Development Analysis approach was carried out in this study. This approach is a framework specifically designed to look at the direction of technological development and possible conditions that occur in the future. First stage of the framework consists of the Delphi study, which is a method for obtaining issues related to maritime and possible conditions in the future. Second stage of the framework, scenario planning, which is a method to illustrate the possibility of future conditions by taking into account the main driving factors; then technology road-mapping illustrate the steps that must be taken to achieve the ideal conditions planned. The Delphi study is applied to develop a consensus-based, prioritized research agenda for Indonesia Foresight Maritime Research Topics in year 2018-2045. Expert investigation of the web-based Delphi method was employed to develop online survey. In this study, a three round modified Delphi survey was conducted. A total of 45 study participants were participated in the third round of study. Some interview study was designed to validate some of the findings. The top 10 priority themes were determined by the web-based Delphi method result and in-depth interview of experts. The results of this study indicate that among the Indonesian maritime researcher and maritime technology expert in this study, they have a high degree consensus on some research topics. In the second stage of the framework, we used “scenario planning” approach. Scenario planning is intended to understand the perception of management in recognizing future alternatives so that appropriate decisions can be taken. Experts and stakeholders have better understanding of the issues pertaining the development of Maritime Frontier Research Topics in Indonesia, including those related to tourism, ocean and fisheries, infrastructures, energy and mineral etc. This study then analyzes the dynamics of strategic environment changes (socio-political-economic context) that determine the construction of Maritime Frontier Research Topics. By using a scenario planning analysis framework, the driving factors and uncertainty factors are formulated which are then developed into a strategic environmental scenario. There are 40 experts participated in two scenario planning workshops. The results of scenario planning exercises map 4 (four) scenarios in 2030 and 4 (four) scenario in 2045. The framework for this foresight technology activities has produced the documents for the development of the Foresight Maritime Research topics that use the combination of Delphi study and scenario planning approach.', 'title': 'Combining Delphi Study and Scenario Planning for Indonesia Research Priorities in Maritime Sector', 'embedding': []}, {'id': 14875, 'abstractText': 'Agile Software Development (ASD) is a knowledge intensive and collaborative activity. The key of acquiring, creating and co-operating the knowledge depends on Knowledge Management (KM). The field of knowledge management helps to improve the productivity of the whole agile software development process from the beginning to the end of the phase. Therefore, investigation of various aspects such as purposes, types of knowledge, technologies and research type is essential. The goal of this study is to conduct a literature review on existing researches on KM initiatives in ASD in order to identify the-state-of-the-art in the area as well as future research opportunities. A mapping study was performed by searching six electronic databases, and we considered studies published until December 2017. The initial resulting set comprised of 404 studies. From this set, a total of 10 studies were selected. for these 10, we performed snowballing and direct search of publications of researchers and research groups that accomplished these studies. Finally, we identified 12 reviewed studies addressing KM in Agile software development in order to extract relevant information on a set of research questions. Although few studies were found that addressed KM in ASD, the mapping shows an increasing attention and interest in the topic in the recent years. Reuse of knowledge of the team is a perspective that has received more attention. Moreover, as a main conclusion, the results show that KM is pointed out as an important strategy for increasing the effectiveness in ASD.', 'title': 'Knowledge Management in Agile Software Development- A Literature Review', 'embedding': []}, {'id': 14876, 'abstractText': 'Since lane geometry information can be used for controlling the pose of an intelligent vehicle, a lane geometry map that contains the lane geometry information should have reliable accuracy. For generating the reliable lane geometry map, lane curve which is detected from a lane detector is an useful information because the lane geometry information can be obtained directly. However, since the detected lane curve contains an uncertainty caused by the noise of the lane detector, the accuracy of the lane geometry map can be degraded. In previous studies, a near point on each detected lane is sampled at each time stamp and accumulated for reducing the noise effects of the lane detector. However, these sampled points also contain the sensing noise of the lane detector and the density of accumulated points depends on the distance interval of data acquisition. In this article, we proposed the probabilistic lane smoothing-based generation method for the reliable lane geometry map. In the probabilistic lane smoothing, the lane geometry map is modeled as the nodes with the uncertainty of its position obtained from the sensor error model. Each node of the lane geometry map is smoothed based on the Bayesian filtering scheme. The evaluation results show that the lane geometry map can be generated by reducing the noise of the detected lane curve. Additionally, the generated lane geometry map is more reliable than the sampling point-based generated map in terms of the accuracy of the distance and heading angle.', 'title': 'Probabilistic Smoothing Based Generation of a Reliable Lane Geometry Map With Uncertainty of a Lane Detector', 'embedding': []}, {'id': 14877, 'abstractText': \"Intelligent Cushion (iCushion) technology is recently booming with embedded pressure array sensors to enable individual-specific sitting experiences. iCushion has the build-in functionality to identify users throughout its use in a continuous and non-intrusive manner. Due to the variability in sitting posture and the angle of seated deflection, the accuracy of user identification remains unstable or unclear with existing solutions. Aiming at this problem, this study develops a two-stage pressure map algorithm based on robust spatial-temporal features. First, pressure maps are collected constantly without limiting the user's posture, based on which an accumulated identity library is established for sitting postures by extracting features from pressure maps. To be specifically, we create a decision tree to classify maps by distances between both ischia and then variances in both areas around ischia in maps are analyzed. Second, the similarity between both maps are measured by the Euclidean distance between feature vectors around ischia for matching maps data. A k-NN voting mechanism is developed to achieve reliability of identification. The resulted iCushion prototype has successfully identified 92.2% of maps with three randomly chosen individuals through four-hour non-stop testing. It holds potentials of non-intrusive and reliable activity recognition in other pervasive applications.\", 'title': 'iCushion: A Pressure Map Algorithm for High Accuracy Human Identification', 'embedding': []}, {'id': 14878, 'abstractText': 'Existing studies on indoor navigation often require such a pre-deployment as floor map, localization system and/or additional (customized) hardwares, or human motion traces, making them prohibitive when the situation deviates from these requirements (e.g., navigating a crowd of panicking people where no localization system or motion traces are available). The main observation inspiring our work without reliance on such pre-deployment is that when there are sufficient participants (e.g., a crowd of panicking people), the WiFi signatures collected by participants can serve as the fingerprints (referred to as location fingerprints) of their unknown locations. By computing relative positions of these location fingerprints we can connect them to form a global map. Such a map reflects the topology of the underlying walkable space and thus holds the potential of offering a navigation path for any intended users. Based on this observation, we design Fly-Navi, a crowdsourcing based indoor navigation system via on-the-fly map generation, and primarily designed for indoor environments with rectilinear and narrow corridors. Specifically, each participant uploads sensory data, and the server then generates a global map (on-the-fly map) through a series of operations such as local map generation, local map stitch and edge computation. On top of the global map, Fly-Navi computes a navigation path to the given destination and tracks the progress. We implement the prototype of Fly-Navi and our experiments show that Fly-Navi can quickly generate a correct global map with the 80-percentile of between-fingerprint distance error less than 3 meters, which is important for computing turning points of the map and hereon offering turn-by-turn instructions, and correctly navigate the intended users to their destinations.', 'title': 'Fly-Navi: A Novel Indoor Navigation System With On-the-Fly Map Generation', 'embedding': []}, {'id': 14879, 'abstractText': 'Coherent mapping becomes important resources in ontology matching process. A process called mapping repair is carried out to ensure the output alignment does not contain incoherent mapping. The goal of repair is to change the incoherent alignment into coherent alignment by removing some unwanted mapping from the alignment. The removal of mapping should be as minimal as possible in order to avoid a major change in input alignment. This article aims to build a mapping repair system that restore incoherent into coherent mapping (or alignment). This study implements mapping weighting as the heuristic function and A* Search method to produce coherent output alignment. Some experiments were conducted, the results showed that proposed system can produce output alignment with zero conflict and 100% coherence degree.', 'title': 'Optimal Path Finding Algorithm Using Weighted Based Heuristic for Incoherent Mapping Repair', 'embedding': []}, {'id': 14880, 'abstractText': 'Virtual network embedding (VNE) is the key technology in network virtualization and has been proven NP-hard. The purpose of VNE is to find the optimal mapping of virtual nodes and links, and minimize the utilization of resources. However, many particle swarm optimization approaches to VNE separate VNE into two independent subproblems (i.e., node mapping and link mapping) and ignore the coordination between node mapping and link mapping. In this paper, a one-stage and dual-heuristic particle swarm optimization (DH-PSO) is devised to solve VNE. To coordinate node mapping and link mapping, firstly, DH-PSO updates positions of particles step by step, and nodes and links are mapped in one stage. Secondly, DH-PSO devises the dual-heuristic strategy to further improve the optimizing capability. The first heuristic strategy is to construct a candidate set and the second strategy is to find the best solution from the candidate set. Hence, not only the network resources but the network paths are taken into account to construct solutions. DH-PSO can be combined with different two-stage approaches to become one-stage. DH-PSO is experimentally studied on different instances. The experimental results verify that the proposed DH-PSO is promising.', 'title': 'One-stage and Dual-heuristic Particle Swarm optimization for Virtual Network Embedding', 'embedding': []}, {'id': 14881, 'abstractText': 'Aiming at the problem that the map operation time occupies more resources in the embedded system. By studying the digital map, then the corresponding map is drawn and the map is optimized. Using the GPS/BEIDOU module to locate and use the Dijkstra algorithm to achieve the shortest path of the navigation module design, the use of the map database to remove the corresponding area of the data mapping map information, in a translation to a certain distance when the scene clear the data, draw a map. In this way, there will be no data left in the view. Since the Dijkstra algorithm is based on the graph, it is necessary to link the roads in the area to create an indirect graph to find the shortest path to the end point. If the point of the line as an indirect node, the composition of the indirect map will be very large. Moreover, the use of the line as an indirect node, we can reduce the number of nodes without the map, to improve the efficiency of Dijkstra algorithm traversal. Based on the open system platform and data standard, this paper designs and develops a secure, stable and low cost embedded GPS/BEIDOU navigation and navigation system. The experimental results are optimized. The results show that the proposed method is efficient for navigation.', 'title': 'An Open Source Map Optimization Platform for Efficient Navigation', 'embedding': []}, {'id': 14882, 'abstractText': 'Single-robot visual SLAM has problems such as slow mapping speed. This study proposed a multi-robot collaborative SLAM and scene reconstruction system based on an RGB-D camera. The system adopts a centralized structure. While several client robots collect RGB-D image data respectively and transmit the data to the server, the server runs a stand-alone visual SLAM system based on ORB-SLAM2 for each client, and stores the real-time mapping data in the map manager. At the same time, it detects whether the maps in the map managers meet the fusion conditions at a certain frame rate, and uses the map fusion algorithm to fuse when the conditions are met. In order to solve the problem that ORB-SLAM2 can only do semi-dense mapping, the system uses the 7-DoF poses of each client output by collaborative SLAM to reconstruct the dense map. We evaluated the performance of the system through simulations. Compared with the single-robot system, the collaborative SLAM in this system has a significant improvement in speed and can maintain high accuracy. We verified the effectiveness of the scene reconstruction algorithm through the scene reconstruction simulation and further proved that high-precision camera poses can be obtained in collaborative SLAM through the reconstruction results.', 'title': 'Multi-robot collaborative SLAM and scene reconstruction based on RGB-D camera', 'embedding': []}, {'id': 14883, 'abstractText': \"Semantic information without spatial characteristics can also be visualised in the form of a map. This map type is usually called map-like visualisation; it is not a representation of a real geo-entity but borrows the map metaphor. In current map-like visualisation works, the manipulation of the cartographic process is poor, and there is a considerable amount of randomness and uncertainty in the map results. For instance, in the process of converting semantic objects to map objects, the map shape is usually uncertain and has many possibilities. Frequently, however, we need shapes to follow certain principles, such as the principles of being close to a certain shape and having a simple geometry, to facilitate recognition and mapping. To achieve a balance between uncertain shapes and design needs, this study enhances the realisation of the cartographer's design idea by improving the controllability in the cartographic process. An optimisation technology, the simulated annealing algorithm, is introduced to control the determination of shape to improve mapping efficiency. Experiments based on real data show that based on this method, the map-like representation not only creates a whole outline close to the pre-defined shape but also realises internal regions with a simple shape.\", 'title': 'Shape Decision-Making in Map-Like Visualization Design Using the Simulated Annealing Algorithm', 'embedding': []}, {'id': 14884, 'abstractText': 'In this article, a novel dense underwater 3-D mapping paradigm based on pose graph simultaneous localization and mapping (SLAM) using an acoustic camera mounted on a rotator is proposed. The demands of underwater tasks, such as unmanned construction using robots, are growing rapidly. In recent years, the acoustic camera, which is a state-of-the-art forward-looking imaging sonar, has been gradually applied in underwater exploration. However, distinctive imaging principles make it difficult to gain an intuitive perception of an underwater environment. In this study, an acoustic camera with a rotator was used for dense 3-D mapping of the underwater environment. The proposed method first applies a 3-D occupancy mapping framework based on the acoustic camera rotating around the acoustic axis using a rotator at a stationary position to generate 3-D local maps. Then, scan matching of adjacent local maps is implemented to calculate odometry without involving internal sensors, and an approximate dense global map is built in real time. Finally, based on a graph optimization scheme, offline refinement is performed to generate a final dense global map. Our experimental results demonstrate that our 3-D mapping framework for an acoustic camera can achieve dense 3-D mapping of underwater environments robustly and accurately.', 'title': 'Acoustic Camera-Based Pose Graph SLAM for Dense 3-D Mapping in Underwater Environments', 'embedding': []}, {'id': 14885, 'abstractText': 'Heterogeneous MPSoCs consisting of integrated CPUs and GPUs are suitable platforms for embedded applications running on hand- held devices such as smart phones. As the handheld devices are mostly powered by battery, the integrated CPU and GPU MPSoC is usually designed with an emphasis on low-power rather than performance. In this paper, we are interested in exploring a power- efficient layer mapping of convolution neural networks (CNNs) deployed on integrated CPU and GPU platforms. Specifically, we investigate the impact of layer mapping of YoloV3-Tiny (i.e., a widely-used CNN in both industry and academia) on system power consumption through numerous experiments on NVIDIA board Jetson TX2. The experimental results indicate that 1) almost all of the convolution layers are not suitable for mapping to CPU, 2) the pooling layer can be mapped to CPU for reducing power consumption, but the mapping may lead to a decrease in inference speed when the layer’s output tensor size is large, 3) the detection layer can be mapped to CPU as long as its floating-point operation scale is not too large, and 4) the channel and upsampling layers are both suitable for mapping to CPU. These observations obtained in this study can be further utilized to guide the design of power-efficient layer mapping strategies for integrated CPU and GPU platforms.', 'title': 'Power-Efficient Layer Mapping for CNNs on Integrated CPU and GPU Platforms: A Case Study', 'embedding': []}, {'id': 14886, 'abstractText': 'A highlighted map, where objects with unique shapes are highlighted, has been studied for mobile robot localization. This map improves the localization accuracy without adding any sensors or online computations for localization. In addition, it can be used in various particle-filter-based localization algorithms. For generating a highlighted map, reinforcement learning has been used. Since this method generates the highlighted map by utilizing a limited number of the actual sensor measurement data, the generated map is vulnerable to unexpected sensor measurement noise. In this paper, the robustification method of a highlighted map is proposed. Our proposed method introduces a virtual obstacle that causes measurement noise, and learns both the worst-case obstacle behavior and the optimal highlighted map simultaneously based on adversarial reinforcement learning. We perform a numerical simulation to verify the robustness of the map.', 'title': 'Adversarial Reinforcement Learning Based Robustification of Highlighted Map for Mobile Robot Localization', 'embedding': []}, {'id': 14887, 'abstractText': \"Chaotic dynamics is widely used to design pseudo-random number generators and for other applications, such as secure communications and encryption. This paper aims to study the dynamics of the discrete-time chaotic maps in the digital (i.e., finite-precision) domain. Differing from the traditional approaches treating a digital chaotic map as a black box with different explanations according to the test results of the output, the dynamical properties of such chaotic maps are first explored with a fixed-point arithmetic, using the Logistic map and the Tent map as two representative examples, from a new perspective with the corresponding state-mapping networks (SMNs). In an SMN, every possible value in the digital domain is considered as a node and the mapping relationship between any pair of nodes is a directed edge. The scale-free properties of the Logistic map's SMN are proved. The analytic results are further extended to the scenario of floating-point arithmetic and for other chaotic maps. Understanding the network structure of a chaotic map's SMN in digital computers can facilitate counteracting the undesirable degeneration of chaotic dynamics in finite-precision domains, also helping to classify and improve the randomness of pseudo-random number sequences generated by iterating the chaotic maps.\", 'title': 'Dynamic Analysis of Digital Chaotic Maps via State-Mapping Networks', 'embedding': []}, {'id': 14888, 'abstractText': 'Autonomous vehicle self-positioning based on 3D light detection and ranging (Lidar) has become popular recently due to disadvantages of global navigation satellite system (GNSS) in urban areas. As LiDAR-based simultaneous localization and mapping (SLAM) methods suffer from error accumulation, state-of-the-art approaches match the point cloud data acquired by LiDAR to the priori known 3D point cloud map to obtain the position of the vehicle within the map. However, 3D point cloud map is very expensive to store and download as it contains an enormous amount of data even for a small area (around 300 million points per km<sup>2</sup>). In this study, rather than using 3D point cloud directly as a map, we focused on the planar surfaces which are mostly available in urban areas, easy to extract, and at the same time clearly observable by LiDAR. Therefore, in our proposed map, we extract the planar surfaces from the 3D point cloud and calculate its uncertainty (deviation) and store them as a prior map. Accordingly, in this map, we can abstract several thousands of points by only one plane. As a result, we can extremely shrink the map size (25 million points to around 1000 planes). Later in the localization phase, we reconstruct Gaussian mixture model for each planar surface based on previously stored deviation, and match LiDAR data to it to obtain the precise location of the vehicle. Experiments conducted in one of the urban areas of Tokyo show that even though we extremely shrank the map size, we could preserve the mean error of the localization less than 43cm comparing to other point cloud based methods.', 'title': 'Autonomous vehicle self-localization based on probabilistic planar surface map and multi-channel LiDAR in urban area', 'embedding': []}, {'id': 14889, 'abstractText': 'Coarse-grain reconfigurable arrays (CGRAs) are emerging accelerators that promise low-power acceleration of compute-intensive loops in applications. The acceleration achieved by CGRA relies on the efficient mapping of the compute-intensive loops by the CGRA compiler, onto the CGRA architecture. The CGRA mapping problem, being NP-complete, is performed in a two-step process, namely, scheduling and mapping. The scheduling algorithm allocates timeslots to the nodes of the data flow graph, and the mapping algorithm maps the scheduled nodes onto the processing elements of the CGRA. On a mapping failure, the initiation interval (II) is increased and a new schedule is obtained for the increased II. Most previous mapping techniques use the iterative modulo scheduling (IMS) algorithm to find a schedule for a given II. Since IMS generates a resource-constrained as-soon-as-possible (ASAP) scheduling, even with increased II, it tends to generate a similar schedule that is not mappable. Therefore, IMS does not explore the schedule space effectively. To address these issues, this article proposes CRIMSON, compute-intensive loop acceleration by randomized IMS and optimized mapping technique that generates random modulo schedules by exploring the schedule space, thereby creating different modulo schedules at a given and increased II. CRIMSON also employs a novel conservative test after scheduling to prune valid schedules that are not mappable. From our study conducted on the top 24 performance-critical loops (run for more than 7% of application time) from MiBench, Rodinia, and Parboil, we found that previous state-of-the-art approaches that use IMS, such as RAMP and GraphMinor could not map five and seven loops, respectively, on a 4×4 CGRA, whereas CRIMSON was able to map them all. For loops mapped by the previous approaches, CRIMSON achieved a comparable II.', 'title': 'CRIMSON: Compute-Intensive Loop Acceleration by Randomized Iterative Modulo Scheduling and Optimized Mapping on CGRAs', 'embedding': []}, {'id': 14890, 'abstractText': 'Depth maps obtained by commercial depth sensors are always in low-resolution, making it difficult to be used in various computer vision tasks. Thus, depth map super-resolution (SR) is a practical and valuable task, which up-scales the depth map into high-resolution (HR) space. However, limited by the lack of real-world paired low-resolution (LR) and HR depth maps, most existing methods use down-sampling to obtain paired training samples. To this end, we first construct a large-scale dataset named \"RGB-D-D\", which can greatly promote the study of depth map SR and even more depth-related real-world tasks. The \"D-D\" in our dataset represents the paired LR and HR depth maps captured from mobile phone and Lucid Helios respectively ranging from indoor scenes to challenging outdoor scenes. Besides, we provide a fast depth map super-resolution (FDSR) baseline, in which the high-frequency component adaptively decomposed from RGB image to guide the depth map SR. Extensive experiments on existing public datasets demonstrate the effectiveness and efficiency of our network compared with the state-of-the-art methods. Moreover, for the real-world LR depth maps, our algorithm can produce more accurate HR depth maps with clearer boundaries and to some extent correct the depth value errors.', 'title': 'Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark Dataset and Baseline', 'embedding': []}, {'id': 14891, 'abstractText': 'The first Canadian wetland inventory (CWI) map, which was based on Landsat data, was produced in 2019 using the Google Earth Engine (GEE) big data processing platform. The proposed GEE-based method to create the preliminary CWI map proved to be a cost, time, and computationally efficient approach. Although the initial effort to produce the CWI map was valuable with a 71% overall accuracy (OA), there were several inevitable limitations (e.g., low-quality samples for the training and validation of the map). Therefore, it was important to comprehensively investigate those limitations and develop effective solutions to improve the accuracy of the Landsat-based CWI (L-CWI) map. Over the past year, the L-CWI map was shared with several governmental, academic, environmental nonprofit, and industrial organizations. Subsequently, valuable feedback was received on the accuracy of this product by comparing it with various in situ data, photo-interpreted reference samples, land cover/land use maps, and high-resolution aerial images. It was generally observed that the accuracy of the L-CWI map was lower relative to the other available products. For example, the average OA in four Canadian provinces using in situ data was 60%. Moreover, including reliable in situ data, using an object-based classification method, and adding more optical and synthetic aperture radar datasets were identified as the main practical solutions to improve the CWI map in the future. Finally, limitations and solutions discussed in this study are applicable to any large-scale wetland mapping using remote sensing methods, especially to CWI generation using optical satellite data in GEE.', 'title': 'Evaluation of the Landsat-Based Canadian Wetland Inventory Map Using Multiple Sources: Challenges of Large-Scale Wetland Classification Using Remote Sensing', 'embedding': []}, {'id': 14892, 'abstractText': 'Task mapping has been a hot topic in multiprocessor system-on-chip software design for decades. During the mapping process, load balance (LB) and communication optimization have been two important performance optimization factors. This paper studies the relations between LB, interprocessor communications, and communication pipeline technique during the mapping process, and proposes an integer linear programming (ILP)-based static task mapping approach, which considers both LB and communication optimization. The approach consists of an optimized ILP model for task mapping with fewer variables compared to previous ILP mapping works. Moreover, to enhance the scalability of the ILP task mapping, the task-processor-cluster algorithm is proposed to reduce the scale of the task graph and the number of processors and then solve the coarse-grained input by the ILP mapping. To increase the adaptability of the ILP task mapping, the improved augmented E-constraint method is further integrated with the ILP formulations to select the best mapping for different applications. Experimental results on a 2/4/8/16/24-CPU platform of both synthetic and real-life benchmarks demonstrate the efficiency of the proposed approach.', 'title': 'A Scalable and Adaptable ILP-Based Approach for Task Mapping on MPSoC Considering Load Balance and Communication Optimization', 'embedding': []}, {'id': 14893, 'abstractText': 'Pre-surgical mapping of sensorimotor and language functions is crucial to reduce neurological deficits in epilepsy and tumor resection surgery. As non-invasive mapping, both resting-state and task-evoked functional MRI has been explored in pre-surgical mapping. In lack of standardized test paradigm, the reliability of fMRI mapping is still a concern for clinical use. In this study, to improve the reliability of fMRI based mapping, task fMRI data from all available task paradigms (motor movement, word repeating and picture naming) were low-pass filtered in the band of resting-state fMRI (0.01-0.08Hz) and concatenated to get more time points. With K-means clustering, it was shown that the sensorimotor network could be reliably parcellated into hand and tongue sub-regions. The resulted parcellations were further verified with invasive ECoG and ECS mapping. Both the accuracy and specificity were better than using the motor-task fMRI only. Especially, for those patients who failed in task fMRI mapping, our method was able to provide accurate mapping as well. Our results also indicate that cortical sensorimotor network pattern is intrinsic and always present during various tasks, which supports the physiological link between the spontaneous and the task-evoked BOLD signals.', 'title': 'Sensorimotor network parcellation for pre-surgical patients using low-pass filtered fMRI', 'embedding': []}, {'id': 14894, 'abstractText': 'Inverse tone mapping is an important topic in High Dynamic Range technology. Recent years, deep learning based image inverse tone mapping methods have been extensively studied and perform better than classical inverse tone mapping methods. However, these methods consider the inverse tone mapping problem as a domain transformation problem from LDR domain directly to HDR domain and ignore the relationship between LDR and HDR. Besides, when using these deep learning based methods to transform frames of videos, it will lead to temporal inconsistency and flickering. In this work, we propose a new way to consider the inverse tone mapping problem and design a deep learning based video inverse tone mapping algorithm to reduce the flickering. Different from previous methods, we first transform LDR resources back to approximate real scenes and use these real scenes to generate the HDR outputs. When generating HDR outputs, we use 3D convolutional neural network to reduce the flickering. We also use methods to further constrain the luminance information and the color information of HDR outputs separately. Finally, we compare our results with existing classical video inverse tone mapping algorithms and deep image inverse tone mapping methods to show our great performance, and we also prove the necessity of each part of our method.', 'title': 'Deep Video Inverse Tone Mapping', 'embedding': []}, {'id': 14895, 'abstractText': \"A quantitative evaluation of a 2D map is important and becomes a challenging task for indoor survey mapping. The paper presents the method for comparison of built 2D maps generated from Google's Cartographer. We focus on the comparison of map quality without ground truth data between a reference map from a set of LiDAR with IMU and wheel-encoder and compared maps from a set of LiDAR with IMU, wheel-encoder, and/or UWB. Three metrics for comparison the map quality are presented in this paper including RMSE, blur detection, and drifts in translation and rotation. The finding shows that a set of LiDAR with IMU and UWB can be comparable to a set of LiDAR with IMU and wheel-encoder in terms of map quality. Moreover, the proposed method can be further used for the relative evaluation of a feasibility study.\", 'title': 'Quality Evaluation Method of 2D SLAM', 'embedding': []}, {'id': 14896, 'abstractText': 'Accurate protein contact map prediction is essential for de novo protein structure prediction. Over the past few years, deep learning has brought a significant breakthrough in protein contact map prediction and optimized deep learning architectures are highly desired for performance improvement. As an emerging deep learning architecture, the generative adversarial network (GAN) has shown the powerful capability of learning intrinsic patterns, which inspires us to comprehensively exploit GAN for predicting accurate protein contact maps. In this study, we present GANcon, a novel GAN-based deep learning architecture for protein contact map prediction, which to the best of our knowledge is the first GAN-based approach in this field. Instead of using a single neural network, GANcon is composed of two competitive networks that are evolving through adversarial learning. The generator network employs a dedicated encoder-decoder architecture that can efficiently capture the underlying contact information from versatile protein features to generate contact maps, while the discriminator network learns the differences between generated contact maps and real ones and promotes the generator network to produce more accurate contact maps. Moreover, to deal with the imbalance problem and take into account the symmetry of contact maps, we also propose a novel symmetrical focal loss, which can further enhance the effectiveness of adversarial learning for better performance. The experimental results on several datasets demonstrate that GANcon outperforms many state-of-the-art methods, indicating the effectiveness of our method for predicting protein contact maps. GANcon is freely available at https://github.com/melissaya/GANcon.', 'title': 'GANcon: Protein Contact Map Prediction With Deep Generative Adversarial Network', 'embedding': []}, {'id': 14897, 'abstractText': 'We present a study on sketch-map interpretation and sketch to robot map matching, where maps have nonuniform scale, different shapes or can be incomplete. For humans, sketch-maps are an intuitive way to communicate navigation information, which makes it interesting to use sketch-maps for human robot interaction; e.g., in emergency scenarios. To interpret the sketch-map, we propose to use a Voronoi diagram that is obtained from the distance image on which a thinning parameter is used to remove spurious branches. The diagram is extracted as a graph and an efficient error-tolerant graph matching algorithm is used to find correspondences, while keeping time and memory complexity low. A comparison against common algorithms for graph extraction shows that our method leads to twice as many good matches. For simple maps, our method gives 95% good matches even for heavily distorted sketches, and for a more complex real-world map, up to 58%. This paper is a first step toward using unconstrained sketch-maps in robot navigation.', 'title': 'Using sketch-maps for robot navigation: Interpretation and matching', 'embedding': []}, {'id': 14898, 'abstractText': 'High resolution 3D mapping of road systems is currently being carried out by expensive Mobile Mapping Systems (MMS) but coverage is limited. Recently Low Cost Sensor (LCS) systems have been developed which use common, low cost, internal MEMS position sensors from mobile phones, but such sensors come with a reduced absolute and relative positional accuracy. This study investigates the registration of LCS maps within MMS maps to improve map coverage and lower costs. MMS and LCS maps of a real world environment are made and registration is performed using feature matching and Iterative Closest Point alignment. Accuracy of ICP alignment is approximately (10cm) and local convergence is possible up to (1m). A combination of feature matching and ICP is used to demonstrate accurate alignment from an initial error of (10m). An example of a LCS map aligned within a MMS map is presented to confirm the use of LCS systems to extend 3D mapping coverage.', 'title': 'Registration of Low Cost Maps within Large Scale MMS Maps', 'embedding': []}, {'id': 14899, 'abstractText': 'Maps can greatly improve vehicle localization using perception sensors that detect features georeferenced in the map. This relies on two assumptions. Firstly, the detected features and the elements of the map have to be correctly associated. Secondly, the features of the map have to be accurately referenced. In this paper, solutions regarding these issues are presented. The case study of localization using a camera detecting road markings is considered. A Kalman smoothing process is used to obtain the best possible estimate of the trajectory that enables to evaluate the reliability of markings stored in the map. A likelihood maximization technique is used to best associate the observed markings to those referenced in the map. By using these two methods, map errors are detected after a first passage in an area and can be mitigated in later passes. Experimental results are reported to evaluate the performance of this approach. It is shown that mapping errors can be correctly handled.', 'title': 'Estimating the reliability of georeferenced lane markings for map-aided localization', 'embedding': []}, {'id': 14900, 'abstractText': \"In order to generate real‐time motion plans, the planning and control module can combine perception inputs, which detect dynamic obstacles in real time, localization inputs, which generate real‐time vehicle poses, and mapping inputs, which capture road geometry and static obstacles. Digital maps, such as Google map, Bing map, and Open Street Map (OSM), were developed for humans instead of for machines, as these digital maps rely heavily on human knowledge and observations. This chapter discusses the details of digital mapping by using OSM as an example. It also discusses details of building high definition (HD) maps and presents a case study on PerceptIn's mapping technology, in which existing OSM is extended for autonomous robot and vehicle navigation. HD maps for autonomous driving systems need to have high precision, usually at centimeter level.\", 'title': 'Mapping', 'embedding': []}, {'id': 14901, 'abstractText': 'Signal map is of great importance, especially in the dawn of 5G network, for site spectrum monitoring, location-based services (LBS), network construction, and cellular planning. Despite its significance, the traditional signal map construction, e.g., through full site survey, could be time-consuming and labor-intensive as the signal varies frequently over time and the accuracy requirement grows rapidly with the emergence of new applications. Even with crowdsourcing scheme, the participants tend to be unevenly distributed in space while the encouragement budgets for the participants could be far from enough to collect adequate high-quality measurements. Therefore, the signal map constructed by crowdsourcing is often sparse and incomplete. To this end, in this paper, we study how to effectively reconstruct and update the signal map in the case of partially measured signal maps with minimum cost and propose an auto-encoder-based active signal map reconstruction method (AER). Our method is mainly innovative in three parts. Firstly, AER can effectively update the signal map with only a small number of observations while also fully using the incomplete historical signals to effectively update the signal map online. Secondly, AER consists of an active query mechanism which quantitatively evaluates the most valuable measurement site for reconstruction, which further reduces the measurement cost to a large extent. Thirdly, to cope with the measurement dynamics, we give a new signal map model describing not only the signal strength but also the signal dynamics, based on which an advanced AER algorithm is proposed. The simulation results demonstrate the advantages and effectiveness of our approach in both accuracy and cost.', 'title': 'Cost-Effective Signal Map Crowdsourcing with Auto-Encoder Based Active Matrix Completion', 'embedding': []}, {'id': 14902, 'abstractText': 'Human visual cortex is organized into several functional re-gions/areas. Identifying these visual areas of the human brain (i.e., V1, V2, V4, etc) is an important topic in neurophysiology and vision science. Retinotopic mapping via functional magnetic resonance imaging (fMRI) provides a noninvasive way of defining the boundaries of the visual areas. It is well known from neurophysiology studies that retino-topic mapping is diffeomorphic within each local area (i.e. locally smooth, differentiable, and invertible). However, due to the low signal-noise ratio of fMRI, the retinotopic maps from fMRI are often not diffeomorphic, making it difficult to delineate the boundaries of visual areas. The purpose of this work is to generate diffeomorphic retinotopic maps and improve the accuracy of the retinotopic atlas from fMRI measurements through the development of a specifically designed registration procedure. Although there are sophisticated existing cortical surface registration methods, most of them cannot fully utilize the features of retinotopic mapping. By considering unique retinotopic mapping features, we form a qua-siconformal geometry-based registration model and solve it with efficient numerical methods. We compare our registration with several popular methods on synthetic data. The results demonstrate that the proposed registration is superior to conventional methods for the registration of retinotopic maps. The application of our method to a real retinotopic mapping dataset also results in much smaller registration errors.', 'title': 'Diffeomorphic Registration for Retinotopic Mapping Via Quasiconformal Mapping', 'embedding': []}, {'id': 14903, 'abstractText': 'Object-based image analysis (OBIA) technique has been representing an evolving paradigm of remote sensing application, along with more high-resolution satellite images available. However, too many derived features from segmented objects also present a new challenge to OBIA applications. In this paper, we present a supervised and adaptive method for ranking and weighting features for object-based classification. The core of this method is the feature weight maps for each land type resulted from prior thematic maps and their corresponding satellite images of study areas. Specifically, first, satellite images to be classified are segmented using an adaptive multiscale algorithm, and the multiple (spectral, shape, and texture) features of segmented objects are calculated. Second, we extract distance maps and feature weight vectors for each land type from the prior thematic maps and corresponding satellite images, to generate feature weight maps. Third, a feature-weighted classifier with the feature weight maps, is applied on the segmented objects to generate classification maps. Finally, the classification result is evaluated. This approach is applied on a Sentinel-2 multispectral satellite image and a Google Map image to produce objected-based classification maps, compared with the traditional feature selection algorithms. The experimental results illustrate that the proposed method is practically efficient to select important features and improve classification performance.', 'title': 'Supervised and Adaptive Feature Weighting for Object-Based Classification on Satellite Images', 'embedding': []}, {'id': 14904, 'abstractText': \"This paper presents the basis of the system for automation of the military trafficability maps development. The system is based on determining the index of trafficability (IOP) for square primary fields of 1 km by 1 km size. It is defined with use of data on land cover elements, obtained from both military and civilian spatial databases. To determine the trafficability, various methods have been used: artificial neural networks (multilayer perceptron and Self Organizing Maps) and GIS multi-criteria spatial analyses. Tests were performed for both military (Vector Map Level 2) and civilian spatial databases (Corine Land Cover and Open Street Map). The article presents a comparison of used methods. In addition to the basic statistical analyses (average value of IOPs, Pearson's correlation matrices, etc.), spatial statistics such as spatial autocorrelation coefficients (Moran I) were calculated. The key experiment was also to compare generated maps to the map of trafficability made by the methods commonly used in the Armed Forces. The result of conducted experiments is therefore choosing the optimal map of trafficability and the answer to the question: which method and data should be used for the best trafficability map elaboration? It was found that the most useful and most faithful map showing the conditions of trafficability is the study elaborated using the Vegetation Roughness Factor method and using Corine Land Cover data.\", 'title': 'Comparison of the military maps of trafficability developed by different methods', 'embedding': []}, {'id': 14905, 'abstractText': 'Vehicle self-Iocalization based on the matching of Light detection and ranging (LiDAR) scans to the normal distribution (ND) map become more popular in recent years due to the price down and miniaturization of the LiDARs. In such methods, the source of self-Iocalization error can be divided into input scan quality, matching algorithm and map. In this work, we focus on the map, as one ofthe high potential sources of error. By investigating the erroneous scenarios in the map and comparing their characteristics, we come up with some criteria and requirements for the map to be able to perform self-Iocalization with a needed error. In this work, we propose four factors for quantified evaluation ofthe map requirements. These factors are feature count factor, layout factor, normal entropy factor, and local similarity factor ofthe map. We evaluated these four factors in a different part ofthe map with different scenarios by comparing them with the self-Iocalization error. Experimental results show that the local similarity factor with 0.59 of correlation with the maximum error has the highest contribution to the Iocalization error. For normal entropy factor, feature count factor, layout factor, correlations are 0.42, 0.36, and 0.34 respectively. By applying these four factors, maximum Iocalization error can be modeled with RMSE and R-squared (R<sup>2</sup>) of 0.44 and 0.598 respectively. Result of this study can be applied to the dynamic determination of the abstraction ratio of the map and sensor fusion as well.', 'title': 'Evaluation of Digital Map Ability for Vehicle Self-Iocalization', 'embedding': []}, {'id': 14906, 'abstractText': 'Multi-destination maps are a kind of navigation maps aimed to guide visitors to multiple destinations within a region, which can be of great help to urban visitors. However, they have not been developed in the current online map service. To address this issue, we introduce a novel layout model designed especially for generating multi-destination maps, which considers the global and local layout of a multi-destination map. We model the layout problem as a graph drawing that satisfies a set of hard and soft constraints. In the global layout phase, we balance the scale factor between ROIs. In the local layout phase, we make all edges have good visibility and optimize the map layout to preserve the relative length and angle of roads. We also propose a perturbation-based optimization method to find an optimal layout in the complex solution space. The multi-destination maps generated by our system are potential feasible on the modern mobile devices and our result can show an overview and a detail view of the whole map at the same time. In addition, we perform a user study to evaluate the effectiveness of our method, and the results prove that the multi-destination maps achieve our goals well.', 'title': 'Generating Multi-Destination Maps', 'embedding': []}, {'id': 14907, 'abstractText': 'Despite having many similar characteristics with cryptography, existing chaotic systems have many security issues that negatively affect the chaos-based cryptographic algorithms that utilize them. This paper proposes a new chaotification method that enhances the chaotic complexity of existing chaotic maps to surmount these issues. The proposed method uses a cosine function alongside a chaotic map in a cascade system. To depict its advantages, we apply it to enhance logistic and Henon maps before analyzing their chaotic properties. Results and comparisons indicate that the new chaotic maps have a wider chaotic range, elevated sensitivity, complex characteristics, high nonlinearity, and an extended cycle length as compared to the original (seed) maps as well as other chaotic maps. We then utilize the modified maps (and their corresponding seed maps) to design simple pseudorandom number generators to study their feasibility when used in cryptographic algorithms. We perform comparisons between the generators derived from both the original and seed maps. Results show that generators based on the new maps outperform their seed counterparts in nearly every aspect. This finding demonstrates the capability of the proposed method in improving the performance of chaos-based cryptographic algorithms.', 'title': 'Digital Cosine Chaotic Map for Cryptographic Applications', 'embedding': []}, {'id': 14908, 'abstractText': 'Map images (e.g., illustrated maps, historical maps, and geographic maps) have been published around the world, not only for giving location but also to attract tourists or hand down the histories of locations. The management of map data, however, has been an open issue for several research fields, including digital library, humanities, and tourism studies. This paper explores an approach for classifying diverse map images by their themes using map content features. Specifically, we present a novel strategy for preprocessing text data that are positioned inside the map images, which are extracted using OCR. The activation of the textual feature-based model is joint with the visual features in an early fusion manner. Finally, we train a classifier model comprising a convolutional layer and a fully connected layer, which predicts the belonging class of the input map. In experiments conducted on a new labeled dataset of map images, we demonstrate that our approach that uses the fused features achieved the best classification performance over single modality. We have made our dataset available on the Internet to facilitate this new task.', 'title': 'A Deep Multimodal Approach for Map Image Classification', 'embedding': []}, {'id': 14909, 'abstractText': \"Opportunistic signals (e.g., WiFi, magnetic fields, and ambient light) have been extensively studied for low-cost indoor localization, especially via fingerprinting. We present an automatic site survey approach to build the signal maps in space-constrained environments (e.g., modern office buildings). The survey can be completed by a single smartphone user during normal walking, say, with a little human intervention. Our approach follows the classical GraphSLAM framework: the front end constructs a pose graph by incorporating the relative motion constraints from the pedestrian dead-reckoning (PDR), the loop-closure constraints by magnetic sequence matching with the WiFi signal similarity validation, and the global heading constraints from the opportunistic magnetic heading measurements; and the back end generates a globally consistent trajectory via graph optimization to provide ground-truth locations for the collected signal fingerprints along the survey path. We then build the signal map (also known as fingerprint database) upon these location-labeled fingerprints by the Gaussian processes regression (GPR) for later online localization. Specifically, we exploit the pseudowall constraints from the GPR variance map of magnetic fields and the observations of ceiling lights to correct the PDR drifts with a particle filter. We evaluate our approach on several data sets collected from both the HKUST academic building and a shopping mall. We demonstrate the real-time localization on a smartphone in an office area, with 50th percentile accuracy of 2.30 m and 90th percentile accuracy of 3.41 m. Note to Practitioners - This paper was motivated by the problem of the efficient signal map construction for fingerprinting-based localization on smartphones. The conventional manual site survey method, known to be time-consuming and labor-intensive, hinders the penetration of fingerprinting methods in practice. This paper suggests a GraphSLAM-based approach to automate this signal map construction process by reducing the survey overhead significantly. A surveyor is merely asked to walk through an indoor venue with an Android smartphone held in hand with a little human intervention. Meanwhile, opportunistic signals (e.g., WiFi and magnetic fields) are captured by smartphone sensors. We construct a GraphSLAM engine to first identify the measurement constraints from these signal observations and then recover the surveyor's walking trajectory by the graph optimization. We can generate signal maps using the captured signals alongside the recovered trajectory. In this paper, we propose a WiFi signal similarity validation method to reduce false positive loop-closures and exploit the magnetic headings to improve the trajectory optimization performance. In addition, we propose to use the generated magnetic field variance map and the lights distribution map for localization. The efficacy of the proposed site survey approach is proven through field experiments, and real-time localization is demonstrated on a smartphone using the generated signal maps. The localization experiment was conducted by a single user with the same Android smartphone that was used in the site survey. Therefore, the usability of signal maps on other devices and the generality to other users have not yet been testified. We will leave these issues in our future work.\", 'title': 'An Automatic Site Survey Approach for Indoor Localization Using a Smartphone', 'embedding': []}, {'id': 14910, 'abstractText': 'Agriculture is one of the most affected sectors by the flood. Spaceborn remote sensing is widely used for flood mapping and monitoring in recent decades. Some applications such as flood crop loss assessment require data with fine temporal resolution to monitor short-lived flood. MODIS is providing remote sensing data with 1-2 days temporal resolution which has frequently been used for flood mapping for a large area. However, incapability to penetrate through the cloud hindered the application of optical remote sensing in flood mapping in many cases. Thus, radar remote sensing especially synthetic aperture radar (SAR) already shows the capability for the flood mapping in cloud condition. However, monitoring of short-lived flood is not possible using freely available SAR data because of the long revisit capacity of these SAR systems. Therefore, microwave remote sensing with fine temporal resolution might be helpful for flood inundation mapping. Soil Moisture Active Passive (SMAP) is a microwave remote sensing initiative which is providing 3-hourly soil moisture data. Therefore, this study tries to map agriculture flood based on SMAP soil moisture data and soil physical properties. Soil moisture above the filed capacity might be the indication of soil inundation. Moreover, It has been observed that there is an increment in soil moisture during the flood. Therefore, this approach considered three conditions to map the flooded pixel: a minimum of 0.05 increment in soil moisture, a soil moisture threshold 0.40 (moisture above the field capacity) and the 72 consecutive hours. To avoid the random increment in soil moisture a 3-day moving window is applied to the time series data. The flood map extracted from SMAP data is validated with FEMA declared inundated crop land. The overall accuracy is 60% and about 32% of commission error. The over estimation of the flood by SMAP data due to the coarse spatial resolution (9km) of SMAP data.', 'title': 'Agriculture flood mapping with Soil Moisture Active Passive (SMAP) data: A case of 2016 Louisiana flood', 'embedding': []}, {'id': 14911, 'abstractText': 'Automotive players have recently shown an increasing interest in high-precision mapping, with the aim of enhancing vehicles safety and autonomy. Nevertheless, the acquisiton, processing and updates of accurate maps remains an economic challenge. Collaborative mapping through vehicles crowdsourcing represents a promising solution to tackle this problem. However, the potential accuracy provided by such an approach has yet to be assessed. In this paper, we evaluate the performances of crowdsourced mapping in real conditions. We build a map of geolocalized landmarks by crowdsourcing observations from multiple vehicles, and applying several successive map updates. We evaluate the map quality through a field-test with multiple vehicle passings. Furthermore, we study the benefits of crowdsourced mapping for vehicles positioning.', 'title': 'Graph-based Approach for Crowdsourced Mapping: Evaluation through Field Experiments', 'embedding': []}, {'id': 14912, 'abstractText': \"Many pervasive computing applications depend upon maps for navigation and support of location based services. Maps are commonly available for outdoor pervasive applications from a variety of sources. An individual can determine their location outdoors on these maps via GPS. Indoor pervasive applications may also need to know the layout of rooms, doorways and hallways of buildings, and the objects and obstacles within them, however indoor maps of buildings are less prevalent. Moreover, indoor maps may need to be dynamic and updated regularly since the layout changes when objects and obstacles are added or removed by people within the building. In this paper, we present iFrame, a dynamic approach that leverages existing mobile sensing capabilities for constructing indoor floor plans. We explore how iFrame users may collaborate and contribute to constructing 2-dimensional indoor maps by merely carrying smartphones or other mobile devices, and to allow their mobile devices to share information with other users' devices. The iFrame approach consists of four steps: 1) Abstract the unknown indoor map as a matrix; 2) Leverage collaborating mobile devices that incorporate three mobile sensing technologies - accelerometers to support dead reckoning, Bluetooth RSSI detection, and WiFi RSSI detection; 3) Combine the three methods by Curve Fit Fusion (CFF), and 4) Extend iFrame from one room to a whole building by shadow rates and anchor points analysis. We conducted a deployment study that shows iFrame is a light-weight and unattended approach that provides a skeleton map of a real building effectively and automatically. The layouts of 12 rooms are reconstructed within 5-10 minutes. Changes of layout in indoor maps can be detected and the resolution of the reconstructed indoor floor plans can be improved when there is an increase in the number of cooperating users.\", 'title': 'iFrame: Dynamic indoor map construction through automatic mobile sensing', 'embedding': []}, {'id': 14913, 'abstractText': \"Massive maps have been shared as Web Map Service (WMS) from various providers, which could be used to facilitate people's daily lives and support space analysis and management. The theme classification of maps could help users efficiently find maps and support theme-related applications. Traditionally, metadata is usually used in analyzing maps content, few papers use maps, especially legends. In fact, people usually considers metadata, maps and legends together to understand what maps tell, however, no study has tried to exploit how to combine them. This paper proposes a method to fuse them with the purpose of classifying map themes, named latent feature based multimodality fusion for theme classification (LFMF-TC). Firstly, a multimodal dataset is created that supports the supervised classification on map themes. Secondly, textual and visual features are designed for metadata, maps, and legends using some advanced techniques. Thirdly, a latent feature based fusion method is proposed to fuse the multimodal features on the feature level. Finally, a neural network classifier is implemented using supervised learning on the multimodal dataset. In addition, a web-based collaboration platform is developed to facilitate users in labeling multimodal samples through an interactive Graphical User Interface (GUI). Extensive experiments are designed and implemented, whose results prove that LFMF-TC could significantly improve the classification accuracy. In theory, the LFMF-TC could be used for other applications with few modifications.\", 'title': 'A Latent Feature-Based Multimodality Fusion Method for Theme Classification on Web Map Service', 'embedding': []}, {'id': 14914, 'abstractText': 'Probing seismic anisotropy of the lithosphere provides valuable clues on the fabric of rocks. We present a 3-D probabilistic model of shear wave velocity and radial anisotropy of the crust and uppermost mantle of Europe, focusing on the mountain belts of the Alps and Apennines. The model is built from Love and Rayleigh dispersion curves in the period range 5–149 s. Data are extracted from seismic ambient noise recorded at 1521 broad-band stations, including the AlpArray network. The dispersion curves are first combined in a linearized least squares inversion to obtain 2-D maps of group velocity at each period. Love and Rayleigh maps are then jointly inverted at depth for shear wave velocity and radial anisotropy using a Bayesian Monte Carlo scheme that accounts for the trade-off between radial anisotropy and horizontal layering. The isotropic part of our model is consistent with previous studies. However, our anisotropy maps differ from previous large scale studies that suggested the presence of significant radial anisotropy everywhere in the European crust and shallow upper mantle. We observe instead that radial anisotropy is mostly localized beneath the Apennines while most of the remaining European crust and shallow upper mantle is isotropic. We attribute this difference to trade-offs between radial anisotropy and thin (hectometric) layering in previous studies based on least-squares inversions and long period data (&gt;30 s). In contrast, our approach involves a massive data set of short period measurements and a Bayesian inversion that accounts for thin layering. The positive radial anisotropy (V<inf>SH</inf> &gt; V<inf>SV</inf>) observed in the lower crust of the Apennines cannot result from thin layering. We rather attribute it to ductile horizontal flow in response to the recent and present-day extension in the region.', 'title': 'Evidence for radial anisotropy in the lower crust of the Apennines from Bayesian ambient noise tomography in Europe', 'embedding': []}, {'id': 14915, 'abstractText': 'In this study, we conducted a test with 20 sighted users to track the work described in earlier studies in an expanded scenario. As we have tried to answer whether adding multi-reference mapping of sonification to auditory graphs could improve the point estimation accuracy in nonvisual condition. We also emphasize the efficiency of the performance of multi-reference graphs using timbre to make them as efficient as mapping using a single pitch. The design of the study was different from the earlier study, who found that multi-reference mapping took a long time against single pitch mapping in auditory graphs. The results help provide empirical evidence that the multi-reference mode provides more accurate results than the single-pitch mode. The evaluation confirms that adding contexts to auditory graphs such as markers could improve the perception of auditory graphs.', 'title': 'Investigation of Multi Reference Point Estimation with Timbre for Effective Mobile Auditory Devices', 'embedding': []}, {'id': 14916, 'abstractText': 'We present an ALMA study of the ∼180 brightest sources in the SCUBA-2 850-μm map of the COSMOS field from the S2COSMOS survey, as a pilot study for AS2COSMOS – a full survey of the ∼1000 sources in this field. In this pilot study, we have obtained 870-μm continuum maps of an essentially complete sample of the brightest 182 sub-millimetre sources (<tex>$S_{850\\\\, \\\\mu \\\\rm m}\\\\gt $</tex> 6.2 mJy) in COSMOS. Our ALMA maps detect 260 sub-millimetre galaxies (SMGs) spanning a range in flux density of <tex>$S_{870\\\\, \\\\mu \\\\rm m}$</tex> = 0.7–19.2 mJy. We detect more than one SMG counterpart in 34 ± 2 per cent of sub-millimetre sources, increasing to 53 ± 8 per cent for SCUBA-2 sources brighter than <tex>$S_{850\\\\, \\\\mu \\\\rm m}\\\\gt $</tex> 12 mJy. We estimate that approximately one-third of these SMG–SMG pairs are physically associated (with a higher rate for the brighter secondary SMGs, <tex>$S_{870\\\\, \\\\mu \\\\rm m}\\\\gtrsim$</tex> 3 mJy), and illustrate this with the serendipitous detection of bright [C II] 157.74-μm line emission in two SMGs, AS2COS 0001.1 and 0001.2 at z = 4.63, associated with the highest significance single-dish source. Using our source catalogue, we construct the interferometric 870-μm number counts at <tex>$S_{870\\\\, \\\\mu \\\\rm m}\\\\gt $</tex> 6.2 mJy. We use the extensive archival data of this field to construct the multiwavelength spectral energy distribution of each AS2COSMOS SMG, and subsequently model this emission with MAGPHYS to estimate their photometric redshifts. We find a median photometric redshift for the <tex>$S_{870\\\\, \\\\mu \\\\rm m}\\\\gt $</tex> 6.2 mJy AS2COSMOS sample of z = 2.87 ± 0.08, and clear evidence for an increase in the median redshift with 870-μm flux density suggesting strong evolution in the bright end of the 870-μm luminosity function.', 'title': 'An ALMA survey of the brightest sub-millimetre sources in the SCUBA-2–COSMOS field', 'embedding': []}, {'id': 14917, 'abstractText': 'Crowdsourcing is a multidisciplinary research area that represents a rapidly expanding field where new applications are constantly emerging. Research in this area has investigated its use for citizen science in data gathering for research and crowdsourcing for industrial innovation. Previous studies have reviewed and categorized crowdsourcing research using qualitative methods. This has led to the limited coverage of the entire field, using smaller discrete parts of the literature and mostly reviewing the industrial aspects of crowdsourcing. This study uses a scientometric analysis of 7059 publications over the period 2006–2019 to map crowdsourcing research to identify clusters and applications. Our results are the first in the literature to map crowdsourcing research holistically. In this article, we classify its usage in the three domains of innovation, engineering, and science, where 11 categories and 26 subcategories are further developed. The results of this article reveal that the most active scientific clusters where crowdsourcing is used are environmental sciences and ecology. For the engineering domain, it is computer science, telecommunication, and operations research. In innovation, idea crowdsourcing, crowdfunding, and crowd creation are the most frequent areas. The findings of this study map crowdsourcing usage across different fields and illustrate emerging crowdsourcing applications.', 'title': 'A Scientometric Exploration of Crowdsourcing: Research Clusters and Applications', 'embedding': []}, {'id': 14918, 'abstractText': \"Apollo basin is located within the large South Pole- Aitken (SPA) basin. The study on Apollo basin will provide interesting information about the basic geologic issues about the lunar farside. In this paper, the normalized brightness (TB) temperature (nT<sub>B</sub>) maps and the TB difference (dT<sub>B</sub>) maps are generated with the Chang'E-2 microwave sounder data to study the microwave thermal emission features of Apollo basin. The results are as follows. First, the mare volcanism in Apollo basin is re-understood according to the nT<sub>B</sub> performances at noon, and they should be originated from the southern part of the Apollo basin and strongly altered by the later impact ejecta. Second, the nT<sub>B</sub> maps indicate that there exists a special material from Dryden crater to Chaffee crater, whose thickness is more than 31 cm but less than 76.9 cm. Third, the similar dT<sub>B</sub> performances at 3.0 GHz indicate the homogeneous regolith thermophysical parameters of Apollo basin in the lateral direction. Fourth, the dTB maps and the discovered cold TB anomaly indicate the homogeneity of the SPA basin at least in the microwave thermophysical parameters. Our study also shows that the scientific study about the lunar surface is not sufficient only by visible data.\", 'title': 'MTE Features of Apollo Basin and Its Significance in Understanding the SPA Basin', 'embedding': []}, {'id': 14919, 'abstractText': 'Cellular automata can be used to rapidly generate complex images, but controlling the character of those images can be difficult. This study continues experimentation with fashion-based cellular automata that generate cavern-like level maps and provides the beginning of a mathematical theory. Fashion-based automata are defined by a competition matrix with different cell states competing to capture territory. This study co-evolves pairs of competition matrices to permit the evolution of automata rules that can be spatially morphed to provide substantially more diverse types of maps than earlier systems using fashion-based cellular automata. As in earlier studies, the cellular automata rules function in local neighborhoods, meaning that the level generation system scales smoothly to any desired level map size. This reusability also permits variation of the type of morph used: a variety of spatial morphing styles are tested with the evolved rules. The theoretical treatment includes the derivation of a normal form for the cellular automata rules that informs the design of the fitness function and has application to understanding the fitness landscape of fashion based automata.', 'title': 'Automatic Generation of Diverse Cavern Maps with Morphing Cellular Automata', 'embedding': []}, {'id': 14920, 'abstractText': \"Volunteered Geographic Information (VGI) is a kind of geographic information which is created, edited, managed and maintained on the basis of common handheld GPS terminals, high resolution remote sensing images and personal spatial cognition. VGI has received extensive attention from scholars at home and abroad for its advantages of wide data coverage, strong current situation, and free use. OpenStreetMap (OSM) is simple in its way of generating and uploading data, and it has a large amount of storage data, which has become a widely used case model in VGI. Since there is no effective classification and management of data, how to extract valuable information from OSM data effectively becomes a bottleneck restricting the application of OSM data. Thematic maps of agricultural information can visually reflect the current status of agriculture in a region and have important reference value for government departments in formulating agricultural policies and economic plans. At present, thematic map production of agricultural information is mainly completed using satellite remote sensing and ground surveys, which requires higher costs. But agriculture is a low-value-added industry. How to reduce the production cost of agricultural thematic maps is an issue that needs urgent solution. In this study, Hapcheon County in South Korea is selected as the research area. First, study the threshold setting for the area of blocks, the area of buildings in the blocks, the density of road lines, the density of cores, and the density of road nodes, etc., and use the recent historical OSM data of the study area to extract the area of cropland; Secondly, the farmland area extracted from the OSM data was compared with the cultivated land area of the combined Unmanned Aerial Vehicle (UAV) and RapidEye image data to determine the optimal threshold; Finally, the farmland information based on the optimal threshold value and OSM's own agricultural geographic information overlay are integrated to realize the agricultural geographic information thematic map production based on OSM data. The research results show that setting thresholds for various impact factors, such as block size, can improve the extraction accuracy of farmland area. Based on the information of cultivated land area extracted by remote sensing image of UAV, the information of farmland based on OSM data has good accuracy (The error is about 15%). The study can achieve rapid acquisition of geographical information of farmland at various regional scales and provide data support for dynamic monitoring of farmland area and agricultural production management.\", 'title': 'Extraction of Farmland Geographic Information Using OpenStreetMap Data', 'embedding': []}, {'id': 14921, 'abstractText': 'In this study, it is aimed to monitor and control the plant production in greenhouses, and increase the efficiency of greenhouses by doing interdisciplinary research in the fields of agriculture, electronics, robotics, and data mining. In the small and outnumbered greenhouses used by farmers, to collect data using static sensor systems is both costly and impractical. In this study, an autonomous mobile robot has been developed and used to collect information from the greenhouse and mapping the greenhouse. By using the autonomous mobile robot, that is designed and produced for the project needs, sensory data such as, RGB-D map of the greenhouse, moisture, temperature, and light, were obtained. As well as the mobile robot could autonomously navigate in the greenhouse, it can be manually controlled by an operator. On the robot, robot operating system (ROS) is used. By using RGB-D mapping and SLAM packages in the ROS, robot can find its position in the greenhouse can measure the temperature, the moisture, and the light density of this location, and generates a three dimensional map of the greenhouse. In this study, data about how it changed the greenhouse environment and the plants grown in the greenhouse was obtained with measurements made at regular intervals of time. The next step of this study is process the data gathered by the robot in real time to get information such as the number of the crops, the phonological phase of the crops or condition of the greenhouse.', 'title': 'Data acquisition from greenhouses by using autonomous mobile robot', 'embedding': []}, {'id': 14922, 'abstractText': 'Different communication-aware mapping techniques were proposed in recent years for improving the performance of distributed systems based on both, off-chip and on-chip networks. Some of these proposals were based on heuristic search for finding pseudo-optimal assignments of tasks and processing elements. However, the technology integration improvements have allowed a significant increase in the number of network nodes, requiring the acceleration of the heuristic search. In this paper, we propose a comparative study of the local search method used in a communication-aware mapping technique, when implemented on different parallel architectures. We compare the performance provided by a version of the local search method when executed on a single Graphics Processing Unit (GPU) with the one provided by the MPI version executed on a supercomputer with the same theoretical performance of the GPU platform, in order to study a fair scenario. We have considered a GPU based on the Fermi architecture, evaluating the improvements achieved by some new architectural features of this platform. The results show that a mixed parallel implementation on a single GPU outperforms the MPI implementation of the local search method. These results validate the GPU implementation as a very cost-effective accelerator for the local search method.', 'title': 'On the Use of GPU for Accelerating Communication-Aware Mapping Techniques', 'embedding': []}, {'id': 14923, 'abstractText': 'Many resource allocation problems in the cloud can be described as a basic Virtual Network Embedding Problem (VNEP): the problem of finding a mapping of a request graph (describing a workload) onto a substrate graph (describing the physical infrastructure). Applications range from mapping testbeds, over the embedding of batch-processing tasks to the embedding of service function chains and come with different mapping restrictions for nodes and edges. The restrictions studied most often are node and edge capacities, node mapping, edge routing and latency restrictions. While the VNEP has been studied intensively, complexity results are only known for specific models and this paper provides a first comprehensive study of the computational complexity of the VNEP by systematically analyzing its hardness for any combination of the above stated mapping restrictions. For all studied variants the NP-completeness of the respective decision problems is shown. Furthermore, NP-completeness results for finding approximate embeddings, which may, e.g., violate capacity constraints by certain factors, are derived. Lastly, it is also shown that all these results pertain when restricting the request graphs to planar and degree-bounded graphs. While theoretic in nature, our results have severe practical implications. Firstly, any optimization variant of the VNEP is NP-hard and cannot be approximated for any of the studied restrictions, unless P = NP. Secondly, we uncover structural hardness properties: the VNEP is NP-hard and inapproximable even if, e.g., only node placement and edge routing restrictions are considered.', 'title': 'On the Hardness and Inapproximability of Virtual Network Embeddings', 'embedding': []}, {'id': 14924, 'abstractText': 'Context: Security bug reports are reports from bug tracking systems that include descriptions and resolutions of security vulnerabilities that occur in software projects. Researchers use security bug reports to conduct research related to software vulnerabilities. A mapping study of publications that use security bug reports can inform researchers on (i) the research topics that have been investigated, and (ii) potential research avenues in the field of software vulnerabilities. Objective: The objective of this paper is to help researchers identify research gaps related to software vulnerabilities by conducting a systematic mapping study of research publications that use security bug reports. Method: We perform a systematic mapping study of research that use security bug reports for software vulnerability research by searching five scholar databases: (i) IEEE Xplore, (ii) ACM Digital Library, (iii) ScienceDirect, (iv)Wiley Online Library, and (v) Springer Link. From the five scholar databases, we select 46 publications that use security bug reports by systematically applying inclusion and exclusion criteria. Using qualitative analysis, we identify research topics investigated in our collected set of publications. Results: We identify three research topics that are investigated in our set of 46 publications. The three topics are: (i) vulnerability classification; (ii) vulnerability report summarization; and (iii) vulnerability dataset construction. Of the studied 46 publications, 42 publications focus on vulnerability classification. Conclusion: Findings from our mapping study can be leveraged to identify research opportunities in the domains of software vulnerability classification and automated vulnerability repair techniques.', 'title': 'Security Bug Report Usage for Software Vulnerability Research: A Systematic Mapping Study', 'embedding': []}, {'id': 14925, 'abstractText': 'Vegetation phenology identification is significance to the exploration of vegetation growth and is also conducive to the impact of phenology on the ecological environment. Recently, vegetation phenology detection is based on a time series of vegetation phenology to index simulation of vegetation growth time indirectly. In this study, we identify the vegetation phenology of deciduous broad-leaved forest through the deep learning method within a single PhenoCam image. The result of the phenology identification of growing regions, the accuracy MAP of daily identification in daily scales mAP up to 10.2%, which could identify the growing period of most deciduous broad-leaved forests. The identification accuracy mAP in the 8-day scale is up to 69%, and the identification mAP accuracy of vegetation could reach 98.2% when it was divided into four categories. The purpose of this study is to detect the phenological growth period of deciduous broad- leaved forest with rapid development, high precision, and fast deep learning methods. It has a great improvement on the current method of calculating the vegetation phenology period by using the traditional measurement and related mathematical and physical models. While obtaining the phenology period more quickly, it can automatically and accurately obtain the growth area and growth period of the study area, making a certain contribution to the study of vegetation phenology.', 'title': 'Vegetation phenology detection of deciduous broad-leaf forest using YOLOv3 from PhenoCam', 'embedding': []}, {'id': 14926, 'abstractText': 'Over the last two decades, high-resolution (HR) mapping has emerged as a powerful technique to study normal and abnormal bioelectrical events in the gastrointestinal (GI) tract. This technique, adapted from cardiology, involves the use of dense arrays of electrodes to track bioelectrical sequences in fine spatiotemporal detail. HR mapping has now been applied in many significant GI experimental studies informing and clarifying both normal physiology and arrhythmic behaviors in disease states. This review provides a comprehensive and critical analysis of current methodologies for HR electrical mapping in the GI tract, including extracellular measurement principles, electrode design and mapping devices, signal processing and visualization techniques, and translational research strategies. The scope of the review encompasses the broad application of GI HR methods from in vitro tissue studies to in vivo experimental studies, including in humans. Controversies and future directions for GI mapping methodologies are addressed, including emerging opportunities to better inform diagnostics and care in patients with functional gut disorders of diverse etiologies.', 'title': 'Methods for High-Resolution Electrical Mapping in the Gastrointestinal Tract', 'embedding': []}, {'id': 14927, 'abstractText': 'EEG signal classification is an important task to build an accurate Brain Computer Interface (BCI) system. Many machine learning and deep learning approaches have been used to classify EEG signals. Besides, many studies have involved the time and frequency domain features to classify EEG signals. On the other hand, a very limited number of studies combine the spatial and temporal dimensions of the EEG signal. Brain dynamics are very complex across different mental tasks, thus it is difficult to design efficient algorithms with features based on prior knowledge. Therefore, in this study, we utilized the 2D AlexNet Convolutional Neural Network (CNN) to learn EEG features across different mental tasks without prior knowledge. First, this study adds spatial and temporal dimensions of EEG signals to a 2D EEG topographic map. Second, topographic maps at different time indices were cascaded to populate a 2D image for a given time window. Finally, the topographic maps enabled the AlexNet to learn features from the spatial and temporal dimensions of the brain signals. The classification performance was obtained by the proposed method on a multiclass dataset from BCI Competition IV dataset 2a. The proposed system obtained an average classification accuracy of 81.09%, outperforming the previous state-of-the-art methods by a margin of 4% for the same dataset. The results showed that converting the EEG classification problem from a (1D) time series to a (2D) image classification problem improves the classification accuracy for BCI systems. Also, our EEG topographic maps enabled CNN to learn subtle features from spatial and temporal dimensions, which better represent mental tasks than individual time or frequency domain features.', 'title': 'EEG Signal Classification Using Convolutional Neural Networks on Combined Spatial and Temporal Dimensions for BCI Systems', 'embedding': []}, {'id': 14928, 'abstractText': 'Mapping urban dynamics at the global scale becomes a pressing task with the increasing pace of urbanization and its important environmental and ecological impacts. In this study, we proposed a new approach to mapping global urban areas from 2000 to 2012 by applying a region-growing support vector machine classifier and a bidirectional Markov random field model to time-series nighttime light data. In this approach, both spectrum and spatial-temporal contextual information are employed for an improved urban area mapping. Our results indicate that at the global level, the urban area increased from 625,000 to 1,039,000 km<sup>2</sup> during 2000-2012. Most urban areas are concentrated in the region between 30°N and 60°N latitudes. The latitudinal distribution of urban areas from this study is consistent with three land-cover products, including European Space Agency Climate Change Initiative Land Cover dataset, Finer Resolution Observation and Monitoring Global Land Cover, and 30-m Global Land Cover dataset. We found that for several major cities, such as Shanghai, urban areas from our study contain some nonurban land-cover types with intensive human activities. The validation using Landsat 7 ETM+ imagery indicates that the overall accuracies of the mapped urban areas for 2000, 2005, 2008, and 2010 are 86.0%, 88.6%, 89.8%, and 88.7%, respectively, and the Kappa coefficients are 0.72, 0.77, 0.79, and 0.78, respectively. This study also demonstrates that the integration of the spatial-temporal contextual information and the use of bidirectional Markov random field model are effective in improving the accuracy and temporal consistency of urban area mapping using time-series nighttime light data.', 'title': 'Mapping Global Urban Areas From 2000 to 2012 Using Time-Series Nighttime Light Data and MODIS Products', 'embedding': []}, {'id': 14929, 'abstractText': 'Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey 88 ML4VIS studies, aiming to answer two motivating questions:what visualization processes can be assisted by MLandhow ML techniques can be used to solve visualization problemsThis survey reveals seven main processes where the employment of ML techniques can benefit visualizations:Data Processing4VIS, Data-VIS Mapping, InsightCommunication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling. The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations.Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this paper can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at https://ml4vis.github.io', 'title': 'A Survey on ML4VIS: Applying MachineLearning Advances to Data Visualization', 'embedding': []}, {'id': 14930, 'abstractText': 'The purpose of this study is to develop a PET detector with a 0.5 mm crystal size for high-resolution preclinical PET imaging. The methods to improve the decoding performances of the detectors, including the wedge-shaped light guides with different thicknesses, different methods to suppress the dark count noises in the SiPMs and different methods to calculate the flood maps, were investigated. The experimental results show that the PET detector developed in this study achieved an extraordinary decoding performance. The ratio of background “noise” in the flood map to the total events is about 40%. The simulation study shows that the intrinsic intra-crystal Compton scatters contribute to 30.32% of the background “noise”. Thus, the optical design of the detector and the electronics and algorithms to generate the flood maps only introduced 10% the background “noise”. We are currently constructing a high-resolution preclinical PET imager using 12 detectors.', 'title': 'Preliminary Optimized Design of a High-resolution PET Detector with a 0.5 mm Crystal Size', 'embedding': []}, {'id': 14931, 'abstractText': 'Abstract Password schemes based on online map locations are an emerging topic in authentication research. GeoPass is a promising such scheme, as it provides satisfactory resilience against online guessing and showed high memorability (97%) in a single-password laboratory study. In this article, we investigate more deeply into the potential of GeoPass through four separate studies. First, in a 2-month-long field study, we found that users in a real-world setting remembered their location passwords 96.1% of the time and showed improvement with more login sessions. Then, in a study of interference effects in Geopass, in which each participant had to remember four separate location passwords, we found that memorability was &lt;70%, with 41.5% of login failures due to interference. Based on these findings, we propose to address interference issues in GeoPass with mental stories, where users are asked to create a meaningful association between their location password and the corresponding account. We tested the efficacy of this approach through a second interference study, where the memorability rate for GeoPass was &gt;97%, with only 3.4% of login attempts failing due to interference. We also conducted a shoulder-surfing study to examine the resilience of GeoPass against this attack. Based on our results, we identify the promising aspects of location passwords that should be further studied in future research.', 'title': 'Exploring the Potential of GeoPass: A Geographic Location-Password Scheme', 'embedding': []}, {'id': 14932, 'abstractText': 'Understanding the current situation of natural disaster damages is a critical step for an effective natural disaster responses, and many pictures uploaded after natural disasters are valuable resources for this purpose. However, many pictures are not associated with location information and it is not easy to connect the image content to locations on a map. There are two reasons for this difficulty. First, pictures are different in terms of directions and heights. Second, the situation of damaged areas in the picture may differ from before the natural disaster. Therefore, there is a mismatch between map fragments and the pictures taken after a disaster. This paper explores the potential of human computation to solve this problem. For this study, we asked people to draw a birds-eye view map of the place in a picture and compared the map with the correct map fragments to see the characteristics of such drawn maps.', 'title': 'Analysis of Hand-drawn Maps of Places in Natural Disaster Pictures', 'embedding': []}, {'id': 14933, 'abstractText': 'This study aims to improve users’ positive experiences in wayfinding in virtual environments through empirical research on the influence of different designs of landmarks on overview maps. The experiment adopted a four (landmark) x two (gender) between-subjects design. Landmarks with symbols, symbols and images, symbols and text, and symbols, images and text were examined. Fifty-six participants were invited to complete three wayfinding tasks and fill out questionnaires. The generated results indicated that: (1) Landmark presentation styles significantly affect wayfinding performance on overview maps in virtual environments. (2) Concerning subjective rationality, system usability and perceived usefulness, the use of text in landmark design can significantly improve users’ evaluations of overview maps. (3) In terms of gender, females’ system usability evaluations and subjective intentions of using overview map with landmarks are significantly more positive than males’.', 'title': 'Wayfinding in Virtual Environments With Landmarks on Overview Maps', 'embedding': []}, {'id': 14934, 'abstractText': 'Autonomous robots are increasingly used in many fields to perform some special tasks. Autonomous transfer vehicles (ATV) in smart factories are one of the most critical components of industry 4.0. Mapping is the key process for the long term successful operation of ATVs. Although, robotic mapping is enough for the autonomous navigation of an ATV, high definition (HD) maps are required to perform the perfect behavior and ensure better interactions with the environment. HD maps can be created manually, but updating the maps is an open problem for sustainability of the performance. In this study, an HD-map update strategy is proposed for ATVs that operate in smart factories.', 'title': 'Updating HD-Maps for Autonomous Transfer Vehicles in Smart Factories', 'embedding': []}, {'id': 14935, 'abstractText': 'Car counting on drone-based images is a challenging task in computer vision. Most advanced methods for counting are based on density maps. Usually, density maps are first generated by convolving ground truth point maps with a Gaussian kernel for later model learning (generation). Then, the counting network learns to predict density maps from input images (estimation). Most studies focus on the estimation problem while overlooking the generation problem. In this paper, a training framework is proposed to generate density maps by learning and train generation and estimation subnetworks jointly. Experiments demonstrate that our method outperforms other density map-based methods and shows the best performance on drone-based car counting.', 'title': 'Drone-Based Car Counting via Density Map Learning', 'embedding': []}, {'id': 14936, 'abstractText': 'This paper addresses a vehicle localization method that fuses aerial maps and lidar data in urban canyon environments where global positioning system (GPS) signals are inaccurate. The boundaries of buildings are extracted from the aerial map and they are matched to point cloud data provided by the lidar. However, most aerial maps contain perspective projection distortions which can be significant in urban canyons with tall buildings. In this study, a new method to correct such projection distortion is proposed and it is applied to precise localization by fusing the corrected map and lidar data. In order to achieve this, the semantic segmentation of an aerial image is performed using a convolutional neural network, and the mutual information between the lidar measurements and the building boundaries is obtained to measure their similarity. A particle filter framework is employed to localize the vehicle and match the map using the mutual information as the weight of a particle. An experimental dataset is then used to validate the feasibility of the proposed method.', 'title': 'Fusing Lidar Data and Aerial Imagery with Perspective Correction for Precise Localization in Urban Canyons', 'embedding': []}, {'id': 14937, 'abstractText': 'Coral reefs are referred to as \"tropical rainforests of the sea\" to provide for richness of biological diversity. However, coral reefs are seriously degrading for global warming effects. Mapping coral reefs is important to spatially interpret the current coral distribution. Now, coral reefs mapping is generally created by visual interpretation with aerial photos in Japan. For effectively mapping coral reefs cover, the study is to understand availability if satellite-based remote sensing technique is able to be substituted in the near future. The paper introduces how to map coral reefs distribution by aerial photo-based visual interpretation and satellite-based analytical process. The result by satellite data is evaluated with the ground truth data and the visible interpretation method. In the error matrix utilizing 73 ground survey points, the overall accuracy was achieved 78%, which indicated similar trend with the ground truth. In error matrix with rasterized visible interpretation map, the overall accuracy was 55%. The reason is why \"less than 5% Coral cover on Sand and Rock bottom\" and \"5-50% Coral cover\" were caused misclassification in the offshore. However, the inshore area in the analytical result is able to detail coral structure, and the satellite image was able to penetrate deeper by the water column correction. Therefore, satellite-based remote sensing is appreciable to apply for the coral reefs mapping. In the future, we need to consider the classification class types and pro-processing to achieve higher accuracy.', 'title': 'Evaluation of Coral Reefs Mapping in Kerama Islands by Satellite-Based Classification', 'embedding': []}, {'id': 14938, 'abstractText': 'Hydrocarbon contamination introduced during point, line and map analyses in a field emission electron probe microanalysis (FE-EPMA) was investigated to enable reliable quantitative analysis of trace amounts of carbon in steels. The increment of contamination on pure iron in point analysis is proportional to the number of iterations of beam irradiation, but not to the accumulated irradiation time. A combination of a longer dwell time and single measurement with a liquid nitrogen (LN<inf>2</inf>) trap as an anti-contamination device (ACD) is sufficient for a quantitative point analysis. However, in line and map analyses, contamination increases with irradiation time in addition to the number of iterations, even though the LN<inf>2</inf> trap and a plasma cleaner are used as ACDs. Thus, a shorter dwell time and single measurement are preferred for line and map analyses, although it is difficult to eliminate the influence of contamination. While ring-like contamination around the irradiation point grows during electron-beam irradiation, contamination at the irradiation point increases during blanking time after irradiation. This can explain the increment of contamination in iterative point analysis as well as in line and map analyses. Among the ACDs, which are tested in this study, specimen heating at 373 K has a significant contamination inhibition effect. This technique makes it possible to obtain line and map analysis data with minimum influence of contamination. The above-mentioned FE-EPMA data are presented and discussed in terms of the contamination-formation mechanisms and the preferable experimental conditions for the quantification of trace carbon in steels.', 'title': 'Quantitative FE-EPMA measurement of formation and inhibition of carbon contamination on Fe for trace carbon analysis', 'embedding': []}, {'id': 14939, 'abstractText': 'Sea ice lead area fraction, distribution of lead length and orientation of leads are subject of this study. Leads are classified from dual-band Sentinel-1 SAR data with an automatic supervised learning classification algorithm. Binary maps are combined from scenes acquired within a three-day interval to provide an Arctic-wide composite lead map. Resolution of these binary maps is 80 meters. Based on these binary maps, lead area fraction is calculated on a 12 km grid. Regional maps of lead area fraction for the Beaufort Sea and the Fram Strait are calculated on a 4 km grid. The Hough transform is used to detect linear features on binary lead maps. The lead length distribution and the orientation of leads are calculated for the Fram Strait region and the Beaufort Sea. Most of the detected leads have length below 15 km. Two pronounced peaks on the lead orientation are found at around 50 and 130 degrees.', 'title': 'Sea Ice Leads Detected From Sentinel-1 SAR Images', 'embedding': []}, {'id': 14940, 'abstractText': 'We use a convolutional neural network to study cosmic string detection in cosmic microwave background (CMB) flat sky maps with Nambu–Goto strings. On noiseless maps, we can measure string tensions down to order 10<sup>−9</sup>, however when noise is included we are unable to measure string tensions below 10<sup>−7</sup>. Motivated by this impasse, we derive an information theoretic bound on the detection of the cosmic string tension Gμ from CMB maps. In particular, we bound the information entropy of the posterior distribution of Gμ in terms of the resolution, noise level and total survey area of the CMB map. We evaluate these bounds for the ACT, SPT-3G, Simons Observatory, Cosmic Origins Explorer, and CMB-S4 experiments. These bounds cannot be saturated by any method.', 'title': 'Information theoretic bounds on cosmic string detection in CMB maps with noise', 'embedding': []}, {'id': 14941, 'abstractText': 'The integrated crosstalk noise (ICN) has been widely used as an alternative to the insertion crosstalk ratio (ICR) for channel crosstalk evaluation in the IEEE 802.3ba standard. In this work, the differential ICN mitigation scheme by using idea of orthogonality is implemented in two adjacent differential pairs first. In the full pin map area of SerDes channel, new pin map patterns based on such scheme are prosed and compared with the conventional pan map patterns. The new pin maps mitigate the differential ICN drastically, yet maintain the G:s ratio. A preliminary study is conducted on fan-out trace routing to maintain the benefit from new pin map patterns.', 'title': 'Differential Integrated Crosstalk Noise (ICN) Mitigation in the Pin Field Area of SerDes Channel', 'embedding': []}, {'id': 14942, 'abstractText': 'Dot map is one of quantitative cartographic presentation methods used for geovisualization. Traditionally, it was a task of a qualified cartographer to distribute dots in the map. Nowadays, when maps are prepared not only by specialists, it is necessary to develop methods for automatic map production, which will be accessible for a wide range of GIS users. The author presented a new approach to automation of dot maps production. In the study, a building object class from Database of Topographic Objects BDOT10k was combined with statistical data on population and average useful floor area of dwelling in given administrative units. Size and value of dots were calculated with use of hexagonal tessellation. The author also introduced an algorithm for hexagons clusterization with use of dot value. Examples of maps prepared with use of the described method were prepared for a couple of districts of Małopolskie Province.', 'title': 'Automation of Dot Maps Production Supported by BDOT10k Database', 'embedding': []}, {'id': 14943, 'abstractText': 'Designers are increasingly using online resources for inspiration. How to best support design exploration without compromising creativity? We introduce and study Design Maps, a class of point-cloud visualizations that makes large user interface datasets explorable. Design Maps are computed using dimensionality reduction and clustering techniques, which we analyze thoroughly in this paper. We present concepts for integrating Design Maps into design tools, including interactive visualization, local neighborhood exploration and functionality to integrate existing solutions to the design at hand. These concepts were implemented in a wireframing tool for mobile apps, which was evaluated with actual designers performing realistic tasks. Overall, designers find Design Maps supporting their creativity (avg. CSI score of 74/100) and indicate that the maps producing consistent whitespacing within cloud points are the most informative ones.', 'title': 'Interactive Exploration of Large-Scale UI Datasets with Design Maps', 'embedding': []}, {'id': 14944, 'abstractText': 'Philosophical principles are very useful in customization of Linux kernel, e.g., the answer for the question: \"For the pointer to the start address of page table, is it a physical address or a virtual address?\" can be derived by one simple philosophical principle: the depth of recursion is limited. This is because if the pointer were a virtual address, there would be another new page table to store the translation information of this virtual address, but who was responsible for storing the translation information of the start address of this new page table? This would result an infinite recursion. So the pointer definitely is a physical address. In fact, the usefulness of philosophical principles comes from the reduction of searching space. And this reduction is very important in customization of Linux kernel, for it could cut down the size of the new code needed to be read. This is especially valuable when considering that Linux kernel is continuously updating and huge now. Another example to further demonstrate the reduction of searching space in customization is showed in the following: in customization of file system in kernel version 3.10, the question: \"Does the Linux kernel itself maintain the consistency between the buffer cache and the page cache?\". This is a hard problem in practice, for without any guidance of philosophical principle, a developer has to read all of the code in Linux kernel to get a precise answer. The tricky part of this question is that if the developer only read a part of the codes and doesn\\'t find any mechanisms for maintenance of cache consistency, the conclusion of non-existence of such mechanisms still can not be drawn, for there\\'s still a possibility that such mechanisms exist in the codes not explored. Besides, if the developer search internet to find the answer, assume that the developer is lucky enough, he/she finally finds one program example on a web page shows that the inconsistency may raise between buffer cache and page cache. He/she still can not get the conclusion that Linux kernel does not maintain such consistency, because that program example maybe is only valid in a specific scenario, e.g. in kernel version 2.26, not 3.10. But we can get a satisfied answer by using the philosophical principle: the cost of management process should be far less than the value created by being managed process. By this principle, it can be drawn that Linux kernel doesn\\'t maintain the consistency between the buffer cache and page cache in kernel 3.10. This is because that the data in buffer cache and page cache is highly dependent on application logic, so if Linux kernel wanted to maintain such consistency, it would have to track all these applications, which cost was much higher than the benefits that these applications could produce. However, the successful application of philosophical principles depends on two factors: firstly, establishment of a mapping between concepts in Linux system and well-known concepts in human society. This is not a new idea, e.g. the word of \"cost\" is a concept first appeared in human society, not in computer science, but nowadays, developers establish a mapping between this concept and concepts in computer science. Although the idea is very old, it is still very effective. Since well-known concepts in human society are familiar to most developers and are what they have in common, the cost of applying philosophical principles is reduced. Besides this, already existing cause-effect relations among concepts in human society can be highly possible to be reused in philosophical deduction in Linux kernel. E.g, in the mapping we established, process is treated as a human and since in religion of human society, God creates humankind, it is natural to derive that there\\'s one process that creates all other processes in Linux system with high probability. Secondly, a concrete model with many qualitative and quantitative details should be the basis of philosophical deduction. We build such model according to our past experiences and the construction of the model follows the philosophical principle: unfold the complexity only when it is necessary. E.g., in this model, for a specific detail, it is covered only when it is required in practice. This is to lower down the cost of modelling huge and continuously evolving Linux kernel. This model is very important, without it, philosophical deduction is impossible. But it is really a hard work, according to our experiences, it needs at least 6-years of work on Linux kernel for one developer to build it. Although philosophical principles are very useful in practice, there\\'s a big gap on the recognition of philosophical principles between academic researchers and industry practioners. E.g., some academic researcher seriously doubts whether the mapping above, which mentioned God, is helpful. In fact, it is, for by this mapping, a developer will know that the existence of the process, which is the origin of all other processes, is highly possible and also that process maybe is not easily observed. This is true, for that process is the process which PID is zero and that process can not be observed by Linux command: \"ps -e\". That process is a very valuable point of customization, e.g., by modifying that process, all processes in the Linux will be affected. Why does this big gap exist? We believe there\\'re at least three reasons: i. The bias on philosophical principles. This usually comes from the observation that some developers establish wrong mapping between the philosophical principles and the objects in real world. But is that true for those that has been verified many times in practice? ii. Wrong expectations. E.g., hope to get the precise answer when applying philosophical principles, instead of reducing the searching space. iii. Some academic researchers do not realize that a good philosophical principle usually is the result of a deep learning process of many years by human brain. Finally, we suggest that more efforts should be put on the studying of philosophical principles in program understanding and we believe that in the near future, the philosophical principles plus AI will be a trend in program understanding.', 'title': 'Application of Philosophical Principles in Linux Kernel Customization', 'embedding': []}, {'id': 14945, 'abstractText': 'While the accuracy requirement for simulation-based efficiency map becomes high for traction motors that have high efficiency over wide drive range. Even a small improvement in efficiency is critical, the accuracy level of the simulation-based efficiency map has not been well studied.To evaluate the accuracy of a finite element analysis (FEA)based efficiency map, efficiency maps were generated with different loss calculation methods for an interior permanent magnet (IPM) motor. The simulation results were compared with a measurement-based efficiency map.The comparison indicated that FEA can reproduce an efficiency map with an error of less than 1% when losses were calculated taking into account high fidelity factors which are; minor hysteresis loops, AC loss, manufacturing degradations, and stray losses. Also, the cause and effect of the loss generation are analyzed by the observation of the magnetic hysteresis and the loss density distribution in the steel sheet and the windings.', 'title': 'Loss Analysis of a Permanent Magnet Traction Motor in a Finite Element Analysis based Efficiency Map', 'embedding': []}, {'id': 14946, 'abstractText': 'Spectrum maps represent the spatial distribution of signal strength in a particular area and are necessary in spectrum management applications such as frequency reuse and prediction of coverage. This paper studies the problem of spectrum map complementation under the condition of limited observations. First, the relationship among the number of radiation sources, the rank of the spectrum map, and the complement performance is discussed. Then, for the problem of complementation performance of spectrum map being not ideal when there are too many radiation sources, an iterative completion method of spectrum map based on difference of observation values is proposed. This method is used to reconstruct the variation of the frequency spectrum map at the adjacent time. Simulation results show that the proposed method is superior to the direct complement method in accuracy and complement performance.', 'title': 'The iterative completion method of the spectrum map based on the difference of measurement values', 'embedding': []}, {'id': 14947, 'abstractText': 'Recently \"entertainment computing\" (EC) technology becomes a hit term in Japan. There is a well-known \"projection mapping\" in this EC. Projection mapping is a video technique that synchronizes real and video, and the fascinating world view that the fusion of both produces attracts worldwide attention. \"Mapping\" of projection mapping has a meaning of pasting a material called video on the surface layer of the projection target. In mapping, by giving images such as light and shadow while utilizing information such as the design and unevenness of the object, realistic multi-dimensional feeling and space feeling can be expressed. In this study, we prototyped an interactive projection mapping that changes according to user\\'s movement by projecting to the user. This time we focused on sports like baseball and soccer, and we projected the ball to the user by projection mapping so that we can experience pitching and lifting.', 'title': 'A Proposal of Interactive Projection Mapping Using Kinect', 'embedding': []}, {'id': 14948, 'abstractText': 'The likelihood of transitions between pairs of land cover and land use classes in a given time interval and environmental context can be used to impose classification restrictions on an image or to evaluate results. This study presents a methodology for using the likelihood of transitions between classes to improve land cover classification, given a base map (a supposedly accurate map for the same area in another date) and a set of previously classified images. These improved land cover classified images were named conditioned classified images. We aimed to classify one Synthetic Aperture Radar image and an optical one, both from June 2010, using two land cover legends in different level of detail for a region in the Brazilian Amazon. We used both a classified image from 2008 (also in two legends levels) and the data from the Programme for the Estimation of Deforestation in Brazilian Amazon (PRODES) from 2008 as base maps, and presented the likelihood of transitions between the considered classes. The proposed methodology resulted in conditioned classified images with higher Overall Accuracy than the one that does not consider the base maps and the likelihood of transitions. The conditioned classified images presented unlabeled areas due to classification errors in the input data. It is important to highlight that these areas are probably misclassified in maps obtained without using likelihood transition and base maps, since they are impossible to occur in the field.', 'title': 'The use of land cover change likelihood for improving land cover classification', 'embedding': []}, {'id': 14949, 'abstractText': 'Recently, wireless localization has attracted great interest. However, a type of localization problem has not received much attention and has not been well studied, in which all locations of nodes are predetermined, but when nodes are deployed to these positions, the relationship between identifiers and locations of nodes is unknown. Applications with such localization problems include smart lighting, structural health, smart roads and industrial monitoring, etc. In this paper, we investigate how to map node identifiers to known locations. Although the locations of nodes are given, it is still challenging to map the node identifier to known location because the combination of mapping possibilities is huge, and incorrect mapping may occur when several known locations are close to each other. To address this problem, we first propose a localization algorithm, called Q-Hop, in which the distance of two nodes is measured by a fine-grained hop count. Then, we propose a mapping method that maps the node positions estimated by Q-Hop to known locations. The simulation results demonstrate that our solution achieves satisfactory mapping accuracy, which will provide helpful insights for applications with such localization problems.', 'title': 'Wireless Localization of Mapping Node Identifiers to Known Positions', 'embedding': []}, {'id': 14950, 'abstractText': 'In an autonomous scientific exploration system, the terrain map generated from mapping process integrates sensing information from multiple aspects and lays the base for decision making processes. With the increasing challenges in planetary exploration, equipping planetary rovers with the principles of terramechanics is becoming more and more common, especially on rough or intricate terrain. However, it is difficult for conventional maps with elevation information only to reflect terrain mechanical properties, which play important roles in terramechanics-based simulation or motion control. This study extracts the dominant parameters in terrain bearing and shearing models, and presents a multi-layered grid map with fundamental geometric and mechanical elements. A corresponding mapping scheme based on dense visual input is designed to reconstruct elevation in the map and predict terrain mechanical parameters of the entire visual field. Experiments are conducted to verify the practicability of the approach proposed in a Mars emulation yard with a rover prototype.', 'title': 'Mapping for Planetary Rovers from Terramechanics Perspective<sup>*</sup>', 'embedding': []}, {'id': 14951, 'abstractText': 'Modern genomic tree breeding studies and forest health monitoring programs demand accurate mapping of tree phenotype. However, conventional field-based approaches to map phenotype are costly in terms of time and money. The recent phenomenal advances in the low-flying Unmanned Airborne Vehicle (UAV) remote sensing platforms together with the availability of high-resolution hyperspectral cameras allow to periodically capture a huge amount of crown spectral details of individual trees. These details can be exploited to map phenotypic class of tree genotypes. State-of-the-art methods that maps tree phenotype often underexploit the information in hyperspectral time-series data by using only a few specific band-based remote sensing (RS) indices to model phenological tree parameters that define the phenotypic response. Thus, we propose a wavelet-based approach to map tree phenotype of trees that a) maximally exploits the spectral and temporal information in band-groups by addressing data redundancy problem, and b) uses spectro-temporal/phenological information in the hyperspectral time-series data to map phenotype class of tree genotype. The improved performance of the proposed method over a RS index-based state-of-the-art one to map trees to a phenotypic class, on a set of 100 trees from 10 genotypes, proves the method to be performing.', 'title': 'A Band Grouping Based Approach for Phenotype-Class Mapping of Tree Genotypes Using Spectro-Temporal Information in Hyperspectral Time-Series UAV Data', 'embedding': []}, {'id': 14952, 'abstractText': 'We propose a deep-learning approach based on generative adversarial networks (GANs) to reduce noise in weak lensing mass maps under realistic conditions. We apply image-to-image translation using conditional GANs to the mass map obtained from the first-year data of Subaru Hyper Suprime-Cam (HSC) Survey. We train the conditional GANs by using 25\\xa0000 mock HSC catalogues that directly incorporate a variety of observational effects. We study the non-Gaussian information in denoised maps using one-point probability distribution functions (PDFs) and also perform matching analysis for positive peaks and massive clusters. An ensemble learning technique with our GANs is successfully applied to reproduce the PDFs of the lensing convergence. About <tex>$60{{\\\\ \\\\rm per\\\\ cent}}$</tex> of the peaks in the denoised maps with height greater than 5σ have counterparts of massive clusters within a separation of 6 arcmin. We show that PDFs in the denoised maps are not compromised by details of multiplicative biases and photometric redshift distributions, nor by shape measurement errors, and that the PDFs show stronger cosmological dependence compared to the noisy counterpart. We apply our denoising method to a part of the first-year HSC data to show that the observed mass distribution is statistically consistent with the prediction from the standard ΛCDM model.', 'title': 'Noise reduction for weak lensing mass mapping: an application of generative adversarial networks to Subaru Hyper Suprime-Cam first-year data', 'embedding': []}, {'id': 14953, 'abstractText': 'Recent advances in Open Source mapping software are opening up new possibilities for online visualization of satellite data. Bhuvan, Indian Geo-platform of ISRO, hosts multi-temporal, multi-sensor and multi-resolution satellite data over internet and is used for various remote sensing applications. Serving satellite images over internet with good quality and faster response is essential for better user experience and increases the use of satellite data online. Map rendering software is generally used for serving satellite data as web map tiles over internet. This software will process satellite images and generate fixed size tiles based on parameters like zoom level, projection and area of interest. In this study, we explore open source map tile rendering software Mapnik and MapServer. The overall implementation process for hosting satellite data as web map tiles is discussed in detail. Using these two software, satellite data is published with different re-sampling methods as map layers. The published map layers are compared for web tile quality and performance under different load conditions. We also discuss various possible methods for improving the performance of tile creation and rendering.', 'title': 'Study of open source web map rendering software for rendering Bhuvan High Resolution Satellite data', 'embedding': []}, {'id': 14954, 'abstractText': 'Aim: To parallelize scanning and save time and cut costs of preclinical studies we have designed a new hotel holding 4 rats in the HRRT, which has a spatial resolution close to that of preclinical PET scanners. In this work we test the quantitative accuracy on phantoms in the hotel using different attenuation corrections methods on the HRRT. Material and Methods: The rat hotel has 4 compartments made of acrylic plastic with an 8 mm base plate and a 3 mm half-cylinder lid. Four 50 ml syringes filled with [<sup>18</sup>F]-FDG in water were used as phantoms and scanned in the rat hotel for 20 min. on the HRRT and a high statistics speed 10 transmission scan was acquired. Three μ-map processing/reconstruction methods - MAP-TR with either human head (HH) or water phantom (WP) prior and TXTV - were used and μ-maps and PET images reconstructed with each of the 3 μ-maps evaluated. Results: The μ-maps all underestimated the LAC of the acrylic plastic material as compared to CT, and the base plate thickness was underestimated. Activity concentrations were thus also underestimated: -4.6% using HH -8.7% using TXTV and -13.8% with WP. No noteworthy local variations were found. Conclusion: We found a global underestimation of PET activity, which was within a ±5% acceptance range using MAP-TR with the human head prior and a long transmission scan (speed 10). Fine tuning HH or TXTV parameters might give further improvements.', 'title': 'Quantification accuracy of a new HRRT high throughput rat hotel using transmission-based attenuation correction: A phantom study', 'embedding': []}, {'id': 14955, 'abstractText': 'Increasing the low spatial resolution of hyperspectral images (HSIs) improves the performance of applications in which the HSIs are used. In this study, a fusion based method is proposed to increase the resolution of HSIs. In the proposed method, low resolution (LR) HSI is fused with the high resolution (HR) RGB image to obtain the HR HSI. In this approach, instead of using the spectral images as in the conventional methods, RGB image is used with the abundance maps of the HSI estimated from the linear unmixing and the spatial resolution is enhanced using these maps. In this method, firstly, endmembers are estimated and LR abundance maps are obtained. Then, HR abundance maps are obtained by minimizing an energy function, which is constructed from the LR abundance maps with the HR RGB image. Finally, HR HSI is obtained from these HR abundance maps. The method is tested with real HSIs. Main contribution of the method is converting fusion problem to a quadratic optimization problem in the abundance map domain without any assumption or prior knowledge. The proposed method solves the fusion problem with a computational time much lower than the state-of-the-art fusion based methods with a competing performance.', 'title': 'Fusion based resolution enhancement in hyperspectral images', 'embedding': []}, {'id': 14956, 'abstractText': 'A highly accurate and robust real-time localization process is crucial for autonomous driving applications. Numerous methods for localization have been proposed, which combine various kinds of input, such as data from environmental sensors, inertial measurement units (IMU), and the Global Positioning System (GPS). Because reliance on a single environmental sensor is a vulnerable approach, the use of multiple environmental sensors is a better alternative. However, the fusion methods from previous studies have not adequately compensated for the drawbacks due to the lack of sensor diversity nor have the methods considered the fail-safe issue. In this paper, we propose a multi-modal fusion-based localization framework that uses multiple map matching sources. The framework contains two independent map matching sources and integrates them in a stochastic situational analysis model. By applying a probabilistic model, the more reliable map matching between the multiple sources is determined and the system stability is verified via a fail-safe action. A number of experiments with autonomous vehicles within actual driving environments have shown that combining multiple map matching sources yield more robust results than the use of a single map matching.', 'title': 'Fail-Safe Multi-Modal Localization Framework Using Heterogeneous Map-Matching Sources', 'embedding': []}, {'id': 14957, 'abstractText': 'Predictive map of geoelectric sections of the North Eurasia, necessary for calculation of propagation of VLF-MF radio waves, is constructed. Taking into account the layered structure of the underlying medium, this map is capable of increasing the accuracy of electromagnetic field calculations by 1.5-3 times as compared to the Morgan-Maxwell map and ITU-R Recommendation P.832-2. The methodology of the geoelectric mapping is described. The studies of electrical properties of layered media by combined radio and geophysical methods in a variety of natural and geological conditions, and the proposed method of geoelectric mapping have resulted in the construction of a new generation of maps showing the electrical properties of the underlying medium that account for the layered structure of the crust and have no analogues in the world.', 'title': 'Predictive map of geoelectric sections of North Eurasia and its application for the radiowaves propagation calculations', 'embedding': []}, {'id': 14958, 'abstractText': 'The need to reduce water pumps size to achieve compact designs adapted to multiple working points opens new fields of study. PMSMs are the preferred choice due to outstanding torque-speed range capabilities. This paper presents a methodology to design and optimize PMSMs by defining the desired torque-speed-efficiency map, adapting its performance to the hydraulic characteristics of the water pump. Once the hydraulic efficiency is known, an initial PMSM reference torque-speed-efficiency map is defined according to the objective motor performance, including the distribution of power losses and the power rating of the selected application. The designer has full freedom to define the efficiency levels and distribution along the torque-speed map. The design optimization algorithm achieves the PMSM characteristics which adjust as much as possible to the defined performance. This methodology uses ultra-fast finite element analysis by applying magneto-static computations and a time-space conversion to compute the iron losses, reducing the computational requirements. The torque-speed-efficiency map is calculated by applying a direct-quadrature electrical model. The objective function uses a novel image comparison technique that allows comparing the similarity between the objective and optimized maps. The methodology is validated experimentally by designing and testing a PMSM adapted to a real WP application', 'title': 'Customized PMSM Design and Optimization Methodology for Water Pumping Applications', 'embedding': []}, {'id': 14959, 'abstractText': 'In this work, we investigated the effect of map presentations and landmarks on wayfinding performance. We carried out an experiment in virtual reality, participants were asked to navigate inside a 3D environment to find targets shown on the maps. We studied two kinds of maps: Skymap, a world-scale, and world-aligned head-up map and a track-up bird’s eye view map. Results showed that neither SkyMap nor landmarks did improve target finding performances. In fact, participants performed better with the track-up map.', 'title': 'Map Displays And Landmark Effects On Wayfinding In Unfamiliar Environments', 'embedding': []}, {'id': 14960, 'abstractText': \"Network function virtualization (NFV) provides an effective way to decouple network functions from the proprietary hardware, allowing the network providers to implement network functions as virtual machines running on standard servers. In the NFV environment, an NFV service request is provisioned in the form of a service function chain (SFC). The SFC defines the exact sequence of actions or virtual network functions (VNFs) that the data stream from the service request is subjected to. These actions or VNFs need to bemapped onto specific physical networks to provide network services for end users. In this paper,we investigate the problem of dependence-aware service function chain (D_SFC) design and mapping. We study how to efficiently map users' service requests over the physical network while taking into consideration the computing resource demand, function dependence of the VNFs, and the bandwidth demand for the service request. We propose an efficient algorithm, namely, Dependence-Aware SFC Embedding With Group Mapping (D_SFC_GM), which integrates the proposed techniques of dependence sorting, independent grouping, adaptive mapping, and tetragon remapping to jointly design and map users' service requests. The proposed D_SFC_GM algorithm takes advantage of VNF's dependence relationships and the available resources in the physical network to efficiently design the chain and reserve the computing/bandwidth in the physical network. The extensive performance analysis in both IP and physical networks shows that the proposed D_SFC_GM significantly outperforms the traditional approach based on topological sorting and sequential embedding.\", 'title': 'Embedding dependence-aware service function chains', 'embedding': []}, {'id': 14961, 'abstractText': 'Local mapping plays an important role in outdoor intelligent vehicle applications and multi-vehicle cooperative local mapping which takes advantage of vehicular communication can bring considerable benefits to this important task. In this paper, a multi-vehicle cooperative local mapping architecture using split covariance intersection filter (Split CIF) is proposed. In the proposed method, a vehicle can flexibly perform cooperative local mapping with other vehicles in decentralized way, without complicated monitoring and controlling of data flow among vehicles; fused maps can be shared freely among vehicles. An efficient and accurate implementation of the Split CIF is also introduced. A simulation-based comparative study demonstrates the potential and advantage of the proposed multi-vehicle cooperative local mapping architecture using Split CIF.', 'title': 'Multi-Vehicle Cooperative Local Mapping Using Split Covariance Intersection Filter', 'embedding': []}, {'id': 14962, 'abstractText': 'Feature maps in Convolutional neural networks are extracted automatically with some initialization methods and training strategies, which greatly economizes the cost of feature engineering. However, correlation between feature maps are not considered in common networks, resulting in the increase of redundant feature maps with the networks becoming more complicated. In this work, we proposed the correlation layer and designed the correlation loss, which can compute the correlation coefficient matrix of the feature maps in the last convolutional layer and optimize the weights distribution respectively. In the training phase, 2 strategies, namely the supervision and initialization are studied with Gaussian and He initialization methods for the baseline. The experimental results on CIFAR-10 dataset demonstrated that the supervision strategy for the multi-task training could efficiently reduce the correlation between the feature maps learned and increase the classification accuracy from 0.39% to 1.14% on the test set.', 'title': 'Feature Correlation Loss in Convolutional Neural Networks for Image Classification', 'embedding': []}, {'id': 14963, 'abstractText': 'Rivers are important elements of the Earth’s ecosystem, and their spatial distribution information is critical for the study of hydrological and biogeochemical processes. Moderate Resolution Imaging Spectroradiometer (MODIS) imagery has been widely used for river mapping due to its high temporal resolution and long-term observation records, which are essential to capture the rapid fluctuation of rivers. However, when the conventional hard classification methods are used, the accuracy of the river maps produced from MODIS data (and especially for those rivers with narrow widths) is often limited because mixed pixels are common in MODIS imagery, due to the coarse spatial resolution. In this paper, a cascaded spectral-spatial information combined deep convolutional neural network (CNN) model for super-resolution river mapping (DeepRivSRM) is proposed to produce Landsat-like fine-resolution river maps from MODIS images. In DeepRivSRM, a CNN made up of a spectral unmixing module and a super-resolution mapping (SRM) module is introduced to handle the spectral and spatial information simultaneously. Moreover, for training the DeepRivSRM model, an adaptive cross-entropy loss function incorporating the fraction information of the rivers is designed to improve the performance of the DeepRivSRM model for small rivers. The proposed method was evaluated with MODIS images from three test sites and was compared with hard classification, a conventional SRM method, and a CNN-based SRM method. The results show that DeepRivSRM can generate more accurate river maps by effectively learning the subpixel-scale spatial-spectral information in MODIS imagery.', 'title': 'A Cascaded Spectral-Spatial CNN Model for Super-Resolution River Mapping With MODIS Imagery', 'embedding': []}, {'id': 14964, 'abstractText': \"Landslide susceptibility maps are vital for natural disaster mitigation activities. It can be a basic information to make appropriate mitigation plan. In the present study, we propose ensemble fuzzy clustering to produce the landslide susceptibility map for Ponorogo. The mapping process is based on five factors which play a dominant role in the occurrence of landslide. The factors are rainfall, land use, slope angle, geology, and elevation. As a result, from 316 areas in Ponorogo, 202 areas were mapped in very low level, 94 areas in medium level and 20 areas in the high level of vulnerability. Finally, the validation of landslide susceptibility map was carried out using Pearson's chi-square test. The chi-square value shows higher value than the critical value. It shows that the model has good accuracy in predicting the landslide susceptibility in Ponorogo. The map was visualized on web-based to make it easier to use and can be used for mitigation activities.\", 'title': 'Landslide susceptibility mapping using ensemble fuzzy clustering: A case study in ponorogo, east java, Indonesia', 'embedding': []}, {'id': 14965, 'abstractText': 'Along the season crop classification based on satellite data is challenging task for Ukraine because of a big diversity of different agricultural crops with different phenology (crop calendars). Taking into account the availability for free of high resolution (10 to 30 meter) optical and SAR data from different satellite, the most resource consuming task is ground data collecting. That is why the proper time of ground surveys and crop classification maps developing is very important. In the study we propose to build three crop classification maps for JECAM Ukraine test site in Kyiv region during the vegetation season. The first one is built in the middle of May to classify winter cereals and rapeseeds. The next crop classification map is developing in July to discriminate major summer crops (spring cereals, maize, soybeans, sunflowers). The final crop map is built in autumn to refine summer crops and sugar beet discrimination. Time series of multi-temporal satellite images with restored missing (clouded and shadowed) data are classified using neural network approach, in particular ensemble of multi-layer perceptrons (MLPs). It is shown, that addition of satellite data from the end of previous year to the spring imagery allows to significantly improve the accuracy of winter crops classification. In July it is possible to deliver the map with major summer crops with overall accuracy higher than 87%, and the overall accuracy of final map at the end of the season is 94%.', 'title': 'Along the season crop classification in Ukraine based on time series of optical and SAR images using ensemble of neural network classifiers', 'embedding': []}, {'id': 14966, 'abstractText': \"This study presented a votable concept mapping approach to promoting students' attentional behavior based on the evidence of mining sequential behavioral patterns. Twenty-nine students from an Educational Research Methodology course were recruited as participants. In the first week, a group-polling method was introduced in class; in the second week, participants were asked to draw concept maps using pen and paper (PnP concept mapping); and in the third week, the polling system and concept maps were integrated (votable concept mapping) and applied. The results showed that the votable concept mapping method was effective to stimulating students' attention during class. It was therefore suggested that teachers adopt methods integrating concept maps and polling tools to stimulate students' attention and thereby promote a positive cycle of attentional behavior in the classroom.\", 'title': \"A Votable Concept Mapping Approach to Promoting Students' Attentional Behavior: Based on the Evidence of Mining Sequential Behavioral Patterns\", 'embedding': []}, {'id': 14967, 'abstractText': 'Dynamic cerebral perfusion computed tomography (DCPCT) imaging has the ability to detect ischemic stroke via hemodynamic maps. However, due to multiple acquisitions protocol, DCPCT scanning imposes high radiation doses on patients and might increase their potential cancer risks. The DCPCT protocol that decreases DCPCT samples by increasing sampling intervals can greatly reduce radiation dose, but this may introduce bias in the hemodynamic maps estimation, affecting the diagnosis. To address this issue, in this study, we present a deep learning network to determine the DCPCT protocol to realize the dose-reduction task, i.e., decreasing DCPCT samples, and the diagnosis-quality task, i.e., improve hemodynamic maps accuracy. Specifically, one interpolation convolutional neural network is fully designed to estimate the DCPCT images at the sampling interval, termed as dynamic cerebral perfusion interpolation network (DCPIN). The present network treats the DCPCT measurements as a \"video\" to characterize the maximum temporal coherence of spatial structure among phases, and interpolates a frame at any arbitrary time step between any two frames. First, a flow computation network is used to estimate the bi-directional optical flow between two input DCPCT frames by linearly fusing to approximate the required intermediate optical flow. Second, another flow interpolation network is designed to refine the flow approximations and predict soft visibility maps. Finally, the estimated flow approximations and visibility maps are merged together to jointly predict the intermediate DCPCT frame. Experimental results on patient data clearly demonstrate that the present DCPIN can achieve promising reconstruction performance, i.e., high-quality DCPCT images and high-accuracy hemodynamic maps.', 'title': 'Task-driven Deep Learning Network for Dynamic Cerebral Perfusion Computed Tomography Protocol Determination', 'embedding': []}, {'id': 14968, 'abstractText': 'This work in progress paper presents the preliminary step towards developing a new educational framework titled “Tracking Evolution Architecture of Cognitive Hierarchy Maps for Engaged Classrooms” (TEACH ME). In this paper, we track the scaffolding of the pre-requisites needed in high school level physics concepts and use this information to design the higher-level scaffolding of the same concepts in a university freshmen physics course. This tracking is performed as a meta-cognitive activity inside the classrooms, using a well-established concept visualization tool called “concept maps”. Further, these maps will be extended to include higher-level concepts in the engineering courses that have the physics course as a pre-requisite. This paper presents the insights from a pilot implementation of this technique in a freshman physics course. We used concept map as an additional instruction method to aid the students to visually connect the pre-requisite and the newly delivered concepts. This will allow students to understand what they know and what they need to learn. The effectiveness of this method is measured using the data collected from class quizzes and tests. The paper summarizes the initial results from this study along with a discussion on how such earlier maps developed at freshmen level can be extended to higher-level engineering classes for creating engaged classrooms using a novel teaching-learning frame work under development titled “Tracking Evolution Architecture of Cognitive Hierarchy Maps for Engaged Classrooms” (TEACH ME).', 'title': 'The first step towards a pre-requisite knowledge tracking architecture for engineering programs', 'embedding': []}, {'id': 14969, 'abstractText': 'The topological G -conjugacy map has an important significance in terms of theory and application. In this paper, we study recurrent point, non-wandering point and periodic shadowing property of topological G -conjugacy on metric G -space. By means of properties of topological G -conjugacy map and by inference, we will give the following conclusions that if Let f: X → X and t: Y → Y be two continuous map of metric G-space X and Y. Suppose the map h: X → Y is a topogical G -conjugacy from f to t and x2 X, then(1)The point x is a G - recurrent point of f if and only if the point h(x) is a G -recurrent point of t. (2)The point x is a G -nonwandering point of f if and only if the point h(x) is a G -nonwandering point of t. (3) The map f has the G -periodic shadowing property if and only if the map t has the G -periodic shadowing property. These results will enrich the theory of the recurrent point, non-wandering point and periodic shadowing property of topological G -conjugacy on metric G-space.', 'title': 'Point Sets and Periodic Shadowing Property of Topological G-Conjugacy on Metric G-Space', 'embedding': []}, {'id': 14970, 'abstractText': 'Precipitable water vapor (PWV) is one of the key parameters in the evolution of extreme weather and climate change. However, current data fusion methods (such as Gaussian Processes, Spherical Cap Harmonics and polynomial fitting) can hardly obtain simultaneously the PWV map with high precision and high spatio-temporal resolution. To solve this problem, a two-step based PWV fusion (TPF) method is proposed, in which a Hybrid PWV Fusion Model (HPFM) and a Spatial and Temporal Fusion Model (STFM) are introduced separately. In the first step, HPFM is established by combining the Global Pressure and Temperature 2 wet (GPT2w) model, spherical harmonic functions, and polynomial fitting to obtain the PWV value with high precision at an arbitrary location in the study area. In the second step, STFM is proposed to generate the PWV map with high temporal resolution taking advantage of site-based GNSS-derived PWV. To validate the performance of the proposed method, GNSS observations, ERA-Interim, and ERA5 reanalysis products are selected in Yunnan Province, China to carry out the experiment. Statistical results show that: (1) HPFM has the ability to obtain atmospheric water vapor with an root mean square (RMS) of less than 3 mm in an arbitrary location of the PWV map; (2) STFM can generate PWV maps with the same temporal resolution as GNSS observations, and the accuracy of the obtained PWV values can be guaranteed. Therefore, the proposed TPF method is proved to have the ability to simultaneously retrieve PWV maps with high accuracy and spatio-temporal resolution.', 'title': 'Two-step precipitable water vapor fusion method', 'embedding': []}, {'id': 14971, 'abstractText': 'There is a growing demand for maps from government, enterprises and the pubulic. This paper studies the data management methods of ancient and modern maps to solve the problems of scattered storage, low openness, inconvenient query and use, lack of system inheritance and so on. Based on the digital processing and database management system, the map database was established after the ancient map was edited. Based on SOA service architecture and Browser/Server mode, the spatio-temporal fusion and operation platform for ancient and modern maps is established. The functions of map unified management, visual display, query and analysis, input and multi-mode export are achieved. Using Android and IOS technology to build mobile applications, improve map management and sharing application mode.', 'title': 'Design and Development of Spatio-Temporal Fusion and Operation Platform for Ancient and Modern Maps', 'embedding': []}, {'id': 14972, 'abstractText': '3D maps such as Google Earth and Apple 3D Map that allow users to easily access 3D models of real world become ubiquitously available recently. Currently, most people use the 3D maps on conventional display devices such as monitors and interface devices such as mouse and keyboard. With recent development of mass-market head mounted displays (HMD) where users can experience virtual reality (VR) with more affordable cost, experiencing 3D maps with virtual reality is expected to be an important application. Although one of the main goals of virtual reality technology is to enhance the sense of immersion, the conventional mouse and keyboard interfaces limit the level of immersion, which makes the interfaces often inappropriate for the 3D map navigation. From this motivation, this paper proposes immersive gesture interfaces for 3D map navigation in HMD-based virtual environments. The gestures are recognized using Kinect in real-time. User studies show that the proposed interface improves the level of immersion.', 'title': 'Immersive gesture interfaces for 3D map navigation in HMD-based virtual environments', 'embedding': []}, {'id': 14973, 'abstractText': 'The present study explores the center-of-mass (Hc) of the ionosphere as the effective ionospheric varying shell height (VSH). We presents global ionospheric maps GIM-Hc produced with IRI-Plas model by assimilating the instant GIM-TEC maps provided by JPL from 1994 to present which allows obtaining global maps of the F2 layer critical frequency, GIM-foF2 (or NmF2) and peak height, GIM-hmF2. Ratio TEC/NmF2 represents the slab-thickness model of the ionosphere, GIM-τ, fitted to foF2, hmF2 peak at each cell of a map. The slab-thickness τ is for the first time tied to hmF2 peak height with its components τbot below hmF2 and topside τtop. Equations for evaluating Hc with IRI-Plas and NeQuick models are derived from Ne(h) profile complemented with the total electron content—height profile TEC(h) from the bottom boundary of the ionosphere (65–80 km over the Earth) to varying altitude h up to 20,200 km (GPS orbit). The position of Hc depends on the altitude range selected at the ionosphere and plasmasphere. We determine Hc from Ne(h) and TEC(h) profile within the borders of τ from the bottom side (hmF2 — τbot) to topside (hmF2 + τtop). Regression linear model of Hc variation with hmF2 is derived which allows estimate of Hc from the F2 layer peak height hmF2. Statistical characteristics of GIM-Hc maps can serve for validation and updating the effective shell height with Hc parameter varying over the globe for improved vertical TEC evaluation from the slant TEC observations.', 'title': 'Modeling center-of-mass of the ionosphere from the slab-thickness', 'embedding': []}, {'id': 14974, 'abstractText': 'Fatal accidents can be avoided in autonomous vehicles by detecting the obstacles, and distance estimation accurately and in real-time is crucial. Still, most of the object detection approaches fail to detect debris and other roadside obstacles. The drawbacks of the object detection approaches are countered using active sensors such as LiDAR, RADAR, and SONAR, but expensive. This study introduces a modified architecture to estimate a depth map based on an unsupervised learning framework. Furthermore, integrating depth map information directly into autonomous vehicles is a slightly critical and challenging task. This paper presents a concept for predicting a potential collision based on the estimated depth map. For this purpose, we designed and developed an efficient and robust algorithm to understand the color encoding used in representing the depth map. Thereby, the possibility of collision regions in real-time identified using predefined threshold values for the color encoding of the predicted depth map. This approach emphasizes integrating the estimated depth map with the level of comprehension of the situation awareness to enhance the ability to recognize and process predicted hazards in the environment. It facilitated the possibility to avoid a dangerous situation in real-time capabilities.', 'title': 'Towards Robust Perception Depth Information For Collision Avoidance', 'embedding': []}, {'id': 14975, 'abstractText': 'Land use and land cover (LULC) mapping are required by some government institutions to manage their natural resources sustainably at various temporal and spatial scales. Our study, set in Ecuador, use Sentinel-2 and Landsat-8 from 2017 to 2019 applying an adapted Phenology-Based Synthesis (PBS) method, which originally only classifies land cover, to additionally classify land use categories and eventually obtain LULC map in yearly period with different spatial resolution. Copernicus Global Land Cover map with 100-meter spatial resolution was used as a reference LULC map and provided a critical analysis on PBS classifications, which showed, a consistent representation of the LULC patterns, with vegetation and non-vegetation types well described across the country. Furthermore, we evaluated the LULC changes which showed conversion transition occurred in each class. Finally, entire Ecuador was mapped confirming that adapted PBS mapping approach could be considered as a reliable, replicable and low-cost LULC monitoring tool.', 'title': 'Mapping and Assessment of Land Use and Land Cover for Different Ecoregions of Ecuador Using Phenology-Based Classification', 'embedding': []}, {'id': 14976, 'abstractText': 'Objective: Expanding the clinical research network from China to the international community will greatly facilitate the collaborative efforts to conquer emerging pandemics globally. This study aims to link clinical drugs in China with those in USA and beyond, by mapping concepts of clinical drugs in a Normalized Chinese Clinical Drug (NCCD) knowledge base with RxNorm and the international RxNorm Extension in the OHDSI research network. Methods: At this initial stage, we mainly focused on chemical drugs with a single active ingredient. A hybrid approach combining automated natural language processing technologies and manual review by domain experts was proposed, to normalization of drug names at different specification levels. The statistics of the mapping results, as well as the validation results using top 1000 frequent drugs in clinical settings of China. Results: 2,247 (25.39%) and 588 (6.64%) semantic clinical drugs in NCCD can be mapped to RxNorm and RxNorm Extension, respectively. Moreover, among the top 1000 most frequent clinical drugs in China, 99.6% of chemical drugs can be mapped to RxNorm/Extension at the ingredient level. This high coverage indicates that mapping Chinese chemical drugs to RxNorm/Extension can effectively support downstream clinical research and operations in the near future. Conclusion: The mapping between NCCD to RxNorm/Extension serves as a promising channel to promote transnational clinical research. The mapping terminologies are publicly accessible through OHDSI Athens. (athena.ohdsi. org/).', 'title': 'NCCD-RxNorm: Linking Chinese Clinical Drugs to International Drug Vocabulary', 'embedding': []}, {'id': 14977, 'abstractText': 'Disease similarity is a useful measure that has potential application to various aspects of medicine. One such application is the mapping of diseases in a two-dimensional plane, which can be the foundation of a useful diagnostic reminder method called the “pivot and cluster strategy.” However, the mapping of diseases using a similarity measure has yet to be explored. This article investigates such a mapping, and quantifies its basic characteristics. We first collected mutual similarity data for 1,550 diseases using a machine learning approach. The calculated similarity data were then used to map the diseases using a “multidimensional scaling” algorithm. Quantitative analysis indicated that it is difficult to express all the diseases on the map and yet still show the similarity information between the items. Then, by restricting the input, the algorithm performed well in practice. To our knowledge, this is the first study to investigate the automated mapping of diseases on a plane for use in clinical practice.', 'title': 'Geometrical mapping of diseases with calculated similarity measure', 'embedding': []}, {'id': 14978, 'abstractText': 'Attenuation correction of brain MRI coils used on PET/MR systems can be prone to error emanating from artifacts in CT-based coil attenuation maps. In this study editing was applied to brain-coil CT images to reduce the impact of metalinduced artifacts on attenuation correction for the GE SIGNA PET/MR. Methods for global and material-specific transformation of coil CT images to linear attenuation coefficient (LAC) maps were evaluated. In addition, CT-based attenuation maps were generated for the coil mirrors and four levels of smoothing were applied to the attenuation maps. A uniform phantom experiment was performed to assess absolute quantification, and both axial and transaxial image uniformity. For the three coils tested, the edited CT attenuation maps improved absolute quantification (mean absolute error 1.2% vs. 7.2% in the central axial 15 cm) and axial uniformity by up to 60% (33% on average) compared to the vendor-supplied attenuation maps, although transaxial uniformity deteriorated by up to 38% (16% on average). No single CT-to-LAC conversion method was found to be optimal across coils and performance metrics. Overall, material-specific CT-to-LAC conversion performed best for image quantification (0.6% vs. 1.5%), whereas global transformation narrowly ranked best for uniformity (1.69% vs. 1.71%). The inclusion of the mirror marginally improved axial uniformity in the majority of cases, but also slightly degraded transaxial uniformity. The impact of changing smoothing was minor, with the system default setting of 10 mm FWHM Gaussian producing the best results overall.', 'title': 'Brain MRI Coil Attenuation Map Processing for the GE SIGNA PET/MR: Impact on PET Image Quantification and Uniformity', 'embedding': []}, {'id': 14979, 'abstractText': 'Large-scale mapping of coastal oil spills and their monitoring over time is a major issue that can be adressed by using hyperspectral images and dedicated processing. Previous researches have shown that it is possible to map the polluted coastline caused by the explosion of the Deepwater Horizon (DwH) platform from AVIRIS images (AVIRIS: Airborne Visible/InfraRed Imaging Spectrometer). But the detection processes required either ground truth or laboratory spectra of hydrocarbons or were not fully automatic.In this paper we focused on an AVIRIS image which covers The Bay Jimmy, located south of New Orleans, and particularly impacted by oil pollution. Two automatic methods were developed to detect oiled coasts. In the first one, we have developed a new spectral index able to detect directly hydrocarbon and less sensitive to noise than indices proposed in previous works. The second one extracts endmembers via Orthogonal Subspace Projection, and then sorts the endmembers in terms of hydrocarbon indices scores, in descending order. Then, the detection map or the abundance map corresponding to the best endmember is used to map oiled areas. Both approaches give results consistent with those of studies previously conducted on the same image, and with maps built from field observations.', 'title': 'Automatic Mapping of Hydrocarbon Pollution Based on Hyperspectral Imaging', 'embedding': []}, {'id': 14980, 'abstractText': 'Context: Heuristic optimization has been of strong focus in the recent modeling of the Resource Constrained Project Scheduling Problem (RCPSP), but lack of evidence exists in systematic assessments. New solution methods arise from random evaluation of existing studies. Objective: The current work conducts a secondary study, aiming to systemize existing primary studies in heuristic optimization techniques applied to solving classes of RCPSPs. Method: The systemizing framework consists of performing a systematic mapping study (SM), following a 3-steped protocol. Results: 371 primary studies have been depicted from the multi-stage search and filtering process, to which inclusion and exclusion criteria have been applied. Results have been visually mapped in several distributions. Conclusions: Specific RCPSP classes have been grounded and therefore a rigorous classification is required before performing a systematic mapping. Focusing on recent developments of the RCPSP (2010-2015, a strong interest has been acknowledged on solution methods incorporating AI techniques in meta- and hyper-heuristic algorithms.', 'title': 'Heuristic optimization for the resource constrained Project Scheduling Problem: A systematic mapping', 'embedding': []}, {'id': 14981, 'abstractText': 'Communication plays an important role in Agile Software Development (ASD). In each ASD practice (e.g., stand-up or retrospective meetings), different communication practices and channels are adopted by different companies. Several works have analyzed the impact of communication channels and practices. However, there are no secondary studies summarizing their impact on ASD. This study presents a Systematic Mapping Study (SMS) that aggregates, summarizes, and discusses the results of 25 relevant primary studies concerning the impact of communication channels and practices in ASD. We followed the well-known systematic mapping methodology in software engineering and analyzed empirical studies published before the end of June 2018. The results of our study have yielded several strategies that can be adopted by practitioners. Communication practices are context dependent. In the case of a distributed team, blended usage of rich-media communication tools, such as shared mind-map tools, videoconferencing, and promoting the exchange of team members between teams, is beneficial. In conclusion, communication can be expensive if teams do not apply the right strategies. Future research direction is to understand how to maximize product quality while reducing communication cost and how to identify the most beneficial communication strategy for the different stages of ASD.', 'title': 'Lessons Learned on Communication Channels and Practices in Agile Software Development', 'embedding': []}, {'id': 14982, 'abstractText': 'This article describes the mathematical modeling of the relationship between genotype and phenotype using statistical methods. The results of the study of cells of laboratory mice are used as the initial data. To build a relationship model, methods of correlation analysis, cluster analysis, ANOVA and its modification - QTL analysis are used. Graph theory is used to model the interaction of proteins in cellular processes. The QTL mapping idea is to phenotype observation and identification of the genome region on which the genotype is associated with the phenotype. With the help of molecular-genetic markers, molecular maps of individual chromosomes and genomes are made, genes and QTLs mapping are performed on them. Thus, genes with the greatest connectivity to phenotype were identified. The correlation between genotype and phenotype is studied across the full genome of the individual in this study. The data provided by the Laboratory of Molecular Genetics of the innate immunity of Petrozavodsk State University were the initial data in this research. The essence of the project, carried out jointly with the laboratory, is to study the arrays of genetic information to identify and model the relationships between the genotype and the phenotype of biological organisms. Genotyping and phenotyping were conducted based on sequencing data (determination of amino acid and nucleotide sequence) of matrix RNA. The result of the study is the construction of a mathematical model of the relationship between phenotype and genotype and the identification of groups of significant phenotypes that affect the cells processes.', 'title': 'Mathematical Modeling of Influence of the Genotype on Cell Processes by Statistical Analysis', 'embedding': []}, {'id': 14983, 'abstractText': 'Seismic ambient noise tomography is applied to central and southern Mozambique, located in the tip of the East African Rift (EAR). The deployment of MOZART seismic network, with a total of 30 broad-band stations continuously recording for 26 months, allowed us to carry out the first tomographic study of the crust under this region, which until now remained largely unexplored at this scale. From cross-correlations extracted from coherent noise we obtained Rayleigh wave group velocity dispersion curves for the period range 5–40\\ue251s. These dispersion relations were inverted to produce group velocity maps, and 1-D shear wave velocity profiles at selected points. High group velocities are observed at all periods on the eastern edge of the Kaapvaal and Zimbabwe cratons, in agreement with the findings of previous studies. Further east, a pronounced slow anomaly is observed in central and southern Mozambique, where the rifting between southern Africa and Antarctica created a passive margin in the Mesozoic, and further rifting is currently happening as a result of the southward propagation of the EAR. In this study, we also addressed the question concerning the nature of the crust (continental versus oceanic) in the Mozambique Coastal Plains (MCP), still in debate. Our data do not support previous suggestions that the MCP are floored by oceanic crust since a shallow Moho could not be detected, and we discuss an alternative explanation for its ocean-like magnetic signature. Our velocity maps suggest that the crystalline basement of the Zimbabwe craton may extend further east well into Mozambique underneath the sediment cover, contrary to what is usually assumed, while further south the Kaapval craton passes into slow rifted crust at the Lebombo monocline as expected. The sharp passage from fast crust to slow crust on the northern part of the study area coincides with the seismically active NNE-SSW Urema rift, while further south the Mazenga graben adopts an N-S direction parallel to the eastern limit of the Kaapvaal craton. We conclude that these two extensional structures herald the southward continuation of the EAR, and infer a structural control of the transition between the two types of crust on the ongoing deformation.', 'title': 'Ambient noise tomography of the East African Rift in Mozambique', 'embedding': []}, {'id': 14984, 'abstractText': 'This study provides a technical review of the current state of immersive virtual museum, heritage, and tourism focusing on workflows and challenges involved in realistic asset creation. The workflow includes two parts i.e. virtualization of historic objects and creation of environment. However, in some instances the environment itself is a cultural heritage site e.g. an old castle that can be considered as historic object. Otherwise, the environment is just a conceptual virtual place (created using traditional 3D modeling methods) to mimic museum experience, embedding smaller historic objects, which are virtualized. Although tools and technologies such as photogrammetry, 3D scanning, or aerial 3D mapping have made the process of virtualization of historic/cultural objects considerably easier for basic users, challenges and limitations still remain as these automatic processes are not always accompanied by flawless outcomes. This study addresses some of those challenges and limitations faced during preparation of experimental immersive virtual museum for exhibition purposes. This covers various ranges of topics from lighting, texturing, and topology to limitations related to opacity, dark colors, and small details. This paper also provides a comprehensive overview of the technical details when it comes to preparation of virtual cultural heritage environments specifically for immersive experiences. Areas such as user interaction, navigation, space optimization, quality and viewing distance, access, purpose and objectives, degree of realism, etc. are covered in this review. The major processes illustrated in this study include photogrammetry, aerial 3D mapping, polygon modeling, 3D sculpting, 3D painting, UV Mapping, etc. The major software/tools used in this workflow include Agisoft Photoscan, Autodesk Remake, Pixologic ZBrush, xNormal, Autodesk 3ds Max, Unity, SteamVR, HTC Vive, including other relevant plugins and scripts. However, this study is not a step by step guide or a tutorial, but a reference for the currently available technologies to create immersive virtual museum, cultural heritage, and tourism aiming to distinguish the lines between different levels of processes involved. The objective is to provide a clear understanding of the challenges involved. Based on the literature review done prior to this study, a comprehensive academic reference (covering the mentioned areas) for digital heritage researchers is lacking (to date). The authors believe that due to the increasing availability and affordability of the current immersive virtual reality technologies for basic users this is a proper time for gathering major processes/challenges involved in creation of such environments and present them in form of a comprehensive reference. Although the main focus of this study is on digital heritage, the processes undertaken and explained can be generalized to be used by researchers in other fields where applicable.', 'title': 'Workflows and Challenges Involved in Creation of Realistic Immersive Virtual Museum, Heritage, and Tourism Experiences: A Comprehensive Reference for 3D Asset Capturing', 'embedding': []}, {'id': 14985, 'abstractText': 'The ionosphere has a time-varying electron content. The electron content has a significant effect on the signals emitted in the ionosphere. For this effect to be examined, it is important to estimate and map the total electron content. In addition, historical archiving is important in terms of the typical behaviors shown by the ionosphere with long-term examinations and the extraction of instant disturbances at certain days and times. Although compressive sensing has application in many areas, the studies on the ionosphere are negligible. In addition to the studies for estimating the total electron content, compressive sensing method was used in this study. It has been shown that the ionosphere maps are sparsely structured and are estimated by compressive sensing methods. The estimation performances of random and clustered observation methods is compared.', 'title': 'Construction of TEC Maps Using Compressive Sensing', 'embedding': []}, {'id': 14986, 'abstractText': 'HD\\xa0170582 is an interacting binary of the double periodic variable (DPV) type, showing ellipsoidal variability with a period of 16.87\\xa0d along with a long photometric cycle of 587\\xa0d. It was recently studied by Mennickent et\\xa0al., who found a slightly evolved B-type star surrounded by a luminous accretion disc fed by a Roche lobe overflowing A-type giant. Here we extend their analysis presenting new spectroscopic data and studying the Balmer emission lines. We find orbitally modulated double-peak Hα and Hβ emissions whose strength also vary in the long term. In addition, Doppler maps of the emission lines reveal sites of enhanced line emission in the first and fourth velocity quadrants, the first one consistent with the position of one of the bright zones detected by the light-curve analysis. We find a difference between Doppler maps at high and low stage of the long cycle; evidence that the emission is optically thicker at high state in the stream-disc impact region, possibly reflecting a larger mass transfer rate. We compare the system parameters with a grid of synthetic binary evolutionary tracks and find the best-fitting model. The system is found to be semi-detached, in a conservative Case-B mass transfer stage, with age 7.68 × 10<sup>7</sup>\\xa0yr and mass transfer rate 1.6 × 10<sup>−6</sup> M<inf>⊙</inf>\\u2009yr<sup>−1</sup>. For five well-studied DPVs, the disc luminosity scales with the primary mass and is much larger than the theoretical accretion luminosity.', 'title': 'Doppler tomography of the Double periodic variable HD\\xa0170582 at low and high stage', 'embedding': []}, {'id': 14987, 'abstractText': 'Advance organizer strategies based on cognitive theories have been found to have an impact on learning in the early studies. However, recent studies have found inconclusive results when type and format of the advance organizers were taken into account. This study investigated the impact of different formats of advance organizers (graphic with text explanation, voice narration, and concept map) on learners understanding of science concepts. A total of 136 high school students participated in this study. A factorial design was utilized for the experiment using format of advance organizers as the independent variable while learners achievement on retention and transfer test regarding the learning content as the dependent variables. The findings showed that using graphical advance organizers, especially the concept map, worked effectively in achieving high school students understanding of the learning content. The results showed that using concept map was likely to conceptually anchored viewers attention on key elements of reading content. When designing instructional materials, it is advised to take the advantage of graphical organizers to achieve understanding of abstract science concepts. Implications and suggestions for effective instructional design and for future studies are provided in this paper.', 'title': \"Investigations of the Effect of Format of Advance Organizers on Learners' Achievement on Understanding of Science Concepts\", 'embedding': []}, {'id': 14988, 'abstractText': 'This research full paper characterizes the literature on academic initiatives to foster computational thinking and programming (CT&amp;P) in Brazilian K-12 education. Context: Mapping and analyzing the diversity of experiences and studies that address CT&amp;P in K-12 education can bring valuable data to researchers. This work delimits such study to the Brazilian scenario to allow a more in-depth view, given the Brazilian context. Previous surveys and systematic mapping studies present recent publications in major Brazilian computing journals and conferences. Although they offer important contributions, they do not comprehensively cover the Brazilian literature on CT&amp;P in K-12 education, since they focus the search in the Brazilian Computer Society publications alone. Objective: This work proposes to characterize the literature on CT&amp;P in K-12 education in Brazil. Results: through a systematic mapping study, we collected information on year, venue, type, K-12 education stage and modality, methodological contexts, and used tools and programming languages from 338 selected primary studies from 2001 until 2016. Conclusions: there is a significant increase in the number of Brazilian studies in the latest years, showing a growing interest on this research area as well as several trends and gaps to be addressed by both researchers and practitioners.', 'title': 'A Mapping Study of Computational Thinking and Programming in Brazilian K-12 Education', 'embedding': []}, {'id': 14989, 'abstractText': \"Mapping and quantification of biomass changes is critical to understanding mangrove carbon sequestration, conservation, and restoration. Few previous studies have focused on mangrove biomass changes based on high spatial resolution images, particularly for disturbed and recovering areas. This study developed an effective model to estimate and map mangrove aboveground biomass dynamic change between 2010 and 2016 on Qi'ao Island in South China. The study area includes native Kandelia candel (K. candel) and planted Sonneratia apetala (S. apetala) mangrove species within the largest planted area in China. Models were developed using WorldView-2 images, digital surface models (DSMs), and the random forest algorithm. Accuracies of the model were assessed using multiyear field samples. DSMs were identified as the most important variable for model accuracy, reducing relative error by up to 3.14%. Three models were developed: a model for 2010, another model for 2016, and a combined model for 2010 and 2016. Compared with the 2010 (RMSE = 41.03 t/ha, RMSEr = 24.31%) and 2016 (RMSE = 39.92 t/ha, RMSEr = 23.40%) models, the combined model (RMSE = 50.99 t/ha, RMSEr = 30.48%) only increased the relative error by 6.17% and 7.08%, respectively. Mangrove biomass maps generated from the most accurate models showed total biomass increased from 23270.43 to 39819.03 tons by up to 71.11% over the study period. K. candel total biomass decreased by 36.5% due to Derris trifoliata challenge. S. apetala total biomass increased by 74.79% due to reforestation programs, achieving aboveground biomass accumulation of 4.17 t/ha for stands that existed in 2010. This study provides insights into biomass dynamic change in disturbed and recovering mangrove areas. Future studies should consider using LiDAR techniques to obtain actual tree height applied for biomass estimation instead of DSM.\", 'title': 'Estimating and Mapping Mangrove Biomass Dynamic Change Using WorldView-2 Images and Digital Surface Models', 'embedding': []}, {'id': 14990, 'abstractText': 'Light detection and ranging (LiDAR) data classification provides useful thematic maps for numerous geospatial applications. Several methods and algorithms have been proposed recently for LiDAR data classification. Most studies focused on object-based analysis because of its advantages over per-pixel-based methods. However, several issues, such as parameter optimization, attribute selection, and development of transferable rulesets, remain challenging in this topic. This study contributes to LiDAR data classification by developing an approach that integrates ant colony optimization (ACO) and rule-based classification. First, LiDAR-derived digital elevation and digital surface models were integrated with high-resolution orthophotos. Second, the processed raster was segmented with the multiresolution segmentation method. Subsequently, the parameters were optimized with a supervised technique based on fuzzy analysis. A total of 20 attributes were selected based on general knowledge on the study area and LiDAR data; the best subset containing 12 attributes was then selected via ACO. These attributes were utilized to develop rulesets through the use of a decision tree algorithm, and a thematic map was generated for the study area. Results revealed the robustness of the proposed method, which has an overall accuracy of ~95% and a kappa coefficient of 0.94. The rule-based approach with all attributes and the k nearest neighbor (KNN) classification method were applied to validate the results of the proposed method. The overall accuracy of the rule-based method with all attributes was ~88% (kappa = 0.82), whereas the KNN method had an overall accuracy of &lt;;70% and produced a poor thematic map. The selection of the ACO algorithm was justified through a comparison with three well-known feature selection methods. On the other hand, the transferability of the developed rules was evaluated by using a second LiDAR dataset at another study area. The overall accuracy and the kappa index for the second study area were 92% and 0.90, respectively. Overall, the findings indicate that the selection of a subset with significant attributes is important for accurate LiDAR data classification with object-based methods.', 'title': 'Integration of Ant Colony Optimization and Object-Based Analysis for LiDAR Data Classification', 'embedding': []}, {'id': 14991, 'abstractText': 'Effective assessment of cyber risks in the increasingly dynamic threat landscape must be supported by artificial intelligence techniques due to their ability to dynamically scale and adapt. This article provides the state of the art of AI-supported security risk assessment approaches in terms of a systematic mapping study. The overall goal is to obtain an overview of security risk assessment approaches that use AI techniques to identify, estimate, and/or evaluate cyber risks. We carried out the systematic mapping study following standard processes and identified in total 33 relevant primary studies that we included in our mapping study. The results of our study show that on average, the number of papers about AI-supported security risk assessment has been increasing since 2010 with the growth rate of 133% between 2010 and 2020. The risk assessment approaches reported have mainly been used to assess cyber risks related to intrusion detection, malware detection, and industrial systems. The approaches focus mostly on identifying and/or estimating security risks, and primarily make use of Bayesian networks and neural networks as supporting AI methods/techniques.', 'title': 'A Systematic Mapping Study on Approaches for Al-Supported Security Risk Assessment', 'embedding': []}, {'id': 14992, 'abstractText': 'Geomorphological and geological studies of the seafloor benefit today from both ROV exploration and from acquisition of high resolution bathymetric data. Although both represent significant improvements to study submarine domains, the understanding of the studied objects is made more difficult than on land given the limited visual perception provided by the ROV camera due to the attenuation of light in the water and the need to use artificial illumination. Likewise, mapping can be performed using GIS software for digital elevation models and its derivatives (e.g. slope or shade raster), mostly in a 2D map view only. So, the submarine studies lack the field survey stage performed in classical onshore works that allows clear visualization and appreciation of the studied objects. Our aim is to develop a solution allowing the visualization of Digital Elevation Models (DEM) and 3D models derived from Structure-from-Motion (SfM) within a virtual reality environment, and to use these data for geomorphological and geological analysis. For this, we use an Oculus Rift headset, Touch controllers, and the Unity game engine, with GIS-like interaction capabilities. The free and open Unity package that we are developing allows, at this stage, data visualization and working at a 1:1 scale in a georeferenced system. The user can therefore move freely within a 3D immersive environment that includes custom topographic data. For quantitative observations, we develop tools (ruler, compass) allowing measurements similar to those performed during geomorphological or geological field work. We also add the possibility to map objects. Digitizing in 3D is achieved with a laser pointed towards the data, providing great precision. The user can thus create pseudo shapefiles using the same three graphic primitives, and that are compatible with standard GIS software. Beside these functionalities, we also implement a spatial user interface displaying help and information and a teleportation tool preventing motion sickness. The users that have tested this solution are enthusiastic and agree that it helps to better appreciate and understand the shape and geometry of the studied objects. It was also used to present and explain 3D models of outcrops to master students. Further developments will port the solution for other headsets, facilitate the data import (e.g., standard file formats for 3D objects and DEMs), create and manage of multiple layers of shapefiles, and include multiplayer online gaming capabilities to allow remote co-working with colleague(s) at other distant locations, or a whole classroom.', 'title': 'Performing submarine field survey without scuba gear using GIS-like mapping in a Virtual Reality environment', 'embedding': []}, {'id': 14993, 'abstractText': \"Startup companies are seeking for a business model by means of innovative products and need external investments until they can stabilize this product in the market and then start to grow to become a mature company. Nevertheless, many of these companies fail before the growth phase, emerging the need to seek for methods that help these companies to achieve their goals. This paper intends to identify the techniques, practices and tools used in software development on startups and from the results present a knowledge base that can be transferable to the industry. A systematic mapping study was conducted using a classification schema to attest the primary studies quality as well as a exclusion and inclusion criteria to select them. A total of 112 studies are found at the end of the searches and 19 of them were selected to form the mapping study. From these studies a total of 24 techniques, 31 practices and 37 tools were found. As a result the mapping study presents a technical knowledge base that seeks to fill the research gap and that can also be used as a starting point for both researchers and startups' practitioners.\", 'title': 'Technical Aspects of Software Development in Startups: A Systematic Mapping', 'embedding': []}, {'id': 14994, 'abstractText': \"Urban growth indicates rapid increasing in urban population, city significance level and economic expansion. The present and historical information is necessary in urban spatial analysis study and future urban growth planning. These fast urban sprawls require mapping and monitoring of urban area using geospatial and temporal imagery. This study examines and assesses urban expansion of Bhilwara city using Remote Sensing (RS) images and Geo-graphical Information System (GIS) with help of Shannon's entropy model. In this research, urban growth areas were mapped and analysed for a period of 64 years using Shannon's entropy by multi spatial and temporal RS data. The key advantages of RS data are its availability over large coverage area and to detect temporal changes. The Survey of India (Sol) toposheet and Landsat data (1972, 1981, 1991 &amp; 2001), Liss-III (2011) and Sentinel 2B (2018) are used for preparation of GIS based outputs maps of the Bhilwara City and surrounding areas. The finding of this study will be helpful in the understanding the present and projecting the future growth scenario. Further, this study will also help in efficient planning for systematic and controlled urban sprawl in the study area.\", 'title': 'Urban Growth Assessment Using Remote Sensing, GIS and Shannon’s Entropy Model: A Case Study of Bhilwara City, Rajasthan', 'embedding': []}, {'id': 14995, 'abstractText': 'Timely and accurately monitoring of cropland use is of critical significance to ensure food security. Shaanxi province of China has experienced major land use and land cover changes due to the rapid urban sprawl and the implement of the Grain for Green Project. This paper aimed to investigate cropland use changes in Shaanxi from 2000 to 2015 based on MODIS EVI time-series data by utilizing shape-matching cropping index mapping method. The shape-matching method we formerly developed was proved to be an efficient method in mapping cropland use intensity. We firstly divided the study area into six different geomorphic zones, and then randomly selected training samples in each zone. After that, the cropping status for each sample was visually interpreted and used as reference data to train the optimal thresholds for detect cropping index in each zone with the shape-matching cropping index mapping model. Then the cropland use maps for 2000 and 2015 was derived. Finally, the cropland use changes were obtained by spatial analysis of these two cropland use maps. Results show that: (1) The dominant land use is double cropping and non-grain crop production in Guangzhong Plain both in 2000 and 2015. And most cropland in other zones was used for non-grain crop production in the two study years. (2) There were totally 2 585.3 km<sup>2</sup> cropland newly developed and 7 042.8 km<sup>2</sup> cropland lost in the past fifteen years. Areas with increased use intensity is 4 084.9 km<sup>2</sup>, while areas with decreased use intensity is 10 889.3 km<sup>2</sup> during this period. (3) Extensive cropland use changes were taken places in the Wind Drift Sand Region, the Loess Plateau, the Guanzhong Plain and the Hanjiang Basin. Our findings could provide great support for local agriculture management.', 'title': 'Cropland Use Change Analysis in Shaanxi Province of China Based on the Shape-Matching Cropping Index Mapping Method', 'embedding': []}, {'id': 14996, 'abstractText': 'The study involves development of ultrasound-based regularized Nakagami imaging (RNI) that improves the quality of Nakagami image. The feasibility of RNI to image the spatial and temporal evolution of hotspot during microwave (MW) hyperthermia experiment was explored on in-vitro polyacrylamide gel (PAG)-agar based phantoms. The normalized cumulative differential regularized Nakagami (NCDRN) maps were estimated from the envelope of beamformed ultrasound radiofrequency (RF) data using proposed RNI technique. The NCDRN maps were estimated at different time instants for the entire duration of the experiment. The experiments were carried out on phantoms at power level of 12 W fed to the microwave antenna. The contour maps of the NCDRN and the ground truth temperature map, obtained using an infra-red (IR) thermal camera corresponding to ultrasound imaging plane, showed that NCDRN was able to locate the axial and lateral co-ordinates of the hotspot with an error of &lt;; 1.5 mm axially and &lt;; 0.4 mm laterally. This preliminary in-vitro study demonstrates that NCDRN maps estimated using the regularized Nakagami imaging may have potential in imaging and evaluating the spatio-temporal evolution of hotspot and may help in the development of ultrasound-based image guided hotspot monitoring system for microwave hyperthermia.', 'title': 'Regularized Nakagami Ultrasound Imaging for Microwave Hyperthermia monitoring', 'embedding': []}, {'id': 14997, 'abstractText': \"Transcranial Magnetic Stimulation is a noninvasive magnetic stimulation of the motor cortex and motor pathways. Responses obtained from the target muscles through TMS can be evaluated electrophysiologically. There are 20 healthy volunteers who use the right hand actively in the study. There are active right hand 20 healthy volunteers in the study. Two sEMG electrodes are placed in the Abductor Pollicis Brevis (APB) and Orbicularis Oris (O.oris) muscles. APB muscle constitutes a hand representative in the cortical motor mapping. O.Oris muscle constitutes a face representative in the cortical motor mapping. The position, amplitude and latency values of muscular action potentials induced by magnetic stimulation are determined by signal processing methods. The results are mapped on 3-D human model. When the right-hand representation map is examined, the hot spot of the APB muscle is found as the C1. While the motor potential of the hot spot is 6.89(±1.67) mV, the latency time is 20.98(±0.87) ms. When the right-face representation map is examined, the hot spot of the O.Oris muscle's found as the F5. While the motor potential of the hot spot is 2.83(±1.81) mV, the latency time is 4.95(±0.05) ms. Moreover, the face representation intersects with the hand representation at some points and is active at distances close to the hand representation.\", 'title': 'Mapping of Hand and Face Representations Areas in Cerebral Cortex with Transcranial Magnetic Stimulation', 'embedding': []}, {'id': 14998, 'abstractText': 'To evaluate the ESHERSs and determine their efficiency to measure environmental sustainability, we tackle this problem as a classification assignment. This study benchmark three ESHERSs: UI GreenMetric, Times Higher Education Impact ranking, and STARS (Sustainability Tracking, Assessment Rating System) by AASHE (the association for the advancement of sustainability in higher education). Next, we recruited a group of experts who mapped the ESHERS indicators to the SDGs indicators. Then, we use NLP techniques to classify (map) the ESHERS indicators to the SDGs indicators. Since most of the ESHERS indicators and the SDGs indicators are in the form of short text, we use the query expansion technique to make the NLP techniques more effective. Each ESHERS indicator and its expanded text represents a document. And, each SDG indicator and its expanded text represents a document. We took the expanded text from the description of the ESHERS indicators and the description of SDG indicators, forming the corpus for our study. Then, we used document similarity to find the similarity between every pair of the corpus documents. We used different similarity measures to see the similarity between the forms. Then, we used a voting system to map the ESHERSs indicators to the SDGs indicators. The proposed system was able to automatically map the underlying ranking systems indicators to the UN SDGs with 99% accuracy compared to the experts mapping.', 'title': 'Automated Mapping of Environmental Higher Education Ranking Systems Indicators to SDGs Indicators using Natural Language Processing and Document Similarity', 'embedding': []}, {'id': 14999, 'abstractText': 'Human skin temperature mapping provides abundant information of physiological conditions of human body, which provides supplementary or alternative indicators for disease monitoring or diagnosis. The existing models of temperature mapping or temperature field distribution of human skin are generally established by finite element method. Due to the complexity of biological systems, it is challenging to achieve high accuracy mathematical models of temperature field of human skin. The goal of this study is to establish human skin temperature three-dimensional (3-D) mapping platform by integrating optical fibers and improved genetic algorithm-back propagation (GA-BP) neural network. The proposed data-driven method is capable of acquiring entire human skin temperature 3-D mapping by simply measuring a few points on human skin. Multiple experiments were conducted to validate the proposed method on different areas of human skin in different ambient environments. In each experiment setting, the measured data and the model output data were compared. The mean absolute error in all the validation experiments is 0.11 °C, which is lower than that in the state of the art using physical modeling for skin temperature prediction and more close to clinical accuracy. The results show that the proposed approach is accurate and reliable, which may provide a platform technology for human skin temperature mapping that can be used in both medical and scientific studies as well as home monitoring.', 'title': 'An Optical Fiber-Based Data-Driven Method for Human Skin Temperature 3-D Mapping', 'embedding': []}, {'id': 15000, 'abstractText': 'The new era of digitization increases the impact of IT on the overall success of organizations. Even though the business generally recognizes the importance of IT, business-IT alignment is still considered a major challenge by IT executives. A widely adapted Enterprise Architecture Management tool addressing this challenge is the business capability map. However, there are only few approaches specifying the creation of business capability maps. This paper presents a case study describing the initiation of a business capability map at a medium-sized, state-controlled organization following an approach to business capability map implementation presented by The Open Group. Based on the case study, we detail each phase of the approach presenting necessary activities and resulting artifacts. Additionally, lessons learned are presented with the major finding being that an involvement of the whole business leadership leads to a better business-IT alignment, a common language, and a better understanding between all business units. Furthermore, a business capability map provides a suitable tool for structuring strategy development.', 'title': 'Reporting from the Implementation of a Business Capability Map as Business-IT Alignment Tool', 'embedding': []}, {'id': 15001, 'abstractText': 'Paddy rice is one of the most important crops to offer daily food for human beings and has a strong influence on food security and market stability. Therefore, it is of significance to obtain first-hand information related to rice such as rice planting extent. In order to timely and accurately obtain the spatial distribution and temporal variations of rice plants, remote sensing techniques have been introduced and applied to achieve this goal, among which the phenological method is unsupervised and can be used to automatically extract rice. However, the phenological methods are strongly dependent on the weather conditions, and the resultant extraction results might be damaged or incomplete. Additionally, the machine learning-based methods are less influenced by the weather contaminations but are restricted by the availability of training samples. In consideration of the challenges of each approach, this study combines these two approaches to extract rice by using the initial phenology-based rice samples as the training samples of machine learning models, thus obtaining more complete results and high accuracy. This strategy is tested in the Central Valley of California, USA, one of the main rice-producing areas in the United States, and is compared with the rice maps derived from Cropland Data Layer (CDL) from the U.S. Department of Agriculture (USDA). The results show that through the incorporation of the phenology- and machine learning-based methods, the derived rice maps present high consistency with the CDL-rice map, and high accuracy is achieved in terms of OA and Kappa values reaching up to 0.96 and 0.86 respectively, while the purely phenology-based rice map shows great misclassifications thus leading to a sharp decrease in the Kappa value (0.49). This comparison proves the effectiveness of this strategy. The satisfactory results in this study show the possibility of extending this strategy to other regions for rice mapping.', 'title': 'Rice extraction in the Central Valley using multi-temporal Landsat images', 'embedding': []}, {'id': 15002, 'abstractText': \"Freehand gesture interaction has long been proposed as a `natural' input method for Augmented Reality (AR) applications, yet has been little explored for intensive applications like multiscale navigation. In multiscale navigation, such as digital map navigation, pan and zoom are the predominant interactions. A position-based input mapping (e.g. grabbing metaphor) is intuitive for such interactions, but is prone to arm fatigue. This work focuses on improving digital map navigation in AR with mid-air hand gestures, using a horizontal intangible map display. First, we conducted a user study to explore the effects of handedness (unimanual and bimanual) and input mapping (position-based and rate-based). From these findings we designed DiveZoom and TerraceZoom, two novel hybrid techniques that smoothly transition between position- and rate-based mappings. A second user study evaluated these designs. Our results indicate that the introduced input-mapping transitions can reduce perceived arm fatigue with limited impact on performance.\", 'title': 'Augmented Reality Map Navigation with Freehand Gestures', 'embedding': []}, {'id': 15003, 'abstractText': \"Concept map has been widely employed to promote knowledge understanding in various academic disciplines. This study developed an extended open-ended concept map to enhance meaningful learning through two phases of concept map construction that calls Extended Scratch-Build (ESB). The study included software development process, experimental use, and results analysis. The functionality test showed that this learning support application could run properly. We examined that the proposed design of the ESB concept map can enhance meaningful learning. The experimental results also indicated that the ESB concept map had a significant impact on learners' knowledge understanding, which was represented by using the paired t-test statistical analysis on the pre-test and post-test scores.\", 'title': 'Extended Scratch-Build Concept Map to Enhance Meaningful Learning', 'embedding': []}, {'id': 15004, 'abstractText': 'In this article, the issue of soil passability classification is discussed, using field measurements and soil spatial databases. The most detailed cartographic studies available in Poland were used: a soil-agricultural map at scale 1: 25 000. Maps concerning the passability of soils were developed by conducting field measurements with specialized equipment - an electronic cone penetrometer and the type of combat vehicle with the number of its passes in the analysed area. For this purpose, two indicators were used: the Cone Index and the Vehicle Cone Index. The passability of soils was classified into three classes of passability (GO, SLOW-GO and NO-GO TERRAIN). The obtained maps were analysed, which showed very good conditions of passability in the study area. Nevertheless, this condition significantly changes with the increase in the number of passes by the analysed combat vehicles. The article shows that it is possible to use the existing database of soils in Poland and the results of penetrometric measurements to develop maps of passability, taking into consideration soil conditions. The developed maps may provide geospatial support in planning military operations and crisis management.', 'title': 'Soil passability analysis in the open terrain', 'embedding': []}, {'id': 15005, 'abstractText': 'Hyperspectral rare earth elements detection in space borne and near-field acquired images becomes more and more important for global exploration. In comparison to classic exploration methods, the benefit of hyperspectral surveys is the fast and in-situ generation of spatial information. Current hyperspectral investigations do more and more include rare earth element mappings - one new tool for hyperspectral rare earth mapping is the REEMAP algorithm. So far it is trained for five rare earth elements (erbium, dysprosium, holmium, neodymium and thulium). Previous versions of REEMAP did not map samarium. The here presented study focusses on the extension of REEMAP to identify samarium and presents a detailed mapping of the samarium and dysprosium occurrences of a two-carbonatite units containing outcrop (rauhaugites - dolomitic carbonatites and rødbergites - hematitic carbonatites) at Fen Complex, Norway. Four absorption bands of samarium were scrutinized for their shape characteristics in order to extend REEMAP for the detection of samarium. REEMAP was extended with these newly defined filter parameters. The mapping result for the investigated outcrop show that two absorption bands proved to be robust enough to be used in the REEMAP algorithm. The two remaining absorption bands are superimposed by H<sub>2</sub>O absorptions and are therefore not recommended for space borne or near-field hyperspectral analyses. However, the resulting samarium map shows the two-rock units represented by different samarium concentration levels and revealed a gradual increase of samarium towards the top of the rauhaugites rock unit. This study shows that REEMAP can be trained for the detection of samarium, especially for two of the investigated absorption bands (1250 and 1567 nm), and that REEMAP helps for in-situ interpretations of REE ore distributions.', 'title': 'Rare earth element detection from near-field to space - samarium detection using the REEMAP algorithm', 'embedding': []}, {'id': 15006, 'abstractText': \"The purpose of this preliminary study is to introduce and investigate concept map as a tool for new teaching and learning strategy in power engineering course in higher education system. Participants included 45 students from second year in semester one and two topics from the power engineering syllabus are selected as a pilot study. The students were asked to create their own concept map and allowed to have a discussion in a group. To understand the impact of this learning style, questionnaires are designed and distributed among the students to gauge the students' perception towards concept map. The results reveal that majority of the students perceive that concept map is a useful method to get a big picture before engaging to a new topic. Our results indicate this concept map has the potential to improve the teaching and learning style process in power engineering course.\", 'title': 'A Preliminary Study of the Implementation of Concept Maps for Teaching and Learning Strategies in a Power Engineering Course', 'embedding': []}, {'id': 15007, 'abstractText': 'In positron emission tomography (PET), a subject may be scanned two or more times to monitor longitudinal functional changes. These images often appear very similar except for localised, relatively small regions of change. This observation led to the proposal of the maximum a posteriori simultaneous longitudinal reconstruction (MAP-SLR) method, which reconstructs longitudinal datasets together with regularisation to encourage sparse difference images between pairs of scans. In this work we extend MAP-SLR to application to a multi-scan treatment response simulation study. To do this, five 2D [18F]fluorodeoxyglucose head scan datasets (designed to emulate a brain tumour longitudinal study) were simulated and then reconstructed with MAP-SLR. The resulting images were compared to: maximum likelihood expectation-maximisation (MLEM) reconstructions; longitudinally smoothed MLEM reconstructions; and MLEM applied to a reference dataset with five times the number of counts. When using MAPSLR, the noise (in terms of regional coefficient of variation) in a longitudinally unchanging white matter region was reduced and with sufficient regularisation these noise levels approached the high counts reference case. In the tumour, whilst a longitudinal bias is obtained with MAP-SLR, the bias is much smaller than that obtained when performing a noise-matched longitudinal smooth on MLEM reconstructions. With an appropriate level of regularisation the tumour bias is small enough to produce reconstructed images which preserve the longitudinal changes seen in the independent dataset MLEM reconstructions, but with noise reduction of 40% in regions which do not change. The results suggest that MAP-SLR is a simple and effective way of achieving noise reduction in longitudinal PET imaging. Future work will involve application specific testing and investigation into the inclusion of other longitudinally defined penalties into the simultaneous reconstruction.', 'title': 'Longitudinal Multi-Dataset PET Image Reconstruction', 'embedding': []}, {'id': 15008, 'abstractText': 'This paper aims to study the structure of the singular manifold of a fragment of protein backbone using the method of robot kinematics. By modeling chemical bonds as rigid links, and torsional motion around chemical bonds as revolute joints, a fragment of protein backbone turns into a kinematic linkage. The singular manifold of the forward kinematic map of this linkage contains important information about the structure of its inverse kinematics map, and the results also apply to the study of the conformation space of protein loops. By splitting the forward kinematic map into an orientation and position map, we derive equations of the singular manifolds for both maps and analyze their geometric and combinatorial structures. We discuss the application of singular manifolds in several problems of computational biology, including minimal parametrization of protein conformation space and conformation space sampling of protein loops.', 'title': 'Structure of the Singular Manifold of Protein Backbone and Its Applications', 'embedding': []}, {'id': 15009, 'abstractText': 'The amount of scientific publications is believed to get doubled every five-years. These publications are stored by citation indexes and digital libraries in the form of complete PDF or/and by extracting terms from these documents. This indexing behavior poses several challenges for the scientific community as well as for digital repositories in terms of handling the advanced requirements of a user. For instance, addressing queries like “Give me those papers that contain the term “Pagerank” in their result section” may not be answered unless the papers are indexed section-wise. This issue has been focused by researchers and international prestigious challenges by top venues in the world like Semantic Publishing Challenge in ESWC. One of the important metadata extraction from research papers is the section information such as IMRAD (Introduction, Methodology, Results, and Discussion). Researchers have presented different approaches to identify and map the section-headings to IMRAD sections. The existing studies have employed parameters like dictionary terms, the template of a paper, and in-text citation frequency to map section-headings onto logical sections. The critical analysis of state-of-the-art revealed that some immensely potential features have been ignored, which might result in accurate mapping. In this study, we propose a novel approach that employs new features along with previously well-known features to map sections-headings to IMRAD. The newly proposed features are: (1) variant of In-text Citation count (2) Figure counts, (3) Table counts, and (4) subheading implicit mapping. The employed data set contains 5000 research papers, collected from CiteSeer. The evaluation of the proposed approach and comparisons with state-of-the-art three approaches revealed an improvement of 18.96%, 21.77%, and 9.50% in average precision with Ding et al, Shahid et al, and Habib et al. respectively. This research has significant implications for citation indexes and digital libraries.', 'title': 'A Systematic Approach to Map the Research Articles’ Sections to IMRAD', 'embedding': []}, {'id': 15010, 'abstractText': 'Due to the emergence of dual cameras for smart phones in recent years, synthetic refocusing using stereo data has become an important issue. For synthetic refocusing to produce satisfactory results, it is crucial to render the image with high-quality depth map, which is often obtained through a refinement process. However, most existing depth map refinement algorithms pay little attention to problems related to notorious occlusion and loss of image details. In this paper, we study how the quality of depth map affects the performance of synthetic refocusing and, based on the study, propose a new method that integrates depth information and RGB image for synthetic refocusing. A notable feature of our approach is that it formulates occlusion filling as a labeling problem and solves it by multi-layer alpha matting, resulting in a depth map with edges well-aligned with the RGB image in the occluded area and giving synthetic refocusing a realistic sensation. An evaluation of our method against previous methods is performed by comparing the estimated depth map and the refocusing results.', 'title': 'Occlusion-and-Edge-Aware Depth Estimation From Stereo Images for Synthetic Refocusing', 'embedding': []}, {'id': 15011, 'abstractText': 'Mapping lithological units of an area using remote sensing data can be broadly grouped into pixel-based (PBIA), sub-pixel based (SPBIA) and object-based (GEOBIA) image analysis approaches. Since it is not only the datasets adequacy but also the correct classification selection that influences the lithological mapping. This research is intended to analyze and evaluate the efficiency of these three approaches for lithological mapping in semi-arid areas, by using Sentinel-2A data and many algorithms for image enhancement and spectral analysis, in particular two specialized Band Ratio (BR) and the Independent component analysis (ICA), for that reason the Paleozoic Massif of Skhour Rehamna, situated in the western Moroccan Meseta was chosen. In this study, the support vector machine (SVM) that is theoretically more efficient machine learning algorithm (MLA) in geological mapping is used in PBIA and GEOBIA approaches. The evaluation and comparison of the performance of these different methods showed that SVM-GEOBIA approach gives the highest overall classification accuracy (OA ≈ 93%) and kappa coefficient (K) of 0, 89, while SPBIA classification showed OA of approximately 89% and kappa coefficient of 0, 84, whereas the lithological maps resulted from SVM-PBIA method exhibit salt and pepper noise, with a lower OA of 87% and kappa coefficient of 0, 80 comparing them with the other classification approaches. From the results of this comparative study, we can conclude that the SVM-GEOBIA classification approach is the most suitable technique for lithological mapping in semi-arid regions, where outcrops are often inaccessible, which complicates classic cartographic work.', 'title': 'Pixel and Object-Based Machine Learning Classification Schemes for Lithological Mapping Enhancement of Semi-Arid Regions Using Sentinel-2A Imagery: A Case Study of the Southern Moroccan Meseta', 'embedding': []}, {'id': 15012, 'abstractText': 'The disastrous effects of global warming urge developed and developing countries to invest in Information Technology solutions to reduce flood risks. More recently, flood risk and hazard mapping system is slowly adopted in the Philippines, a developing country. However, acceptance of flood risk and hazard mapping system in the Philippines is underrepresented in the literature. Hence, this paper aims to present users’ acceptance of flood risk and hazard mapping system by using the Technology Acceptance Model, through a survey conducted among different users. Results of the study show an affirmative acceptance of flood risk and hazard mapping system. Also, the relationships between perceived usefulness and behavioral intention, perceived ease of use and behavioral intention, perceived ease of use and perceived usefulness, relevance and perceived usefulness, domain knowledge and perceived usefulness, self-efficacy and perceived ease of use, computer literacy and perceived ease of use are all observed. This study contributes by providing the first empirical evidence of investigation on acceptance of flood risk and hazard mapping system in the Philippines.', 'title': 'User Acceptance of Flood Risk and Hazard Mapping System: A Field Survey in the Philippines', 'embedding': []}, {'id': 15013, 'abstractText': 'Autonomous driving in an urban environment with surrounding agents remains challenging. One of the key challenges is to accurately predict the traversability map that probabilistically represents future trajectories considering multiple contexts: inertial, environmental, and social. To address this, various approaches have been proposed; however, they mainly focus on considering the individual context. In addition, most studies utilize expensive prior information (such as HD maps) of the driving environment, which is not a scalable approach. In this study, we extend a deep inverse reinforcement learning-based approach that can predict the traversability map while incorporating multiple contexts for autonomous driving in a dynamic environment. Instead of using expensive prior information of the driving scene, we propose a novel deep neural network to extract contextual cues from sensing data and effectively incorporate them in the output, i.e., the reward map. Based on the reward map, our method predicts the ego-centric traversability map that represents the probability distribution of the plausible and socially acceptable future trajectories. The proposed method is qualitatively and quantitatively evaluated in real-world traffic scenarios with various baselines. The experimental results show that our method improves the prediction accuracy compared to other baseline methods and can predict future trajectories similar to those followed by a human driver.', 'title': 'Incorporating Multi-Context Into the Traversability Map for Urban Autonomous Driving Using Deep Inverse Reinforcement Learning', 'embedding': []}, {'id': 15014, 'abstractText': \"Short grass grazing lawn patches are significant components of habitat heterogeneity in southern African savannah ecosystems. Accurate maps of grazing lawn distribution is essential to enhance understanding of important ecosystem processes such as mega-herbivore population dynamics, nutrient cycling and plant community composition. The inherent heterogeneity of savannah landscapes however creates significant challenges for accurate discrimination of vegetation components and thus grazing lawn detection. Recent studies favour very high spatial resolution (VHR) multi-spectral imagery for dealing with this challenge. However, such data are costly for use in operational management. Planet Labs, through Norway's International Climate and Forests Initiative (NICFI), now grant free access to high-resolution, analysis-ready mosaics over the tropics, with great potential for fine-scale vegetation mapping. However, the spectral characteristics of these data are limited and fail to resolve the spectral similarity of different savannah vegetation components. We address these issues using Gram-Schmidt transformation to fuse Planet Basemaps and Sentinel-2A images for grazing lawn detection within the Lower Sabie region of Kruger National Park, South Africa. The original and fused images were classified using a random forest approach. Overall, the fused image achieved the best grazing lawn detection accuracy (0.85) and general map accuracy (0.72) results compared to Sentinel-2 (0.67 and 0.62) and Planet basemap (0.64 and 0.62 respectively). Our findings provide a foundation for cost-effective and accurate high spatial resolution vegetation mapping in heterogenous savannah landscapes. Further studies will investigate the potential of multi-temporal fused data and object-based approaches for enhanced savannah vegetation mapping\", 'title': 'Fusion of Sentinel-2 Data with High Resolution Open Access Planet Basemaps for Grazing Lawn Detection in Southern African Savannahs', 'embedding': []}, {'id': 15015, 'abstractText': \"Path planning in a dynamic environment is required not only efficiently but also with high safety. To achieve this, it is necessary to understand people's walking activity in the space, and studies of planning paths focused on walking paths of pedestrian have been widely done. In these studies, information getting from walking paths does not include spatial features of people's walking activity such as dynamics of their walking. A walking behavior is not always smooth from the viewpoints of changes of walking speed and moving directions. For example, people should accelerate or decelerate their walking speed according to environmental settings or their purposes of walking. In addition, moving directions of people's walking also depend on the environmental structures and settings. Therefore, people's walking activity should be understood including walking dynamics. To apply the information of the walking activity to the path planning for mobile robot navigation, there are two requirements; one is that the information should be described in form that a robot system is able to use, and another one is that it should be described without lacking spatial information. To realize these, we propose that an environmental map describing the walking activity of people including walking dynamics. More specifically, we build the environmental map as a grid map that is one of the often-used maps for a mobile robot. From the results of the experiment, it was shown that the proposed environmental map was possible to determine spatial features.\", 'title': 'Environmental Map Building to Describe Walking Dynamics for Determination of Spatial Feature of Walking Activity', 'embedding': []}, {'id': 15016, 'abstractText': 'Speaking style conversion (SSC) is the technology of converting natural speech signals from one style to another. In this study, we aim to provide a general SSC system for converting styles with varying vocal effort and focus on normal-to-Lombard conversion as a case study of this problem. We propose a parametric approach that uses a vocoder to extract speech features. These features are mapped using parallel machine learning models from utterances spoken in normal style to the corresponding features of Lombard speech. Finally, the mapped features are converted to a Lombard speech waveform with the vocoder. A total of three vocoders (GlottDNN, STRAIGHT, and Pulse model in log domain (PML)) and three machine learning mapping methods (standard GMM, Bayesian GMM, and feed-forward DNN) were compared in the proposed normal-to-Lombard style conversion system. The conversion was evaluated using two subjective listening tests measuring perceived Lombardness and quality of the converted speech signals, and by using an instrumental measure called Speech Intelligibility in Bits (SIIB) for speech intelligibility evaluation under various noise levels. The results of the subjective tests show that the system is able to convert normal speech into Lombard speech and that there is a trade-off between quality and Lombardness of the mapped utterances. The GlottDNN and PML stand out as the best vocoders in terms of quality and Lombardness, respectively, whereas the DNN is the best mapping method in terms of Lombardness. PML with the standard GMM seems to give a good compromise between the two attributes. The SIIB experiments indicate that intelligibility of converted speech compared to that of normal speech improved in noisy conditions most effectively when DNN mapping was used with STRAIGHT and PML.', 'title': 'Vocal Effort Based Speaking Style Conversion Using Vocoder Features and Parallel Learning', 'embedding': []}, {'id': 15017, 'abstractText': 'Semantic Mapping is a semantics-based interactive system that enables intuitive virtual content placement for projection mapping in intelligent environments. Our semantic mapping system embeds semantic information of the environment to provide a user with an easy way to control and place projected virtual items in the physical world. In contrast to traditional projection mapping that involves manual adjustments, this semantic mapping system enables efficient manipulation of virtual content through inputs from users via speech or text. To build the system, we first use a commercial depth camera for scene reconstruction and an end-to-end deep learning framework for semantic segmentation at the instance level. We illustrate the system by developing a prototype for a set of proof-of-concept, room-scale applications. The accuracy study and user study results show that the system can provide users with accurate semantic information for effective virtual content placement.', 'title': 'Semantic Mapping: A Semantics-based Approach to Virtual Content Placement for Immersive Environments', 'embedding': []}, {'id': 15018, 'abstractText': 'In this paper, the co-moving Lyapunov exponent estimation problem for a unidirectionally traffic coupled map lattice model with hyperbolic tangent local map is studied. The traffic behavior in the coupled map lattice model shows nonlinear characteristics similar to the car-following model. The nonlinear feedback method is used to study the control of the chaotic system of the unidirectionally traffic coupled map lattice model. The stability of spatiotemporal chaos in the coupled map lattice is realized. The results of numerical simulation show that there is a relationship between control results and control parameters when controlling spatiotemporal chaos to a uniform stable state in a certain phase space compression parameter region.', 'title': 'Spatiotemporal Chaos and Control of a Unidirectionally Traffic Coupled Map Lattice Model', 'embedding': []}, {'id': 15019, 'abstractText': 'The generation of benthic habitat maps relies either on direct in-situ observations made by SCUBA divers swimming in a rectilinear fashion, or on costly remote sensing techniques involving either ROVs or sonar technology. The recent commercialisation of off-the-shelf underwater drones has enhanced benthic mapping possibilities by providing a cost-effective alternative. Despite still requiring ground truthing, such drones do not rely extensively on boat-support services. In this study, the applicability and feasibility of using an underwater high-resolution optical platform to automate the generation of benthic maps is investigated. A PowerVision PowerRay [1] was used to capture underwater imagery in an embayment along the north-east coast of the island of Malta (central Mediterranean). A Machine Learning method based on Self-Organizing Maps was then implemented to automate the classification process. Results produced from this technique were evaluated in terms of their accuracy through comparisons with a benthic habitat map of the same area that was generated through conventional means in a previous study.', 'title': 'Automatic Benthic Habitat Mapping using Inexpensive Underwater Drones', 'embedding': []}, {'id': 15020, 'abstractText': \"We show how a model of visual salience that was originally developed to explain human visual search performance can suggest display design choices that reduce search time for items. The statistical saliency model proposes that the time to find an item on a visual display depends on the similarity between a target item's features and the statistical distribution of display features. In the present study, observers rated the amount of display clutter on a set of MapQuest maps containing colored pushpins. We identified a group of “high-clutter” maps and a group of “low-clutter” maps. Next, we used the statistical saliency model to choose colors for new pushpins placed on those maps. We show that the model's color assignments depend on the colors the display contains. Map designs produced using this method were tested in a visual search experiment. Search time decreased as a pushpin's predicted salience increased. In addition, choosing low salience colors led to slower search times for items on high-clutter displays than for items on low-clutter displays. The method we describe works with real images and does not require any parameter fitting. This study provides evidence that computational models of visual perception have potential as display design tools.\", 'title': 'The Statistical Saliency Model Can Choose Colors for Items on Maps', 'embedding': []}, {'id': 15021, 'abstractText': 'The increase in the crime rate numbers and a rise in the need to find better solutions to handle information about criminality is affected by the ever-changing socio-economical order of the world. Despite the number of solutions implemented for reducing crime (against women), cities continue to have an unsafe environment. The prime drawback lies in the inability to provide a prompt response in real-time when in danger. Thus, the effective utilization of technology in public safety management is important. The present state of the art solutions focus on technological innovations with limited human intervention and are insufficient in ensuring the safety of the women as and when required. To dig deeper into the root cause of preventing a crime from occurring in a particular place, it is vital to analyze the parameters and factors contributing to the crime in a community. This research applies the Information Communication Technologies (ICT) along with harnessing big data tools to identify crime hotspots and patterns. After a comprehensive literature review, it has been noted that there are different social-economic factors affecting the crime in an area. The proposed work aims to integrate the socio-economic attributes leading to increasing crime against women. Interpolation strategies used for thematic maps generation also play a major role in predicting and studying the area affected by a crime. This research initially identifies the various social-economical parameters that affect crime against women. Some of them to mention include unemployment, illiteracy, population, sex ratio, traffic, age, no. of schools, and location of liquor shops. Subsequently, a comparison of major interpolation methods used in crime mapping: Inverse Distance Weighted (IDW), Kriging, and Spline are formulated to understand the overall contribution of socio-economic factors on the crime thematic map to further ascertain if one parameter poses substantially more important than the other. The comparison of different Interpolation techniques used in pixel by pixel error analysis on high definition satellite images of the crime site, of resolution as high as 2.5m × 2.5m, is created using visualization libraries like Matplotlib and Seaborn. Finally, the thematic maps are created using the best Interpolation technique chosen and help in predicting the pattern of the crime. The proposed framework developed using Geographic Information System (GIS) based visualization and big data tools for crime mapping can then be applied in the development of user interactive platforms and designing safety strategies to help the needy in real-time. To validate the methodology, a case study is performed with real data, in the Jhunjhunu district of Rajasthan, India.', 'title': 'Comparison of Different Spatial Interpolation Techniques to Thematic Mapping of Socio-Economic Causes of Crime Against Women', 'embedding': []}, {'id': 15022, 'abstractText': 'Visual relation detection methods rely on object information extracted from RGB images such as 2D bounding boxes, feature maps, and predicted class probabilities. We argue that depth maps can additionally provide valuable information on object relations, e.g. helping to detect not only spatial relations, such as standing behind, but also non-spatial relations, such as holding. In this work, we study the effect of using different object features with a focus on depth maps. To enable this study, we release a new synthetic dataset of depth maps, VG-Depth, as an extension to Visual Genome (VG). We also note that given the highly imbalanced distribution of relations in VG, typical evaluation metrics for visual relation detection cannot reveal improvements of under-represented relations. To address this problem, we propose using an additional metric, calling it Macro Recall@K, and demonstrate its remarkable performance on VG. Finally, our experiments confirm that by effective utilization of depth maps within a simple, yet competitive framework, the performance of visual relation detection can be improved by a margin of up to 8%.', 'title': 'Improving Visual Relation Detection using Depth Maps', 'embedding': []}, {'id': 15023, 'abstractText': 'Cropping patterns are closely related to food production, cropland intensification, water resource management, greenhouse gas emission and regional climate alteration. Timely and accurate mapping of cropping patterns is urgently needed in many disciplines. However, the existing cropland-related datasets are informative at the global level, but lack regional-scale details about cropland utilizations. Thus, there is a need for better information on the area and distribution of cropping patterns at regional scales. In this study, we developed a phenology-based cropping pattern (PBCP) mapping method based on remote sensing vegetation index time series. The new method first extracted vegetation phenological metrics (start of season (SOS), end of season (EOS), growing season length (GSL) and growth amplitude (GA)) from the vegetation index time series. Then, it identified crop seasons by using the minimum crop GSL, the minimum crop GA and the maximum crop GSL, which were derived from the training samples. Finally, cropping patterns were classified based on a set of decision rules. The case study in Henan province of China showed that, the results indicated that: (1) compared with cropping index derived from the supervised classification of Landsat-5 TM images, the PBCP method provided cropping index with satisfactory accuracy of 85.3%. (2) Validation sample analysis indicated that the cropping pattern mapping accuracy was 84% for the PBCP method. Different to current cropping pattern mapping methods, the PBCP method considered crop planting information in three years in deciding the cropping pattern to map the dominant cropping patterns. It can provide new insights in agriculture related land use analysis.', 'title': 'A Phenology-Based Cropping Pattern (PBCP) Mapping Method Based on Remotely Sensed Time-Series Vegetation Index Data', 'embedding': []}, {'id': 15024, 'abstractText': \"The ocean temperature front is a narrow transition zone where the temperature changes dramatically, which can be described by gradient of temperature. The temporal and spatial variations and the patterns of ocean fronts are of great concern to researchers by tedious observation and comparison of many spatial distribution maps of ocean front at different moments. However, a particular number of spatial states may only reflect certain spatial or temporal aspects of ocean front. This study designed a collaborative interactive visualization system to simultaneously integrate temporal and spatial analysis of ocean fronts with experts' knowledge, obtaining higher analysis efficiency and more comprehensiveness. The interactive statistical charts facilitate focus + context selection of points of interest in time and space, while the interactive Map-View and Map-Gallery support spatial analysis from overview to details. Moreover, this paper uses an unsupervised learning model named Self-Organizing Mapping network (SOM) to conduct spatio-temporal cluster analysis on different ocean fronts near the China Sea. The clustering results can be customized by user's colors specification, evaluated and interactively adjusted by researchers' knowledge. The spatio-temporal patterns of clustering result can easily mined by collaborative linkage of multi-graphs including unified distance matrix (U-Matrix), component plane, feature parallel coordinates plot, Map-View and other charts. The effectiveness and usability of the proposed system are demonstrated with two case studies.\", 'title': 'OFViser: An Interactive Visual System for Spatiotemporal Analysis of Ocean Front', 'embedding': []}, {'id': 15025, 'abstractText': 'Purpose: Explore the overview of dust and heavy metal research in typical regions of China and present them in the form of knowledge maps. Through in-depth research and visual analysis, a deeper understanding of the research hotspots of dust and heavy metals is expected to provide some help for later researchers. Method: The key words of “heavy metal exposure” and “heavy metals, dust” were searched in the Chinese Journal Full-text Thematic Database (CNKI), and the relevant literature on dust and heavy metals was gradually selected through extensive reading and intensive reading according to the literature inclusion and exclusion criteria. Relevant documents were summarized and compiled using NoteExpress software. General processing of data was performed by Ucinet and Netdraw software, and the correlation analysis was performed by SPSS25.0 and Stata software. Finally, RevMan software was selected to complete the meta-analysis. Result: A total of 815 documents were hit, and 67 documents were successfully selected, including 4063 sample sizes. A knowledge visualization map of dust and heavy metal health risk assessment in typical domestic regions was established, and the knowledge distribution of dust and heavy metal research was visually displayed and formed a focus. By studying clustering, we obtained the network map of keyword. Through Meta analysis, the analysis results of typical forest map and funnel map are obtained. Conclusion: For the study of heavy metals in dust, the research data in the Southeast area is relatively abundant. According to the results of the meta-analysis, the health risk assessment of typical dust heavy metals in basic research is credible. Degree is higher.', 'title': 'Meta Analysis of Dust Heavy Metals based on Knowledge Map Visualization', 'embedding': []}, {'id': 15026, 'abstractText': 'Intracranial hypertension is an acute, life-threatening neurological condition that can lead to high risk of mortality. Its prompt identification and timely management are key to functional recovery and resuscitation of the patient. The objective of the present study is to propose quantitative measures for the early assessment of intracranial hypertensive (IH) episodes in traumatic brain injured (TBI) patients and to explore the association between intra-individual variability and IH events. To achieve this, we identified fifty-nine IH events in twelve TBI patients, and analyzed intracranial pressure (ICP), mean arterial pressure (MAP) and heart rate (HR). The notion of Granger causal (GC) analysis was adopted to quantify the bi-directional information flow patterns among ICP, MAP and HR. Additionally, the coefficient of variations of GC values was estimated to quantify intra-individual variations. The present study shows that GC values of ICP-to-MAP, MAP-to-ICP and HR-to-ICP decrease during an IH event while the GC value of HR-to-MAP increases during an IH event. Moreover, it was also observed that TBI patients show more inconsistency during ICP elevations. Our findings suggest that directional communications across cardiovascular (MAP and HR) and cerebrovascular (ICP) mechanisms are associated with the onset of intracranial hypertension. These derived GC measures may also be utilized as functional bio-markers in physiological diagnostics.', 'title': 'Variations in Information Flow Patterns Following Intracranial Hypertensive Events in Traumatic Brain Injured Patients', 'embedding': []}, {'id': 15027, 'abstractText': \"Concept mapping is one of the methods of knowledge mapping. The existing studies portray that concept mapping can be very useful for improving students' learning and achievements. Many engineering students taking accounting course find it challenging to understand the accounting topics and it is difficult for them to achieve good results in this course. This study is aimed to review the available literature on concept mapping and its use in teaching accounting to engineering students. Forward and backward snowballing technique has been used to find the relevant literature on concept mapping. The available literature has been divided into three categories. The review results depicted a profound research gap regarding the research questions raised.\", 'title': 'Teaching accounting to engineering students and the use of concept mapping: a preliminary review', 'embedding': []}, {'id': 15028, 'abstractText': 'In this paper, we consider the transformation of laser range measurements into a top-view grid map representation to approach the task of LiDAR-only semantic segmentation. Since the recent publication of the SemanticKITTI data set, researchers are now able to study semantic segmentation of urban LiDAR sequences based on a reasonable amount of data. While other approaches propose to directly learn on the 3D point clouds, we are exploiting a grid map framework to extract relevant information and represent them by using multi-layer grid maps. This representation allows us to use well-studied deep learning architectures from the image domain to predict a dense semantic grid map using only the sparse input data of a single LiDAR scan. We compare single-layer and multi-layer approaches and demonstrate the benefit of a multi-layer grid map input. Since the grid map representation allows us to predict a dense, 360° semantic environment representation, we further develop a method to combine the semantic information from multiple scans and create dense ground truth grids. This method allows us to evaluate and compare the performance of our models not only based on grid cells with a detection, but on the full visible measurement range.', 'title': 'Exploiting Multi-Layer Grid Maps for Surround-View Semantic Segmentation of Sparse LiDAR Data', 'embedding': []}, {'id': 15029, 'abstractText': 'Retinotopic mapping, the mapping of visual input on the retina to cortical neurons, is an important topic in vision science. Typically, cortical neurons are related to visual input on the retina using functional magnetic resonance imaging (fMRI) of cortical responses to slowly moving visual stimuli on the retina. Although it is well known from neurophysiology studies that retinotopic mapping is locally diffeomorphic (i.e., smooth, differentiable, and invertible) within each local area, the retinotopic maps from fMRI are often not diffeomorphic, especially near the fovea, because of the low signal-noise ratio of fMRI. The aim of this study is to develop and solve a mathematical model that produces diffeomorphic retinotopic mapping from fMRI data. Specifically, we adopt a geometry concept, the Beltrami coefficient, as the tool to define diffeomorphism, and model the problem in an optimization framework. Efficient numerical methods are proposed to solve the model. Experimental results with both synthetic and real retinotopy data demonstrate that the proposed method is superior to conventional smoothing methods.', 'title': 'Diffeomorphic Smoothing for Retinotopic Mapping', 'embedding': []}, {'id': 15030, 'abstractText': 'In the aftermath of a disaster, there is an urgent need for base maps to support relief efforts, especially in developing countries. In response to this, the OpenStreetMap project has been leveraged to produce maps of disaster-affected areas in a collaborative way. However, there has been little investigation aimed at explaining the collaborative mapping activity itself. This study presents an exploratory case study on how the collaborative mapping activities that followed the Nepal Earthquake in 2015 were coordinated and structured, i.e. how volunteers were organized, and what were the main outcomes of their activity in the context of disaster management. The results show that a large number of remote contributors spread across the world carried out concerted efforts to support the relief work. Moreover, coordination mechanisms were used by local actors to share their knowledge with remote mappers, and, hence, to improve the accuracy of the map.', 'title': 'Potential of Collaborative Mapping for Disaster Relief: A Case Study of OpenStreetMap in the Nepal Earthquake 2015', 'embedding': []}, {'id': 15031, 'abstractText': 'It is important to reduce the human influence on nature resources by identifying an appropriate land use. Moreover, it is essential to carry out scientific land evaluation. Such kind of analysis allows identifying the main factors for agricultural production and enables decision makers to develop crop managements in order to increase the land capability. The key is to match the type of intensity of land use with its natural capability. Therefore; in order to benefit from these areas and invest them to obtain good agricultural production, they must be organized and managed in full. Lebanon suffers from the unorganized agricultural use. We take South Lebanon as a study area; it is the most fertile ground and has a variety in crops. The study aims to identify and locate the most suitable area to cultivate thirteen types of permanent trees which are: apples, avocadoes, stone fruits in coastal regions and stone fruits in mountain regions, bananas, citrus, loquats, figs, pistachios, mangoes, olives, pomegranates and grapes. Several geographical factors are taken as criterion for selection of the best location to cultivate. Soil, rainfall, PH, temperature, and elevation, are main input to create final map. Input data of each factor is managed, visualized and analyzed using Geographical Information System (GIS). Managements GIS tools were implemented to produce input maps capable of identifying suitable areas related to each index. The combination of the different indices map generates the final output map of the suitable place to get best permanent tree productivity. The output map is reclassified into three suitability classes: low, moderate, and high suitability. Results show different locations suitable for different kinds of trees. Results also reflected the importance of GIS in helping decision makers finding a most suitable location for every tree to get more productivity and a verity in crops.', 'title': 'Geographic Information System-Based Map for Agricultural Management in South-Lebanon', 'embedding': []}, {'id': 15032, 'abstractText': 'We present a user study comparing a pre-evaluated mapping approach with a state-of-the-art direct mapping method of facial expressions for emotion judgment in an immersive setting. At its heart, the pre-evaluated approach leverages semiotics, a theory used in linguistic. In doing so, we want to compare pre-evaluation with an approach that seeks to directly map real facial expressions onto their virtual counterparts. To evaluate both approaches, we conduct a controlled lab study with 22 participants. The results show that users are significantly more accurate in judging virtual facial expressions with pre-evaluated mapping. Additionally, participants were slightly more confident when deciding on a presented emotion. We could not find any differences regarding potential Uncanny Valley effects. However, the pre-evaluated mapping shows potential to be more convenient in a conversational scenario.', 'title': 'Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion', 'embedding': []}, {'id': 15033, 'abstractText': \"Objective: Tumor stiffening in pancreatic adenocarcinoma (PDAC) has been linked to cancer progression and lack of therapy response, yet current elastography tools cannot map stiffness in a whole tumor field-of-view with biologically relevant spatial resolution. Therefore, this study was developed to assess stiffness heterogeneity and geometrical patterns across whole PDAC xenograft ex vivo tumors. Methods: The ex vivo elastography (EVE) mapping system was capable of creating stiffness map at 300-micron spatial resolution under a 5-20 mm field of view relevant to whole tumor assessment. The stiffness value at each location was determined by compression testing and an absolute tumor Young's modulus map was calculated based on the calibration between the system and ultrasound elastography (R<sup>2</sup> = 0.95). Results: Two PDAC tumor lines AsPC-1 and BxPC-3 implanted in xenograft models were assessed to show tumor stiffness and its linear relationship to collagen content (R<sup>2</sup> = 0.59). EVE was able to capture stiffness heterogeneity ranging between 5 and 100 kPa in pancreatic tumors with collagen content up to 25%. More importantly, data shows the inverse relationship of local stiffness to local drug distribution (R<sup>2</sup> = 0.66) and vessel patency (R<sup>2</sup> = 0.61) in both PDAC tumor lines. Conclusion: The results suggested that elastography could be utilized to predict drug penetration in PDAC tumors or assess response to biological modifying adjunct therapies. Significance: This study presents the first attempt to map out stiffness on a biologically relevant spatial scale across whole PDAC tumor slices with spatial resolution in the hundreds of microns.\", 'title': 'High-Resolution <italic>Ex Vivo</italic> Elastography to Characterize Tumor Stromal Heterogeneity In Situ in Pancreatic Adenocarcinoma', 'embedding': []}, {'id': 15034, 'abstractText': \"Recently, super-resolution techniques have been energetically studied for the purpose of reusing the low resolution image contents. Although a lot of approaches to achieve the appropriate super-resolution have been proposed such as non-linear filtering, total variation regularization, deep learning etc., the characteristic of the viewpoint distribution of the observer has not been effectively utilized. Because applying super-resolution to unimportant regions in an image may hinder the observer's attention to seeing the display, it leads to a low subjective evaluation. This paper proposes the region-wise super-resolution algorithm based on the view-point distribution of observer. However, we cannot obtain the viewpoint distribution map for an image without the pre-experiment using the device such as eye mark recorder, therefore, the saliency map is utilized in this paper. Numerical examples show that the proposed algorithm using saliency map achieves a higher subjective evaluation than the previous study based on the non-linear filtering based super-resolution. Furthermore, in numerical examples, the proposed algorithm using the saliency map is shown to give the similar results of the algorithm using the viewpoint distribution map obtained by the pre-experiment using eye mark recorder.\", 'title': 'Region-Wise Super-Resolution Algorithm Based On the Viewpoint Distribution', 'embedding': []}, {'id': 15035, 'abstractText': 'In this paper, we propose and evaluate a simple mechanism to accelerate iterative machine learning algorithms implemented in Hadoop map-reduce (stock), and Apache Spark. In particular, we describe a technique that enables data parallel tasks in map-reduce and Spark to be dynamically and adaptively scheduled on CPU or GPU, based on availability and load. We examine the extent of performance improvements, and correlate them to various parameters of the algorithms studied. We focus on end-to-end performance impact, including overheads associated with transferring data into and out of the GPU, and conversion between data representations in the JVM and on GPU. We also present three optimizations that, in our analysis, can be generalized across many iterative machine learning applications. We present a case study where we accelerate four iterative machine learning applications - multinomial logistic regression, multiple linear regression, K-Means clustering and principal components analysis using singular value decomposition, implemented in three data analytics frameworks - Hadoop Map-Reduce (HMR), IBM Main-Memory Map-Reduce (M3R) and Spark. We observe that the use of GPGPUs decreases the execution time of these applications on HMR by up to 8X, M3R by up to 18X, and Spark by up to 25X. Through our empirical analysis, we offer several insights that can be helpful in designing middleware and cluster managers to accelerate map-reduce and Spark applications using GPUs.', 'title': 'Adaptively Accelerating Map-Reduce/Spark with GPUs: A Case Study', 'embedding': []}, {'id': 15036, 'abstractText': 'Extracting information about variations within urban areas using satellite imagery has generally focused on mapping individual buildings or slum versus non-slum areas. While these data are useful, they can run into issues in very dense urban areas, additionally slums have a subjective definition. In previous research we have found that contextual features are related to population, census variables, poverty, and other values, but have not explored which urban attributes (i.e., buildings and roads) these features represent. In this study we seek to determine the correlation between contextual features calculated on Very High Spatial Resolution (VHSR) satellite data and urban attributes derived from Open Street Map (OSM) for portions of multiple cities in Sri Lanka. Results indicate that individual contextual features are highly correlated with building area, building density, road area, road density, total built up areas and other features. Moreover, when multiple contextual features are combined within a model, they can explain from 70 to 92 percent of the variance of these urban features within the study area. This indicates that contextual features are very strong indicators of urban variability and can be used to map differences within the urban setting. This may allow us to forgo having to map each building and road individually for mapping urban areas in future projects.', 'title': 'Evaluating the Relationship Between Contextual Features Derived from Very High Spatial Resolution Imagery and Urban Attributes: A Case Study in Sri Lanka', 'embedding': []}, {'id': 15037, 'abstractText': \"Recently, various applications including data analytics and machine learning have been developed for geo-distributed cloud data centers. For those applications, the ways of mapping parallel processes to physical nodes (i.e., “process mapping”) could significantly impact the performance of the applications because of non-uniform communication cost in geo-distributed environments. What's more, the different data privacy requirements in geo-distributed data centers pose additional constraints on process mapping solutions. While process mapping has been widely studied in grid/cluster environments, few of the existing studies have considered the problem in geo-distributed cloud environment, which is a challenging task due to the multi-level data privacy constraints, heterogeneous network performance and process failures. In this paper, we introduce the special privacy requirements in geo-distributed data centers and formulate the geo-distributed process mapping problem as an optimization problem with multiple constraints. We develop a new method to efficiently find good process mapping solutions to the problem. Experimental results on real clouds (including Amazon EC2 and Windows Azure) and simulations demonstrate that our proposed approach can achieve significant performance improvement compared to the state-of-the-art algorithms.\", 'title': 'Privacy Regulation Aware Process Mapping in Geo-Distributed Cloud Data Centers', 'embedding': []}, {'id': 15038, 'abstractText': 'IONOLAB is an interdisciplinary research group dedicated for handling the challenges of near earth environment on communication, positioning and remote sensing systems. IONOLAB group contributes to the space weather studies by developing state-of-the-art analysis and imaging techniques. On the website of IONOLAB group, www.ionolab.org, four unique space weather services, namely, IONOLAB-TEC, IRI-PLAS-2015, IRI-PLAS-MAP and IRI-PLAS-STEC, are provided in a user friendly graphical interface unit. Newly developed algorithm for ionospheric tomography, IONOLAB-CIT, provides not only 3-D electron density but also tracking of ionospheric state with high reliability and fidelity. The algorithm for ray tracing through ionosphere, IONOLAB-RAY, provides a simulation environment in all communication bands. The background ionosphere is generated in voxels where IRI-Plas electron density is used to obtain refractive index. One unique feature is the possible update of ionospheric state by insertion of Total Electron Content (TEC) values into IRI-Plas. Both ordinary and extraordinary paths can be traced with high ray and low ray scenarios for any desired date, time and transmitter location. 2-D regional interpolation and mapping algorithm, IONOLAB-MAP, is another tool of IONOLAB group where automatic TEC maps with Kriging algorithm are generated from GPS network with high spatio-temporal resolution. IONOLAB group continues its studies in all aspects of ionospheric and plasmaspheric signal propagation, imaging and mapping.', 'title': 'Space weather studies of IONOLAB group', 'embedding': []}, {'id': 15039, 'abstractText': \"As a meta-heuristic algorithm that simulates the intelligence of gray wolves, grey wolf optimizer (GWO) has a wide range of applications in practical problems. As a kind of local search, chaotic local search (CLS) has a strong ability to get rid of the local optimum due to its integration of chaotic maps. To enhance GWO, CLS is always incorporated into GWO to increase its population diversity and accelerate algorithm's convergence. However, it is still unclear that how may chaotic maps should be used in CLS and how to embed them into GWO. To address these challenging issues, this paper studies both single and multiple chaotic maps incorporated GWOs. Extensive comparative experiments are conducted based on IEEE Congress on Evolutionary Computation (CEC) benchmark test suit. The results show that CLS incorporated GWOs generally perform better than the original GWO, suggesting the effectiveness of such hybridization. Moreover, a remarkable finding of this work is that the piecewise linear chaotic map (PWLCM) and Gaussian map have the most potential to improve the search performance of GWO. Additionally, CLS incorporated GWOs also perform significantly better than some other state-of-the-art meta-heuristic algorithms. This study not only gives more insights into the mechanism of how CLS makes influence on GWO, but also finds that the most suitable choice of chaotic map for it.\", 'title': 'Comparative Study on Single and Multiple Chaotic Maps Incorporated Grey Wolf Optimization Algorithms', 'embedding': []}, {'id': 15040, 'abstractText': 'This chapter demonstrates methods for estimating population and assessing urban environmental quality (UEQ) through case studies. The population estimation study intended to combine the statistical‐based and dasymetric‐based methods and to redistribute census population. The case study of population estimation aimed to compare the effectiveness of the spectral response‐based and the land‐use (LU)‐based methods for population estimation of US census block groups and to produce a more accurate presentation of population distribution by combining the dasymetric mapping with LU‐based methods. The UEQ study intended to evaluate the UEQ changes from 1990 to 2000 in Indianapolis, Indiana, using the integrated techniques of remote sensing and geographic information system. Specifically, the chapter intends to derive an UEQ index based on the synthetic indicators of physical variables extracted from Landsat images and socioeconomic variables derived from US census data and to develop a new method for assessing UEQ change over the time.', 'title': 'Remote Sensing of Socioeconomic Attributes', 'embedding': []}, {'id': 15041, 'abstractText': 'Digital markets and digitalization were developed and were driven by online shopping, consumer lifestyles, and entrepreneurial spirit. It has been research yet the notion about digital market review study which showed the big picture using data from all countries. This research aims to map the status of the digital market studies indexed Scopus using the bibliometrics approach. The study conducted the bibliometrics method and analyzed research data using the service of analyzing search results from Scopus and the VOSviewer application. Research data of 337 academic documents published from 1976 to 2019 obtained from the Scopus database in March 2020. The results revealed about the most productive countries, research institutes, and individual researchers in digital market studies are the United States, Yale University, and Alexander Schachinger. The most intensive subject area and sources of publications in the digital market studies are computer science and Ink World. Besides, Spain was one of the collaborative researcher group map. This research proposed a convergence axis classification consisting of digital market studies to characterize the body of knowledge generated from four decades of studies: Management information system, E-commerce, Digital printing, Data, and Digitalization, abbreviated as MEDDD themes.', 'title': 'A Study of Digital Market Status Using The Bibliometric Approach During Four Decades', 'embedding': []}, {'id': 15042, 'abstractText': 'The development of the field of GIS and remote sensing has greatly contributed to the development and discovery of many archaeological monuments. Therefore, our study discusses the documentation of the historical spatial features of Qary, Sudan using GIS and remote sensing. In addition to the absence of an integrated information application that contributes to the documentation and development of these landmarks through the support of decision makers. The objectives of the study are to collect and classify spatial and descriptive data and information for the Qary area, to design the spatial database of the spatial features of the Qari region, to implement a spatial spatial application of the archaeological spatial features of the Qari region, The study followed descriptive field applied methodology. The study reached several results, the most important of which are: About 20 archaeological sites were recorded and recorded in the Qari area, dating back to different historical periods from the Stone Age to the Islamic Period. Including settlement sites - church tombs. This diversity and richness in the sites shows that the area of Qari has remained a contemporary of all the archaeological periods in the Sudan. (Stone Age (300.00 - 3700 BC)) 8 Post-Merwa (6th century AD) AD 1504 AD (4 monuments, Islamic period (1505 - 1820 AD) 5 monuments, and 2 unidentified teachers)). The spatial database of the spatial and spatial features of the Qari region was constructed. A spatial application of the spatial features of the Qari region was completed. Numerous maps and archaeological sites were developed for the Qari region (map of all the monuments in the Stone Age, the Marwa period, the Christian period, , The study recommends designing the Web and Mobile application to support the decision maker and guide the tourist, and recommends the preparation of a prospective study to implement the three-dimensional area.', 'title': 'GIS &amp; RS-Based Archaeologies Site Documents: Gari Region, Khartoum, Sudan', 'embedding': []}, {'id': 15043, 'abstractText': 'Study on green computing continues to develop but is limited to one country and/or one field. From a bibliometric overview, this study aims to visually study mapping and research trends in the field of green computing on an international scale. This study used bibliometric techniques with secondary data from Scopus. Analyze and visualize data using the VOSViewer program and the analyze search results function on Scopus. This study analyzed 2,596 scientific documents published from 1979 to 2020. According to the research, the Chinese Academy of Sciences, China, and Rajkumar Buyya from the University of Melbourne had the most active organization and individual scientists in green computing research. China and IEEE Access were the most countries and disseminated outlets of green computing. There were five category maps of collaborative researchers from around the world. Based on the identification of a collection of knowledge created from forty-one years of publication, this research proposes a grouping of green computing study themes: Computer science, Environmental management, Mobile computing, Energy, and Sustainability, abbreviated as CEMES study themes.', 'title': 'Four Decades of the Green Computing Study: A Bibliometric Overview', 'embedding': []}, {'id': 15044, 'abstractText': \"Background: Good API documentation facilitates the development process, improving productivity and quality. While the topic of API documentation quality has been of interest for the last two decades, there have been few studies to map the specific constructs needed to create a good document. In effect, we still need a structured taxonomy that captures such knowledge systematically.Aims: This study reports emerging results of a systematic mapping study. We capture key conclusions from previous studies that assess API documentation quality, and synthesise the results into a single framework.Method: By conducting a systematic review of 21 key works, we have developed a five dimensional taxonomy based on 34 categorised weighted recommendations.Results: All studies utilise field study techniques to arrive at their recommendations, with seven studies employing some form of interview and questionnaire, and four conducting documentation analysis. The taxonomy we synthesise reinforces that usage description details (code snippets, tutorials, and reference documents) are generally highly weighted as helpful in API documentation, in addition to design rationale and presentation.Conclusions: We propose extensions to this study aligned to developer utility for each of the taxonomy's categories.\", 'title': 'What should I document? A preliminary systematic mapping study into API documentation knowledge', 'embedding': []}, {'id': 15045, 'abstractText': 'Genetic mapping is an approach in identifying genes and processes. Genetic maps are essential tools for analyzing DNA sequence data, not only providing a blueprint of the genome but also unlocking linkage patterns between genetic markers, chromosomal regions with more than one sequence variant. Studying these linkage patterns enables diverse applications to identifying the biological underlying feature of problems in health, agriculture, and the study of biodiversity. Genetic mapping provides a mean to understand the basis of genetic and biochemical diseases and provides genetic markers. Mapping studies can be done in a single large pedigree; the larger the number of affected individuals sampled the better the estimate of recombination between the gene causing the disease and one or more nearby genetic marker. This work proposes an algorithm for improving the methods to detect breast cancer by analyzing the DNA data and detect the issue in the DNA samples. This work based on the big data and machine learning techniques to get classifications for all samples. All samples will be classified into two main classes. This work evaluates the performance of different classification algorithms on the dataset. It also provides a website application as the tool that can help specialist predict the of breast cancer based on stated genetic mutation.', 'title': 'Machine Learning Model for Breast Cancer Prediction', 'embedding': []}, {'id': 15046, 'abstractText': \"Objective: To enable non-invasive dynamic metabolic mapping in rodent model studies of mitochondrial function using <sup>31</sup>P-MR spectroscopic imaging (MRSI). Methods: We developed a novel method for high-resolution dynamic <sup>31</sup>P-MRSI. The method synergistically integrates physics-based models of spectral structures, biochemical modeling of molecular dynamics, and subspace learning to capture spatiospectral variations. Fast data acquisition was achieved using rapid spiral trajectories and sparse sampling of (k, t, T)-space; image reconstruction was accomplished using a low-rank tensor-based framework. Results: The proposed method provided high-resolution dynamic metabolic mapping in rat hindlimb at spatial and temporal resolutions of 4 × 4 × 2 mm<sup>3</sup> and 1.28 s, respectively. This allowed for in vivo mapping of the time-constant of phosphocreatine resynthesis, a well established index of mitochondrial oxidative capacity. Multiple rounds of in vivo experiments were performed to demonstrate reproducibility, and in vitro experiments were used to validate the accuracy of the estimated metabolite maps. Conclusions: A new model-based method is proposed to achieve high-resolution dynamic 31P-MRSI. The proposed method's ability to delineate metabolic heterogeneity was demonstrated in rat hindlimb. Significance: Abnormal mitochondrial metabolism is a key cellular dysfunction in many prevalent diseases such as diabetes and heart disease; however, current understanding of mitochondrial function is mostly gained from studies on isolated mitochondria under nonphysiological conditions. The proposed method has the potential to open new avenues of research by allowing in vivo and longitudinal studies of mitochondrial dysfunction in disease development and progression.\", 'title': 'High-Resolution Dynamic <sup>31</sup>P-MR Spectroscopic Imaging for Mapping Mitochondrial Function', 'embedding': []}, {'id': 15047, 'abstractText': \"Convolutional neural networks (CNN) are powerful tools in many medical imaging applications including denoising. However, a major concern in deploying a CNN in safety-critical areas is to access its prediction accuracy on out-of-distribution test samples. Our previous study showed that while a CNN could generate similar ensemble standard deviation levels on inputs with different noise levels, the magnitude and location of bias could be substantially different. These observations imply that although CNNs can generate visually high-quality images from each individual dataset, the bias might be higher in one image than the other, which might lead to misdiagnosis. Therefore, it is crucial for us to understand when and where a CNN is uncertain about its prediction in real clinical datasets. In this study, we investigate the use of a Bayesian Convolutional Neural Network (BCNN) to estimate an uncertainty map of its prediction, and assess its correlations with the ensemble bias of its own and a conventional CNN's prediction ensemble bias, respectively. The BCNN is implemented by adding Monte Carlo dropout layers after each convolution and ReLU layer, which is equivalent to imposing a Bernoulli prior distribution on the network weights. At the testing stage, we acquire multiple stochastic forward passes through the network to generate the uncertainty map. We applied BCNN and CNN on two 18F-FDG scans, with one whole-body scan which is considered as an in-distribution test sample, and one brain only study which is considered as an out-of-distribution sample. For the whole-body scan, we generated 10 noise realizations for ensemble analysis using bootstrap. Our results demonstrate that BCNN can generate a prediction uncertainty map that highly correlates with the ensemble bias map. This technique could provide important guidance on interpreting the denoising results from a neural network where ground truth is unknown.\", 'title': 'Estimating Ensemble Bias using Bayesian Convolutional Neural Network', 'embedding': []}, {'id': 15048, 'abstractText': 'Image-based fusion is a state-of-art process to extract vital information by combining the two or more images acquired from different satellite sensors. Recently launched (26th September 2016) ISRO’s (Indian Space Research Organization) Ku-band (13.5 GHz) based Scatterometer Satellite (SCATSAT-1) as an active microwave sensor can offer the day-night, all-weather monitoring services, which are not possible with the optical-based visible and infrared remote sensing satellites. Therefore, the fusion of optical and microwave data offers the cloud-free detection of earth surface transitions and helps in emergency response to natural hazards, security, and defence. The objectives of the proposed framework are (a) nearest-neighbour based fusion (NNF) of ISRO’s SCATSAT-1 and NASA’s (National Aeronautics and Space Administration) moderate resolution imaging spectroradiometer (MODIS) optical data, (b) generation of thematic maps using artificial neural network (ANN) based classification of the fused data, (c) detection of spatiotemporal variations via post-classification comparison (PCC) based change detection, (d) cross-referencing with well-defined fusion methods, i.e. Gram-Schmidt (GS), Brovey Transformation (BT) and Ehlers, and (e) Impact analysis of clouds on the input dataset and fusion methods. This study has been conducted over the Western Himalayas to estimate the snow cover changes under cloudy conditions with two datasets i.e., winter and monsoon. The experimental outcomes confirm the efficacy of the proposed framework in the effective removal of clouds, generation of classified maps, and change maps. The present study includes an exhaustive list of applicative situations for cloud-free monitoring using freely and daily based SCATSAT-1 and MODIS datasets.', 'title': 'Image-Fusion of Ku-band based SCATSAT-1 and MODIS data for Cloud-free Change Detection over Western Himalayas', 'embedding': []}, {'id': 15049, 'abstractText': 'Simultaneous localization and mapping (SLAM) during communication is emerging. This technology promises to provide information on propagation environments and transceivers’ location, thus creating several new services and applications for the Internet of Things and environment-aware communication. Using crowdsourcing data collected by multiple agents appears to be much potential for enhancing SLAM performance. However, the measurement uncertainties in practice and biased estimations from multiple agents may result in serious errors. This study develops a robust SLAM method with measurement plug-and-play and crowdsourcing mechanisms to address the above problems. First, we divide measurements into different categories according to their unknown biases and realize a measurement plug-and-play mechanism by extending the classic belief propagation (BP)-based SLAM method. The proposed mechanism can obtain the time-varying agent location, radio features, and corresponding measurement biases (such as clock bias, orientation bias, and received signal strength model parameters), with high accuracy and robustness in challenging scenarios without any prior information on anchors and agents. Next, we establish a probabilistic crowdsourcing-based SLAM mechanism, in which multiple agents cooperate to construct and refine the radio map in a decentralized manner. Our study presents the first BP-based crowdsourcing that resolves the “double count\" and “data reliability\" problems through the flexible application of probabilistic data association methods. Numerical results reveal that the crowdsourcing mechanism can further improve the accuracy of the mapping result, which, in turn, ensures the decimeter-level localization accuracy of each agent in a challenging propagation environment.', 'title': 'Enabling Plug-and-Play and Crowdsourcing SLAM in Wireless Communication Systems', 'embedding': []}, {'id': 15050, 'abstractText': 'This study presents the results of a field experiment conducted for assessing the crop health status of several barley and oat crop fields in Prince Edward Island, Canada. The crop fields were mapped with an Unmanned Aircraft System (UAS) and the crop health status was assessed through the Green Area Index (GAI) and vegetation indices (VIs). GAI maps were produced from the UAS imagery and VIs using machine learning pipelines with several regression algorithms (Multiple Linear Models, Support Vector Machines, Random Forests, and Artificial Neural Networks) along with a feature selection strategy. The Random Forests algorithm was shown to be the best algorithm for GAI prediction with an average relative Root Mean Square Error of 10.86% and a Mean Absolute Error of 0.67. The resulting GAI maps and the regression feature space were classified with Random Forests to discriminate between vigorous and stressed crop areas. We achieved a mean overall accuracy of 94%. The limits of the study are also presented.', 'title': 'Evaluation of crop health status with UAS multispectral imagery', 'embedding': []}, {'id': 15051, 'abstractText': 'Direct reconstruction of parametric images from raw dynamic PET data has the potential of producing lower noise images than obtained using intermediate frame-based reconstructed images, due to the accurately characterized statistical properties of raw PET data. The goal of this study was to extend a previous direct parametric reconstruction algorithm (PMOLAR). PMOLAR uses the Expectation Maximization (EM) algorithm to estimate the parametric maps. Previous versions of PMOLAR were based on the one-tissue (1T) compartment model (PMOLAR-1T). The 1T model is suitable for some PET tracers, but most reversible PET radioligand kinetics are better described by more complex models, mainly the two-tissue (2T) compartment model. Alternatively, Logan Graphical Analysis (GA) can be applied to all reversible PET tracers. In this study PMOLAR was adapted to a new model based on GA. Two versions of the new reconstruction algorithm were investigated. PMOLAR-GA was evaluated on human data acquired on the High Resolution Research Tomograph (HRRT) after injection of [<sup>11</sup>C]PBR28, a radiotracer used to study neuroinflammation. The new model derived from GA was first compared to previous modeling methods (MA1, LEGA, 2T) suitable for [<sup>11</sup>C]PBR28 on fits of region of interest time-activity curves. Then the two new versions of PMOLAR were evaluated on a human 4D PET data set. PMOLAR-GA parametric maps were compared to frame-based parametric maps. Using routine reconstruction settings, frame based parametric maps were visually very noisy and biased (between 27±4% and 222±121% on a regional level, depending on the modeling method). While the PMOLAR-GA mean regional values were very close to reference values (11%±9% or 8%±9%), with visually lower noise at the voxel level.', 'title': 'Direct EM reconstruction of parametric images from list-mode brain PET using a novel model based on logan graphical analysis', 'embedding': []}, {'id': 15052, 'abstractText': 'Natural calamities triggered by erratic weather conditions like cyclone, earthquakes, hail storms, and flood incurs substantial loss to the infrastructure and crops of the region. Countries across the globe are prone to such natural calamities. In India, specifically coastal parts are vulnerable to tropical cyclones. In 2018 east coast districts of Tamil Nadu and Andhra Pradesh, India were affected by the three cyclones namely Titli (11 Oct. 2018), Gaja (16 Nov. 2018) and Pethai (17 Dec. 2018) causing severe damage to seasonal crops such as Rice, Coconut and Areca Nut plantations. Traditional survey-based methods of crop loss assessment are time-consuming and labor-intensive.This study addresses the problem of near-real-time qualitative crop loss assessment due to tropical Gaja cyclone using the temporal data from Sentinel 1 and 2 satellites. The crop damage assessment study has been undertaken for Gaja cyclone in the affected district of Thanjavur, Tamil Nadu, India. The major crops cultivated in the district are Kharif Rice (locally called as Samba and Late Samba) and Coconut plantations. The study addresses qualitative loss assessment in terms of crop area affected. As a first step, we used time series data of Sentinel1 (VV and VH backscatter) available between Aug.-Nov. 2018 to map the Kharif rice area. Also, cloud-free Sentinel 2 scenes available during Mar.-May. 2018 have been used to map the Coconut area. Field visits were conducted to collect the geo-tagged plot boundaries for the rice crop and coconut plantations. The data collected through field visits was used both for model training and crop loss assessment. Google maps satellite layer was used as a base map for identification of other non-crop classes (i.e., forest, water, settlement, etc.). The overall accuracy of crop area classification was 87.23% for rice and 92.22% for coconut.Further, to estimate the crop loss, crop layers along with the NDVI were considered. Two crop loss scenarios, namely minimum damage and maximum damage, were identified for both the crops. The mean NDVI composite before the event, i.e., 1-15 Nov. 2018 was considered as the base. In case of maximum loss scenario, short term NDVI composite available immediately after the event, i.e., 17-25 Nov. 2018 was selected. After the cyclone, long term NDVI composite of the mean (i.e., 17 Nov.13 Dec. 2018) was used to assess the minimum loss scenario. Using field observations, the crop loss was categorized as severe loss, medium loss, low loss, and no loss. Results showed that the coconut plantations in Pattukkottai, Peravurani, and Papanasam blocks of Tanjavur are affected by the cyclone. The significant rice crop loss has been observed in Thanjavur, Orattanadu, Pattukkottai blocks. We have found the remote sensing based crop loss observations are matching with the government reports based on field observations. The remote sensing observations with human participatory sensing (i.e., field observations) has the potential for near-real-time crop loss assessment.', 'title': 'Near Real Time Crop Loss Estimation using Remote Sensing Observations', 'embedding': []}, {'id': 15053, 'abstractText': 'Sugarcane is a high biomass crop that requires large quantities of water for maximum yield. The study aims to estimate daily actual water consumption or crop evapotranspiration ( ETc) at field scales for practical applications with real data, remote sensing (RS) observation using novel methodologies (ML: machine learning and LSM: land surface model). Northeast Thailand is the study area chosen and three Sugarcane growing seasons (2016 to 2019) is the duration of the study. Similarities between the crop coefficient ( Kc) curve and a satellite-derived leaf area index (LAI) showed potential for estimation of Kc maps. A regression model has been developed to establish LAI vs Kc relation and used to derive daily Kc maps. With a view to computing daily reference evapotranspiration ( ETo) driven by weather, RS data, soil texture, land charac-teric etc, a high resolution LSM has been customized. The results shows ETc distribution low at initial and early development stages, while ETc tends to be high during grand growth and yield formation stages. Significant spatiotemporal variation has been observed across fields. Analysis of 19 fields for complete three seasons has been undergone with regard to yield response to water consumption and outcomes confirm standard yield reduction due to water stress. The daily ETc maps aided to demonstrate the variability of crop water use during growing season at field scales. Further, using ETc maps at the field scale in near real time, growers can supply optimal water to maximize yield, leading to water conservation in scale.', 'title': 'Crop Evapotranspiration Estimates for Sugarcane Based on Remote Sensing and Land Surface Model in Thailand', 'embedding': []}, {'id': 15054, 'abstractText': 'The inversion of remote sensing images is crucial for soil moisture mapping in precision agriculture. However, the large size of remote sensing images complicates their management. Therefore, this study proposes a remote sensing observation sharing method based on cloud computing (ROSCC) to enhance remote sensing observation storage, processing, and service capability. The ROSCC framework consists of a cloud computing-enabled sensor observation service, web processing service tier, and a distributed database tier. Using MongoDB as the distributed database and Apache Hadoop as the cloud computing service, this study achieves a high-throughput method for remote sensing observation storage and distribution. The map, reduced algorithms and the table structure design in distributed databases are then explained. Along the Yangtze River, the longest river in China, Hubei Province was selected as the study area to test the proposed framework. Using GF-1 as a data source, an experiment was performed to enhance earth observation data (EOD) storage and achieve large-scale soil moisture mapping. The proposed ROSCC can be applied to enhance EOD sharing in cloud computing context, so as to achieve soil moisture mapping via the modified perpendicular drought index in an efficient way to better serve precision agriculture.', 'title': 'ROSCC: An Efficient Remote Sensing Observation-Sharing Method Based on Cloud Computing for Soil Moisture Mapping in Precision Agriculture', 'embedding': []}, {'id': 15055, 'abstractText': \"The current COVID-19 pandemic has seen a lot of higher institutions of learning embracing the e-learning systems. Although these e-learning systems promise to deliver solutions to teaching and learning in this pandemic era, a key challenge is motivating the learner to engage with the e-learning system continuously. Most e-learners quickly get bored and lose motivation in the course of learning. While there exist many strategies such as chatrooms and sporadic question and answer sessions to keep learners involved in e-learning platforms, they have always achieved minimal connectedness among e-learners. Facial emotions have been identified as an effective tool for interpreting learning experience in learners. This study, therefore, examines the use of facial emotions expressed by learners to interpret their learning affect in an e-learning session. This work also explores a standardized mapping mechanism between facial emotions exhibited and their respective learning affects. The study identifies the physical changes in the face of a learner and uses it to estimate their facial emotions and then based on the mapping mechanism, maps emotional states to a student's learning affect. Experiments include the use of a convolutional neural network for the classification of seven facial emotions. The research study tests different network architectures to find optimal architecture, using the FER2013 dataset. Results from the mapping are statistically analyzed and compared with responses provided by participants who participated in the live testing of the system. Results show that facial emotions, which are a form of non-verbal communication, can be used to estimate the learning affect of a student and provides a new avenue to enhance the current e-learning platforms.\", 'title': 'Estimating Student Learning Affect Using Facial Emotions', 'embedding': []}, {'id': 15056, 'abstractText': 'The cold dark matter model predicts that dark matter haloes are connected by filaments. Direct measurements of the masses and structure of these filaments are difficult, but recently several studies have detected these dark-matter-dominated filaments using weak lensing. Here we study the efficiency of galaxy formation within the filaments by measuring their total mass-to-light ratios and stellar mass fractions. Specifically, we stack pairs of luminous red galaxies (LRGs) with a typical separation on the sky of 8 h<sup>−1</sup> Mpc. We stack background galaxy shapes around pairs to obtain mass maps through weak lensing, and we stack galaxies from the Sloan Digital Sky Survey to obtain maps of light and stellar mass. To isolate the signal from the filament, we construct two matched catalogues of physical and non-physical (projected) LRG pairs, with the same distributions of redshift and separation. We then subtract the two stacked maps. Using LRG pair samples from the Baryon Oscillation Spectroscopic Survey at two different redshifts, we find that the evolution of the mass in filament is consistent with the predictions from perturbation theory. The filaments are not entirely dark: Their mass-to-light ratios (M/L = 351 ± 137 in solar units in the rband) and stellar mass fractions (M<inf>stellar</inf>/M = 0.0073 ± 0.0030) are consistent with the cosmic values (and with their redshift evolutions).', 'title': 'How dark are filaments in the cosmic web?', 'embedding': []}, {'id': 15057, 'abstractText': 'Knowledge mapping has been studied widely in knowledge management context, but the terminology of knowledge map system (KMSs) is not been much studied and it needs to identify state the art KMSs topic from a literature review to propose the future research. The method refers to the systematic literature review as guidelines from Kitchenham, this research gathers, synthesizes, and analyses some paper based on keyword “knowledge map system” or “knowledge mapping system” and “knowledge management”, where it published from 2008 until 2017 on four international electronic databases and using predefined review protocol. We obtained 9 articles used in this study and find that most design systems use customized with some computational technics. The scope is still a lot of focus on enterprise and virtual community, accordingly it needs to be developed further system for the country as public services and also validation method that combines elements of content and software.', 'title': 'Knowledge Mapping System Implementation in Knowledge Management: A Systematic Literature Review', 'embedding': []}, {'id': 15058, 'abstractText': 'Accurate and timely information of the extent and changes in corn cultivation are of great significance for agricultural production management, food security and global environment change studies. Due to the high temporal repeat interval, near global coverage and rich spectral bands, MODIS time series data has been demonstrated particularly suitable for detecting the seasonal dynamics of different crops. However, their inherently coarse spatial resolution limit the accuracies of corn identification in regions with small fields or complex agricultural landscapes. In this study, we investigate the potential of using the random forest regression (RF-r) model to map sub-pixel corn cultivation based on time-series of MODIS data. Corn in Heilongjiang province, China in year 2011 was selected as a case study. Five time series of vegetation indices (155 features) derived from different spectral channels of MOD09A1 data were used as candidate features for the RF-g model. The out-of-bag strategy and backward elimination approach were applied to select the optimal spectra-temporal feature subset for corn identification. These derived corn maps were assessed in two ways: (1) wall-to-wall pixel comparison with the Landsat-based reference map to evaluate the classification performance in terms of spatial distribution. (2) prefectural- and county-level comparison with census data to evaluate its classification performance in terms of area estimates. Results show 61 optimal spectro-temporal features for corn cultivation were selected, which achieved the highest classification accuracy, with squared R (R<sup>2</sup>) of 0.7586 and the root mean squared error (RMSE) of 0.085. MODIS-derived corn cultivation area had good agreements with the census data, with R<sup>2</sup> of 0.73 and RMSE of 238.07 km<sup>2</sup> across 43 counties, R<sup>2</sup> of 0.83 and RMSE of 1155.57 km<sup>2</sup> across 12 prefectures. These promising results indicate the great potential of RF-g method in mapping sub-pixel crop distributions based on coarse spatial resolution images.', 'title': 'Mapping sub-pixel corn distribution using MODIS time-series data and a random forest regression model', 'embedding': []}, {'id': 15059, 'abstractText': 'Seven years after the beginning of a massive wastewater injection project in eastern Colombia, local earthquake activity increased significantly. The field operator and the Colombian Geological Survey immediately reinforced the monitoring of the area. Our analysis of the temporal evolution of the seismic and injection data together with our knowledge of the geological parameters of the region indicate that the surge of seismicity is being induced by the re-injection of produced water into the same three producing reservoirs. Earthquake activity began on known faults once disposal rates had reached a threshold of ∼2 × 10<sup>6</sup> m<sup>3</sup> of water per month. The average reservoir pressure had remained constant at 7.6 MPa after several years of production, sustained by a large, active aquifer. Surface injection pressures in the seismically active areas remain below 8.3 MPa, a value large enough to activate some of the faults. Since faults are mapped throughout the region and many do not have seismicity on them, we conclude that the existence of known faults is not the only control on whether earthquakes are generated. Stress conditions of these faults are open to future studies. Earthquakes are primarily found in four clusters, located near faults mapped by the operator. The hypocentres reveal vertical planes with orientations consistent with focal mechanisms of these events. Stress inversion of the focal mechanisms gives a maximum compression in the direction ENE-WSW, which is in agreement with borehole breakout measurements. Since the focal mechanisms of the earthquakes are consistent with the tectonic stress regime, we can conclude that the seismicity is resulting from the activation of critically stressed faults. Slip was progressive and seismic activity reached a peak before declining to few events per month. The decline in seismicity suggests that most of the stress has been relieved on the main faults. The magnitude of a large majority of the recorded earthquakes was lower than 4, as the pore pressure disturbance did not reach the mapped large faults whose activation might have resulted in larger magnitude earthquakes. Our study shows that a good knowledge of the local fault network and conditions of stress is of paramount importance when planning a massive water disposal program. These earthquakes indicate that while faults provide an opportunity to dispose produced water at an economically attractive volume–pressure ratio, the possibility of induced seismicity must also be considered.', 'title': 'Seismicity induced by massive wastewater injection near Puerto Gaitán, Colombia', 'embedding': []}, {'id': 15060, 'abstractText': 'We propose a novel hybrid Convolutional Neural Network (CNN) model with one-versus-one approach to forecast solar flare occurrence with the outputs of four classes (No-flare, C, M, and X) within 24 h. We train and test our model using the same data sets as in Zheng, Li &amp; Wang, and then compare our results with previous models using the true skill statistic (TSS) as primary metric. The main results are as follows. (1) This is the first time that the CNN model in conjunction with one-versus-one approach is used in solar physics to make multiclass flare prediction. (2) In the four-class flare prediction, our model achieves quite high mean scores of TSS = 0.703, 0.489, 0.432, and 0.436 for No-flare, C, M, and X class, respectively, which are much better than or comparable to those of previous studies. In addition, our model obtains TSS scores of 0.703 ± 0.070 for ≥C-class and 0.739 ± 0.109 for ≥M-class predictions. (3) This is the first attempt to open the black-box CNN model to study the visualization of feature maps for interpreting the prediction model. Furthermore, the visualization results indicate that our model pays attention to the regions with strong gradient, strong intensity, high total intensity, and large range of the intensity in high-level feature maps. The median gradient and intensity, the total intensity, and the range of the intensity for high-level feature maps increase approximately with the increase of flare level.', 'title': 'Hybrid deep convolutional neural network with one-versus-one approach for solar flare prediction', 'embedding': []}, {'id': 15061, 'abstractText': 'Mangrove usually found on the outfall or along the beach and commonly the area is a mudflats area. Mangroves have many important roles to the environment and to the marine ecosystem and also protect land from the erosion, flood and even tsunami. Mangrove can be a shelter when there is tempest that come from the sea and can reduce the erosion to the land that cause from the wave. Mangrove have main important role for ecosystem, mapping of the mangrove should be done. But to get the information to map the mangrove area might be difficult since the area of the mangrove is a mudflats area and might have constraint to get the information to map the mangrove area. In this study using remote sensing technology, mapping for mangrove area are easier since it is use satellite image and no need to go to the field which might have difficult and probably have to take longer time to get the data. This study is about to detect mangrove area by using high resolution satellite imagery. Since this study is to detect mangrove area by using high resolution satellite imagery and there are many types of satellite imagery are available, Quickbird images have been used to detect mangrove area. Visual interpretation technique being used to choose the training area and supervised and unsupervised classification will be performed.', 'title': 'Mangrove area detection by using high resolution satellite imagery', 'embedding': []}, {'id': 15062, 'abstractText': 'The paper compares the potential of WorldView-3 (WV-3) and Sentinel-2 (S-2) satellite data for mapping naturally occurring asbestos (NOA) outcrops to be used by geologists in the planning phase of environmental monitoring. The wide distribution as well as the variety and extent of asbestos-bearing rocks make the selected area a significant case study for the evaluation of the feasibility of multispectral VNIR-SWIR (0.425-2.330 μm) remote sensing observations for NOA outcrops mapping, in those areas where the density of vegetation allows their spectral identification. Different classification procedures were used to produce NOA outcrops maps for the study area. In our study, we found in general a good agreement (k &gt; 0.8) between the produced NOA outcrops maps and the extensive available in situ data for the accessible locations.', 'title': 'Worldview-3 and Sentinel-2 Imagery for Mapping Naturally Occurring Asbestos (NOA) in Serpentinites Rocks in Southern Italy', 'embedding': []}, {'id': 15063, 'abstractText': 'The present work aimed to map the various requirements engineering techniques and methodologies for the construction of educational technologies through a systematic mapping study. The research protocol of this mapping included the planning, execution, and analysis of the results phases. This research added information about the diversity of methods and techniques of requirements engineering applied in the process of developing educational technologies. However, this research indicated that the phenomena related to the pedagogical practice experienced by teachers and students are sometimes relegated in this process. This work analyzed the primary studies of the last ten years, making it possible to identify gaps in this area of research to propose research and the generation of new forms of work in the development of educational systems.', 'title': 'Requirements Elicitation and Specification for Educational Technology Development: A Systematic Literature Mapping', 'embedding': []}, {'id': 15064, 'abstractText': 'The article presents the results of a study on the spatial distribution of acts of vandalism and alcohol consumption in prohibited areas. The study made use of data on offences committed in Krakow, reported by citizens of the city and illustrated on an interactive map - the National Safety Risk Map. The gathered data was calibrated and generalised. Then, a map was prepared illustrating the spatial distribution of both categories of offences in different sub-districts in Krakow, as well as a choropleth map of both types of offence. Two methods were used for examining the interrelationship between these offences - regression analysis using the Poisson regression model with a linear correction of the dispersion and dual kernel density estimation with a \"hot spot\" analysis. The study constitutes the first systematic attempt to analyse NSRM data with regard to the spatial distribution and spatial co-occurrence of two common offences in Krakow.', 'title': 'A Spatial Analysis of Selected Categories of Offences in Krakow Based on Data from the National Safety Risk Map', 'embedding': []}, {'id': 15065, 'abstractText': 'This article deals with the computational complexity issue of graphbased simultaneous localization and mapping (SLAM). SLAM allows a robot that is navigating in an unknown environment to build a map of this environment while simultaneously determining the robot pose on this map. Graph-based SLAM is a smoothing method that uses a graph to represent and solve the SLAM problem. We first propose a graph construction that takes advantage of the incremental and sparse characteristics of graph-based SLAM. This incremental construction is exploited to perform several algorithmic optimizations. Second, we present a study of using a heterogeneous architecture to implement the graph-based SLAM algorithm. Indeed, the emergence of recent heterogeneous embedded architectures should lead to a great advance in the design of embedded systems-based robotics applications. As a result of this study, an algorithm-architecture mapping is proposed for a central processing unit-graphics processing unit (CPU-GPU)-based architecture. The study also investigates how this kind of architecture can speed up graph-based SLAM by offloading some critical compute-intensive tasks of the algorithm on the GPU. Some common data sets are used to compare our implementations to the state of the art.', 'title': 'Graph-Based Simultaneous Localization and Mapping: Computational Complexity Reduction on a Multicore Heterogeneous Architecture', 'embedding': []}, {'id': 15066, 'abstractText': 'In finite precision implementations of chaotic maps all trajectories are eventually periodic. The goal of this brief is to develop methods for systematic study of effects of finite precision computations on dynamical behaviors of discrete maps and to carry out a study of the logistic map in this context. In particular, we are interested in finding all cycles when the logistic map is implemented in single and double precision and studying properties of these cycles including the size of the basin of attraction, and the maximum and average convergence times.', 'title': 'Periodic Orbits of the Logistic Map in Single and Double Precision Implementations', 'embedding': []}, {'id': 15067, 'abstractText': 'Evolutionary multitasking (EMT) is a newly emerging research topic in the community of evolutionary computation, which aims to improve the convergence characteristic across multiple distinct optimization tasks simultaneously by triggering knowledge transfer among them. Unfortunately, most of the existing EMT algorithms are only capable of boosting the optimization performance for homogeneous problems which explicitly share the same (or similar) fitness landscapes. Seldom efforts have been devoted to generalize the EMT for solving heterogeneous problems. A few preliminary studies employ domain adaptation techniques to enhance the transferability between two distinct tasks. However, almost all of these methods encounter a severe issue which is the so-called degradation of intertask mapping. Keeping this in mind, a novel rank loss function for acquiring a superior intertask mapping is proposed in this article. In particular, with an evolutionary-path-based representation model for optimization instance, an analytical solution of affine transformation for bridging the gap between two distinct problems is mathematically derived from the proposed rank loss function. It is worth mentioning that the proposed mapping-based transferability enhancement technique can be seamlessly embedded into an EMT paradigm. Finally, the efficacy of our proposed method against several state-of-the-art EMTs is verified experimentally on a number of synthetic multitasking and many-tasking benchmark problems, as well as a practical case study.', 'title': 'Affine Transformation-Enhanced Multifactorial Optimization for Heterogeneous Problems', 'embedding': []}, {'id': 15068, 'abstractText': 'Electrocardiogram recordings during opucal mapping experiments in heart tissue are commonly used tu monitor the health of the preparation and to obtain dominant frequencies during arrhythmic and defibrillatory studies. However the use of ECG reconstructed from optical mapping is seldom used and to date it has not been strictly validated. In this manuscript we present the first detailed validation and comparison of Optical Mapping ECG, or OM-ECG, with standard ECG recordings by calculating the electrostatic potential in space as a function of the voltage measured optically and describe the different approximations that can be used to obtain unipolar or bipolar ECG recordings. We found that in small/medium hearts, such as rabbits, leads that are aligned apex to base only require activation recording from one surface (anterior or posterior) for the OM-ECG to match the ECG while leads aligned left to right may require both an anterior and posterior optical mapping recording. The discrepancy between leads is due to symmetries in the ventricular activations. In the case of ischemic hearts where activations even-out more, the match between the OM-ECG and standard ECG may require only one surface recording for both left-right and base-apex leads. We believe that this methodology has two main and direct applications in the study of cardiac dynamics. The first is during studies of defibrillation where information after the shock may be crucial in the development of new strategies, OM-ECGs do not suffer the current artifacts of standard ECGs during shocks and can be calculated during the entire activation. We present examples in rabbit ventricles where even low amplitude pacing artifacts are captured by the ECG but do not appear in the OM-ECG. The second use of this technique is for reconstructions of intramural dynamics in larger hearts where differences between the ECG and OM-ECG obtained from anterior and posterior recordings can be used to derive the intramural activation.', 'title': 'Electrocardiogram reconstruction from high resolution voltage optical mapping', 'embedding': []}, {'id': 15069, 'abstractText': 'The SZZ approach for identifying fix-inducing changes traces backwards from a commit that fixes a defect to those commits that are implicated in the fix. This approach is at the heart of studies of characteristics of fix-inducing changes, as well as the popular Just-in-Time (JIT) variant of defect prediction. However, some types of commits are invisible to the SZZ approach. We refer to these invisible commits as Ghost Commits. In this paper, we set out to define, quantify, characterize, and mitigate ghost commits that impact the SZZ algorithm during its mapping (i.e., linking defect-fixing commits to those commits that are implicated by the fix) and filtering phases (i.e., removing improbable fix-inducing commits from the set of implicated commits). We mine the version control repositories of 14 open source Apache projects for instances of mapping-phase and filtering-phase ghost commits. We find that (1) 5.66%11.72% of defect-fixing commits of defect-fixing commits only add lines, and thus, cannot be mapped back to implicated commits; (2) 1.05%4.60% of the studied commits only remove lines, and thus, cannot be implicated in future fixes; and (3) that no implicated commits survive the filtering process of 0.35%14.49% defect-fixing commits. Qualitative analysis of ghost commits reveals that 46.5% of 142 addition-only defect-fixing commits add checks (e.g., null-ness or emptiness checks), while 39.7% of 307 removal-only commits clean up (unused) code. Our results suggest that the next generation of SZZ improvements should be language-aware to connect ghost commits to implicated and defect-fixing commits. Based on our observations, we discuss promising directions for mitigation strategies to address each type of ghost commit. Moreover, we implement mitigation strategies for addition-only commits and evaluate those strategies with respect to a baseline approach. The results indicate that our strategies achieve a precision of 0.753, improving the precision of implicated commits by 39.5 percentage points.', 'title': 'The Ghost Commit Problem When Identifying Fix-Inducing Changes: An Empirical Study of Apache Projects', 'embedding': []}, {'id': 15070, 'abstractText': 'Electromagnetic (EM) scattering studies from seashore regions are attracting more and more attentions in recent years. As an important part of the seashores, sand beaches should also be treated seriously. In this paper, the statistical simulation of EM scattering from Gaussian-texture sandbeach surface is studies. As the frequency of incident wave is relatively small, with long wavelength, the roughness of the sand scale is neglected and the facet-based small-slope approximation (SSA) method is employed to the EM scattering calculations of the Gaussian-texture sandbeach surface. When the spatial correlated scattering map of small sandbeach area is derived together with its correspond statistical properties by the facet-based EM scattering model, the amplitude of the spatial correlated scattering map is generated through the MNLT method, which is of high efficiency in the generation. From the comparisons of texture feature, statistical characteristics between the results derived from EM model and the statistical approach, it is demonstrated that the statistical approach can give a fast and effective simulation of spatial correlated scattering map.', 'title': 'Statistical Simulation of EM Scattering from Gaussian-Texture Sandbeach Surface', 'embedding': []}, {'id': 15071, 'abstractText': 'Technical analysis is a widely used method for forecasting the price direction on the financial time series data. This method requires the use of different number and types of analysis algorithms (technical indicators) together. Although these algorithms show successful performance on small-scale financial time series data, significant performance decreases are detected when the size of data increased. On the large-scale financial time series data, it is necessary to implement these algorithms based on the Map-Reduce programming model and examine the performance of the algorithms which are implemented based on this model comparatively. For this purpose, seven different indicators are studied within the scope of this study, new versions of these indicators are implemented using Map-Reduce parallel data processing model and performance comparisons are made with these algorithms. As a result of these comparisons on single-node and multi-node, significant performance gains have been obtained using Map-Reduce programming model.', 'title': 'Technical Analysis on Financial Time Series Data Based on Map-Reduce Programming Model: A Case Study', 'embedding': []}, {'id': 15072, 'abstractText': \"In this paper, we propose two different scanning methods, drone scanning method, and mobile scanning method, using a stereo camera for estimation of the tree's diameter at breast height (DBH). The mobile scanning method is widely used in mapping forest environments and for forest inventory due to its cost-effectiveness and accessibility. Drone scanning has gained wide attention in recent years because of the development of unmanned aerial technology. This study evaluates two existing fitting algorithms to determine the most suitable method for DBH extraction, namely, the least-squares ellipse fitting algorithm and the random Hough transform (RHT) circle fitting algorithm. A stereo camera is used to generate 3D point cloud maps of the pine forest area. The trees in the study area are segmented using clustering algorithms before performing DBH extraction. A three-dimensional point cloud map of pine trees in the study area is generated, and DBH of each individual tree is estimated. From the experimental results, the drone scanning method was found to be more appropriate than the mobile scanning method for the application of forest mapping. From our experimental results, the drone scanning method with the RHT circle fitting algorithm achieved an average precision accuracy of 92.67%.\", 'title': 'Estimation of Tree Diameter at Breast Height using Stereo Camera by Drone Surveying and Mobile Scanning Methods', 'embedding': []}, {'id': 15073, 'abstractText': 'Visual defect inspection and classification are significant steps of most manufacturing processes in the semiconductor and electronics industries. Known and unknown defects on wafer maps tend to cluster, and these spatial patterns provide valuable process information for supporting manufacturing in determining the root causes of abnormal processes. In previous studies, data augmentation-based deep learning (DL) techniques were most commonly used for the identification of wafer map defect patterns (WMDP). Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations were manually designed for the WMDP problem. In this study, we propose a DL-based method with automatic data augmentation for the WMDP task. Basically, it focuses on learning effective discriminative features, from wafer maps, through a deep network structure. The network consists of a convolution-based variational autoencoder (CVAE) sequentially. First, we pre-trained the CVAE on large training data in an unsupervised manner. Second, we fine-tuned the encoder of the CVAE, which was followed by a neural network (NN) classifier, in a supervised manner. Additionally, we describe a simple procedure for automatically searching for improved data augmentation policies. The policy mainly consists of five image processing functions: rotation, flipping, shifting, shearing range, and zooming. The effectiveness of the proposed method was demonstrated through experimental results obtained from a simulation dataset and a real-world wafer map dataset (WM-811K). This study provides guidance for the application of deep learning in semiconductor manufacturing processes to improve product quality and yield.', 'title': 'Unsupervised Pre-Training of Imbalanced Data for Identification of Wafer Map Defect Patterns', 'embedding': []}, {'id': 15074, 'abstractText': \"Introduction: Studies of the movement of the chest wall show their potential in the diagnosis of heart diseases. Few studies have focused on mapping these movements especially in the lower inaudible frequency band. Aims: This study evaluates Body Surface Mapping (BSM) as a method for describing mechanical cardiac activity. Methods: The chest wall's velocity was measured with a Laser Doppler Vibrometer (LDV) on six healthy subjects. The measuring procedure was repeated for 30 points positioned in a grid at the subjects chest. An electrocardiogram (ECG) and respiration was measured to support the signal processing. The heart movement were described using amplitude maps, constructed from the integrated LDV signal components of 1-20 Hz. Results: The impact of the cardiac motion on the displacement of the chest wall was shown as a typical pattern of the changes of the amplitude maps as a function of time. Conclusion: The results had a high reproducibility and were in concordance with existing evidence, thus indicating BSM to be a valid method for characterization of the mechanical cardiac activity.\", 'title': 'Body surface mapping of the mechanical cardiac activity', 'embedding': []}, {'id': 15075, 'abstractText': \"Now a day's Unmanned Aerial Vehicle (UAV) replaces the aeroplane flown by a pilot, and a small high-resolution digital camera replaces the large metric camera, the combination of both are used as a platform for acquiring aerial images, so as called UAV photogrammetry. This study concentrates on the use and the capabilities of UAV photogrammetry for producing topographic maps, and to assess the accuracy of these maps. For that, a lightweight fixed-wing UAV eBee - Sensefly was used as a platform for acquiring aerial digital images of the study area. Before the flight mission, ground control points were established, Leica GS15 GPS determined their 3D coordinates in two sessions of static observations. The digital images were processed using Pix4Dmapper software for producing orthophotos and digital surface models. For accuracy assessment, the root mean square errors (RMSE) is used in which 2.0 cm in Easting, 2.1 cm in Northing and 7.5 cm in Elevation were obtained for orthomosaic and DTM respectively. Based on these assessments, the results showed that the accuracy achieved is following (ASPRS Accuracy Standards for Digital Geospatial Data) within the second and third classes of these standards for horizontal and vertical accuracies. In conclusion, this study shows that UAV photogrammetry can be applied for producing digital maps, orthophotos, contour lines, digital terrain model, digital surface model, and line maps all of them complies with international standards.\", 'title': 'Accuracy Assessment of UAV photogrammetry for Large Scale Topographic Mapping', 'embedding': []}, {'id': 15076, 'abstractText': 'In this study, a real-time image stitching method is proposed for cost-effective high resolution wide angle video shooting. In the first stage, the images taken from more than one camera with fixed position are stitched with a classical algorithm. After the parameters calculated in the first stage are stored, the image pixels are mapped using the stored parameters and ArUco markers. The mapping process is used for real-time panoramic video shooting after it has been calculated for that particular camera setup once. The images taken from the multi camera are combined by remapping with a GPU based approach. Therefore, registered mapping can be used in different environments without changing the position and lenses of the cameras. As a case study, real-time panoramic video is shot with two cost-effective cameras in football matches. Deep-learning based autonomous pilot video shooting is then performed on the high resolution panoramic video obtained. In experiments, 36 FPS speed has been reached by using a standard desktop computer and it has been seen that image quality measurements are at reasonable levels.', 'title': 'Real-Time Image Stitching For Multiple Camera Panoramic Video Shoot: A Case Study in Football Matches', 'embedding': []}, {'id': 15077, 'abstractText': 'Analysis and exploration of spatio-temporal data such as traffic flow and vehicle trajectories have become important in urban planning and management. In this paper, we present a novel visualization technique called route-zooming that can embed spatio-temporal information into a map seamlessly for occlusion-free visualization of both spatial and temporal data. The proposed technique can broaden a selected route in a map by deforming the overall road network. We formulate the problem of route-zooming as a nonlinear least squares optimization problem by defining an energy function that ensures the route is broadened successfully on demand while the distortion caused to the road network is minimized. The spatio-temporal information can then be embedded into the route to reveal both spatial and temporal patterns without occluding the spatial context information. The route-zooming technique is applied in two instantiations including an interactive metro map for city tourism and illustrative maps to highlight information on the broadened roads to prove its applicability. We demonstrate the usability of our spatio-temporal visualization approach with case studies on real traffic flow data. We also study various design choices in our method, including the encoding of the time direction and choices of temporal display, and conduct a comprehensive user study to validate our embedded visualization design.', 'title': 'Embedding Spatio-Temporal Information into Maps by Route-Zooming', 'embedding': []}, {'id': 15078, 'abstractText': \"Contributions: A concept-map-based remedial learning system is presented to enhance students' grasp of the learning concepts of the IEEE floating-point standard and microprocessor without interlocked pipeline stages (MIPSs) encoding according to their understanding of these learning concepts. Background: Concept maps have been used to represent the knowledge structures of learning topics. This study presents another usage of concept maps, illustrating the prerequisite relationships between and among the learning concepts in a concept map. Simulation-based systems for learning the IEEE floating-point standard and MIPS encoding have been implemented. A simple remedial learning system that helps students to learn the IEEE floating-point standard and MIPS encoding would therefore be valuable, but had yet to be designed. Intended Outcomes: Students' understandings of the IEEE floating-point standard and MIPS encoding are expected to enhance via studying the remedial materials generated by this system. Application Design: A one-group-pretest-posttest design was utilized. The students first took a pretest to get their grasp of the learning concepts, and then studied the remedial learning materials according to their understanding of the learning concepts in the pretest. Findings: 1) The score progress of the IEEE floating-point learners was significant after they undertook remedial learning; 2) the score progress of the low-achieving students was significantly greater than that of the high-achieving students, in their learning of the IEEE float-pointing standard; 3) the score progress of the MIPS encoding learners was significant after they undertook remedial learning; and 4) the score progress of the low-achieving students was significantly greater than that of the high-achieving students in their learning of MIPS encoding, with weak evidence.\", 'title': 'A Concept Map-Based Remedial Learning System With Applications to the IEEE Floating-Point Standard and MIPS Encoding', 'embedding': []}, {'id': 15079, 'abstractText': 'Great inland freshwater lakes play an important role in regulating inland water resources, and the usage of synthetic aperture radar (SAR) images for the accurate waterline mapping is an effective technical means to study the dynamic changes of great inland lakes. In this article, the Danjiangkou (DJK) reservoir is selected as a study case, and a novel waterline mapping method with four main parts is proposed to monitor the water area dynamically. First, a coarse segmentation method is implemented to extract the initial waterline. Second, a strategy of division in local regions is given to speed up the subsequent processes. Third, a combination of a speckle filter and an improved geometric active contour model is used for refined segmentation. Finally, a change detection method is used to study the changing lake. Furthermore, six SAR images obtained by the Gaofen-3 (GF-3) and Sentinel-1A (S-1A) satellites in the DJK reservoir, Hongze lake and Poyang lake are tested to verify the universality of the proposed water area extraction method. The results demonstrate excellent performances with an accuracy of over 97% and an average contour offset under 0.7 pixels. Besides, the time-series analysis of the DJK reservoir is applied based on the mapped waterlines of 37 SAR images collected from January to December in 2017. Comparing with the changing tendency of the water level surveyed in the DJK reservoir, the waterline mapping results and the filed survey data have great consistency, which further proves the validity of the proposed method, also presents the significant potential of the GF-3 and S-1A SAR images for managing the water resource.', 'title': 'Dynamic Waterline Mapping of Inland Great Lakes Using Time-Series SAR Data From GF-3 and S-1A Satellites: A Case Study of DJK Reservoir, China', 'embedding': []}, {'id': 15080, 'abstractText': 'Local climate zone (LCZ) classification system provides standard urban morphological classification for urban heat island studies and weather and climate modelling. Based on the definition of the LCZ, various semi-supervised classification approaches have been proposed to generate LCZ maps for different cities using available satellite data. Given that the acquisition of training data is labor intensive, it is practical to develop new models that are suitable for LCZ classification for any cities without the need for training data/samples. In this study, a novel domain-adaptation co-training approach with self-paced learning is designed to generate LCZ maps for new cities with which valid training samples from existing cities are explored and transferred to new target cities for classification. Experimental results show that the proposed approach could derive LCZ maps for the four testing cities, with an overall accuracy of 69.8%, which is over 10% more accurate than conventional approaches. Compared with conventional approaches, the novel approach does not need prior knowledge about the target cities, and it can automatically generate worldwide LCZ maps to support urban-climate studies for cities in the world.', 'title': 'A co-training approach to the classification of local climate zones with multi-source data', 'embedding': []}, {'id': 15081, 'abstractText': 'We propose a new cloud gaming platform to address the limitations of the existing ones. We study the rendering pipeline of 2D planar maps, and convert it into the server and client pipelines. While doing so naturally gives us a distributed rendering platform, compressing 2D planar maps for transmission has never been studied in the literature. In this paper, we propose a compression component for 2D planar maps with several parametrized modules, where the optimal parameters are identified through real experiments. The resulting cloud gaming platform is evaluated through extensive experiments with diverse game scenes. The evaluation results are promising, compared to the state-of-the-art x265 codec, our platform: (i) achieves better perceptual video quality, by up to 0.14 in SSIM, (ii) runs fast, where the client pipeline takes ≤ 0.83 ms to render each frame, and (iii) scales well for ultra-high-resolution displays, as we observe no bitrate increase when moving from 720p to 1080p, 2K, and 4K displays. The study can be extended in several directions, e.g., we plan to leverage the temporal redundancy of the 2D planar maps, for even better performance.', 'title': 'Optimizing next-generation cloud gaming platforms with planar map streaming and distributed rendering', 'embedding': []}, {'id': 15082, 'abstractText': \"It is essential use descriptive, visual, and statistical methods to interpret human resource data and processes. This study aims to increase the efficacy in calculating the data analysis of employees through the training of talent mapping to the students of psychology. This study used the design of a non-equivalent control group with pretest and posttest design. The measurement tool was the people analytics-efficacy scale adapted from the combination of Bandura's self-efficacy scale and the concept of people analytics. The result of this study concluded that talent mapping training was able to increase the confidence of psychology students. It means that high people analytics-efficacy do not only perceive themselves to be able to complete talent mapping operations on a larger or more difficult scale but also show higher confidence in their ability to successfully perform existing talent mapping operations.\", 'title': 'Talent Mapping Training to Improve People Analytics Efficacy of University Students', 'embedding': []}, {'id': 15083, 'abstractText': 'Mean aortic pressure (MAP) is a major determinant of perfusion in all organs systems. The ability to forecast MAP would enhance the ability of physicians to estimate prognosis of the patient and assist in early detection of hemodynamic instability. However, forecasting MAP is challenging because the blood pressure (BP) time series is noisy and can be highly non-stationary. The aim of this study was to forecast the mean aortic pressure five minutes in advance, using the 25 Hz time series data of previous five minutes as input. We provide a benchmark study of different deep learning models for BP forecasting. We investigate a left ventricular dwelling transvalvular micro-axial device, the Impella, in patients undergoing high-risk percutaneous intervention. The Impella provides hemodynamic support, thus aiding in native heart function recovery. It is also equipped with pressure sensors to capture high frequency MAP measurements at origin, instead of peripherally. Our dataset and the clinical application is novel in the BP forecasting field. We performed a comprehensive study on time series with increasing, decreasing, and stationary trends. The experiments show that recurrent neural networks with Legendre Memory Unit achieve the best performance with an overall forecasting error of 1.8 mmHg.', 'title': 'Aortic Pressure Forecasting With Deep Learning', 'embedding': []}, {'id': 15084, 'abstractText': 'This research is devoted to the study and analysis of modern methods of building routes. The article describes the data provided by the Open street map, google maps API and ways of working with them, as well as various routing algorithms used in Graphhopper, Yandex.Maps, and other routing services. In the course of the study, based on the material studied about working with maps, graphs, and routing services, a web application was created. It is aimed at solving the problem of optimizing the route for the needs of the user.', 'title': 'Research of Modern Routing Systems', 'embedding': []}, {'id': 15085, 'abstractText': \"Landslide is an activity from balance disruption which triggers the movement of a mass of soil and rock down a sloped section of land. Boyolali Regency is one of 35 regencies in Central Java Province which has high vulnerability to landslide. In order to reduce the number of casualties and property loss, this study aims to create a new model of landslide prone area map using the parameters which cause landslide, such as rainfall, soil types, drainage, slope, and land cover. The parameters are processed and analyzed using the combination of the scoring method and the polygon thiessen method. The scoring method is implemented to determine landslide prone areas, while the polygon thiessen method is applied to do the overlay and spatial mapping of landslide prone areas. The hypothesis proposed in this study is that the combination of scoring method and the polygon thiessen can map landslide prone areas in Boyolali Regency accurately. The result of the study shows that the model of the landslide prone area's accuracy is 83.3%. The landslide prone area map shows the four sub districts in Boyolali Regency which meet the criteria of high landslide vulnerability level are Solo, Ampel, Musuk and Cepogo Sub Districts.\", 'title': 'A New Model of Landslide Prone Map Using a Combination of Scoring and Polygon Thiessen Methods', 'embedding': []}, {'id': 15086, 'abstractText': \"Fast and precise noninvasive evaluation of tissue mechanical properties is of high importance in ultrasound shear wave elastography. In this study, we present an updated, faster version of the local phase velocity-based imaging (LPVI) method used to create images of local phase velocity in soft tissues. The updated LPVI implementation uses 1-D Fourier transforms in spatial dimensions separately in comparison to its original implementation. A directional filter is applied upon the shear wave field to extract the left-to-right (LR) and right-to-left (RL) propagating shear waves. A local shear wave phase velocity map is recovered based on both LR and RL waves. Finally, a 2-D shear wave velocity map is reconstructed by combining the LR and RL phase velocity maps. LPVI performance for shear wave displacement and velocity-wave motion data is examined. A study of LPVI used for only one data acquisition with multiple focused ultrasound push beams is presented. The lesion placement with respect to the pushes and whether two sequential pushes provided different results from two simultaneous radiation force pushes was investigated. The addition of white Gaussian noise to the wave motion data was also tested to examine the LPVI method's performance. Robust and accurate shear wave phase velocity maps are reconstructed using the proposed LPVI method using numerical tissue-mimicking phantoms with inclusions. Results from the numerical phantom study showed that the reconstructed, asymmetric inclusions, for various axial locations, are better preserved for shear wave particle velocity signals compared with particle displacement motion data.\", 'title': 'Fast Local Phase Velocity-Based Imaging: Shear Wave Particle Velocity and Displacement Motion Study', 'embedding': []}, {'id': 15087, 'abstractText': 'In robotics studies, problems can be given as mapping, location estimation, exploration and navigation. Navigation problem; It can be subdivided into path planning, path tracking and obstacle avoidance. In the study, in the Gazebo simulation environment, a 4-wheel robot and a laser distance sensor were used. 2D mapping, path planning and path following with respect to the 2D map are implemented on Robot Operating System (ROS). Potential-based path planning was compared with the A* algorithm. Potential-based path tracking was compared with the dynamic window approach (DWA). In tests conducted in the Gazebo simulation environment, it was observed that the potential-based approach yielded successful results in path planning and path tracking.', 'title': 'Potential Field Path Planning and Potential Field Path Tracking', 'embedding': []}, {'id': 15088, 'abstractText': 'Development of applications in knowledge mapping requires a system architecture approach to be able to explain conceptual models that define structure, behaviour and view of the system. There are many system architectures that are made for knowledge mapping but are very diverse, and there are no general guidelines that help a scholar used as references in making them. Therefore this research proposes a universal architectural system that can be used to create a knowledge mapping system. With the literature study method and content analysis on the system architecture used in previous studies, this study proposes four layers which can generally conduct in creating architectural systems, namely: Acquisition layer, database layer, knowledge mapping process layer and user interface layer.', 'title': 'Toward a Common System Architecture for Knowledge Mapping', 'embedding': []}, {'id': 15089, 'abstractText': \"This proposal is about a study we recently published in the IEEE Transaction of Software Engineering journal [4]. Context: Collaborative software engineering (CoSE) deals with methods, processes and tools for enhancing collaboration, communication, and co-ordination (3C) among team members. CoSE can be employed to conceive different kinds of artifacts during the development and evolution of software systems. For instance, when focusing on software design, multiple stakeholders with different expertise and responsibility collaborate on the system design. Model-Driven Software Engineering (MDSE) provides suitable techniques and tools for specifying, manipulating, and analyzing modeling artifacts including metamodels, models, and transformations. Collaborative MDSE consists of methods or techniques in which multiple stakeholders manage, collaborate, and are aware of each others' work on a set of shared models. A collaborative MDSE approach is composed of three main complementary dimensions: (i) a model management infrastructure for managing the life cycle of the models, (ii) a set of collaboration means for allowing involved stakeholders to work on the modelling artifacts collaboratively, and (iii) a set of communication means for allowing involved stakeholders to exchange, share, and communicate information within the team. Collaborative MDSE is attracting several research efforts from different research areas (e.g., model-driven engineering, global software engineering, etc.), resulting in a variegated scientific body of knowledge on the topic. Objective: In this study we aim at identifying, classifying, and understanding existing collaborative MDSE approaches. More specifically, our goal is to assess (i) the key characteristics of collaborative MDSE approaches (e.g., model editing environments, model versioning mechanisms, model repositories, support for communication and decision making), (ii) their faced challenges and limitations, and (iii) the interest of researchers in collaborative MDSE approaches over time and their focus on the three dimensions of collaborative MDSE. Method: In order to achieve this, we designed and conducted a systematic mapping study on collaborative MDSE. Starting from over 3,000 potentially relevant studies, we applied a rigorous selection procedure resulting in 106 selected papers, further clustered into 48 primary studies, along a time span of nineteen years. A suitable classification framework has been empirically defined and rigorously applied for extracting key information from each selected study. We collated, summarized, and analyzed extracted data by applying scientifically sound data synthesis techniques. Results: In addition to a number of specific insights, our analysis revealed the following key findings: (i) there is a growing scientific interest on collaborative MDSE in the last years; (ii) multi-view modeling, validation support, reuse, and branching are more rarely covered with respect to other aspects about collaborative MDSE; (iii) different primary studies focus differently on individual dimensions of collaborative MDSE (i.e., model management, collaboration, and communication); (iv) most approaches are language-specific, with a prominence of UML-based approaches; (v) few approaches support the interplay between synchronous and asynchronous collaboration. Conclusion: This study gives a solid foundation for a thorough identification and comparison of existing and future approaches for collaborative MDSE. Those results can be used by both researchers and practitioners for identifying existing research/technical gaps to attack, better scoping their own contributions to the field, or better understanding or refining existing ones.\", 'title': 'Collaborative Model-Driven Software Engineering: A Classification Framework and a Research Map [Extended Abstract]', 'embedding': []}, {'id': 15090, 'abstractText': 'Amplification can occur in a graben as a result of strong earthquake-induced ground motion. Thus, in seismic hazard and seismic site response studies, it is of the utmost importance to determine the geometry of the bedrock depth. The main objectives of this study were to determine the bedrock depth and map the depth-to-bedrock ratio for use in land use planning in regard to the mitigation of earthquake hazards in the Eskişehir Basin. The fundamental resonance frequencies (f<inf>r</inf> ) of 318 investigation sites in the Eskişehir Basin were determined through case studies, and the 2-D S-wave velocity structure down to the bedrock depth was explored. Single-station microtremor data were collected from the 318 sites, as well as microtremor array data from nine sites, seismic reflection data from six sites, deep-drilling log data from three sites and shallow drilling log data from ten sites in the Eskişehir Graben. The fundamental resonance frequencies of the Eskişehir Basin sites were obtained from the microtremor data using the horizontal-to vertical (H/V) spectral ratio (HVSR) method. The phase velocities of the Rayleigh waves were estimated from the microtremor data using the spatial autocorrelation (SPAC) method. The fundamental resonance frequency range at the deepest point of the Eskişehir Basin was found to be 0.23–0.35\\ue251Hz. Based on the microtremor array measurements and the 2-D S-wave velocity profiles obtained using the SPAC method, a bedrock level with an average velocity of 1300 m\\ue251s<sup>−1</sup> was accepted as the bedrock depth limit in the region. The log data from a deep borehole and a seismic reflection cross-section of the basement rocks of the Eskişehir Basin were obtained and permitted a comparison of bedrock levels. Tests carried out using a multichannel walk-away technique permitted a seismic reflection cross-section to be obtained up to a depth of 1500–2000\\ue251m using an explosive energy source. The relationship between the fundamental resonance frequency in the Eskişehir Basin and the results of deep drilling, shallow drilling, shear wave velocity measurement and sedimentary cover depth measurement obtained from the seismic reflection section was expressed in the form of a nonlinear regression equation. An empirical relationship between f<inf>r</inf>, the thickness of sediments and the bedrock depth is suggested for use in future microzonation studies of sites in the region. The results revealed a maximum basin depth of 1000\\ue251m, located in the northeast of the Eskişehir Basin, and the SPAC and HVSR results indicated that within the study area the basin is characterized by a thin local sedimentary cover with low shear wave velocity overlying stiff materials, resulting in a sharp velocity contrast. The thicknesses of the old Quaternary and Tertiary fluvial sediments within the basin serve as the primary data sources in seismic hazard and seismic site response studies, and these results add to the body of available seismic hazard data contributing to a seismic microzonation of the Eskişehir Graben in advance of the severe earthquakes expected in the Anatolian Region.', 'title': 'An investigation into the bedrock depth in the Eskisehir Quaternary Basin (Turkey) using the microtremor method', 'embedding': []}, {'id': 15091, 'abstractText': 'We extend the velocity channel analysis (VCA), introduced by Lazarian &amp; Pogosyan, of the intensity fluctuations in the velocity slices of position–position–velocity (PPV) spectroscopic data from Doppler broadened lines to study statistical anisotropy of the underlying velocity and density that arises in a turbulent medium from the presence of magnetic field. In particular, we study analytically how the anisotropy of the intensity correlation in the channel maps changes with the thickness of velocity channels. In agreement with the earlier VCA studies, we find that the anisotropy in the thick channels reflects the anisotropy of the density field, while the relative contribution of density and velocity fluctuations to the thin velocity channels depends on the density spectral slope. We show that the anisotropies arising from Alfvén, slow and fast magnetohydrodynamical modes are different; in particular, the anisotropy in PPV created by fast modes is opposite to that created by Alfvén and slow modes, and this can be used to separate their contributions. We successfully compare our results with the recent numerical study of the PPV anisotropies measured with synthetic observations. We also extend our study to the medium with self-absorption as well as to the case of absorption lines. In addition, we demonstrate how the studies of anisotropy can be performed using interferometers.', 'title': 'Extending velocity channel analysis for studying turbulence anisotropies', 'embedding': []}, {'id': 15092, 'abstractText': 'Software developers build complex systems using plenty of third-party libraries. Documentation is key to understand and use the functionality provided via the libraries APIs. Therefore, functionality is the main focus of contemporary API documentation, while cross-cutting concerns such as security are almost never considered at all, especially when the API itself does not provide security features. Documentations of JavaScript libraries for use in web applications, e.g., do not specify how to add or adapt a Content Security Policy (CSP) to mitigate content injection attacks like Cross-Site Scripting (XSS). This is unfortunate, as security-relevant API documentation might have an influence on secure coding practices and prevailing major vulnerabilities such as XSS. For the first time, we study the effects of integrating security-relevant information in non-security API documentation. For this purpose, we took CSP as an exemplary study object and extended the official Google Maps JavaScript API documentation with security-relevant CSP information in three distinct manners. Then, we evaluated the usage of these variations in a between-group eye-tracking lab study involving N=49 participants. Our observations suggest: (1) Developers are focused on elements with code examples. They mostly skim the documentation while searching for a quick solution to their programming task. This finding gives further evidence to results of related studies. (2) The location where CSP-related code examples are placed in non-security API documentation significantly impacts the time it takes to find this security-relevant information. In particular, the study results showed that the proximity to functional-related code examples in documentation is a decisive factor. (3) Examples significantly help to produce secure CSP solutions. (4) Developers have additional information needs that our approach cannot meet. Overall, our study contributes to a first understanding of the impact of security-relevant information in non-security API documentation on CSP implementation. Although further research is required, our findings emphasize that API producers should take responsibility for adequately documenting security aspects and thus supporting the sensibility and training of developers to implement secure systems. This responsibility also holds in seemingly non-security relevant contexts.', 'title': '\"I just looked for the solution!\" - On Integrating Security-Relevant Information in Non-Security API Documentation to Support Secure Coding Practices', 'embedding': []}, {'id': 15093, 'abstractText': \"Grain is the relationship between the people's livelihood and social stability of the special goods, is the stability of the world's important strategic materials, but also is the human survival and social development of a solid foundation, the world attaches great importance to this. Grain production is closely related to climate factors, especially in the current stage of serious ecological environment destruction. Climate change has become an important factor affecting grain production and security. The scholars speculate that in the next 20–50 years, China's agriculture will be seriously affected by climate factors, and the grain production will be adversely affected. Analysis of meteorological factors affecting grain production, with a view to the development of food policy and the implementation of grain security system control to provide rationalization proposals. Wheat is one of the main grain crops in China, and annual planting area and total yield are second only to rice and maize. Henan is a major grain production province in China, which mainly grow winter wheat. During the whole growth and development of winter wheat, the meteorological factors such as temperature, light and rainfall are important environmental factors that affect the normal growth of winter wheat. Many scholars at home and abroad have done a lot of fruitful work in the field of meteorology and wheat production. Chinese studies on meteorology and grain production can be traced back to the Northern Wei Dynasty. China's outstanding agronomist Jia Sixie, in his book “Qi Min Yao Shu” described in detail the seasons, climates, crops and their mutual relationships. Most of the above research methods are based on the principle of statistics and fuzzy mathematics to study the effect of meteorological factors on the yield of wheat growth period. The grain production system itself is a grey system, which is influenced by many influencing factors. The large sample of data defects, for food production system is no longer applicable, and grey system theory just to make up for the mathematical statistics method defects. The existing literature review focuses on the impact of meteorological factors on the yield of grain during the whole growth period, there is little literature about combining phenology changes to study the influence of meteorological factors on different growth stages of crops, the results are not well explained its various growth stages of the impact mechanism. In order to explore the influence of meteorological factors in different growth stages of crops, this paper firstly chooses winter wheat in Henan province as research object by dividing the winter wheat growth period into nine growth stages according to the phonological map. Then the study selects the yield data and daily average meteorological data of winter wheat in Henan province from 2005 to 2015 and uses the exponential smoothing method to calculate the meteorological yield of winter wheat, and finally establishes the meteorological factors set and the grey relational analysis model to doubly analyze the influence of meteorological factors on winter wheat in different growth stages. The results show that in the case of single analysis, the average relative humidity has the greatest influence on meteorological yield of winter wheat during sowing stage, tillering stage, overwintering stage and grouting stage; the mean wind speed has the greatest influence on meteorological yield of winter wheat during the period of reviving stage, jointing stage and maturing stage; the average daily temperature has the greatest influence on meteorological yield of winter wheat during the seeding stage; the duration of illumination has the greatest influence on meteorological yield of winter wheat during the heading stage. In the case of double quantitative analysis, the meteorological yield of winter wheat was influenced by the average relative temperature and the daily maximum temperature at the seedling stage, the average relative humidity of the tillering stage and the overwintering stage and the mean wind speed of jointing stage. Generally speaking, the results of this study are in agreement with previous studies, And this article more comprehensive and specific study of the whole growth stage of winter wheat different meteorological factors on the impact of meteorological production. Through the study of the influence of meteorological factors on grain yield, it is of great significance to the sustainable development of agriculture in Henan province and to improve the economic strength of Henan province, which is of theoretical significance to the social and economic development of the province and the improvement of grain yield.\", 'title': 'Quantitative analysis of the influence of meteorological factors at different growth stages on yield of winter wheat based on grey relational analysis', 'embedding': []}, {'id': 15094, 'abstractText': 'Estimating a depth map and, at the same time, predicting the 3D pose of an object from a single 2D color image is a very challenging task. Depth estimation is typically performed through stereo vision by following several time-consuming stages, such as epipolar geometry, rectification and matching. Alternatively, when stereo vision is not useful or applicable, depth relations can be inferred from a single image as studied in this paper. More precisely, deep learning is applied in order to solve the problem of estimating a depth map from a single image. Then, that map is used for predicting the 3D pose of the main object depicted in the image. The proposed model consists of two successive neural networks. The first network is based on a Generative Adversarial Neural network (GAN). It estimates a dense depth map from the given color image. A Convolutional Neural Network (CNN) is then used to predict the 3D pose from the generated depth map through regression. The main difficulty to jointly estimate depth maps and 3D poses using deep networks is the lack of training data with both depth and viewpoint annotations. This contribution assumes a cross-domain training procedure with 3D CAD models corresponding to objects appearing in real images in order to render depth images from different viewpoints. These rendered images are then used to guide the GAN network to learn the mapping from the image domain to the depth domain. By exploiting the dataset as a source of training data, the proposed model outperforms state-of-the-art models on the PASCAL 3D+ dataset. The code of the proposed model is publicly available at https://github.com/SaddamAbdulrhman/Depth-and-Viewpoint-Estimation/tree/master.', 'title': 'Adversarial Learning for Depth and Viewpoint Estimation From a Single Image', 'embedding': []}, {'id': 15095, 'abstractText': 'Narrative sensemaking is a fundamental process to understand sequential information. Narrative maps are a visual representation framework that can aid analysts in this process. They allow analysts to understand the big picture of a narrative, uncover new relationships between events, and model connections between storylines. As a sensemaking tool, narrative maps have applications in intelligence analysis, misinformation modeling, and computational journalism. In this work, we seek to understand how analysts construct narrative maps in order to improve narrative map representation and extraction methods. We perform an experiment with a data set of news articles. Our main contribution is an analysis of how analysts construct narrative maps. The insights extracted from our study can be used to design narrative map visualizations, extraction algorithms, and visual analytics tools to support the sensemaking process.', 'title': 'Narrative Sensemaking: Strategies for Narrative Maps Construction', 'embedding': []}, {'id': 15096, 'abstractText': 'Residue contact maps contain important information for understanding the structure and function of proteins, thus contact map prediction is an important problem in the bioinformatics field. In recent years, deep learning has become a very popular tool in many research fields. However, the studies of using deep models to predict residue contact maps are very few. This work attempts to identify contact maps based on a recent breakthrough in deep learning, the residual network. The residual network distinguishes itself from other deep convolutional networks in that it incorporates a structure improvement called identity mapping to enable the neural network to go much deeper without consequent training difficulty. Moreover, dilated convolution is employed into this network to obtain a better performance by enlarging the receptive field of the network. A prediction is made based on the input features of a protein and all the features are input into the network at the same time. The experiments demonstrate that the dilated residual network outperforms the original residual network in contact map prediction. We test the networks on 3 test sets: CAMEO, CASP11 and a self-built independent test set. On top L/5 long-range contacts of the three test sets, the accuracy of the dilated network is higher than the non-dilated one by 5.2 %, 4.6% and 2.9%, respectively. Furthermore, it is confirmed that applying different networks on different features is a worse idea than taking them in together. The accuracy on top L/5 long-range contacts of the latter network is higher than the former one by 9.8% on the CAMEO set, 7.0% on the CASP11 set and 5.9% on the self-built independent test set.', 'title': 'Prediction of protein contact map by fully convolutional dilated residual network', 'embedding': []}, {'id': 15097, 'abstractText': 'We present a novel framework for automatic and efficient synthesis of historical handwritten Arabic text. The main purpose of this framework is to assist word spotting and keyword searching in handwritten historical documents. The proposed framework consists of two main procedures: building a letter connectivity map and synthesizing words. A letter connectivity map includes multiple instances of the various shape of each letter, since a letter in Arabic usually has multiple shapes depends in its position in the word. Each map represents one writer and encodes the specific handwriting style. The letter connectivity map is used to guide the synthesis of any Arabic continuous subword, word, or sentence. The proposed framework automatically generates the letter connectivity map annotation from a several pages historical pages previously annotated. Once the letter connectivity map is available our framework can synthesis the pictorial representation of any Arabic word or sentence from their text representation. The writing style of the synthesized text resembles the writing style of the input pages. The synthesized words can be used in word-spotting and many other historical document processing applications. The proposed approach provides an intuitive and easy-to-use framework to search for a keyword in the rest of the manuscript. Our experimental study shows that our approach enables accurate results in word spotting algorithms.', 'title': 'Automatic Synthesis of Historical Arabic Text for Word-Spotting', 'embedding': []}, {'id': 15098, 'abstractText': 'This paper presents a novel wireframe map structure for resource-constrained robots operating in a rectilinear 2D environment. The wireframe representation compactly represents geometry, in addition to transient situations such as occlusions and boundaries of unexplored regions. We formulate a particle filter to suit this sparse wireframe map structure. Functions for calculating the likelihood of scans, merging wireframes, and resampling are developed to accommodate this map representation. The wireframe structure with the particle filter allows for severe discrete map errors to be corrected, leading to accurate maps with small storage requirements. We show in a simulation study that the algorithm attains a map of an environment with 1 % error, compared to an occupancy grid map obtained with GMapping which attained 23% error with the same storage requirements. A simulation mapping a large environment demonstrates the algorithms scalability.', 'title': 'Wireframe Mapping for Resource-Constrained Robots', 'embedding': []}, {'id': 15099, 'abstractText': 'This study was conducted to create driving episodes using machine-learning-based algorithms that address long-term memory (LTM) and topological mapping. This paper presents a novel episodic memory model for driving safety according to traffic scenes. The model incorporates three important features: adaptive resonance theory (ART), which learns time-series features incrementally while maintaining stability and plasticity for time-series data; self-organizing maps (SOMs), which represent input data as a map with topological relations using self-mapping characteristics; and counter propagation networks (CPNs), which label category maps using input features and counter signals. Category maps represent driving episode information that includes driving contexts and facial expressions. The bursting states of respective maps produce LTM, which is created on ART as episodic memory. Evaluation of the experimentally obtained results show the possibility of using recorded driving episodes with image datasets obtained using an event data recorder (EDR) with two cameras. Using category maps, we visualize driving features according to driving scenes on a public road and an expressway.', 'title': 'Adaptive learning based driving episode description on category maps', 'embedding': []}, {'id': 15100, 'abstractText': 'One of the essential technologies required for environmental recognition of an autonomous vehicle is a localization technique that recognizes the position and orientation of the vehicle. In contrast to previous localization techniques that generate map data from sensor data itself, there is an increasing number of studies using high definition (HD) digital maps. The map-based localization technology consists of predicting the position of the next step through the ego-motion of the vehicle and determining the position through map matching. In this paper, we propose a robust ego-motion estimation and map matching technology for robust vehicle localization. First, we propose a visual odometry (VO) model for robust ego-motion estimation and a vehicle planar motion model based on in-vehicle sensors to improve the robustness of VO in the absence of image features. We also introduce a new line segmentation matching model and a geometric correction method of extracted road marking from an inverse perspective mapping (IPM) for robust map matching techniques. The technology proposed in this paper has been verified in various ways through real autonomous vehicles and successfully acquired the autonomous driving license of the Republic of Korea.', 'title': 'Robust Ego-motion Estimation and Map Matching Technique for Autonomous Vehicle Localization with High Definition Digital Map', 'embedding': []}, {'id': 15101, 'abstractText': 'NAND flash memory has been the default storage component in mobile systems. One of the key technologies for flash management is the address mapping scheme between logical addresses and physical addresses, which deals with the inability of in-place-updating in flash memory. Demand-based page-level mapping cache is often applied to match the cache size constraint and performance requirement of mobile storage systems. However, recent studies showed that the management overhead of mapping cache schemes is sensitive to the host I/O patterns, especially when the mapping cache is small. This paper presents a novel I/O scheduling scheme, called MAP, to alleviate this problem. The proposed scheduling approach reorders I/O requests for performance improvement from two angles: Prioritizing the requests that will hit in the mapping cache, and grouping requests with related logical addresses into large batches. Experimental results show that MAP improved upon traditional I/O schedulers by 30% and 8% in terms of read and write latencies, respectively.', 'title': 'I/O scheduling with mapping cache awareness for flash based storage systems', 'embedding': []}, {'id': 15102, 'abstractText': 'Concept maps are significant tools able to support several tasks in the educational area such as curriculum design, knowledge organization and modeling, students’ assessment and many others. They are also successfully used in learning activities in which students have to represent domain knowledge according to teacher’s assignment. In this context, the development of Learning Analytics approaches would benefit of methods that automatically compare concept maps. Detecting concept maps similarities is relevant to identify how the same concepts are used in different knowledge representations. Algorithms for comparing graphs have been extensively studied in the literature, but they do not appear appropriate for concept maps. In concept maps, concepts exposed are at least as relevant as the structure that contains them. Neglecting the semantic and didactic aspect inevitably causes inaccuracies and the consequently limited applicability in Learning Analytics approaches. In this work, starting from an algorithm which compares didactic characteristic of concept maps, we present an extension which exploits a semantic approach to catch the actual meaning of the concepts expressed in the nodes of the map.', 'title': 'Enriching Didactic Similarity Measures of Concept Maps by a Deep Learning Based Approach', 'embedding': []}, {'id': 15103, 'abstractText': 'Hospitals are known as one of the most important types of infrastructure systems as they are expected to provide health care services continuously, irrespective of the hazardous conditions. Therefore, a framework should be developed to evaluate the existing level of safety of hospitals. As the initial step, the hazards which can affect need to be identified and the level of exposure should be determined. Among these hazards, geological and hydro-meteorological hazards which falling under natural hazards, play a major role towards the safety measures in hospitals. It is because the number of affected people and the affected buildings are higher under geological and hydro-meteorological hazards. Therefore, it is essential to develop exposure maps under these natural hazards. As the first step, hazard maps of Tsunami, floods, landslides, etc. could be obtained. After that, a layer hospitals could be created and then the exposure map could be obtained by locating the hospital layer on the hazard maps. This would illustrates the hazards affecting each hospital in Sri Lanka. This paper discusses the process of developing a framework of the development of exposure maps of Sri Lankan hospitals by superimposing the hospital layer in Sri Lanka on the hazard maps, using a mapping software “ArcGIS” with case studies.', 'title': 'A Framework to Develop Multi-Hazard Maps to Identify the Natural Hazards which Affect the Safety of Sri Lankan Hospitals', 'embedding': []}, {'id': 15104, 'abstractText': 'This paper establishes novel methods for vehicle localization and mapping using a 1D linear automotive radar array in conjuncture with pre-existing lidar maps, and tests if the generated radar map can be made to be 3 dimensional. The basic design of this study was to implement a SLAM (Simultaneous Localization And Mapping) system that co-registers radar data to radar data, and/or register radar data to lidar data. After the execution of experiments, it was established that it is possible to localize the car by relating observed radar data to pre-made lidar maps, and to continually add to a cumulative map made with the radar data that can further aid the localization process. Furthermore, the radar map created using the 1D linear automotive array can be extended to 3D with proposed processing chain, though more experiments to establish the full potential of this capability are recommended.', 'title': 'Localization and 3D Mapping using 1D Automotive Radar Sensor', 'embedding': []}, {'id': 15105, 'abstractText': \"Abstract syntax tree (AST) mapping algorithms are widely used to analyze changes in source code. Despite the foundational role of AST mapping algorithms, little effort has been made to evaluate the accuracy of AST mapping algorithms, i.e., the extent to which an algorithm captures the evolution of code. We observe that a program element often has only one best-mapped program element. Based on this observation, we propose a hierarchical approach to automatically compare the similarity of mapped statements and tokens by different algorithms. By performing the comparison, we determine if each of the compared algorithms generates inaccurate mappings for a statement or its tokens. We invite 12 external experts to determine if three commonly used AST mapping algorithms generate accurate mappings for a statement and its tokens for 200 statements. Based on the experts' feedback, we observe that our approach achieves a precision of 0.98-1.00 and a recall of 0.65-0.75. Furthermore, we conduct a large-scale study with a dataset of ten Java projects containing a total of 263,165 file revisions. Our approach determines that GumTree, MTDiff and IJM generate inaccurate mappings for 20%-29%, 25%-36% and 21%-30% of the file revisions, respectively. Our experimental results show that state-of-the-art AST mapping algorithms still need improvements.\", 'title': 'A Differential Testing Approach for Evaluating Abstract Syntax Tree Mapping Algorithms', 'embedding': []}, {'id': 15106, 'abstractText': 'In this letter, we propose a method for accurately estimating the 6-Degree Of Freedom (DOF) pose in an urban environment when a High Definition (HD) map is available. An HD map expresses 3D geometric data with semantic information in a compressed format and thus is more memory-efficient than point cloud maps. The small capacity of HD maps can be a significant advantage for autonomous vehicles in terms of map storage and updates within a large urban area. Unfortunately, existing approaches failed to sufficiently exploit HD maps by only estimating partial pose. In this study, we present a full 6-DOF localization against an HD map using an onboard stereo camera with semantic information from roads. We introduce an 8-bit representation for road information, which allow for effective bitwise operation when matching between query data and the HD map. For the pose estimation, we leverage a particle filter followed by a full 6-DOF pose optimization. Our experimental results show a median error of approximately 0.3 m in the lateral and longitudinal directions for a drive of approximately 11 km. These results can be used by autonomous vehicles to correct the global position without using Global Positioning System (GPS) data in highly complex urban environments. The median operation speed is approximately 60 msec supporting 10 Hz.', 'title': 'HDMI-Loc: Exploiting High Definition Map Image for Precise Localization via Bitwise Particle Filter', 'embedding': []}, {'id': 15107, 'abstractText': 'Detection of near-ground objects occluded by above-ground vegetation from airborne light detection and ranging (lidar) measurements remains challenging. Our hypothesis is that the probability of obstruction due to objects above ground at any location in the forest environment can be reasonably characterized solely from airborne lidar data. The essence of our approach is to develop a data-driven learning scheme that creates high-resolution two-dimensional (2-D) probability maps for obstruction in the under-canopy environment. These maps contain information about the probabilities of obstruction (clutter map) and lidar undersampling (uncertainty map) in the near-ground space. Airborne and terrestrial lidar data and field survey data collected within the forested mountainous environment of Shenandoah National Park, Virginia, USA are utilized to test and evaluate the proposed approach in this work. A newly developed individual tree detection algorithm is implemented to estimate the undersampled stem contributions to the probability of obstruction. Results show the effectiveness of the tree detection algorithm with an accuracy index (AI) of between 61.5% and 80.7% (tested using field surveys). The estimated clutter maps are compared to the maps created from terrestrial scans (i.e., ground truth) and the results show the root-mean-square error (RMSE) of 0.28, 0.32, and 0.34 at three study sites. The overall framework in deriving near-ground clutter and uncertainty maps from airborne lidar data would be useful information for the prediction of line-of-sight visibility, mobility, and above-ground forest biomass.', 'title': 'Estimation of 2-D Clutter Maps in Complex Under-Canopy Environments From Airborne Discrete-Return Lidar', 'embedding': []}, {'id': 15108, 'abstractText': 'Recent progress in 3D sensor devices and in semantic mapping allows to build very rich HD 3D maps very useful for autonomous navigation and localization. However, these maps are particularly huge and require important memory capabilities as well computational resources. In this paper, we propose a new method for summarizing a 3D map (Mesh)as a set of compact spheres in order to facilitate its use by systems with limited resources (smartphones, robots, UAVs,...). This vision-based summarizing process is applied in a fully automatic way using jointly photometric, geometric and semantic information of the studied environment. The main contribution of this research is to provide a very compact map that maximizes the significance of its content while maintaining the full visibility of the environment. Experimental results in summarizing large-scale 3D map demonstrate the feasibility of our approach and evaluate the performance of the algorithm.', 'title': 'Summarizing Large Scale 3D Mesh', 'embedding': []}, {'id': 15109, 'abstractText': 'This study focuses on effects of uniform and full height map correction methods for dewarping book spread images in an automated book reader design for individuals with visual impairment and blindness. The design concept could also be applied to address the challenging process of book digitization. The method is dependent on the geometry of the book reader setup for acquiring the 3-D maps that yield a high reading accuracy. The experiments were performed on a testing dataset consisting of 142 pages with their corresponding depth maps that were extracted. The accuracy of the book spread images was quantified and measured by introducing the corrected images to an Optical Character Recognition engine. Initially, the book spreads were tested by placing them with a standard alignment which yielded an average accuracy of 95.55% and 96.11% with the uniform maps and the full height maps, respectively. Rotations of the book spreads are introduced in a separate test to see if the two proposed methods are tolerant to unsuspected misaligned placements of the book. These tests yield an average accuracy of 90.63% for the corrections with a uniform map and 94.75% with the full height maps.', 'title': 'Uniform vs Full Height Maps Using a Time of Flight Device for Dewarping Book Spread Images in the Design of an Automated Book Reader', 'embedding': []}, {'id': 15110, 'abstractText': 'This paper considers the method of the winter crop classification map producing in terms of climatic and weather abnormal conditions in 2020. Given that the traditional method of construction involves the use of a training sample, which is collected in ground surveys along the roads. This sample could not be collected under the strict quarantine regime, that is why the classification map was created based on the sample obtained as a result of the photointerpretation. Both, optical Sentine1-2 and SAR Sentinel-l satellite data were used. This is due to the fact, that the period of the winter crop classification map producing fell exactly on the period of time (April and May 2020), when the area of study Odesa region (as well as the whole territory of Ukraine) had a high percentage of cloud cover. At the same time, radar imaging techniques allow us to bypass obstacles such as clouds, but also have lower sampling quality. Therefore, it was decided to combine the obtained classification maps based on radar and optical data by fuzzy logic, considering the degree of belonging of each pixel by the value of the normalized difference vegetation index (NDVI).As a result, the obtained classification maps based on photointerpretation sample have an accuracy close to 95%. The fuzzy logic method allows to increase this value by selecting only the best pixels from classification maps based on radar and optical satellite data.', 'title': 'Losses Assessment for Winter Crops Based on Satellite Data and Fuzzy Logic', 'embedding': []}, {'id': 15111, 'abstractText': 'Almost all of the state-of-the-art object detectors employ convolutional neural network (CNN) to extract feature. However, how to fully utilize spatial information is a challenge. In this paper, we propose an effective framework for object detection. Our motivation is that multi-scale representation and context are extremely important for object detection. For multi-scale representation, our mothed combines hierarchical feature maps to a fusion map, which has abundant spatial information and high-level semantics. For context, we exploit spatial information by stacking multi-region feature maps. The network is learned end-to-end, by minimize an objective function. Our network achieves competitive results, 75.9% mAP on PASCAL VOC 2007, 72.0% mAP on PASCAL VOC 2012 and 23.2% mAP on MS COCO. The speed of the network is 10 fps. Our studies demonstrate that multiscale representation and context can further improve performance of object detection.', 'title': 'Multi-scale Fusion with Context-aware Network for Object Detection', 'embedding': []}, {'id': 15112, 'abstractText': 'In semantic mapping, which connects semantic information to an environment map, it is a challenging task for robots to deal with both local and global information of environments. In addition, it is important to estimate semantic information of unobserved areas from already acquired partial observations in a newly visited environment. On the other hand, previous studies on spatial concept formation enabled a robot to relate multiple words to places from bottom-up observations even when the vocabulary was not provided beforehand. However, the robot could not transfer global information related to the room arrangement between semantic maps from other environments. In this paper, we propose SpCoMapGAN, which generates the semantic map in a newly visited environment by training an inference model using previously estimated semantic maps. SpCoMapGAN uses generative adversarial networks (GANs) to transfer semantic information based on room arrangements to a newly visited environment. Our proposed method assigns semantics to the map of an unknown environment using the prior distribution of the map trained in known environments and the multimodal observations made in the unknown environment. We experimentally show in simulation that SpCoMapGAN can use global information for estimating the semantic map and is superior to previous methods. Finally, we also demonstrate in a real environment that SpCoMapGAN can accurately 1) deal with local information, and 2) acquire the semantic information of real places.', 'title': 'SpCoMapGAN: Spatial Concept Formation-based Semantic Mapping with Generative Adversarial Networks', 'embedding': []}, {'id': 15113, 'abstractText': 'Maps representing the detailed features of the road network are becoming more and more important for autonomous driving and next generation driver assistance systems. The mapping of the road network by specially equipped vehicles through the well-known map providers leads to usually quarterly map updates, which might result in problems encountered by autonomous vehicles in the case that the road information is outdated. Furthermore, the provided maps could lack details relevant to autonomous vehicles and next generation driver assistance systems. As an alternative, road network data can be acquired by common vehicles and subsequently fused at the backend-side to detailed, up-to-date maps. The deduction of maps consisting of point-shaped landmarks by utilizing approaches to SLAM is not trivial but well studied. However, the road network is comprised of many also non-point-shaped landmarks, such as crossroads, roundabouts, sign gantries, traffic islands and crosswalks. The reduction of these complex landmarks to point-shaped ones or the independent consideration of their basic features, such as points and lines, involves a severe loss of useful information and/or accuracy and is, therefore, not appreciated. Instead, we propose a novel approach to the parametric description of complex landmarks as a directed acyclic graph and to handle complex landmark observations by an extended formulation of Bundle-Adjustment-based Full-SLAM. We focus on the challenges imposed by the fusion of complex landmarks, such as attribute interdependencies, dynamic attribute counts, and partial observations of attributes due to occlusions. Furthermore, we show the convergence and consistency of our approach to the fusion of complex landmark observations by a hybrid, real-world scenario consisting of roundabout and traffic sign observations. The approach is presented in a way that allows a straightforward adaptation to other kinds of complex landmark observations by defining an appropriate parametric description.', 'title': 'Parametric fusion of complex landmark observations present within the road network by utilizing bundle-adjustment-based Full-SLAM', 'embedding': []}, {'id': 15114, 'abstractText': 'Spatial sensing is a fundamental requirement for applications in robotics and augmented reality. In urban spaces such as malls, airports, apartments, and others, it is quite challenging for a single robot to map the whole environment. So, we employ a swarm of robots to perform the mapping. One challenge with this approach is merging sub-maps built by each robot. In this work, we use wireless access points, which are ubiquitous in most urban spaces, to provide us with coarse orientation between sub-maps, and use a custom ICP algorithm to refine this orientation to merge them. We demonstrate our approach with maps from a building on campus and evaluate it using two metrics. Our results show that, in the building we studied, we can achieve an average Absolute Trajectory error of 0.2m in comparison to a map created by a single robot and average Root Mean Square mapping error of 1.3m from ground truth landmark locations.', 'title': 'WISDOM: WIreless Sensing-assisted Distributed Online Mapping', 'embedding': []}, {'id': 15115, 'abstractText': \"In this paper, we study extensions to the Gaussian processes (GPs) continuous occupancy mapping problem. There are two classes of occupancy mapping problems that we particularly investigate. The first problem is related to mapping under pose uncertainty and how to propagate pose estimation uncertainty into the map inference. We develop expected kernel and expected submap notions to deal with uncertain inputs. In the second problem, we account for the complication of the robot's perception noise using warped Gaussian processes (WGPs). This approach allows for non-Gaussian noise in the observation space and captures the possible nonlinearity in that space better than standard GPs. The developed techniques can be applied separately or concurrently to a standard GP occupancy mapping problem. According to our experimental results, although taking into account pose uncertainty leads, as expected, to more uncertain maps, by modeling the nonlinearities present in the observation space WGPs improve the map quality.\", 'title': 'Warped Gaussian Processes Occupancy Mapping With Uncertain Inputs', 'embedding': []}, {'id': 15116, 'abstractText': 'This paper proposes a modular teleoperation software platform based on the robot operating system (ROS), which is flexible and portable for different master and slave devices. The main issues for the modularity are to develop its control modules in MATLAB/Simulink and access ROS by the robotics system toolbox, which will facilitate the design, analysis, and adjustment of the control algorithm in teleoperation systems greatly. In addition, a case study for this kind of modular teleoperation system is carried out, which is composed of a Geomagic Touch as the master device and a robotic arm of the Rethink Baxter robot as the slave device. Due to the asymmetric structure property between the master and slave devices, the proper workspace mapping which can cover the whole workspace of the slave robot and simultaneously achieve an excellent mapping accuracy becomes a challenging issue. Different from the traditional methods, a hybrid workspace mapping algorithm is proposed, which uses the joint space mapping to cover the whole workspace of the slave device and the operating space mapping to conduct the elaborate manipulation. A smoothing switch law between the joint space mapping and the operating space mapping is also designed. Comparative experiments are carried out, and the results verify the effectiveness and excellent performance of the proposed design methodology.', 'title': 'Modular Development of Master-Slave Asymmetric Teleoperation Systems With a Novel Workspace Mapping Algorithm', 'embedding': []}, {'id': 15117, 'abstractText': 'In this study, Farmland Fertility Algorithm, which is one of the meta-heuristic optimization algorithms, has been investigated in detail. Three different approaches and different chaotic maps were used to generate the initial population of the algorithm. The chaotic mapped algorithms were tested with quality test functions and the performance of the algorithm was presented and interpreted through comparative tables and graphs. As a result of the tests, it was observed that the results of the approach of Generating the Whole Population and all dimensions with chaotic maps and the third approach in which the population was produced with one dimensional chaotic maps were generally similar or better with the Farmland Fertility Algorithm. In the second approach, it has been observed that the Farmland Fertility Algorithm, in which the tent (Tent) chaotic map is applied, produces more successful results for almost all dimensions and iterations. In the third approach, successful results were obtained in most of the chaotic map algorithms.', 'title': 'Generating Initial Population of Farmland Fertility Algorithm with Chaotic Maps', 'embedding': []}, {'id': 15118, 'abstractText': 'The visualization of the Pareto optimal solution set is one of important issues of the decision-making process on the multi-objective optimization problem. The Pareto optimal solution visualization method using the self-organizing maps (SOM) is one of promising visualization methods. This method has two shortcomings in the Pareto optimal solution representation capability. One is that the maps have incorrect points that represent non-Pareto optimal solutions. The other is that the coverage of the maps for the edge region of the Pareto optimal solution set is not good. This study proposes a Pareto optimal solution visualization method using SOM-NG. In SOM, winner nodes affect neighbor nodes on the map space irrespective of similarity on the input data space. This causes the above-mentioned shortcomings. SOM-NG can form maps with considering similarity on the input space; hence, the shortcomings are expected to be overcome. In addition, in the proposed method, the learning parameter optimization is introduced. The effectiveness of the proposed method is confirmed on the incorrectness and the coverage of maps through numerical experiments.', 'title': 'A Pareto optimal solution visualization method using SOM-NG with learning parameter optimization', 'embedding': []}, {'id': 15119, 'abstractText': \"This paper points out that when a fire occurs in some special semi-closed places, the masses cannot quickly and effectively escape from the site safely, propose a method that self-established maps and target recognition through the robot at the site and guides the masses to escape quickly through motion planning. Self-established maps include three parts: self-exploration, simultaneous localization and mapping and motion planning. Self-exploration uses an active exploration method based on information theory to guide the robot to move the maximum information gain between the current point and each point on the two-dimensional map as the target point; simultaneous localization and mapping apply the core idea of particle filtering. The particle group is scattered at the initial position and the weight of each particle is determined by scanning matching method. The point with the largest particle weight is selected as the current position of the robot. Thus constructing of the dimension raster map; The motion planning is divided into global planning and local planning. The global path planning uses the Dijkstra algorithm to find the global optimal path in the two-dimensional grid map. The local path planning uses the DWA (Dynamic Window Approach) algorithm to find the best speed in the local range. Target recognition uses a template matching algorithm to determine the threshold by multiple experiments to achieve accurate target recognition. This paper analyzes and studies the key technologies of robot' s self-construction and target recognition on the turtlebot experimental platform, and completes the technical feasibility verification of the project, so as to reduce the personal injury in the actual scene application. In addition, all processes can be operated and monitored on the remote computer.\", 'title': 'Feasibility Verification of Independent Mapping Technology under Semi-Closed Fire Conditions', 'embedding': []}, {'id': 15120, 'abstractText': 'Since visualization is a powerful technique to reinforce human cognition, real time robots can be used to study its environment to make a map or a model. Mapping is a fundamental task in navigation through unknown environments. Due to its increased application in robotics, surveillance, tracking system used in security and many other areas researchers have propelled to continuously experiment in more efficient and competitive techniques. In this paper we present an experimental evaluation for mapping system that robustly generates 2D maps using a RGB-D sensor. The evaluation was done for our mobile system which comprises a Kinect. This work mainly presents research on earlier related techniques for mapping evaluation and a comparison of SLAM algorithms. The paper concludes with the analysis of mapping accuracy with the ground truth. Experiment indicate that Gmapping gives an accurate solution for 2D mapping among many ROS compatible algorithms for computationally limited robots.', 'title': 'Accuracy Evaluation of SLAM Algorithms in RGB-D Sensors', 'embedding': []}, {'id': 15121, 'abstractText': 'Since the mid-1980s, eight eruptions have occurred along the Juan de Fuca and Gorda Ridges in the NE Pacific. MBARI has examined seven of them, including the April 2015 eruption at Axial Seamount, using autonomous underwater vehicles (AUVs) to map at 1-m lateral resolution and remotely operated vehicles (ROVs) to observe and sample the lava flows and hydrothermal deposits. We have done similar work on the Alarcon Rise at the mouth of the Gulf of California, where all eruptions pre-date our studies of the ridge system, but are recent enough to host impressive active hydrothermal systems.AUV sonar data were processed with MB-System software and the resulting maps were brought into a geographical information system (GIS) for display and analysis. The highresolution AUV maps were used at sea as basemaps in a real-time GIS to guide our ROV dives with greater efficiency than possible before. From the maps and observations, we defined lava flow boundaries and channel systems, assessed fault activity, calculated flow sizes, evaluated age relationships between flows, and then sampled lavas for chemistry. Those data were coupled with age dates from sediment samples to place changes in eruption styles, lava chemistry, and hydrothermal systems on the ridges into the previously elusive framework of time. Using these tools, we have constructed geologic field maps and volcanic histories of the spreading ridges, much like volcanologists do on land, but all unprecedented for submarine volcanoes. The synergistic high-resolution mapping and targeted ROV sampling of the ridges has permitted better understanding of when, how, how much, and how often spreading ridges erupt, and how their magmatic and hydrothermal systems change over time.', 'title': 'High-resolution AUV mapping and ROV sampling of mid-ocean ridges', 'embedding': []}, {'id': 15122, 'abstractText': 'NAND flash memory has been the default storage component in embedded systems. One of the key technologies for flash management is the address mapping scheme between logical addresses and physical addresses, which deals with the inability of in-place-updating in flash memory. Demand-based page-level mapping cache is often applied to match the cache size constraint and performance requirement of embedded storage systems. However, recent studies showed that the management overhead of mapping cache schemes is sensitive to the host I/O patterns, especially when the mapping cache is small. This paper presents a novel I/O scheduling scheme, called MAP+, to alleviate this problem. The proposed scheduling approach reorders I/O requests for performance improvement from two angles. Prioritizing the requests that will hit in the mapping cache, and grouping requests with related logical addresses into large batches. Batches of requests are reordered to further optimize request waiting time. Experimental results show that MAP+ improved upon traditional I/O schedulers by 48% and 18% in terms of read and write latencies, respectively.', 'title': 'An I/O Scheduling Strategy for Embedded Flash Storage Devices With Mapping Cache', 'embedding': []}, {'id': 15123, 'abstractText': 'High-precision and reliable localization is current research focus in the area of autonomous vehicles. Previous studies rely on either high-cost sensors or some specific characteristics, which means that the methods are limited to only a bit given situations. In this paper, a road DNA based localization method is proposed. It could afford high-precision result and does not have the shortcomings of previous methods at the same time. The scenery on both sides of the roads are used to generate the prior-map. The map is presented as grid map by the joint probability of occupation and reflectivity. With this type of map, different environments show different properties, which means that this method is not limited to specific environments and is effective in most cases. It costs much less memory than the previous maps. The map and live road scene flatting are both generated by data collected by low-cost LIDAR. Normalized Information Distance is utilized to align the live road scene flatting with the road DNA. Experiments show the validation and precision of this method.', 'title': 'Road DNA based localization for autonomous vehicles', 'embedding': []}, {'id': 15124, 'abstractText': \"Supporting blind and visually impaired people with digital maps that can enhance spatial awareness and learning about the surrounding environment is a challenge. This study introduces an audio-tactile you-are-here map system that presents map elements and users' updated location on a mobile pin-matrix display for blind and visually impaired people. In addition to panning and zooming a map, in the system, a set of tactile map symbols consisting of raised and lowered pins have been proposed to present varying map elements. A field test with eight visually impaired subjects and eight blindfolded subjects who did not have experience with tactile maps and Braille was conducted. Subjects would locate surrounding streets easily after a short-time training, as well as nearby point-of-interests (POIs). The results of the evaluation indicated that the mean relative distance error was significantly lower with nearby POIs, which do not require panning while exploring, than far away POIs. Furthermore, it is important to improve the portability of the proposed prototype and develop a one-hand map exploration method.\", 'title': 'Exploration of Location-Aware You-Are-Here Maps on a Pin-Matrix Display', 'embedding': []}, {'id': 15125, 'abstractText': 'In recent years, the core technology enable the rapid rise of autonomous vehicles like the solving the Simultaneous Localization And Mapping (SLAM) problem on the embedded system. And the gmapping package on the ROS platform providing laser-based SLAM is widely used and studied. However, implementing it on embedded system imposes two challenges to us: running ROS requires large CPU consumption and the algorithm of SLAM is computationally intensive. In this paper, we present a system implementation of the gmapping on an ARM based embedded hardware not installing ROS and we also improve the architecture of the mapping system by parallel execution of mapping and estimating pose. The mapping process is running on the embedded system and it can create a 2-D occupancy map from laser and pose data collected by a mobile robot with low CPU load and time consumption. Experimental results carried out with mobile robots in indoor environments illustrate the accuracy of our implementation and the low consumption of time and CPU load of the mapping system.', 'title': 'Indoor mapping using gmapping on embedded system', 'embedding': []}, {'id': 15126, 'abstractText': 'Ventricular tachycardia (VT) is a life-threatening arrhythmia, which can be treated by catheter intervention. Accurate identification of the underlying reentrant circuit is often challenging, yet it is key to successful ablation of the VT. In practice, the cardiologist often uses electrocardiography (ECG) data provided by various catheter mapping techniques, including parameters acquired during sinus rhythm (voltage maps, presence of fragmented/late potentials) and during controlled pacing from different sites of the ventricle, so-called pace-mapping. A novel method is presented here to automatically extract the key information from pace-mapping data with automated detection of paced heartbeats from the surface ECG signals, using wavelet detection of pacing spikes and combined time/energy criteria, and automated delineation of paced beats, QRS peak, and QRS onset. This allows the generation of correlation gradient maps (indicating QRS morphology changes as the catheter is moved) and stimulus-to-QRS maps (sQRS, indicating the delay between pacing and activation of the healthy myocardium). The delineator is shown to be in good agreement with manual annotations from experts in a retrospective study of 18 VT ablation procedures. Paced-QRS detection had 95.2% sensitivity and 98.4% positive predictive value. Resulting sQRS maps had a mean absolute error of 11.1 ms, which was in the same range as the inter-observer errors (9.7 ms). The automatic processing drastically reduces the need for manual annotations. Therefore it makes it feasible to process and visualize, during the procedure, all the relevant parametric maps, which can be analyzed jointly to identify VT circuits and corresponding ablation targets.', 'title': 'A Paced-ECG Detector and Delineator for Automatic Multi-Parametric Catheter Mapping of Ventricular Tachycardia', 'embedding': []}, {'id': 15127, 'abstractText': 'Estimating 3D hand poses from RGB images is essential to a wide range of potential applications, but is challenging owing to substantial ambiguity in the inference of depth information from RGB images. State-of-the-art estimators address this problem by regularizing 3D hand pose estimation models during training to enforce the consistency between the predicted 3D poses and the ground-truth depth maps. However, these estimators rely on both RGB images and the paired depth maps during training. In this study, we propose a conditional generative adversarial network (GAN) model, called Depth-image Guided GAN (DGGAN), to generate realistic depth maps conditioned on the input RGB image, and use the synthesized depth maps to regularize the 3D hand pose estimation model, therefore eliminating the need for ground-truth depth maps. Experimental results on multiple benchmark datasets show that the synthesized depth maps produced by DGGAN are quite effective in regularizing the pose estimation model, yielding new state-of-the-art results in estimation accuracy, notably reducing the mean 3D endpoint errors (EPE) by 4.7%, 16.5%, and 6.8% on the RHD, STB and MHP datasets, respectively.', 'title': 'DGGAN: Depth-image Guided Generative Adversarial Networks for Disentangling RGB and Depth Images in 3D Hand Pose Estimation', 'embedding': []}, {'id': 15128, 'abstractText': 'In making a game, a map is an important component. In making maps, several techniques can be used, one of which uses the Procedural Content Generator (PCG) method. In making maps using PCG can apply the Perlin Noise algorithm, as a generator engine for making maps automatically. The Algorithm Perlin Noise can make a noise gradient of and store values from 1 to 0 in each pixel. This value can be utilized as the height value of a 3D map formed from a point which is then connected to a surface called a mesh. The bigger the mesh, the more detailed a map will be. However, there are obstacles in its formation, namely the burden of the processor in processing the map. The level of detail (LOD) in a mesh will affect the workload of the processor, so we need a dynamic LOD. In this study, game performance measurements were performed using the average FPS with the application of Dynamic LOD and LOD Statistics. The performance test managed to increase processor performance to the maximum extent but did not reduce the overall performance of the game. In table 3 the connectedness calculation uses the person correlation method that the connectedness between the CPU and Vertex has a value of -0.81942 which means that if the CPU goes up the vertices go down, and the value of the connectedness between the CPU and LOD is 0.92299 which means if the CPU performance goes up, the LOD will go up, this indicates The CPU is optimized to run the rendering process and can optimize processor performance.', 'title': 'Optimizing Game Performance with Dynamic Level of Detail Mesh Terrain Based on CPU Usage', 'embedding': []}, {'id': 15129, 'abstractText': 'Concept mapping is a great activity to capture and describe ideas. Several tools that are developed to support concept mapping activities. Several studies show that computer- supported concept mapping tools would contribute to a more efficient collaborative learning performance than using a traditional pencil and paper. The interaction style on how they use and experience with the tool also may differ. A computer- supported concept map authoring tool has been developed in this research to support concept mapping for the Kit-Build Concept Map framework. This research evaluates and analyzes the user experience of two the concept map authoring tools, where one of them incorporates keywords and propositions suggestions support features to construct a concept map from an English reading. The evaluation and analysis are performed by using the short version of the User Experience Questionnaire. The result shows that both the tools are having a positive user experience. The tool that provides support features is hedonically better than the basic tool even though it is not always better pragmatically.', 'title': 'User Experience Evaluation on Computer- Supported Concept Map Authoring Tool of Kit-Build Concept Map Framework', 'embedding': []}, {'id': 15130, 'abstractText': 'Motivated by the concept of circuit design in digital circuit, this paper proposes a one-dimensional (1D) nonlinear model (1D-NLM) for producing 1D discrete-time chaotic maps. Our previous works have designed four nonlinear operations of generating new chaotic maps. However, they focus only on discussing individual nonlinear operations and their properties, but fail to consider their relationship among these operations. The proposed 1D-NLM includes these existing nonlinear operations, develops two new nonlinear operations, discusses their relationship among different nonlinear operations, and investigates the properties of different combinations of these operations. To show the effectiveness of 1D-NLM in generating new chaotic maps, as examples, we provide four new chaotic maps and study their dynamics properties from following three aspects: equilibrium point, stability, and bifurcation diagram. Performance evaluations are provided using the Lyapunov exponent, Shannon entropy, correlation dimension, and initial state sensitivity. The evaluation results show that these new chaotic maps have more complex chaotic behaviors than existing ones. To demonstrate the performance of 1D-NLM in practical applications, we use a pseudo-random number generator (PRNG) to compare new and existing chaotic maps. The randomness test results indicate that new chaotic map generated by 1D-NLM shows better performance than existing ones in designing PRNG.', 'title': 'One-Dimensional Nonlinear Model for Producing Chaos', 'embedding': []}, {'id': 15131, 'abstractText': 'Non-acoustic sensors are widely used in speech signal processing tasks, and their immunity to the background acoustic noise shows great benefits to traditional speech enhancement. To avoid using acoustic speech disturbed by strong noise, spectra mapping from throat microphone (TM) speech to acoustic microphone (AM) speech has been studied. However, there is a distinguished difference between the spectra of the two kinds of speech, and the mapping relationship is different in the low-band and high-band spectra, which limits the performance of the traditional full-band spectra mapping model. In this paper, to improve the perceived quality and intelligibility of TM speech, we investigate the low-band and high-band spectral structure between TM and AM speech, respectively, and propose a spectra-based band-division mapping framework for TM speech enhancement based on the investigation. In the framework, the low-band target spectra of AM speech are mapped based on the equalization method, and the high-band spectra are mapped from the full-band TM speech spectra, which are lack of high-frequency components. The overall framework can be viewed as a combination of spectra equalization in the low band and spectra generation in the high band. Both the objective and subjective evaluation results show clear advantages over the existing TM speech enhancement method based on the full-band spectra mapping.', 'title': 'A Spectra-Based Equalization-Generation Combined Framework for Throat Microphone Speech Enhancement', 'embedding': []}, {'id': 15132, 'abstractText': \"In this study, we propose Salient SLAM, a real-time 3D map generation with saliency information based on human visual characteristics. We used a Fine-Grained technique to generate a saliency map on the spatial domain. Then, we integrated this saliency map to a real-time feature-based Visual SLAM technique. As a result, we can obtain a three-dimensional feature map with saliency information and enable the ability to use this saliency information in robots' autonomous and exploration. Finally, the Salient SLAM map point is leveled into four levels where the higher the level, the higher the score of saliencies in the environment. In the experiments, we investigated and analyzed the characteristics of each saliency level in global feature maps according to the type of objects and visual patterns.\", 'title': 'Integrating Human Visual Perception in Scene Understanding by Saliency Visual SLAM', 'embedding': []}, {'id': 15133, 'abstractText': 'This study proposes to develop, build and implement the IoT of the mobile network robot integrated with features of mapping and location in an internal environment, to trace the best route and obtain a fast and efficient change to detect changes in the environment as an identifier of falls in the elderly. It is observed that it is applied research to aggregate several available algorithms. The robot was provided with internal mapping and location capabilities to map the best route and achieve fast and active movements in a retirement home for the elderly. The mobile robot was also set up to monitor and assist in the transport of medicines and notify the caregiver of any incident with the elderly within its environment. A mobile app controls system and robot development. The main phases are highlighted: definition and acquisition of the model and the components used (mechanical structure, microcontroller, sensors and actuators); application development of user-system interaction; development and construction of a robot, auxiliary modules (environment) and central module; and integration experiments. Monitoring and mapping of the environment are performed using Wall-Following, Simultaneous Location and Mapping (SLAM) and Sensor Fusion techniques. The precise movements of the robot are assured through a combination of navigation and control techniques. Moreover, the robot received internal mapping and location, resources to map the best route and obtain quick and active movements in Institutions of Long Stay for the Elderly (L.T.C.F.). Besides, the performance of the following algorithms was analyzed and compared: Breadth-First Search (B.F.S.), Depth First Search (D.F.S.), and Wall-Following. The B.F.S. algorithm obtained the best results for the minimum path.', 'title': 'Integration of the Mobile Robot and Internet of Things to Monitor Older People', 'embedding': []}, {'id': 15134, 'abstractText': 'In a fuzzy cognitive map-based forecasting model, causal relationships (represented with a weight matrix) are constant. This may hinder the applicability of such a model. In this paper, we propose an adaptive fuzzy cognitive map-based forecasting model. Different from the existing models, the proposed model is made of a collection of fuzzy cognitive maps. Maps are constructed according to the clustering results of the so-called premises covering an entire time series. Subsequently, we use an optimization algorithm to train parameters of each fuzzy cognitive map individually. The proposed model construction procedure allows forming fuzzy cognitive maps that more flexible and, thus, suitable for forecasting of long time series. In experimental studies on synthetic time series and real time series, the proposed model performed very well in comparison with the original fuzzy cognitive map-based forecasting model and another two forecasting models.', 'title': 'A New Adaptive Fuzzy Cognitive Map-Based Forecasting Model for Time Series', 'embedding': []}, {'id': 15135, 'abstractText': 'In multi-label image classification, each image is always associated with multiple labels and labels are usually correlated with each other. The intrinsic relation among labels can definitely contribute to classifier training. However, most previous studies on active learning for multi-label image classification purely mine label correlation based on observed label distribution. They ignore the mapping relation between examples and their labels. This mapping relation also implicates label relationship. Ignoring the mapping relation leads to an uncomprehensive label correlation estimation and results in a bad performance for classification. In this paper, we propose a novel multi-label active learning with low-rank mapping for image classification, called LMMAL, to solve this issue. More precisely, we train a low-rank mapping matrix to signify the mapping relation between the feature space and the label space of a certain multi-label dataset. Using this low-rank mapping relation, we exploit a full label correlation. Subsequently, an effective sampling strategy is designed by integrating this potential information with uncertainty to select the most informative example-label pairs. In addition, we extend LMMAL with automatic labeling (denoted as AL-LMMAL) to further reduce the annotation workload of active learning. Empirical results demonstrate the effectiveness of our approaches.', 'title': 'Multi-label active learning with low-rank mapping for image classification', 'embedding': []}, {'id': 15136, 'abstractText': 'Near realtime flood mapping in densely populated urban areas is critical for emergency response. The strong heterogeneity of urban areas poses a big challenge for accurate near realtime flood mapping. However, previous studies on automatic methods for urban flood mapping perform infeasible in near realtime or fail to generalize well to other floods, for several reasons. First, multitemporal pixel-wise flood mapping requires accurate image registration, hindering the efficiency of large-scale processing. Although automatic image registration has been investigated, precisely coregistered multitemporal image sequence requires time-consuming fine tuning. Additionally, the floods may lead to the loss of many corresponding image points across multitemporal images for accurate coregistration. Second, existing unsupervised methods generally rely on hand-crafted features for floodwater detection. Such features may not well represent the patterns of floodwaters in different areas due to inconsistent weather conditions, illumination, and floodwater spectra. This article proposes a self-supervised learning framework for patch-wise urban flood mapping using bitemporal multispectral satellite imagery. Patch-wise change vector analysis is used with patch features learned through a self-supervised autoencoder to produce patch-wise change maps showing potentially flood-affected areas. Postprocessing including spectral and spatial filtering is applied to these patch-wise change maps to remove nonflood related changes. Final flood maps and parameter sensitivities were evaluated using several performance metrics. Two flood events from areas with differing degrees of urbanization were considered: Hurricane Harvey flood (2017) in Houston, Texas, and Hurricane Florence flood (2018) in Lumberton, North Carolina. The proposed method shows strong performance for self-supervised urban flood mapping.', 'title': 'Urban Flood Mapping With Bitemporal Multispectral Imagery Via a Self-Supervised Learning Framework', 'embedding': []}, {'id': 15137, 'abstractText': 'Localization within high definition maps is a key problem for autonomous navigation as vehicles need to extract information from them. In addition, many navigation tasks are defined with respect to map features. For instance, estimating the cross-track and along-track gaps of a vehicle with respect to a given path is critical for lane keeping or intersection management. Map-based localization is also important for cooperative tasks like platooning in curved roads or lane changing. This work studies different methods to compute map-based coordinates defined as curvilinear abscissa, lateral distance and heading with respect to paths in high definition maps. Four approaches using polylines, lanelets and splines are compared. Thanks to real experiments, the discontinuity issues of polylines used in current high definition maps are evaluated and we discuss advantages and drawbacks of splines-based and lanelet methods. We also report experimental results corresponding to a platoon of two vehicles in curved roads and evaluate the effects of the use of low cost GNSS receivers.', 'title': 'Map-based curvilinear coordinates for autonomous vehicles', 'embedding': []}, {'id': 15138, 'abstractText': \"Wildfire is a significant driver of forest and land cover change in the central interior of British Columbia, Canada. Fuel type maps are a primary input to fire behavior calculations and simulation studies that assess wildfire threat at the landscape level. However, these thematic maps are not easily produced at the scale and speed needed to assess and mitigate wildfire threat on an annual basis. The objective of this research was to explore how an artificial neural network could be used with remotely sensed satellite imagery to map and update fuel types on an annual basis. We applied the artificial neural network over a 40 000-km<sup>2</sup> landscape in central interior British Columbia that burned from a megafire in 2017. Fuel maps were generated for the years 2014-2018, assessed through an independent validation, and evaluated against an existing fuel type map. The highest cross-validation overall accuracy during training was 66.5% and overall accuracy from the independent validation was 63.1%. Generally, the maps had fair agreement with the existing fuel type map (circa 2016), with Cohen's Kappa ranging from 0.28 in 2018 to 0.35 in 2015. Several recommendations are provided for future research using artificial neural networks for fuel typing such as assuring quality of training samples through rigorous standards, designing the network architecture, choosing appropriate cost functions and regularization, incorporating learning of temporal features, and identifying novel fuel types from the output activations.\", 'title': 'FuelNet: An Artificial Neural Network for Learning and Updating Fuel Types for Fire Research', 'embedding': []}, {'id': 15139, 'abstractText': 'With the increased amount of digitized historical documents, information extraction from them gains pace. Historical maps contain valuable information about historical, geographical and economic aspects of an era. Retrieving information from historical maps is more challenging than processing modern maps due to lower image quality, degradation of documents and the massive amount of non-annotated digital map archives. Convolutional Neural Networks (CNN) solved many image processing challenges with great success, but they require a vast amount of annotated data. For historical maps, this means an unprecedented scale of manual data entry and annotation. In this study, we first manually annotated the Third Military Mapping Survey of Austria-Hungary historical map series conducted between 1884 and 1918 and made them publicly accessible. We recognized different road types and their pixel-wise positions automatically by using a CNN architecture and achieved promising results.', 'title': 'Automatic Detection of Road Types From the Third Military Mapping Survey of Austria-Hungary Historical Map Series With Deep Convolutional Neural Networks', 'embedding': []}, {'id': 15140, 'abstractText': 'Extraction of residential areas plays an important role in remote sensing image processing. Extracted results can be applied to various scenarios, including disaster assessment, urban expansion, and environmental change research. Quality residential areas extracted from a remote sensing image must meet three requirements: well-defined boundaries, uniformly highlighted residential area, and no background redundancy in residential areas. Driven by these requirements, this study proposes a global and local saliency analysis model (GLSA) for the extraction of residential areas in high-spatial-resolution remote sensing images. In the proposed model, a global saliency map based on quaternion Fourier transform (QFT) and a global saliency map based on adaptive directional enhancement lifting wavelet transform (ADE-LWT) are generated along with a local saliency map, all of which are fused into a main saliency map based on complementarities. In order to analyze the correlation among spectrums in the remote sensing image, the phase spectrum information of QFT is used on the multispectral images for producing a global saliency map. To acquire the texture and edge features of different scales and orientations, the coefficients acquired by ADE-LWT are used to construct another global saliency map. To discard redundant backgrounds, the amplitude spectrum of the Fourier transform and the spatial relations among patches are introduced into the panchromatic image to generate the local saliency map. Experimental results indicate that the GLSA model can better define the boundaries of residential areas and achieve complete residential areas than current methods. Furthermore, the GLSA model can prevent redundant backgrounds in residential areas and thus acquire more accurate residential areas.', 'title': 'Global and Local Saliency Analysis for the Extraction of Residential Areas in High-Spatial-Resolution Remote Sensing Image', 'embedding': []}, {'id': 15141, 'abstractText': 'The limited scale and genre coverage of labeled data greatly hinders the effectiveness of supervised models, especially when analyzing spoken languages, such as texts transcribed from speech and informal text including tweets and product comments in Internet. In order to effectively utilize multiple labeled datasets with heterogeneous annotations for the same task, this paper proposes a coupled sequence labeling model that can directly learn and infer two heterogeneous annotations simultaneously, using Chinese part-of-speech (POS) tagging as our case study. The key idea is to bundle two sets of POS tags together (e.g., “[NN, n]<sup>n</sup>), and build a conditional random field (CRF) based tagging model in the enlarged space of bundled tags with the help of ambiguous labeling. To train our model on two nonoverlapping datasets that each has only one-side tags, we transform a one-side tag into a set of bundled tags by concatenating the tag with every possible tag at the missing side according to a predefined context-free tag-to-tag mapping function, thus producing ambiguous labeling as weak supervision. We design and investigate four different context-free tag-to-tag mapping functions, and find out that the coupled model achieves its best performance when each one-side tag is mapped to all tags at the other side (namely complete mapping), indicating that the model can effectively learn the loose mapping between the two heterogeneous annotations, without the need of manually designed mapping rules. Moreover, we propose a context-aware online pruning strategy that can more accurately capture mapping relationships between annotations based on contextual evidences and thus effectively solve the severe inefficiency problem with our coupled model under complete mapping, making it comparable with the baseline CRF model. Experiments on benchmark datasets show that our coupled model significantly outperforms the state-of-the-art baselines on both one-side POS tagging and annotation conversion tasks. The codes and newly annotated data are released for research usage.<sup>1</sup>', 'title': 'Coupled POS Tagging on Heterogeneous Annotations', 'embedding': []}, {'id': 15142, 'abstractText': \"In this work, we propose the Road Data Enrichment (RoDE), a framework that fuses data from heterogeneous data sources to enhance Intelligent Transportation System (ITS) services, such as vehicle routing and traffic event detection. We describe RoDE through two services: (i) Route service, and (ii) Event service. For the first service, we present the Twitter MAPS (T-MAPS), a low-cost spatiotemporal model to improve the description of traffic conditions through Location-Based Social Media (LBSM) data. As a case study, we explain how T-MAPS is able to enhance routing and trajectory descriptions by using tweets. Our experiments compare T-MAPS' routes against Google Maps' routes, showing up to 62% of route similarity, even though T-MAPS uses fewer and coarse-grained data. We then propose three applications, Route Sentiment (RS), Route Information (RI), and Area Tags (AT), to enrich T-MAPS' suggested routes. For the second service, we present the Twitter Incident (T-Incident), a low-cost learning-based road incident detection and enrichment approach built using heterogeneous data fusion. Our approach uses a learning-based model to identify patterns on social media data which is then used to describe a class of events, aiming to detect different types of events. Our model to detect events achieved scores above 90%, thus allowing incident detection and description as a RoDE application. As a result, the enriched event description allows ITS to better understand the LBSM user's viewpoint about traffic events (e.g., jams) and points of interest (e.g., restaurants, theaters, stadiums).\", 'title': 'Road Data Enrichment Framework Based on Heterogeneous Data Fusion for ITS', 'embedding': []}, {'id': 15143, 'abstractText': 'With increasing of the spatial resolution of satellite imaging sensors, object-based image analysis (OBIA) has been gaining prominence in remote sensing applications. However, scale selection in multi-scale segmentation and OBIA remains a challenge, which directly reduces efficiency of land cover mapping. In this study, we presented an object-based land cover mapping using adaptive scale segmentation. Central to our method is the use of inherent features of segmented objects to determine whether an object should be segmented with a small scale in a top-down segmentation procedure. We firstly used inherent features of a segmented object to determine whether this object should be segmented with a smaller scale in a top-down segmentation procedure, producing a segmentation map with optimal scales. Then, an object-based SVM classifier was applied on the adaptive-scale segmentation map to yield a land-cover map. We have applied this method on a ZY-3 multi-spectral satellite image to produce land cover map, compared with the results using the traditional mean shift algorithm with fixed scales. The experimental results illustrate that the proposed method is practically helpful and efficient to improve the performance of land cover mapping.', 'title': 'Object-based land cover mapping using adaptive scale segmentation from ZY-3 satellite images', 'embedding': []}, {'id': 15144, 'abstractText': 'Solar panel mapping has gained a rising interest in renewable energy field with the aid of remote sensing imagery. Significant previous work is based on fully supervised learning with classical classifiers or convolutional neural networks (CNNs), which often require manual annotations of pixel-wise ground-truth to provide accurate supervision. Weakly supervised methods can accept image-wise annotations which can help reduce the cost for pixel-level labelling. Inevitable performance gap, however, exists between weakly and fully supervised methods in mapping accuracy. To address this problem, we propose a pseudo supervised deep convolutional network with label correction strategy (PS-CNNLC) for solar panels mapping. It combines the benefits of both weak and strong supervision to provide accurate solar panel extraction. First, a convolutional neural network is trained with positive and negative samples with image-level labels. It is then used to automatically identify more positive samples from randomly selected unlabeled images. The feature maps of the positive samples are further processed by gradient-weighted class activation mapping to generate initial mapping results, which are taken as initial pseudo labels as they are generally coarse and incomplete. A progressive label correction strategy is designed to refine the initial pseudo labels and train an end-to-end target mapping network iteratively, thereby improving the model reliability. Comprehensive evaluations and ablation study conducted validate the superiority of the proposed PS-CNNLC.', 'title': 'Pseudo Supervised Solar Panel Mapping based on Deep Convolutional Networks with Label Correction Strategy in Aerial Images', 'embedding': []}, {'id': 15145, 'abstractText': 'Flood is causing devastating damages every year all over the world. One way to improve the readiness of stakeholders (res-cue authorities, policy makers, and communities) is by providing flood extent maps promptly after the disaster, preferably in an automated way and with a minimum number of satellite imagery to reduce costs. The web application developed in this paper aims to address this problem by mapping the flood extent automatically from SAR images. This web application is portable since it runs on the internet browser, and allows to perform the classification of the flooding in an automated fashion. Another strong point is the rapidity of the processing: the whole processing time was around 3 to 5 minutes for a subset of 20 million pixels. The inundation map returned by our algorithm was validated against vector files mapped by the United Nations Institute for Training and Research (UNITAR) for the same flood event. Regarding the dataset needed in this study, a pair of a preflood SAR image and an optical image of the same area were used to build a training dataset of water and non-water classes. The learning phase is immediately followed by the classification of the post-flood SAR image into a binary flood map. The web application described in this paper was built with open-source Python libraries which are backed by large communities (Django, Scikit-learn among others). The flood map was eventually displayed on OpenStreetMap maps provided by Mapbox.', 'title': 'A Web Application for the Automatic Mapping of the Flood Extent on Sar Images', 'embedding': []}, {'id': 15146, 'abstractText': 'Rapid development of affordable and portable consumer depth cameras facilitates the use of depth information in many computer vision tasks such as intelligent vehicles and 3D reconstruction. However, depth map captured by low-cost depth sensors (e.g., Kinect) usually suffers from low spatial resolution, which limits its potential applications. In this paper, we propose a novel deep network for depth map super-resolution (SR), called DepthSR-Net. The proposed DepthSR-Net automatically infers a high-resolution (HR) depth map from its low-resolution (LR) version by hierarchical features driven residual learning. Specifically, DepthSR-Net is built on residual U-Net deep network architecture. Given LR depth map, we first obtain the desired HR by bicubic interpolation upsampling and then construct an input pyramid to achieve multiple level receptive fields. Next, we extract hierarchical features from the input pyramid, intensity image, and encoder-decoder structure of U-Net. Finally, we learn the residual between the interpolated depth map and the corresponding HR one using the rich hierarchical features. The final HR depth map is achieved by adding the learned residual to the interpolated depth map. We conduct an ablation study to demonstrate the effectiveness of each component in the proposed network. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art methods. In addition, the potential usage of the proposed network in other low-level vision problems is discussed.', 'title': 'Hierarchical Features Driven Residual Learning for Depth Map Super-Resolution', 'embedding': []}, {'id': 15147, 'abstractText': \"The continuous memristor models have been applied to various chaotic circuits. However, the discrete memristor models and their applications to discrete maps haven't attracted much attention, yet. In this brief, we first present a discrete memristor model and analyze its characteristics. By coupling the model into the Logistic map, a memristive Logistic map is further achieved. Due to the existence of line fixed point, the memristive Logistic map can be unstable or critically stable, depending on its control parameters and initial state. Using several analysis methods, we study the control parameters-relied dynamical behaviors of the memristive Logistic map and disclose its hyperchaotic attractors. The numerical results show that the discrete memristor can efficiently improve the chaos complexity in the Logistic map. In addition, digital experiments are designed to validate the numerical results.\", 'title': 'Memristor-Coupled Logistic Hyperchaotic Map', 'embedding': []}, {'id': 15148, 'abstractText': 'Heart failure is associated with substantial mortality and morbidity and remains the most common diagnosis in older patients. Based on experimental electrophysiologic studies, cardiac resynchronization therapy (CRT) for heart failure results in a maximum resynchronization effect when applied to the most delayed left ventricular (LV) site. Current clinical practice is to identify the optimal site using separate visualisation of scar and activation information. These must be mentally mapped into 3D, which is challenging and time-consuming for the electrophysiologist. The aim of this work is to improve patient planning for CRT by mapping propagation of mechanical activation from cardiac magnetic resonance (CMR) onto a three-dimensional plus time (3D+t) model map to assist the cardiologist in determining the optimal LV pacing site. Automatic motion analysis of the 16-segment patient-specific LV anatomical model, automatically segmented from cine MR data, was done and regional volume change curves as a function of the cardiac cycle along with intraventricular dyssynchrony indices were extracted. The regional volume information computed was then mapped onto all phases of the 3D+t CMR data, which provides a 3D+t mechanical activation map over the whole cardiac cycle. This workflow was tested on 7 patients and 3 healthy volunteers. This mapping of the regional change of volume across the LV during ventricular pacing could facilitate the selection of the optimum pacing segment at the planning stage of the procedure, and consequently decrease the number of inadequate responders to CRT.', 'title': 'Dynamic mapping of ventricular function from cardiovascular magnetic resonance imaging', 'embedding': []}, {'id': 15149, 'abstractText': 'Several humanoid robots will require to navigate in unsafe and unstructured environments, such as those after a disaster, for human assistance and support. To achieve this, humanoids require to construct in real-time, accurate maps of the environment and localize in it by estimating their base/pelvis state without any drift, using computationally efficient mapping and state estimation algorithms. While a multitude of Simultaneous Localization and Mapping (SLAM) algorithms exist, their localization relies on the existence of repeatable landmarks, which might not always be available in unstructured environments. Several studies also use stop-and-map procedures to map the environment before traversal, but this is not ideal for scenarios where the robot needs to be continuously moving to keep for instance the task completion time short. In this paper, we present a novel combination of the state-of-the-art odometry and mapping based on LiDAR data and state estimation based on the kinematics-inertial data of the humanoid. We present experimental evaluation of the introduced state estimation on the full-size humanoid robot WALK-MAN while performing locomotion tasks. Through this combination, we prove that it is possible to obtain low-error, high frequency estimates of the state of the robot, while moving and mapping the environment on the go.', 'title': 'A Study on Low-Drift State Estimation for Humanoid Locomotion, Using LiDAR and Kinematic-Inertial Data Fusion', 'embedding': []}, {'id': 15150, 'abstractText': 'Image segmentation in robotics is an ongoing research field in which neural networks have shown promising performance. In this paper, we introduce MapSegNet, a deep convolutional neural network for indoor map segmentation, which is able to segment the indoor maps into smaller units, including rooms, corridors, windows, and furniture. The proposed model consists of an encoder phase to capture context and a corresponding decoder phase to increase the resolution of feature maps to the original resolution. A design for skip connections is introduced with fused multi-scale feature maps between the encoder and the decoder phases. The proposed skip connection increases the flow of information between the phases and improves the model generalization. We evaluate empirical studies based on abstract maps and detailed maps. While abstract maps include empty rooms and corridors, the detailed maps contain indoor space with more objects such as furniture. We investigate the effectiveness of the proposed method by employing various indoor maps and compare its performance with similar neural network models on multiple datasets. The results show that the proposed model is able to achieve more accurate recognition or lower computation cost compared to other state-of-the-art segmentation techniques.', 'title': 'MapSegNet: A Fully Automated Model Based on the Encoder-Decoder Architecture for Indoor Map Segmentation', 'embedding': []}, {'id': 15151, 'abstractText': 'Objective: This study demonstrates how functional connectivity (FC) patterns are affected in direct relation to the lobe that is mostly affected by seizures. Methods: The novel idea of penalized FC (pFC) maps is compared against standard FC maps in the four fundamental EEG frequency sub-bands. The FC measure between any two specific electrodes is scaled depending on the probability of true FC between them, and their power content with respect to the two electrodes of maximum power within each frequency sub-band. The algorithm is automated and introduces adaptive power penalization based on the power distribution of the different sub-bands. Results: The pFC maps were found to be more effective at suppressing the local connectivity in the lobes that are less affected by the interictal epileptiform discharges (IEDs). More precisely, given the least amount of power penalization, pFC maps of the theta sub-band reveal statistical significance in terms of increased local connectivity margin of the affected region as compared to the standard FC maps. However, they cannot be solely relied upon as other sub-bands could alternatively show high local connectivity across different patients within the region of interest. Conclusion: penalized functional connectivity maps intrinsically provide more information regarding the whole brain network in context to regions of interest where the active lobe is determined by the neurologists to contain the focal source. Significance: Findings suggest that (1) the significant sub-band varies from patient to patient while remaining relatively consistent within the IED segments of a same patient, and (2) the pFC maps have an advanced capability in terms of pinpointing to a region of interest of the active lobe, and as such can play a critical role in providing insight as to a region of interest where the 3D source might be located when solving the ill-posed inverse problem.', 'title': 'Penalized Functional Connectivity Maps for Patients With Focal Epilepsy', 'embedding': []}, {'id': 15152, 'abstractText': 'Zoning Improvement Plan (ZIP) Code polygon maps, obtained from different data sources, do not match, thus creating uncertainty in spatial analysis. In this dissertation, we want to combine multiple source of ZIP-code polygon data into a coherent system that maps a given location to a ZIP-code. For this purpose, we want to harness the wisdom of the crowd by combining various polygon map data sets with various sources of volunteered geographical information. The system that we want to build will employ traditional classification methods to map a given spatial coordinate to a distribution of ZIP-codes using the (not publicly available) United States Postal Service (USPS) map as an authoritative ground truth. In our first studies, we train a Naïve Bayes classifier using multiple (publicly available) ZIP Code polygon maps. In addition, we enrich our classification by using a lazy K-Nearest Neighbor classifier to predict the ZIP Codes for a given location using Twitter. Feeding the result of this classifier to the Naïve Bayes classification our experimental evaluation shows an improvement in classification accuracy, compared to using only map data.', 'title': 'ZIP-Code Classification Using Spatial and Crowdsourced Data', 'embedding': []}, {'id': 15153, 'abstractText': \"Geomagnetic signals are attractive media for indoor localization since they have less noise than RF and don't require additional equipment installation for signal generation. The fingerprinting technique used in geomagnetic field based indoor positioning systems (IPS) estimates the position by matching the magnetic vector sampled from the current location with the magnetic vectors recorded in the magnetic field map. However, since the magnetic field is represented by a 3-dimensional vector, the values of a magnetic vector may change depending on the user's orientation or the grip position. Thus, the sampled magnetic vector may have different values from the vector values stored in the magnetic field map depending on the sensor's orientation. This may substantially lower the positioning accuracy. To avoid this problem, the existing studies use only the magnitude of a magnetic vector, but this reduces the uniqueness of the fingerprint, which may also degrade the positioning accuracy. In this paper we propose a vector calibration algorithm which can adjust the sampled magnetic vector to the vector of the magnetic field map by using the parametric equation of a circle. This can minimize mismatching with the magnetic field map. To implement this, we need to compute the relative rotation angle from the moving direction of the current user to the moving direction during the field map collection. Since we can measure the moving direction by using the gyroscope and accelerometer, we can compute this relative rotation angle dynamically. To evaluate our vector calibration algorithm, we compare the value mismatches with and without vector calibration for 6 random-walk paths in our campus testbed of 2470 square meters. Our results show that with the calibration, we can decrease the difference between the sampled magnetic vector and the magnetic field map vectors from 17.61$\\\\mu$T to 2.38 $\\\\mu$T in x dimension, from 17.24$\\\\mu$T to 2.59 $\\\\mu$T in y dimension, and from 6.86 $\\\\mu$T to 2.16 $\\\\mu$T in z dimension on average. This translates to 83% reduction in the map mismatch compared to the numbers without calibration. In addition, we also demonstrate the effectiveness of the calibration by applying the algorithm to our long short-term memory (LSTM)-based IPS.\", 'title': 'Magnetic Vector Calibration for Real-Time Indoor Positioning', 'embedding': []}, {'id': 15154, 'abstractText': 'Map matching is an important part of map routing in modern transportation applications. In the map matching process, the effect of two main components, i.e. (i) the features of road networks, and (ii) the design aspects of geolocation services, is still not well understood. In this paper, using a combination of probabilistic analysis and simulations we study the effects of the above factors on the map matching process. Using a Maximum Aposteriori Probability (MAP) estimator, we offer some design guidelines that could improve the performance of map matching algorithms.', 'title': 'A probabilistic study of map matching for transportation applications', 'embedding': []}, {'id': 15155, 'abstractText': 'This paper presents novel methods for computing fixed points of positive concave mappings and for characterizing the existence of fixed points. These methods are particularly important in planning and optimization tasks in wireless networks. For example, previous studies have shown that the feasibility of a network design can be quickly evaluated by computing the fixed point of a concave mapping that is constructed based on many environmental and network control parameters such as the position of base stations, channel conditions, and antenna tilts. To address this and more general problems, given a positive concave mapping, we show two alternative but equivalent ways to construct a matrix that is guaranteed to have spectral radius strictly smaller than one if the mapping has a fixed point. This matrix is then used to build a new mapping that preserves the fixed point of the original positive concave mapping. We show that the standard fixed point iterations using the new mapping converges faster than the standard iterations applied to the original concave mapping. As exemplary applications of the proposed methods, we consider the problems of power and load estimation in networks based on the orthogonal frequency division multiple access (OFDMA) technology.', 'title': 'Elementary Properties of Positive Concave Mappings With Applications to Network Planning and Optimization', 'embedding': []}, {'id': 15156, 'abstractText': 'In this paper, we are studying the optimization of the number and positions of access points required to ensure the radio coverage of an indoor environment. In order to estimate the propagation effects in an indoor environment on radio wave propagation, we present existing path loss models and we list the corresponding advantages and drawbacks. In the context of this paper, simplicity and reduced computation time are the main constraints of the choice of an appropriate indoor propagation model. The accuracy of the model is of second order of importance. We choose an empirical model to estimate the indoor radio coverage. In a second step, we propose and develop an algorithm that divides an indoor environment map into sub- maps. Each of these sub-maps requires one access point to ensure its coverage. Finally, in order to test the good coverage of the whole map, each access point is placed at the centroid of the corresponding sub-map and the coverage map is calculated. The results obtained shows an enhancement of the coverage, and a determination of the minimum number of access points required in the indoor environment.', 'title': 'Optimization of indoor radio coverage', 'embedding': []}, {'id': 15157, 'abstractText': 'The entrenched instability of solar power output throttles its further integration into power grids worldwide. Thus the precise solar power forecasting (SPF) is helpful for the improvement of power grid stability and better exploitation of clean solar energy. As an important role of ultra-short-term SPF, sky images always contain volatile clouds, which results in tempestuous variation of the output of PV plants. Therefore, an accurate model that can capture the mapping relationship between sky image data and solar irradiance data is significant for fulfilling the ultra-short-term SPF. To fill the gap in the content of this research field, this paper proposes two end to end irradiance mapping models based on deep learning technologies, namely convolutional neural network (CNN) and long short-term memory (LSTM) neural network. Then the mapping performance of the above two mapping models is compared to that of traditional artificial neural network (ANN) based model. In all the aforementioned models, it should be noted that the solar irradiance data output by mapping models is in one-to-one correspondence with the input sky image data in time. The deterministic and probability methods are applied to statistically evaluate the mapping result of CNN and LSTM models. Our case study shows that deep learning architectures, especially the CNN based model, are good at mapping sky images to corresponding surface solar irradiance.', 'title': 'Deep Learning Based Irradiance Mapping Model for Solar PV Power Forecasting Using Sky Image', 'embedding': []}, {'id': 15158, 'abstractText': 'A striking example of brain organisation is the stereotyped arrangement of cell preferences in the visual cortex for edges of particular orientations in the visual image. These “orientation preference maps” appear to have remarkably consistent statistical properties across many species. However fine scale analysis of these properties requires the accurate reconstruction of maps from imaging data which is highly noisy. A new approach for solving this reconstruction problem is to use Bayesian Gaussian process methods, which produce more accurate results than classical techniques. However, so far this work has not considered the fact that maps for several other features of visual input coexist with the orientation preference map and that these maps have mutually dependent spatial arrangements. Here we extend the Gaussian process framework to the multiple output case, so that we can consider multiple maps simultaneously. We demonstrate that this improves reconstruction of multiple maps compared to both classical techniques and the single output approach, can encode the empirically observed relationships, and is easily extendible. This provides the first principled approach for studying the spatial relationships between feature maps in visual cortex.', 'title': 'Estimating Cortical Feature Maps with Dependent Gaussian Processes', 'embedding': []}, {'id': 15159, 'abstractText': 'Autonomous cars are currently widely studied in the automotive and robotics industries because autonomous driving can satisfy the needs of human drivers regarding safety, efficiency, and comfortable driving. Behavior and motion planning for an autonomous car is one of the main requirements of autonomous driving. For autonomous cars, a local route along with its road geometry and attributes of the area is required. There are two ways to obtain the local route: perception-based local route (PBLR) and precise map-based local route (MBLR) methods. First, this paper analyzes the characteristic of both the PBLR and MBLR inference methods. Then based on this analysis, this paper proposes a hybrid local route generation algorithm that chooses the best local route between the PBLR and MBLR options, according to the perceived performance and the precise map availability. To effectively create an expensive precise map, a mapping region classification algorithm is presented to selectively choose the mapping area, where the precise map must be constructed for the MBLR inference. The hybrid local route generation algorithm with the mapping region classification allows the area used for autonomous driving to be extended while reducing the cost due to the precise map. The advantages of the proposed algorithm were verified with experiments in real traffic conditions.', 'title': 'Hybrid Local Route Generation Combining Perception and a Precise Map for Autonomous Cars', 'embedding': []}, {'id': 15160, 'abstractText': 'This study elaborates on a comprehensive design methodology of fuzzy cognitive maps (FCMs). Here, the maps are regarded as a modeling vehicle of time series. It is apparent that whereas time series are predominantly numeric, FCMs are abstract constructs operating at the level of abstract entities referred to as concepts and represented by the individual nodes of the map. We introduce a mechanism to represent a numeric time series in terms of information granules constructed in the space of amplitude and change of amplitude of the time series, which, in turn, gives rise to a collection of concepts forming the corresponding nodes of the FCMs. Each information granule is mapped onto a node (concept) of the map. We identify two fundamental design phases of FCMs, namely 1) formation of information granules mapping numeric data (time series) into activation levels of information granules (viz., the nodes of the map), and 2) optimization of information granules at the parametric level, viz., learning (estimating) the weights between the nodes of the map. The learning is typically realized in a supervised mode on a basis of some experimental data. A construction of information granules is realized with the aid of fuzzy clustering, namely fuzzy C-means. The optimization is realized with the use of particle swarm optimization. The proposed approach is illustrated in detail by a series of experiments using a collection of publicly available data.', 'title': 'Design of Fuzzy Cognitive Maps for Modeling Time Series', 'embedding': []}, {'id': 15161, 'abstractText': 'Chaotic systems are widely studied in various research areas such as signal processing and secure communication. Existing chaotic systems may have drawbacks such as discontinuous chaotic ranges and incomplete output distributions. These drawbacks may lead to the defects of some chaos-based applications. To accommodate these challenges, this paper proposes a two-dimensional (2D) modular chaotification system (2D-MCS) to improve the chaos complexity of any 2D chaotic map. Because the modular operation is a bounded transform, the improved chaotic maps by 2D-MCS can generate chaotic behaviors in wide parameter ranges while existing chaotic maps cannot. Three improved chaotic maps are presented as typical examples to verify the effectiveness of 2D-MCS. The chaos properties of one example of 2D-MCS are mathematically analyzed using the definition of Lyapunov exponent. Performance evaluations demonstrate that these improved chaotic maps have continuous and large chaotic ranges, and their outputs are distributed more uniformly than the outputs of existing 2D chaotic maps. To show the application of 2D-MCS, we apply the improved chaotic maps of 2D-MCS to secure communication. The simulation results show that these improved chaotic maps exhibit better performance than several existing and newly developed chaotic maps in terms of resisting different channel noise.', 'title': 'Two-Dimensional Modular Chaotification System for Improving Chaos Complexity', 'embedding': []}, {'id': 15162, 'abstractText': \"Real-time interaction has become increasingly important. At the same time, panoramic video has gradually become popular. In this paper, the problem we study is predicting the Field-of-View(FoV) at the future moment when people are enjoying a dynamic panoramic immersive video. Existing methods either estimate the future viewing area based on the previous trajectory, or predict the FoV based on salient region in video frames. Here, we design a new model to predict the viewing points in future moments. Firstly, we predict a point from the viewer's previous viewing trajectory using LSTM(Long Short-Term Memory) network. At the mean time, panoramic video frames are mapped to 6 patches by cube map in advance. The modified VGG-16 network is used for each patch image to perform saliency detection. Then, these 6 salient maps are combined to a single salient map as output. A 3layer convolutional neural network to refined the salient map is utilized. Finally, the salient map of corresponding moment is combined with the predicted point by LSTM input to a two-layer fully connected network to produce the final predicted point. The experiment results show that our model's prediction accuracy is higher than the traditional prediction algorithm and has better performance than the model without using the second CNN network.\", 'title': 'Viewport Prediction for Panoramic Video with Multi-CNN', 'embedding': []}, {'id': 15163, 'abstractText': 'Simultaneous Localization and Mapping (SLAM) is a fundamental problem for autonomous mobile robots (AMRs). AMRs are widely used in automated warehousing, factory material transfer systems, flexible assembly systems, and other intelligent transportation sites. The visual Inertial Odometry (VIO) which consists of the camera and inertial-measurement-unit (IMU), is a popular approach to achieve accurate 6-DOF state estimation. However, such locally accurate VIO is prone to drift and cannot provide a global consistent map. The prerequisite for re-localizing the robot and ensuring precise autonomous navigation is an accurate and global consistent map of its environment. In this study, we propose a stereo visual-inertial mapping system. The front-end is a robust stereo VIO based on a tightly-coupled sliding window optimization. The core of the back-end is the global Bundle-Adjustment (BA) which is a nonlinear optimization, in which IMU is also added as a time-domain constraint. Meanwhile, stereo-camera-IMU extrinsic calibration is performed in BA to improve mapping accuracy. The selection principles of keyframes and map points are also designed according to the AMRs application characteristics. Further, the forward and backward Perspective-n-Point (PNP) method is also adopted to avoid the loop-detection mismatch. The performance of the system was validated and compared against other state-of-the-art algorithms. The findings revealed the effectiveness and robustness of this stereo visual-inertial mapping algorithm.', 'title': 'Stereo Visual Inertial Mapping Algorithm for Autonomous Mobile Robot', 'embedding': []}, {'id': 15164, 'abstractText': 'MapReduce includes three phases of map, shuffle, and reduce. Since the map phase is CPU-intensive and the shuffle phase is I/O-intensive, these phases can be conducted in parallel. This paper studies a joint scheduling optimization of overlapping map and shuffle phases to minimize the average job makespan. Challenges come from the dependency relationship between map and shuffle phases, since the shuffle phase may wait to transfer the data emitted by the map phase. A new concept of the strong pair is introduced. Two jobs are defined as a strong pair, if the shuffle and map workloads of one job equal the map and shuffle workloads of the other job, respectively. We prove that, if the entire set of jobs can be decomposed to strong pairs of jobs, then the optimal schedule is to pairwisely execute jobs that can form a strong pair. Following the above intuition, several offline and online scheduling policies are proposed. They first group jobs according to job workloads, and then, execute jobs within each group through a pairwise manner. Real data-driven experiments validate the efficiency and effectiveness of the proposed policies.', 'title': 'Optimizing MapReduce Framework through Joint Scheduling of Overlapping Phases', 'embedding': []}, {'id': 15165, 'abstractText': 'Mapping the quasi-circular vegetation patches (QVPs) is the most basic and necessary step for studying the mechanisms of vegetation pattern formation, spontaneous plant colonization, and ecosystem maintenance and degradation in the Yellow River Delta (YRD), China. It is well known that the use of multi-seasonal image data may be expected to obtain better results for mapping the vegetation than the use of a single date image because of vegetation phenology and the contrasts between vegetation and background. Although the spring GF-1 images have been used to detect the QVPs, the potential of multi-seasonal fused GF-1 multispectral images for mapping the QVPs has not been well explored. With the objective to fill this gap, we evaluated the potential of the winter, spring, summer, and autumn fused GF-1 multispectral images for mapping the QVPs with object-based example-based feature extraction with support vector machine classification method to understand the seasonal effect on the QVPs mapping accuracy. The results showed that classification based on the spring data acquired on 6 April 2014 (OA=99.8%, kappa=0.988) was more accurate than the classification base on the other tree seasonal images. The lowest classification accuracy was obtained from the autumn data acquired on September 21 2015 (OA=93.1%, kappa=0.885). We recommend the he spring and winter images to map the QVPs in the YRD. In the future, more machine learning techniques should be applied to classify and compare the potential of the different seasonal fused GF-1 multispectral images for mapping the QVPs.', 'title': 'Comparisons of Different Seasonal Fused GF-1 Multispectral Images for Mapping Quasi-circular Vegetation Patches', 'embedding': []}, {'id': 15166, 'abstractText': \"Multibeam echosounder (MBES) technology has been constantly evolving since its commercial introduction in the late 1970s. The early systems were large and designed to efficiently acquire bathymetric data in deep water. As the underlying sonar technologies improved and computing power increased, systems became smaller and capable of operating on a wider range of vessels over a broader range of depths. Modern deep seafloor exploration, and our present understanding of the geomorphological and biophysical processes that shape it, are closely linked to advances in multibeam echosounder technology. Low to mid-frequency (12-30 kHz) acoustic waves generated by MBES sonars can penetrate kilometers of water column and remotely measure the deep seafloor and shallow subsurface. Reflectivity measurements of the seafloor and water column can also be extracted from MBES datasets, but until the last decade of the 20th century, only the bathymetric swath data was being utilized. In the 1990s, scientists began taking advantage of the multibeam acoustic wave's reflected energy, or backscatter, to interpret information on seafloor geometry (slope), physical characteristics (hardness and roughness), and intrinsic properties, such as composition, surficial and volumetric scattering. Analyzing the geophysical signature of reflected acoustic beams has proven an effective quantitative and qualitative tool to remotely characterize the lithologic composition and geologic nature of the seafloor. Analyzing seafloor backscatter and most recently, backscatter intensities in the water column, has been used for a wide range of applications, including fisheries research, marine biomass assessment, benthic habitat mapping, geological classification, subsea engineering and geohazard mitigation, and hydrocarbon seep studies. This presentation will briefly look at the evolution of MBES technology before focusing on how modern MBES surveys, using the latest generation technology, can deliver a comprehensive characterization of the seafloor and the waters above, as opposed to bathymetry data alone. With The Nippon Foundation-GEBCO Seabed 2030 Project now underway, and planning for the United Nations (UN) Decade of Ocean Science for Sustainable Development having recently commenced, modern MBES technology will play a critical role in bridging ocean bathymetry and ocean observation to improve our understanding of the ocean, its seafloor and its processes. One of the key R&amp;D priorities of the Decade is a comprehensive map (digital atlas) of the ocean. Modern multibeam surveys will support not only bathymetric mapping, but also physical, biological, chemical, geologic, ecosystem, cultural and resource mapping of the world's oceans. Such an approach can feed both Seabed 2030 and the Decade to deliver, as the UN has so eloquently stated, “the ocean we want for the future we need.” Keywords- multibeam; sonar; mbes; echosounder; bathymetry; hydrography; marine geology; mapping; ocean mapping; seeps; seabed seeps; hydrocarbon seeps; exploration; ocean exploration; geophysical; survey; geophysical survey; ocean; seafloor; characterization; seafloor characterization; backscatter; multibeam backscatter; water column; observation; ocean observation; Seabed 2030, United Nations Decade of Ocean Science for Sustainable Development; Ocean Decade.\", 'title': 'How Modern Multibeam Surveys Can Dramatically Increase Our Understanding of the Seafloor and Waters Above to Support the United Nations Decade of Ocean Science for Sustainable Development', 'embedding': []}, {'id': 15167, 'abstractText': 'In this paper, we study the three-dimensional (3D) path planning for a cellular-connected unmanned aerial vehicle (UAV) to minimize its flying distance from given initial to final locations, while ensuring a target link quality in terms of the expected signal-to-interference-plus-noise ratio (SINR) at the UAV receiver with each of its associated ground base stations (GBSs) during the flight. To exploit the location-dependent and spatially varying channel as well as interference over the 3D space, we propose a new radio map based path planning framework for the UAV. Specifically, we consider the channel gain map of each GBS that provides its large-scale channel gains with uniformly sampled locations on a 3D grid, which are due to static and large-size obstacles (e.g., buildings) and thus assumed to be time-invariant. Based on the channel gain maps of GBSs as well as their loading factors, we then construct an SINR map that depicts the expected SINR levels over the sampled 3D locations. By leveraging the obtained SINR map, we proceed to derive the optimal UAV path by solving an equivalent shortest path problem (SPP) in graph theory. We further propose a grid quantization approach where the grid points in the SINR map are more coarsely sampled by exploiting the spatial channel/interference correlation over neighboring grids. Then, we solve an approximate SPP over the reduced-size SINR map (graph) with reduced complexity. Numerical results show that the proposed solution can effectively minimize the flying distance/time of the UAV subject to its communication quality constraint, and a flexible trade-off between performance and complexity can be achieved by adjusting the grid quantization ratio in the SINR map. Moreover, the proposed solution significantly outperforms various benchmark schemes without fully exploiting the channel/interference spatial distribution in the network.', 'title': 'Radio Map-Based 3D Path Planning for Cellular-Connected UAV', 'embedding': []}, {'id': 15168, 'abstractText': 'Carry chains on FPGAs have traditionally been only used for fast binary arithmetic operations. In this paper, we propose using the carry chain to implement general logic as a means of reducing the critical path delay and raising performance. To achieve this, we use a Majority-Inverter Graph (MIG) to represent the application during technology mapping, since carry functionality directly maps to the majority logic function. This aligns the subject graph of technology mapping with the capabilities of the carry chain. We first map an application to LUTs, then determine a chain of critical LUTs containing paths of majority “gates” that we deem beneficial for mapping onto the carry chain. We place such paths onto the carry chains, with the remaining logic in LUTs. In an experimental study using a suite of benchmarks, we observe that the proposed approach yields a post-place-and-route critical path delay that is superior to using delay-optimized mapping, yet without the significant area penalty. With carry-chain optimizations, area-delay product is improved by 9% vs. baseline LUT mappings.', 'title': 'Post-LUT-Mapping Implementation of General Logic on Carry Chains Via a MIG-Based Circuit Representation', 'embedding': []}, {'id': 15169, 'abstractText': 'In this paper, we propose a semantic simultaneous localization and mapping (SLAM) framework for rescue robots, and report its use in navigation tasks. Our framework can generate not only geometric maps in the form of dense point-clouds but also corresponding point-wise semantic labels generated by a semantic segmentation convolutional neural network (CNN). The semantic segmentation CNN is trained using our RGB-D dataset of the RoboCup Rescue-Robot-League (RRL) competition environment. With the help of semantic information, the rescue robot can identify different types of terrains in a complex environment, so as to avoid specific obstacles or to choose routes with better traversability. To reduce the segmentation noise, our approach utilizes depth images to perform filtering on the segmentation results of each frame. The overall semantic map is then further improved in the point-cloud voxels. By accumulating results of multiple frames in the voxels, semantic maps with consistent semantic labels are obtained. To show the advantage of having a semantic map of the environment, we report a case study of how the semantic map can be utilized in a navigation task to reduce the arrival time while ensuring safety. The experimental result shows that our semantic SLAM framework is capable of generating a dense semantic map for the complex RRL competition environment, with which the arrival time of the navigation time is effectively reduced.', 'title': 'Semantic RGB-D SLAM for Rescue Robot Navigation', 'embedding': []}, {'id': 15170, 'abstractText': 'Localization of human long bones in ultrasound images has quite complex challenges. This is due to a representation of the reflection of a sound wave emitted by a B-scan sensor. The ultrasound scan does not only display bone specimens, but also contains muscles, soft tissue, and other parts under the skin tissue Therefore we need a system that can automatically recognize bone specimens in ultrasound images. This study implements deep learning based learning systems using the convolutional neural network (CNN) method with YOLOv3. The training results from the network detector with IoU threshold 0.5 can recognize bone objects in mAP<sub>@50</sub>, mAP<sub>@75</sub> and mAP<sub>@50:95</sub> with values of 99.98, 97.68 and 85.67 respectively. And for the results of training the network detector with IoU threshold 0.75 can recognize bone objects in mAP<sub>@50</sub>, mAP<sub>@75</sub> and mAP<sub>@50:95</sub> with values of 99.96, 97.46 and 86.35 respectively.', 'title': 'Human Bone Localization in Ultrasound Image Using YOLOv3 CNN Architecture', 'embedding': []}, {'id': 15171, 'abstractText': 'Commercial maps often offer traffic awareness which is critical for many location based services. On the other hand free and open map services (such as government maps or OSM) are traffic oblivious and hence are of limited value for such services. In this paper we show that coarse information available from a commercial map routing API, can be dissected into fine-grained per-road-segment traffic information which can be reused in any application requiring traffic-awareness. Our system MapReuse queries a commercial map for a (relatively small) number of routes, and uses the returned routes and expected travel times, to infer travel time on each individual edge of the road network. Such fine-grained travel time information can be used not only to infer travel time on any given route but also to compute complex spatial queries (such as traffic-aware isochrone map) for free. We test our system on four representative metropolitan areas: Bogota, Doha, NYC and Rome, and report very encouraging results. Namely, we observe the median and mean percentage errors of MapReuse, measured against the travel times reported by the commercial map, to be in the range of 4% to 8%, implying that MapReuse is capable to accurately reconstruct the traffic conditions in all four studied cities.', 'title': 'MapReuse: Recycling Routing API Queries', 'embedding': []}, {'id': 15172, 'abstractText': \"In this work, we argue that Location-Based Social Media (LBSM) feeds may offer a new layer to improve traffic and transit comprehension. Initially, we showed the significant correlation between Twitter's feed and traditional traffic sensors. Then, we presented the Twitter MAPS (T-MAPS) a low-cost spatiotemporal model to improve the description of traffic conditions through tweets. T-MAPS enhance traditional traffic sensors by carrying the human lens into the transportation system. We conducted a case study by running T-MAPS and Google Maps route recommendation, in which, we showed T-MAPS viability, as an additional traffic descriptor. As a result, we noticed the median of route similarity reached 62%, and for a quarter of the evaluated trajectories, the similarity achieved between 75% and 100%. Also, we presented three route description services, based on natural language analyzes, Route Sentiment (RS), Route Information (RI), and Area' Tags (AT) aiming to enhance the route information.\", 'title': 'Enriching Traffic Information with a Spatiotemporal Model based on Social Media', 'embedding': []}, {'id': 15173, 'abstractText': 'Choropleth maps are among the most common visualization techniques used to present geographical data. These maps require an equal-area projection but there are no clear criteria for selecting one. We collaborated with 20 social scientists researching on the Global South, interested in using choropleth maps, to investigate their design choices according to their research tasks. We asked them to design world choropleth maps through a survey, and analyzed their answers both qualitatively and quantitatively. The results suggest that the design choices of map projection, center, scale, and color scheme, were influenced by their personal research goals and the tasks. The projection was considered the most important choice and the Equal Earth projection was the most common projection used. Our study takes the first substantial step in investigating projection choices for world choropleth maps in applied visualization research.', 'title': 'Mapping the Global South: Equal-Area Projections for Choropleth Maps', 'embedding': []}, {'id': 15174, 'abstractText': 'The accuracy of the naval map representation determines the quality of route construction when solving the problem of automatic control of ships movement in a difficult navigation environment. Using a high-precision raster map requires a large amount of memory to store data. Vector representation of the map is an alternative approach to the problem of storing data about terrain, it allows to store only the data containing the terrain contours. In this paper we implement and research the algorithms which allow to vectorize the raster maps by the set of isoline slices using irregular grid of heights. These algorithms approximate the map with a set of elevation slices, recursively search for nested isolines on the slice, build and optimize contours of the isolines on the slice. We conduct a study that shows a significant reduction in the amount of stored data in vector form compared to raster format. The proposed format for storing map data will be used to represent realistic landscapes in the tasks of building ship routes.', 'title': 'Raster to Vector Map Convertion by Irregular Grid of Heights', 'embedding': []}, {'id': 15175, 'abstractText': \"The use of mobile robots to explore and supervise confined and uneven environments such as pipes, caves, and galleries improves operational safety by removing human operators from these dangerous areas. In many types of inspections, the robot must generate realistic, colored, and geometrically accurate maps, which experts can use to study and assess the environment remotely at a safe location. This paper investigates two approaches for visual reconstruction of confined environments: a point cloud registration combined with visual odometry and the RTAB-Map SLAM method. Real experiments performed inside a closed corridor and within an underground gold mine show that both approaches are suitable for estimating the robot's pose and perform 3D mapping. Preliminary results indicate that the point cloud registration generates denser maps suitable for visual inspection, and RTAB-Map provides less noisy maps proper for navigation.\", 'title': 'Investigation of Visual Reconstruction Techniques Using Mobile Robots in Confined Environments', 'embedding': []}, {'id': 15176, 'abstractText': 'Total Suspended Sediment (TSS) is one of the factors to determine water quality. The large number of TSS indicated the turbid water. Highly turbid water can affect water biophysics. Therefore, mapping TSS distribution is important for managing and preserving coastal areas. Sentinel-2A image uses to create multitemporal TSS from January 2017 to May 2018. This study goals are testing Sentinel-2A image to create TSS map and making TSS temporal map. Image transformations that used were Normalized Difference Suspended Sediment Index (NDSSI), Normalized Suspended Material Index (NSMI) and Band Ratio using green, red, and blue band channels. The best image transformation that analysis with Stepwise Regression is Band Ratio image. The results of laboratory tests showed minimum TSS content of 0.98 mg/L and maximum content of 4.56 mg/L. The accuracy value from TSS mapping with a band ratio image reach 80.51%, indicates that the use of band ratios (b3/b4) is representative to TSS mapping. TSS distribution is dynamic, indicated by TSS temporal map. Dynamic content of TSS influenced by the season. TSS content in wet seasons (October-April) is higher than in dry season (April-October).', 'title': 'Utilization of Sentinel-2A imagery For Mapping The dynamics of Total Suspended Sediment at The River Mouth of The Padang City', 'embedding': []}, {'id': 15177, 'abstractText': 'Automatic identification of weld seam types by welding robot is a key link in intelligent welding as some adjustment scheme (e.g., welding trajectory planning, initial welding position, welding parameters) vary with the weld seam types. However, the variable welding environment and various weld seam profile omnifarious affect the robustness of weld seam types identification. To overcome the challenges derived from the weld seam diversity, in this paper, the silhouette-mapping was selected as the weld seam intermedium and a multi-type weld seam automatic identification system based on vision sensor was introduced. Two different laser sources were adopted to obtain robust silhouette-mapping features in proper gestures. Based on the silhouette-mapping data (stripe-mapping and spot-mapping), the related image processing algorithms were carried out to achieve automatic identification. Specifically, the bidirectional deviation search method was proposed to locate the spot-mapping area based on the stripe-mapping image. Aiming at the characteristics of the spot-mapping image, a carefully designed CNN (convolutional neural network) model was used to classify types. Experimental results prove that the silhouette-mapping and CNN are an effective combination for the multi-type weld seam identification, and a total of 97.6% of weld seam types were correctly predicted. Some weld-related studies include welding features extraction, and welding quality detection may improve its accuracy on the basis of determining weld seam types.', 'title': 'Automatic Identification of Multi-Type Weld Seam Based on Vision Sensor With Silhouette-Mapping', 'embedding': []}, {'id': 15178, 'abstractText': 'Mapping for terrestrial ecosystem service has exponentially soared in recent years, which provides a reliable theoretical foundation for acknowledging functions, valuation and management of various kinds of ecosystem services. This study was conducted by collecting peer reviewed papers from 2000 to 2018, and establishing a relevant database of 113 papers. Forest, grassland, wetland and desert were selected as four basic components of terrestrial ecosystem. As a result, researches on mapping for terrestrial ecosystem services in each continent could be found, and medium geographical scale is the most widely preferred by researchers. Services origined from different ecosystem might share similar qualities, thus it is hard to identify them when researches refer to only one ecosystem or specific services. Models combining with relevant approaches applied in mapping can complement each other. In conclusion, maps for demonstrating hotspots and effects of climate changes are promising to make a significant progress in the future. Moreover, it is essential for districts and countries to select adaptable mapping methods and models according to their own demands. Drivers like needs for management and governmental planning, cognition of ecosystem services and disservices will motivate researches and the application of mapping forward.', 'title': 'Mapping for terrestrial ecosystem services: a review', 'embedding': []}, {'id': 15179, 'abstractText': 'This paper presents a speech encryption model that uses two different chaotic maps (Hennon and Logistic maps) based on the OFDM technique. The purpose of using the encrypted chaotic key in OFDM techniques is to improve the Bit Error Rate (BER) performance in receiver side which has robustness against AWGN effects, thus increasing the security of data transmitted. In the last few years, a significant research effort has been devoted, concerning secret chaotic key as compared with the conventional OFDM modulation schemes. Chaotic maps have a wideband, non-periodic, unpredictable nature, very sensitive to initial conditions; so chaotic map is used for security. This paper uses two chaotic maps Logistic &amp; Hennon each individually to create encrypted undercover keys and compares a model performance between them. The comparison showed that the performance is similar to the two chaotic maps with a minimal difference between them (After SNR=16 dB the BER=10<sup>-3</sup>). As for residual Intelligibility Measurements, they are very close and there is little difference in the limits (0.01-0.1) for R, SSNR and CD test. The suggested model has been carried out by employing MATLAB by utilizing SIMULINK (R2018b). Execution of this program is studied, the outcomes are good.', 'title': 'Performance Comparison of Hybrid Chaotic Maps Based on Speech Scrambling for OFDM Techniques', 'embedding': []}, {'id': 15180, 'abstractText': \"Reflexion Modelling is a popular method used in industry for Software Architectural Consistency Checking (SACC). However, it involves a mapping step that is manual and tedious. There exist techniques and tools that attempt to automate mapping, yet they are either limited in their approach or they require an initial set of manually pre-mapped entities. This study proposes a novel technique, InMap, that improves the mapping process in reflexion modelling by both providing versatility and eliminating the constraint of needing a set of manually pre-mapped entities in order to automate mapping. Using a software's architecture descriptions, InMap applies information retrieval concepts to the software's source code to interactively provide mapping recommendations to an architect. For the six systems InMap was evaluated on, the recommendations it provided achieved an average recall of 0.97, and an average precision of 0.82. InMap also achieved higher, f<sub>1</sub>-scores in comparison to existing techniques that require premapping. This provides a basis for improving industry tools that use reflexion modelling or similar SACC methods.\", 'title': 'InMap: Automated Interactive Code-to-Architecture Mapping Recommendations', 'embedding': []}, {'id': 15181, 'abstractText': 'In this letter, we propose an end-to-end stitching network, which takes two images with a narrow field of view (FOV) as inputs, and produces a single image with a wide FOV. Our method estimates multiple homographies to cover the depth differences in the scene and is therefore robust against parallax distortion. In particular, global warping maps are generated using estimated multiple homographies and adjusted by local displacement maps. The final result is made by warping input images multiple times using the warping maps and then merging warped images with the weight maps. Multiple homographies, local displacement maps, and weight maps are generated simultaneously by our stitching network. To train the stitching network, we construct a dataset using the CARLA simulator. Then, using this dataset, our network is trained by end-to-end supervised learning based on appearance matching loss and depth layer loss. In experiments, we show that our method is superior to existing methods both qualitatively and quantitatively. Also, we provide various empirical studies for in-depth analysis as well as the result of the expansion to 360<sup>°</sup> panoramas.', 'title': 'End-to-End Image Stitching Network via Multi-Homography Estimation', 'embedding': []}, {'id': 15182, 'abstractText': 'Despite the significant progress in the understanding of the phenomenon of lightning and the physics behind it, locating and mapping its occurrence remain a challenge. Such localization and mapping of very high frequency (VHF) lightning radiation sources provide a foundation for the subsequent research on predicting lightning, saving lives, and protecting valuable assets. A major technical challenge in attempting to map the sources of lightning is mapping accuracy. The three common electromagnetic radio frequency-based lightning locating techniques are magnetic direction finder, time of arrival, and interferometer (ITF). Understanding these approaches requires critically reviewing previous attempts. The performance and reliability of each method are evaluated on the basis of the mapping accuracy obtained from lightning data from different sources. In this work, we review various methods for lightning mapping. We study the approaches, describe their techniques, analyze their merits and demerits, classify them, and derive few opportunities for further research. We find that the ITF system is the most effective method and that its performance may be improved further. One approach is to improve how lightning signals are preprocessed and how noise is filtered. Signal processing can also be utilized to improve mapping accuracy by introducing methods such as wavelet transform in place of conventional cross-correlation approaches.', 'title': 'Lightning Mapping: Techniques, Challenges, and Opportunities', 'embedding': []}, {'id': 15183, 'abstractText': 'Convolutional neural networks (CNN) are widely used in various computer vision applications. Recently, there have been many studies on FPGA-based CNN accelerators to achieve high performance and power efficiency. Most of them have been on CNN-based object detection algorithms, but researches on image super-resolution have been rarely conducted. Fast super-resolution CNN (FSRCNN), well known for CNN-based super-resolution algorithm, are a combination of multiple convolutional layers and a single deconvolutional layer. Since the deconvolutional layer generates high-resolution (HR) output feature maps from low-resolution (LR) input feature maps, its execution cycles are larger than those of the convolutional layer. In this paper, we propose a novel architecture of the FPGA-based CNN accelerator with the efficient parallelization. We develop a method of transforming a deconvolutional layer into a convolutional layer (TDC), a new methodology for the deconvolutional neural networks (DCNN). There is a massive parallelization source in the deconvolutional layer where multiple outputs within the same output feature map are created with the same inputs. When this new parallelization technique is applied to the deconvolutional layer, it generates the LR output feature maps the same as the convolutional layer. Thus, the performance of the accelerator increases without any additional hardware resources because the kernel size required to generate the LR output feature maps is smaller. In addition, if there is a DSP underutilization problem in the deconvolutional layer that some of the processors are in an idle state, the proposed method solves this problem by allowing more output feature maps to be processed in parallel. Experimental results show that the proposed TDC method achieves up to 81 times higher throughput than the state-of-the-art DCNN accelerator with the same hardware resources. We also improve the speed by 7.8 times by having all layers in the hourglass-type FSRCNN to be processed in inter-layer parallelism without additional DSP usage.', 'title': 'Optimizing FPGA-based convolutional neural networks accelerator for image super-resolution', 'embedding': []}, {'id': 15184, 'abstractText': 'In semiconductor manufacturing systems, defects on wafer maps tend to cluster and then these spatial patterns provide important process information for helping operators in finding out root-causes of abnormal processes. Promptly recognizing wafer map defects is an effective way to increase manufacturing process stability and then to improve yields. Deep learning has been widely applied and obtained many successes in image and visual analysis. This paper proposes an effective deep learning method, enhanced stacked denoising autoencoder (ESDAE) with manifold regularization for wafer map pattern recognition (WMPR) in manufacturing processes. This study will concentrate on developing a deep learning model to learn effective discriminative features from wafer maps through a deep network architecture for WMPR improvement. An indication based on ESDAE is developed for detecting map defects online. An ESDAE-based classifier is finally developed to implement recognition of wafer map defects. The most motivation for developing deep learning and manifold regularization techniques is to achieve higher accuracy and applicability than that of some regular recognizers. The effectiveness of the proposed method has been demonstrated by experimental results from a real-world wafer map dataset (WM-811K).', 'title': 'Enhanced Stacked Denoising Autoencoder-Based Feature Learning for Recognition of Wafer Map Defects', 'embedding': []}, {'id': 15185, 'abstractText': 'Super-resolution mapping (SRM) aims to generate a fine spatial resolution land cover map from input coarse spatial resolution fraction images. The spatial prior model used to describe the spatial land cover patterns at the fine spatial resolution is crucial to the SRM analysis. At present, the learning-based SRM algorithm has shown its advantage, because more information of spatial land cover patterns can be captured from available training fine spatial resolution land cover maps. In practice, for learning-based SRM, the training fine spatial resolution land cover maps should include various spatial land cover pattern examples as rich as possible. However, gathering training fine spatial resolution land cover maps is always a hard work. In order to overcome this shortcoming, this study proposes an approach to provide additional transformed examples (land cover maps) in the learning-based SRM approach. By transforming the original fine spatial resolution land cover maps with rotation and mirroring operations, the number of available training examples can increase eight times. The proposed SRM algorithm is compared with several popular SRM algorithms using both synthetic and real images. Experimental results indicate that more spatial details can be produced when the tranformed samples are applied in the learning-based SRM algorithm and the result produced by the proposed method has higher accuracies than the SRM results used for comparison.', 'title': 'Learning-Based Super-Resolution Land Cover Mapping with Additional Transformed Examples', 'embedding': []}, {'id': 15186, 'abstractText': 'Visualizing and communicating insights through maps offers an intuitive and familiar way to explore large-scale dynamic relational data. In this paper, we present VideoMap, which is a novel approach for presenting and interacting with relational video content by taking advantage of the map metaphor. VideoMap employs a metaphor to visualize video content by elements of a map with the aim of enabling exploration of video content as if reading a map. Video content is visualized in a hierarchal structure from a very large scale to a small scale of finely detailed representation. VideoMap recognizes a small set of sketch gestures for semantic zooming in and out, annotating the map, and automatically completing path navigation. To achieve this, VideoMap synthesizes map-derived visuals and binds them to the underlying data by operating the map with sketch interaction to facilitate interactive exploration. Extensive user studies were conducted to evaluate VideoMap, and the results demonstrated the effectiveness of VideoMap for facilitating the exploration and understanding of large video content.', 'title': 'Visualizing and Analyzing Video Content With Interactive Scalable Maps', 'embedding': []}, {'id': 15187, 'abstractText': 'This paper details a system for an autonomous navigation of electric wheelchair in outdoor urban area. The goal for this study is to achieve autonomous navigation on paved roads including the slopes in a busy street. We propose the autonomous navigation system using “2D drivable map” that explains steps, obstacles and paved/unpaved areas. 2D drivable map generated from 3D point cloud map by robot itself. We detected smoothly connected planes and obstacles by region growing segmentation based on normal in 3D point cloud map and projected the vertically onto a 2D map. A paved/unpaved area segmentation is based on intensity of 2D map. The experimental results show the 2D drivable map correctly explains steps and obstacles in urban areas and detects changes from roads to lawns. An electric wheelchair achieved autonomous navigation in urban areas without colliding with obstacles or mounting lawns.', 'title': 'Autonomous Navigation of Electric Wheelchairs in Urban Areas on the Basis of Self-Generated 2D Drivable Maps', 'embedding': []}, {'id': 15188, 'abstractText': \"Building efficient embedded deep learning systems requires a tight co-design between DNN algorithms, hardware, and algorithm-to-hardware mapping, a.k.a. dataflow. However, owing to the large joint design space, finding an optimal solution through physical implementation becomes infeasible. To tackle this problem, several design space exploration (DSE) frameworks have emerged recently, yet they either suffer from long runtimes or a limited exploration space. This article introduces ZigZag, a rapid DSE framework for DNN accelerator architecture and mapping. ZigZag extends the common DSE with uneven mapping opportunities and smart mapping search strategies. Uneven mapping decouples operands (W/I/O), memory hierarchy, and mappings (temporal/spatial), opening up a whole new space for DSE, and thus better design points are found by ZigZag compared to other SotAs. For this, ZigZag uses an enhanced nested-for-loop format as a uniform representation to integrate algorithm, accelerator, and algorithm-to-accelerator mapping. ZigZag consists of three key components: 1) an analytical energy-performance-area Hardware Cost Estimator, 2) two Mapping Search Engines that support spatial and temporal even/uneven mapping search, and 3) an Architecture Generator that auto-explores the wide memory hierarchy design space. Benchmarking experiments against published works, in-house accelerator, and existing DSE frameworks, together with three case studies, show the reliability and capability of ZigZag. Up to 64 percent more energy-efficient solutions are found compared to other SotAs, due to ZigZag's uneven mapping capabilities.\", 'title': 'ZigZag: Enlarging Joint Architecture-Mapping Design Space Exploration for DNN Accelerators', 'embedding': []}, {'id': 15189, 'abstractText': 'Spectrum maps based on power spectral density (PSD) of spatial signals are helpful for cognitive users (CUs) to accurately sense and utilize the spectrum holes, achieving interference coordination among network nodes, and enhancing link robustness of mobile CUs. As an image matrix based data, the cost of spectrum map dissemination in wireless networks cannot be ignored. Disseminating spectrum maps of excessively high resolution would consume too much bandwidth resources, which prevents the growth space of data traffics. In this paper, we study the spectrum map dissemination problem in the next generation data-driven cognitive radio network scenario and propose a method to obtain the optimal spectrum map resolution and quantization bit of PSD for dissemination. The experiment results demonstrate that we can obtain the tradeoff between spectrum map size and network throughput, so as to achieve better network throughput with relatively low map dissemination consumption.', 'title': 'Research on Spectrum Map Dissemination for Data-Driven Cognitive Radio Networks', 'embedding': []}, {'id': 15190, 'abstractText': 'Underwater mapping is important for many studies such as underwater cable/pipe platform placement and monitoring, bridge piers placement, dam construction, geological and geophysical studies. The position and orientation information of the sea surface vehicle is measured from the Global Positioning System (GPS) and Inertial Measurement Unit (IMU) sensors are placed on the vehicle, and the height from the seafloor is measured from a single beam sonar. External disturbance effects such as waves and wind cause oscillations in the surface vehicle. In bathymetric measurements, which are of great importance in mapping, errors due to oscillations occur. In underwater mapping, the orientation effect of the sea surface vehicle should be known in order to minimize these errors. In the study, to minimize these error sources, data from 3 different IMUs integrated into the sea surface vehicle are fused with sensor fusion algorithms such as the Integrated Navigation System (INS) and Support Vector Machine (SVM). In this study, a machine learning-based SVM integrated navigation system with minimum error in optimum positioning of the surface vehicle under external disturbance effects is proposed. With the INS, the performance of the machine learning-based SVM sensor fusion algorithm is analyzed comparatively.', 'title': 'Sensor Fusion Based on Integrated Navigation Data of Sea Surface Vehicle with Machine Learning Method', 'embedding': []}, {'id': 15191, 'abstractText': 'Objective: The objective of this study was to define the clinical relevance of in vivo electrophysiologic (EP) studies in a rat model of chronic ischemic heart failure (CHF). Methods: Electrical activation sequences, voltage amplitudes, and monophasic action potentials (MAPs) were recorded from adult male Sprague-Dawley rats six weeks after left coronary artery ligation. Programmed electrical stimulation (PES) sequences were developed to induce sustained ventricular tachycardia (VT). The inducibility of sustained VT was defined by PES and the recorded tissue MAPs. Results: Rats in CHF were defined (p &lt;; 0.05) by elevated left ventricular (LV) end-diastolic pressure (5 ± 1 versus 18 ± 2 mmHg), decreased LV + dP/dt (7496 ± 225 versus 5502 ± 293 mmHg/ s), LV - dP/dt (7723 ± 208 versus 3819 ± 571 mmHg), LV ejection fraction (79 ± 3 versus 30 ± 3%), peak developed pressure (176 ± 4 versus 145 ± 9 mmHg), and prolonged time constant of LV relaxation Tau (18 ± 1 versus 29 ± 2 ms). The EP data showed decreased (p &lt;; 0.05) electrogram amplitude in border and infarct zones (Healthy zone (H): 8.7 ± 2.1 mV, Border zone (B): 5.3 ± 1.6 mV, and Infarct zone (I): 2.3 ± 1.2 mV), decreased MAP amplitude in the border zone (H: 14.0 ± 1.0 mV, B: 9.7 ± 0.5 mV), and increased repolarization heterogeneity in the border zone (H: 8.1 ± 1.5 ms, B: 20.2 ± 3.1 ms). With PES we induced sustained VT (&gt;15 consecutive PVCs) in rats with CHF (10/14) versus Sham (0/8). Conclusions: These EP studies establish a clinically relevant protocol for studying genesis of VT in CHF. Significance: The in vivo rat model of CHF combined with EP analysis could be used to determine the arrhythmogenic potential of new treatments for CHF.', 'title': 'In vivo Electrophysiological Study of Induced Ventricular Tachycardia in Intact Rat Model of Chronic Ischemic Heart Failure', 'embedding': []}, {'id': 15192, 'abstractText': 'Phonation is controlled by complex synergism of muscles over the front neck region. Proper evaluation of the muscular activities in this region would not only help to estimate phonation function, but may also provide characteristics to diagnose dysphonia. While surface electromyography (sEMG) technique has been used to study the physiological aspects of phonation in previous studies, it remains unclear if the phonating function could be dynamically characterized by the sEMG signals of the neck muscles associated with phonation. In this study, almost 80 channels of high-density (HD) sEMG signals were acquired from four healthy subjects when the vowel /a/ was phonated across different pitches by them. The root mean square (RMS) of the HD sEMG signals was computed within a series of segmented analysis windows and used to construct dynamic sEMG topographic maps. And the RMS maps represented the energy distribution of the front neck muscles, which would provide both the temporal and spatial information in accordance with the physiological and biomechanical principles of phonation. Our pilot results from the sEMG topographic maps across different pitch levels showed that the muscular activities consistently increased with the enhancement of the pitch levels. This pilot study suggests that HD sEMG might be a potential tool to visualize the distribution of the muscular activities and observe the coordination of muscular contractions during phonation. Also, it might pave way for proper screening and diagnosis of dysphonia as well as its associated pathologies.', 'title': 'A pilot study on the evaluation of normal phonating function based on high-density sEMG topographic maps', 'embedding': []}, {'id': 15193, 'abstractText': 'This study investigates the interplay between load and power in load coupled interference networks. In more detail, the first objective of this study is to derive a positive concave mapping having as its fixed point the power allocation inducing a desired network load. Knowledge of this mapping is important for many theoretical and practical reasons. First, it opens up the possibility of applying many existing algorithms to compute the power inducing a desired load, which is an estimation problem typically used to obtain energy efficient network configurations. With the results in this study, systems designers can now select an algorithm based on practical considerations such as the computational complexity, memory requirements, and convergence speed. In particular, we show that algorithms based on simple fixed point iterations already have many advantages over the previously only known method for the power estimation problem. Second, knowledge of specific properties of the mapping, such as concavity, enables us to use standard tools in convex analysis to analyze the network, and it may also give rise to novel optimization tools for self-organizing networks. The second main objective of this study is the development of a truly distributed algorithm for power estimation in real networks. This algorithm uses only information that is readily available at base stations, and it does not require any additional signaling overhead. These two characteristics make the proposed algorithm especially useful in ultra-dense wireless networks, one of the main visions for 5G networks.', 'title': 'Low Complexity Iterative Algorithms for Power Estimation in Ultra-Dense Load Coupled Networks', 'embedding': []}, {'id': 15194, 'abstractText': 'Predicting the next foot placement of humans during walking can help improve compliant interactions between humans and walking aid robots. Previous studies have focused on foot placement estimation with wearable inertial sensors after heel-strike, but few have predicted foot placements in advance during the early swing phase. In this study, a Bayesian inference-based foot placement prediction approach was proposed. Possible foot placements were modeled as a probability distribution grid map. With selected foot motion feature events detected sequentially in the early swing phase, the foot placement probability map could be updated iteratively using the feature models we built. The weighted center of the probability distribution was regarded as the predicted foot placement. Prediction errors were evaluated with collected walking data sets. When testing with the data from inertial measurement units, the prediction errors were (5.46 cm ± 10.89 cm, -0.83 cm ± 10.56 cm) for cross-velocity walking data and (-4.99 cm ± 12.31 cm, -11.27 cm ± 7.74 cm) for cross-subject–cross-velocity walking data. The results were comparable to previous works yet the prediction could be made earlier. For the subject who walked with more stable gaits, the prediction error can be further decreased. The proposed foot placement prediction approach can be utilized to help walking aid robots adjust their pose before each heel-strike event during walking, which will make human–robot interactions more compliant. This study is also expected to inspire additional probabilistic gait analysis works.', 'title': 'A Probability Distribution Model-based Approach for Foot Placement Prediction in the Early Swing Phase with a Wearable IMU Sensor', 'embedding': []}, {'id': 15195, 'abstractText': \"Urban commerce and its distribution have always been an important part of urban research. However, most previous studies were based on statistical data and did not reflect real street experience. Thanks to the Street View image and deep learning technology, researchers are able to carry out large scale studies from real human visual experience. In this article, we aim at sensing the commercial spaces in cities. In order to achieve this ultimate goal, deep learning is applied to process the raw data of Street View image. We disassemble the goal into three tasks: firstly, obtaining all the Street View images in a specific area; then classifying the Street View images according to the commercial facilities in it; and finally creating a visualization of the detected data into a map. For the first task, we get the road network coordinate information from the openstreetmap (OSM) website, set the sampling point on the road, and then download the Street View images of the sampling points' coordinate through the API provided by Baidumap. For the second task, we adopt a two-level learning strategy rather than directly using Deep Convolutional Neural Network for classification. For the final task, we choose the heat map as the expression of the results and draw the map by the existing GIS software. Furthermore, the results from this study can be conveniently combined with other data because of the use of street-network-based data structure. An application of this method combines with street-network data, the calculation of a city's 15-minute commercial service circle coverage is also shown in this study.\", 'title': 'Urban Commerce Distribution Analysis Based on Street View and Deep Learning', 'embedding': []}, {'id': 15196, 'abstractText': 'The paper introduces a fundamental technological problem with collecting high-speed eye tracking data while studying software engineering tasks in an integrated development environment. The use of eye trackers is quickly becoming an important means to study software developers and how they comprehend source code and locate bugs. High quality eye trackers can record upwards of 120 to 300 gaze points per second. However, it is not possible to map each of these points to a line and column position in a source code file (in the presence of scrolling and file switching) in real time at data rates over 60 gaze points per second without data loss. Unfortunately, higher data rates are more desirable as they allow for finer granularity and more accurate study analyses. To alleviate this technological problem, a novel method for eye tracking data collection is presented. Instead of performing gaze analysis in real time, all telemetry (keystrokes, mouse movements, and eye tracker output) data during a study is recorded as it happens. Sessions are then replayed at a much slower speed allowing for ample time to map gaze point positions to the appropriate file, line, and column to perform additional analysis. A description of the method and corresponding tool, Déjà Vu, is presented. An evaluation of the method and tool is conducted using three different eye trackers running at four different speeds (60Hz, 120Hz, 150Hz, and 300 Hz). This timing evaluation is performed in Visual Studio and Eclipse IDEs. Results show that Déjà Vu can playback 100% of the data recordings, correctly mapping the gaze to corresponding elements, making it a well-founded and suitable post processing step for future eye tracking studies in software engineering.', 'title': 'Automated Recording and Semantics-Aware Replaying of High-Speed Eye Tracking and Interaction Data to Support Cognitive Studies of Software Engineering Tasks', 'embedding': []}, {'id': 15197, 'abstractText': \"The sustainability of agricultural sector is becoming an increasing concern in Indonesia, with the shock of the palm oil's products export restrictions to several countries. Therefore, it is vital to avoid the same case for all other export-oriented agricultural products, such as cocoa. This study aims to map the challenges and opportunities in developing a sustainable cocoa supply chain. The study commenses with mapping the role of stakeholders along the supply chain, studying the supply chain mapping literature in a sustainable perspective, and identifying the indicators used to measure the supply chain. The results of this study propose a conceptual model to be utilized as a base reference for research of the agricultural supply chain sustainability model development in Indonesia.\", 'title': 'Literature Review of a Multi Actor Analysis for Developing a Sustainable Agriculture Supply Chain (Case Study of Cocoa)', 'embedding': []}, {'id': 15198, 'abstractText': 'Glacial landforms are a significant element of landscape in many regions of Earth. The increasing availability of high-resolution digital elevation models (DEMs) provides an opportunity to develop automated methods of glacial landscape exploration and classification. In this study, we aimed to: 1) identify glacial landforms based on high-resolution DEM datasets; 2) determine relevant geomorphometric and spectral parameters and object-based features for the mapping of glacial landforms; and 3) develop an accurate workflow for glacial landform classification based on DEM. The developed methodology included the extraction of secondary features from DEM, feature selection with the Boruta algorithm, object-based image analysis, and random forest supervised classification. We applied the workflow for three study sites: one in Svalbard and two in Poland. It allowed the identification of six categories of glacial landforms: till plains, end moraines, hummocky moraines, outwash/glaciolacustrine plains, valleys, and kettle holes. The majority of relevant secondary features represented DEM spectral parameters calculated from 2-D Fourier analysis. The supervised classification models with the highest performance exhibited up to 96% overall accuracy with regard to a groundtruth dataset. This study showed that glacial landforms can be identified using novel image-processing methodology and spectral parameters of high-resolution DEM. The complete classification workflow developed herein provides a solution for the transparent generation of thematic maps of glacial landforms that may be reproducible and transferrable to various glacial regions worldwide.', 'title': 'Exploration of Glacial Landforms by Object-Based Image Analysis and Spectral Parameters of Digital Elevation Model', 'embedding': []}, {'id': 15199, 'abstractText': 'The dengue disease has been reported as a major cause of morbidity and mortality for the last 40 years worldwide including in Malaysia. According to the Malaysian Statists Department, total reported number of dengue cases for the year 2008 to 2010 is 136,992 and it is increasing every year. According to Ministry of Health (MoH) of Malaysia, in urban area 77% of people are suffering from dengue virus compared to 23% people suffer in rural areas. Several studies on dengue are based on their serotype, epidemiology, weather forecasting, but this thesis look into the prediction modelling based on clustering algorithm. Since, the motivation to work on this projects comes from old existing studies which only represent the spatial map with undefined number of incidences and no clustering model exist. Since, in our study we have proposed the spatial- temporal mapping along with the Clustering techniques which comes with k-mean as the initial step to generate the clusters of incidences after that to optimize K-means, K-NN techniques is applied to find best fit K values. After that Gaussian Mixture Model is applied to find density of the dengue incidences, since where K-means is used to find the centroid of the incidences. To process the Gaussian Mixture Model, Estimation Maximization (EM) algorithm is used to relate the cluster with their respective clusters. To optimize the EM algorithm Bayesian Information Criteria (BIC) is performed which gives the best fitting model of BIC and optimizes the EM algorithm. In the end, Geographical Information System (GIS) technique is use to visualize vulnerability mapping to locate the accurate prediction location for dengue incidences in state Selangor of Malaysia (area of study). This research work discusses and implements the visualization and prediction modelling based on machine learning concepts, for the vector borne diseases (dengue). The results are tested for a region (Petaling district of Selangor state) in Malaysia and they showed good performance in predicting the dengue incidences. Thus, the proposed method is able to localize the nature of dengue incidence that can further be utilized for vector disease controlled process. The results confirms location of the predicted coordinates based on the previous data for the year 2014.', 'title': 'Spatial-Temporal Visualization of Dengue Incidences Using Gaussian Kernel', 'embedding': []}, {'id': 15200, 'abstractText': 'As a first step of genomics signal processing, alphabetical sequence is mapped to numerical. The choice of mapping techniques depends on the application and affects the result of the study. Since biological function is the result of amino acids interactions, a significant method for alphabetical to numerical conversion of sequence is to use the physico-chemical and biochemical properties of amino acids. AAindex database is a rich collection of such properties that can be used for numerical representation of protein. Each of these properties gives a viewpoint in the study of biological functions. Taking into account all AAindex indices leads to a multi-viewpoint representation and provides more options to observe and study the target biological phenomena. But this advantage increases variables number, space dimension and computation time. Since there is correlation between AAindex databases, to handle the issue of space dimension increasement, compact versions of correlated indices are extracted. This paper aims at the construction of new indices through clustering of AAindex database with correlation distance. The results suggest that due to the correlation of these new maps with groups of AAindex indices (in clusters); they have the potential to be used for numerical representation of protein sequence in different studies.', 'title': 'A multivariate clustering of AAindex database for protein numerical representation', 'embedding': []}, {'id': 15201, 'abstractText': 'For medical imaging tasks, it is a prevalent practice to have a multi-modality image dataset, as experts prefer using multiple medical devices to diagnose a disease. Each device can show different aspects of segmentation, which in our case, is magnetic resonance imaging (MRI) brain tumor segmentation. For such medical imaging tasks, researchers tend to combine all modalities as an input into the network for feature extraction, and neglect the complexity between different modalities. It is no longer novel to use an encoder-decoder-based model and residual connections to transfer information from high-resolution maps to lower-resolution maps in medical segmentation tasks. In this work, we propose a multimodal fusion network with bi-directional feature pyramid network (MM-BiFPN) using an individual encoder to extract the features of each of the four modalities (FLAIR, T1-weighted, T1-c, and T2-weighted) to focus on the exploitation of the complex relationships among the modalities. In addition, by using the bi-directional feature pyramid network (Bi-FPN) layer, we focus on the aggregation of multiple modalities to study the cross-modality relationship and multi-scale features. Our experiment was conducted on the brain segmentation challenge datasets, the MICCAI BraTS2018 and MICCAI BraTS2020 datasets. We also implemented two ablation studies on our model with different cross-scale modalities fusion networks, as well as a study on different modality settings to see the effect each modality brings in detecting tumor content. With missing modalities, our method achieves a comparable result, demonstrating that our method is robust for brain tumor segmentation.', 'title': 'MM-BiFPN: Multi-Modality Fusion Network with Bi-FPN for MRI Brain Tumor Segmentation', 'embedding': []}, {'id': 15202, 'abstractText': 'A quick response to a large-scale natural disaster such as earthquake and tsunami is vital to mitigate further loss. Remote sensing, especially the spaceborne sensors, provides the possibility to monitor a very large scale area in a short time and with regular revisit circle. Damage ranges and damage levels of the destructed urban areas are extremely important information for rescue planning after an event. Rapid mapping of the urban damage levels with synthetic aperture radar (SAR) is still challenging. Compared with single-polarization SAR, fully polarimetric SAR (PolSAR) has a better potential to understand the urban damage from the viewpoint of scattering mechanism investigation. In radar polarimetry, the dominant double-bounce scattering mechanism in an urban area is primarily induced by the ground-wall structures and can reflect the changes of these structures. In this sense, urban damage level in terms of destroyed ground-wall structures can be indicated by the reduction of the dominant double-bounce scattering mechanism, which is the basis of this study. This work first establishes and validates the linear relationship between the urban damage level and the proposed polarimetric damage index using polarimetric model-based decomposition. Then, efforts are focused on the development of a rapid urban damage level mapping technique which mainly includes two steps of urban area extraction and polarimetric damage level estimation. The 3.11 East Japan earthquake and tsunami inducing great-scale destruction are adopted for study using L-band multitemporal spaceborne PolSAR data. Experimental studies demonstrate that the estimated damage levels are closely consistent to the ground-truth. The final urban damage level map for the full scene is generated thereafter. Results achieved in this study further validate the necessity of exploring fully polarimetric technique for damage assessment.', 'title': 'Urban Damage Level Mapping Based on Scattering Mechanism Investigation Using Fully Polarimetric SAR Data for the 3.11 East Japan Earthquake', 'embedding': []}, {'id': 15203, 'abstractText': 'Context: As software applications become more complex and competition between companies demands shorter time-to-market, the methodologies to develop these applications needs to adapt to this new environment of continuous change. New strategies are required to allow effective management of changing requirements, continuous deliveries, testing, integration, etc. Software reuse emerges as a potential solution to these demands. This work presents results from a systematic mapping study aimed at assessing the use of software reuse in the context of continuous software development. Objective: To identify, analyse, and classify the published works in order to provide an overview of current challenges and the trends in the application of systematic software reuse to the continuous software development. Method: We use a systematic mapping study, posing a set of questions to map the research space, defining exclusion/inclusion criteria and developing a classification schema. We limit our study to peer-reviewed works published up to December 2016. Results: Our study includes a set of fifteen works. We classified the works according to reuse processes, CSD activities and type of research. Component-based development is the most frequent reuse process, while the three CSD activities (Deployment, integration and testing) are mentioned equally. Quality, productivity and reduction of costs are the reuse effects more reported. Conclusions: We have not found previous systematic reviews in this field. The main outcome is that we do not find causal relations between reuse processes and CSD activities. The effect of reuse do not relate to the context of the experience reported. There is a lack of empirical evidence and reported data. This is a poorly investigated area offering a promising domain for future research.', 'title': 'Software Reuse and Continuous Software Development: A Systematic Mapping Study', 'embedding': []}, {'id': 15204, 'abstractText': 'Understanding the geometry and kinematics of the broad line region (BLR) of active galactic nuclei (AGN) is important to estimate black hole masses in AGN and study the accretion process. The technique of reverberation mapping (RM) has provided estimates of BLR size for more than 100 AGN now; however, the structure of the BLR has been studied for only a handful number of objects. Towards this, we investigated the geometry of the BLR for a large sample of 57 AGN using archival RM data. We performed systematic modelling of the continuum and emission line light curves using a Markov chain Monte Carlo method based on Bayesian statistics implemented in PBMAP (Parallel Bayesian code for reverberation−MAPping data) code to constrain BLR geometrical parameters and recover velocity integrated transfer function. We found that the recovered transfer functions have various shapes such as single-peaked, double-peaked, and top-hat suggesting that AGN have very different BLR geometries. Our model lags are in general consistent with that estimated using the conventional cross-correlation methods. The BLR sizes obtained from our modelling approach is related to the luminosity with a slope of 0.583\\xa0±\\xa00.026 and 0.471\\xa0±\\xa00.084 based on H\\u2009β and H\\u2009α lines, respectively. We found a non-linear response of emission line fluxes to the ionizing optical continuum for 93 <tex>${{\\\\ \\\\rm per\\\\ cent}}$</tex> objects. The estimated virial factors for the AGN studied in this work range from 0.79 to 4.94 having a mean at 1.78\\xa0±\\xa01.77 consistent with the values found in the literature.', 'title': 'Estimation of the size and structure of the broad line region using Bayesian approach', 'embedding': []}, {'id': 15205, 'abstractText': 'In comparative genomics, one goal is to find similarities between genomes of different organisms. Comparisons using genome features like genes, gene order, and regulatory sequences are carried out with this purpose in mind. Genome rearrangements are mutational events that affect large extensions of the genome. They are responsible for creating extant species with conserved genes in different positions across genomes. Close species — from an evolutionary point of view — tend to have the same set of genes or share most of them. When we consider gene order to compare two genomes, it is possible to use a parsimony criterion to estimate how close the species are. We are interested in the shortest sequence of genome rearrangements capable of transforming one genome into the other, which is named <italic>rearrangement distance</italic>. Reversal is one of the most studied genome rearrangements events. This event acts in a segment of the genome, inverting the position and the orientation of genes in it. Transposition is another widely studied event. This event swaps the position of two consecutive segments of the genome. When the genome has no gene repetition, a common approach is to map it as a permutation such that each element represents a conserved block. When genomes have replicated genes, this mapping is usually performed using strings. The number of replicas depends on the organisms being compared, but in many scenarios, it tends to be small. In this work, we study the rearrangement distance between genomes with replicated genes considering that the orientation of genes is unknown. We present four heuristics for the problem of genome rearrangement distance with replicated genes. We carry out experiments considering the exclusive use of the reversals or transpositions events, as well as the version in which both events are allowed. We developed a database of simulated genomes and compared our results with other algorithms from the literature. The experiments showed that our heuristics with more sophisticated rules presented a better performance than the known algorithms to estimate the evolutionary distance between genomes with replicated genes. In order to validate the application of our algorithms in real data, we construct a phylogenetic tree based on the distance provided by our algorithm and compare it with a know tree from the literature.', 'title': 'Heuristics for Genome Rearrangement Distance With Replicated Genes', 'embedding': []}, {'id': 15206, 'abstractText': 'With the advent of Service-Oriented Architecture (SOA), services can be registered, invoked, and combined by their identical Quality of Services (QoS) attributes to create a new value-added application that fulfils user requirements. Efficient QoS-aware service composition has been a challenging task in cloud computing. This challenge becomes more formidable in emerging resource-constrained computing paradigms such as the Internet of Things and Fog. Service composition has regarded as a multi-objective combinatorial optimization problem that falls in the category of NP-hard. Historically, the proliferation of services added to problem complexity and navigated solutions from exact (none-heuristics) approaches to near-optimal heuristics and metaheuristics. Although metaheuristics have fulfilled some expectations, the quest for finding a high-quality, near-optimal solution has led researchers to devise hybrid methods. As a result, research on service composition shifts towards the hybridization of metaheuristics. Hybrid metaheuristics have been promising efforts to transcend the boundaries of metaheuristics by leveraging the strength of complementary methods to overcome base algorithm shortcomings. Despite the significance and frontier position of hybrid metaheuristics, to the best of our knowledge, there is no systematic research and survey in this field with a particular focus on strategies to hybridize traditional metaheuristics. This study’s core contribution is to infer a framework for hybridization strategies by conducting a mapping study that analyses 71 papers between 2008 and 2020. Moreover, it provides a panoramic view of hybrid methods and their experiment setting in respect to the problem domain as the main outcome of this mapping study. Finally, research trends, directions and challenges are discussed to benefit future endeavours.', 'title': 'Hybrid Metaheuristics for QoS-aware Service Composition: A Systematic Mapping Study', 'embedding': []}, {'id': 15207, 'abstractText': 'Forest is the largest terrestrial ecosystem on the earth. Quantitative evaluation of the impacts of the land surface slope on forest spatial distributions is of great significance for a deeper understanding of functions and stability of forest ecosystem, scientific planning and rational management of forest resources. The superposition analysis of map of vegetation and digital elevation model (DEM) is an effective method to study impacts of the land surface slope on forest spatial distributions. In the past time, the data of land surface slope was mainly obtained by field measurement which had some problems of time-consuming, labor-intensive and high investigation cost. With the development of space technology, DEM data can be used to obtain land surface slope data rapidly and efficiently which has been widely used in digital forestry construction. However, studies on the influence of land surface slope on forest spatial distribution by using DEM data are rare at home and abroad. Dali City of Yunnan Province was selected as the research area in this study. The contour line vector is used to establish DEM of this area, then collect the slope data and divide the slope grades into five groups: flat slopegentle slopemoderate slopesteep slope and sharp slope. Supervised classification and visual interpretation were executed to interpret and classify the map of vegetation of Dali. By putting map of forest distribution and DEM togetherthe relationship between forest distribution and slope was analyzed and the trend of forest spatial distribution was found out. The results showed that Dali city is relatively flat and the terrain is complex and diverse. The woodland has a large area distribution in Dali City, which is related to the monsoon climate of the subtropical plateau in Dali. The shrub forests were mainly distributed on moderate slope and steep slope, and the coniferous forests were mainly distributed on gentle slope and moderate slope. As the slope changes, the distribution of shrubberies increases and decreases sharply, indicating that shrubberies are highly dependent on slope. Coniferous forests have a large area distribution at each grade, indicating that they are less dependent on slope. In general, with the increase of slope, both forest types showed a trend of increasing first and then decreasing, indicating that the surface slope has an impact on the spatial distribution of forests. This study can offer reference to the rational and scientific management of forest resources.', 'title': 'Impacts of the Land Surface Slope on Forest Spatial Distributions', 'embedding': []}, {'id': 15208, 'abstractText': 'Based on the analysis of the current literature, mathematical modelling of the studied phenomenon was carried out. It was conducted according to the graph theory for vehicles transporting dangerous, oversized, and valuable cargo, moving within a transport network. The mapping of the parking areas included organisational, technical, and security aspects. The main algorithm determines the driving routes based on the parkings that meet the requirements specified by the carrier. These routes can ultimately be analysed and evaluated based on the parameters resulting from the formulated criterion functions. To verify the performance of the algorithm, its implementation was carried out in a computer environment using the Neo4j graph database. The transport network, consisting of 462 transport nodes and 602 transport links, was mapped based on the real national road network of the province in Poland. Data from 113 parking locations in the study area were included in the analysis. The research covered a total of three case studies, one for each vehicle type requiring specific parking conditions, for various input data. The results obtained allowed us to assess the validity of selecting particular routes and evaluating the scalability of the solution. The result of this work is a method, which, on the one hand, fits the current transportation requirements. On the other hand, it lends itself to scaling, extension by additional logical constraints, and is compatible with modern parking systems.', 'title': 'Parking lots assignment algorithm for vehicles requiring specific parking conditions in Vehicle Routing Problem', 'embedding': []}, {'id': 15209, 'abstractText': \"This study aims to obtain a future mapping of NO2 and SO2 pollutant levels in the city of Bandung and it's around. Two mathematical models are used in this study namely the Generalized Space-Time Autoregressive (GSTAR) and Simple Kriging models. The GSTAR model is an estimator model based on time and this study is used to predict future pollutant levels at specific location points. While the Simple Kriging model is a spatial model which in this study is used to interpolate or estimate pollutant levels around points previously estimated using the GSTAR model. The pollutant level estimation of Simple Kriging is then used to map the pollutant levels in Bandung in the next few years. Ljung-Box statistical test results show that the GSTAR model is feasible to use. The assumption of stationary data on the simple Kriging model is determined using the Augmented Dickey-Fuller (ADF) test, while the determination of the semivariogram model is determined using the RMSE calculation. The ADF test showed that stationary data was obtained for the first derivative, whereas the most appropriate theoretical semivariogram model is an exponential semivariogram. Pollutant mapping results in 2021-2024 show that the northern and southern Bandung regions have higher levels of pollutant levels than other locations, although significant increases only occurred for NO2 levels.\", 'title': 'Prediction and Mapping of Air Pollution in Bandung Using Generalized Space Time Autoregressive and Simple Kriging', 'embedding': []}, {'id': 15210, 'abstractText': 'We study the task of single person dense pose estimation. Specifically, given a human-centric image, we learn to map all human pixels onto a 3D, surface-based human body model. Existing methods approach this problem by fitting deep convolutional networks on sparse annotated points where the regression on both surface coordinate components for each body part is uncorrelated and optimized separately. In this work, we devise a novel, unified regression loss function that explicitly characterizes the correlation between the two surface coordinate components, achieving significant improvements in both accuracy and efficiency. Furthermore, based on an observation that the image-to-surface correspondence is intrinsically invariant to geometric transformations from input images, we propose to enforce a geometric equivariance consistency on the target mapping, thereby allowing us to enable reliable supervision on large amounts of unlabeled pixels. We conduct comprehensive studies on the effectiveness of our approach using a quite simple network. Extensive experiments on the DensePose-COCO dataset show that our model achieves superior performance against previous state-of-the-art methods with much less computation complexity. We hope that our work would serve as a solid baseline for future study in the field. The code will be available at https://github.com/Johnqczhang/densepose.pytorch.', 'title': 'Single Person Dense Pose Estimation via Geometric Equivariance Consistency', 'embedding': []}, {'id': 15211, 'abstractText': 'The purpose of this study was to develop a consensus-based computed tomographic (CT) atlas that defines lymph node stations in radiotherapy for lung cancer based on the lymph node map of the International Association for the Study of Lung Cancer (IASLC). A project group in the Japanese Radiation Oncology Study Group (JROSG) initially prepared a draft of the atlas in which lymph node Stations 1–11 were illustrated on axial CT images. Subsequently, a joint committee of the Japan Lung Cancer Society (JLCS) and the Japanese Society for Radiation Oncology (JASTRO) was formulated to revise this draft. The committee consisted of four radiation oncologists, four thoracic surgeons and three thoracic radiologists. The draft prepared by the JROSG project group was intensively reviewed and discussed at four meetings of the committee over several months. Finally, we proposed definitions for the regional lymph node stations and the consensus-based CT atlas. This atlas was approved by the Board of Directors of JLCS and JASTRO. This resulted in the first official CT atlas for defining regional lymph node stations in radiotherapy for lung cancer authorized by the JLCS and JASTRO. In conclusion, the JLCS–JASTRO consensus-based CT atlas, which conforms to the IASLC lymph node map, was established.', 'title': 'The Japan Lung Cancer Society–Japanese Society for Radiation Oncology consensus-based computed tomographic atlas for defining regional lymph node stations in radiotherapy for lung cancer', 'embedding': []}, {'id': 15212, 'abstractText': \"With the advancement in the technology, objects can be represented effectively in their 3D digital models which accurately represents their physical counterparts. Navigation services and mapping based on geographical data have become very popular in supporting our everyday lives. Much of these services are currently available mostly for outdoor purposes, however applications for indoor purposes are being explored where most of the human activities takes place. This can help transform cities into “Smart Cities”. The aim of this study is to develop an indoor mapping system for data collection in a building environment by exploring new, efficient and cost effective scanning devices. The conventional devices currently in use are expensive which makes them difficult to implement for large scale applications. The data will be collected using a 3D scanning camera technology which develops depth maps of various locations. Xbox's Kinect Sensor and Stereolab's ZED camera are being used and compared in this study. Comparisons based on resolution, lighting, accuracy, speed and memory are being made in this study. Their pros and cons over conventional scanning devices are also discussed. The study shows the possibility of using this technology in a large scale building environment in an autonomous method for the future. This technology can then be potentially used for commercial purposes especially to track progress at construction sites, security purposes, facility management, retail and augmented reality applications.\", 'title': 'Indoor mapping for smart cities — An affordable approach: Using Kinect Sensor and ZED stereo camera', 'embedding': []}, {'id': 15213, 'abstractText': 'In the current competitive world, producing quality products has become a prominent factor to succeed in business. In this respect, defining and following the software product quality metrics (SPQM) to detect the current quality situation and continuous improvement of systems have gained tremendous importance. Therefore, it is necessary to review the present studies in this area to allow for the analysis of the situation at hand, as well as to enable us to make predictions regarding the future research areas. The present research aims to analyze the active research areas and trends on this topic appearing in the literature during the last decade. A Systematic Mapping (SM) study was carried out on 70 articles and conference papers published between 2009 and 2019 on SPQM as indicated in their titles and abstract. The result is presented through graphics, explanations, and the mind mapping method. The outputs include the trend map between the years 2009 and 2019, knowledge about this area and measurement tools, issues determined to be open to development in this area, and conformity between conference papers, articles and internationally valid quality models. This study may serve as a foundation for future studies that aim to contribute to the development in this crucial field. Future SM studies might focus on this subject for measuring the quality of network performance and new technologies such as Artificial Intelligence (AI), Internet of things (IoT), Cloud of Things (CoT), Machine Learning, and Robotics.', 'title': 'Software Product Quality Metrics: A Systematic Mapping Study', 'embedding': []}, {'id': 15214, 'abstractText': \"Understanding the 3-D geometric structure of the Earth's surface has been an active research topic in photogrammetry and remote sensing community for decades, serving as an essential building block for various applications such as 3-D digital city modeling, change detection, and city management. Previous research studies have extensively studied the problem of height estimation from aerial images based on stereo or multiview image matching. These methods require two or more images from different perspectives to reconstruct 3-D coordinates with camera information provided. In this letter, we deal with the ambiguous and unsolved problem of height estimation from a single aerial image. Driven by the great success of deep learning, especially deep convolutional neural networks (CNNs), some research studies have proposed to estimate height information from a single aerial image by training a deep CNN model with large-scale annotated data sets. These methods treat height estimation as a regression problem and directly use an encoder-decoder network to regress the height values. In this letter, we propose to divide height values into spacing-increasing intervals and transform the regression problem into an ordinal regression problem, using an ordinal loss for network training. To enable multiscale feature extraction, we further incorporate an Atrous Spatial Pyramid Pooling (ASPP) module to extract features from multiple dilated convolution layers. After that, a postprocessing technique is designed to transform the predicted height map of each patch into a seamless height map. Finally, we conduct extensive experiments on International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam data sets. Experimental results demonstrate significantly better performance of our method compared to state-of-the-art methods.\", 'title': 'Height Estimation From Single Aerial Images Using a Deep Ordinal Regression Network', 'embedding': []}, {'id': 15215, 'abstractText': 'Currently, the use of Information and Communication Technology (ICT) is developing to meet the demands of a better life. This enhances studies to try to improve services to meet these demands. is one to shape a better experience. development needs to be done in a structured way so that they are integrated and efficiently implement the latest technology to build a digital economy. For this reason, it is necessary to apply good governance in planning for the development of technology-based services, so that the development of smart cities towards digital prosperity can be realised. The most important thing that needs to map out is prioritising services in smart cities development.In this study, we conducted a literature study and observations of ICT implementation in the development of services in Indonesia. Then we do a mapping between services that can shape into a digital economy by referring to digital welfare introduced by Atkinsons.Our results show that the initial program that supports faster productivity growth is an ICT service that makes up smart cities is a priority that must be considered by the central and local governments.', 'title': 'The Governance Strategies To Build Smart City Towards Digital Prosperity', 'embedding': []}, {'id': 15216, 'abstractText': 'This paper presents the study of mineral and vegetation in explored fields around the San Juan coal mines west of Farmington, New Mexico. The purpose of this research work is to map the mineral rock &amp; vegetation for statistically analyzing the study area. Pre-processing of Hyperspectral imagery (HSI) data is required for conversion from digital value to reflectance. Minimum Noise Fraction (MNF) and Pure Pixel Index (PPI) method is used for extraction of Endmember fraction. Spectral signature matching procedure is done with U.S. Geological Survey (USGS) Spectral library, which contain spectra of individual species that have been acquired at test sites representatives of varied terrain and climatic zones, observed in the field under natural conditions. Spectral Angle Mapper (SAM) technique is used for spectral analysis and mapping of image. Finally study area is mapped in two classes namely Carnallite mineral and Sagebrush vegetation plants. Land covered by Sagebrush plant is 8.31% and Carnallite is 1.41% of study area.', 'title': 'Mapping of the carnallite mineral and sagebrush vegetation plant by using hyperspectral remote sensing and usgs spectral library', 'embedding': []}, {'id': 15217, 'abstractText': 'The use of games in software engineering education is not new. However, recent technologies have provided new opportunities for using games and their elements to enhance learning and student engagement. The goal of this paper is twofold. First, we discuss how game related methods have been used in the context of software engineering education by means of a systematic mapping study. Second, we investigate how these game related methods support specific knowledge areas from software engineering. The systematic mapping study identified 106 primary studies describing the use of serious games, gamification and game development in software engineering education. Based on this mapping, we aimed to track the learning goals of each primary study to the knowledge areas defined in ACM/IEEE curricular recommendations. As a result, we observed that \"Software Process\", \"Software Design\" and \"Profession Practice\" are the most recurring knowledge areas explored by game related approaches in software engineering education. We also uncover possible research opportunities for game-related education methods.', 'title': 'Games for learning: bridging game-related education methods to software engineering knowledge areas', 'embedding': []}, {'id': 15218, 'abstractText': 'Quantitative acoustic microscopy (QAM) at 500 MHz permits measuring acoustic properties, such as speed of sound (SOS), attenuation (A) and acoustic impedance (Z), of tissue microstructure with a spatial resolution of 4 μm. Although high-frequency QAM has been shown to be a suitable tool for measuring the acoustic properties of several soft tissues, very few studies have been performed at 500 MHz, and none in cancerous lymph nodes (LNs). However, data at such fine resolutions are essential to improve our understanding of ultrasound scattering at lower frequencies (25 MHz) from LNs to detect clinically significant micrometastases. In this study, quantitative acoustic microscopy (QAM) at 500 MHz was performed to obtain 2D maps of speed of sound (c), attenuation (A), acoustic impedance (Z), and other derived acoustical parameters of nodal tissue microstructure with a spatial resolution of 4 μm. Thin section (i.e., 6-μm) of lymph nodes were scanned using a custom-built QAM system based on a novel F-1.08, 500-MHz transducer. The system digitized radio-frequency (RF) signals at 2.5 GHz with 12-bit accuracy.2D QAM maps of the acoustic parameters were obtained using custom a signal-processing algorithm. Following QAM scanning, the samples were stained using hematoxylin and eosin and imaged by light microscopy. The study illustrates that fine-resolution maps of acoustic properties of lymph nodes can be obtained and can provide previously unavailable information. Future studies will investigate the use of 2DZMs to improve the model of ultrasound scattering at 26 MHz. The new lymphnode-specific, ultrasound-scattering models could improve sensitivity and specificity of current QUS approaches for detecting metastatic regions in freshly excised lymph nodes from cancer patients.', 'title': '500-MHz quantitative acoustic microscopy imaging of unstained fixed 6-µm thin sections from cancerous human lymph nodes', 'embedding': []}, {'id': 15219, 'abstractText': 'Semi-supervised learning aims to learn prediction models from both labeled and unlabeled samples. There has been extensive research in this area. Among existing work, generative mixture models with Expectation-Maximization (EM) is a popular method due to clear statistical properties. However, existing literature on EM-based semi-supervised learning largely focuses on unstructured prediction, assuming that samples are independent and identically distributed. Studies on EM-based semi-supervised approach in structured prediction is limited. This paper aims to fill the gap through a comparative study between unstructured and structured methods in EM-based semi-supervised learning. Specifically, we compare their theoretical properties and find that both methods can be considered as a generalization of self-training with soft class assignment of unlabeled samples, but the structured method additionally considers structural constraint in soft class assignment. We conducted a case study on real-world flood mapping datasets to compare the two methods. Results show that structured EM is more robust to class confusion caused by noise and obstacles in features in the context of the flood mapping application.', 'title': 'Semi-supervised Learning with the EM Algorithm: A Comparative Study between Unstructured and Structured Prediction', 'embedding': []}, {'id': 15220, 'abstractText': 'This paper presents a Bayesian framework for estimating a Probabilistic Linear Discriminant Analysis (PLDA) model in the presence of noisy labels. True class labels are interpreted as latent random variables, which are transmitted through a noisy channel, and received as observed speaker labels. The labeling process is modeled as a Discrete Memoryless Channel (DMC). PLDA hyperparameters are interpreted as random variables, and their joint posterior distribution is derived using mean-field Variational Bayes, allowing maximum a posteriori (MAP) estimates of the PLDA model parameters to be determined. The proposed solution, referred to as VB-MAP, is presented as a general framework, but is studied in the context of speaker verification, and a variety of use cases are discussed. Specifically, VB-MAP can be used for PLDA estimation with unreliable labels, unsupervised PLDA estimation, and to infer the reliability of a PLDA training set. Experimental results show the proposed approach to provide significant performance improvements on a variety of NIST Speaker Recognition Evaluation (SRE) tasks, both for data sets with simulated mislabels, and for data sets with naturally occurring missing or unreliable labels.', 'title': 'Bayesian Estimation of PLDA in the Presence of Noisy Training Labels, with Applications to Speaker Verification', 'embedding': []}, {'id': 15221, 'abstractText': \"Satellite remote sensing is playing an increasing role in the rapid mapping of damage after natural disasters. In particular, synthetic aperture radar (SAR) can image the Earth's surface and map damage in all weather conditions, day and night. However, current SAR damage mapping methods struggle to separate damage from other changes in the Earth's surface. In this study, we propose a novel approach to damage mapping, combining deep learning with the full time history of SAR observations of an impacted region in order to detect anomalous variations in the Earth's surface properties due to a natural disaster. We quantify Earth surface change using time series of interferometric SAR coherence, then use a recurrent neural network (RNN) as a probabilistic anomaly detector on these coherence time series. The RNN is first trained on pre-event coherence time series, and then forecasts a probability distribution of the coherence between pre- and post-event SAR images. The difference between the forecast and observed co-event coherence provides a measure of confidence in the identification of damage. The method allows the user to choose a damage detection threshold that is customized for each location, based on the local behavior of coherence through time before the event. We apply this method to calculate estimates of damage for three earthquakes using multiyear time series of Sentinel-1 SAR acquisitions. Our approach shows good agreement with observed damage and quantitative improvement compared to using pre- to co-event coherence loss as a damage proxy.\", 'title': 'Deep Learning-Based Damage Mapping With InSAR Coherence Time Series', 'embedding': []}, {'id': 15222, 'abstractText': 'Mapping of dangerous and flammable gas levels in outdoor environments, such as the oil pipeline and the industrial areas, is needed to monitor gas leaks. The use of sensor networks will require a large number of the sensors for a wide area. The mobile robot can be used as a sensor node that moves to map the desired area. In this study, we have developed a gas level mapping system for outdoor environment using solar-powered mobile robot. The proportional-integral-derivative control method is used by the robot to maneuver and follow the direction setpoint of provided Global Position System waypoints. The experimental results show that the use of solar panel can increase the duration of mobile robot operation. The results of gas concentration mapping by the mobile robot can be displayed online at the website integrated with Google Maps in both personal computer and smartphone.', 'title': 'Online Gas Mapping in Outdoor Environment using Solar-Powered Mobile Robot', 'embedding': []}, {'id': 15223, 'abstractText': \"Mapping and modeling the complex ecosystems and their changes over time are key issues in spatial ecology, biogeography, ecosystem ecology and biodiversity researches. This paper attempts to propose a simple, practical and automatic method to produce the ecosystem map in mountainous areas by fusing multi-source data and the related knowledge. The multi-source data included the 30m-resolution land cover map and the vegetation map of China (1:1 000 000). Three fusion strategies were contained in the proposed approach: hard matching, buffer matching and merged categories matching. Meanwhile, the related spatial distribution knowledge and the law of spatial distance decay were used to determine the optimal vegetation type, when more than two vegetation types are matched simultaneously. Taking the Southwestern China as study area, a new 30m-resolution ecosystem map with 144 ecosystem types was generated by the proposed method, which was used to establish the red list of ecosystems and evaluate the condition of biodiversity in the White Paper: China's Biodiversity.\", 'title': 'Ecosystem mapping in mountainous areas by fusing multi-source data and the related knowledge', 'embedding': []}, {'id': 15224, 'abstractText': 'In many robotic applications, especially for the autonomous driving, understanding the semantic information and the geometric structure of surroundings are both essential. Semantic 3D maps, as a carrier of the environmental knowledge, are then intensively studied for their abilities and applications. However, it is still challenging to produce a dense outdoor semantic map from a monocular image stream. Motivated by this target, in this paper, we propose a method for large-scale 3D reconstruction from consecutive monocular images. First, with the correlation of underlying information between depth and semantic prediction, a novel multi-task Convolutional Neural Network (CNN) is designed for joint prediction. Given a single image, the network learns low-level information with a shared encoder and separately predicts with decoders containing additional Atrous Spatial Pyramid Pooling (ASPP) layers and the residual connection which merits disparities and semantic mutually. To overcome the inconsistency of monocular depth prediction for reconstruction, post-processing steps with the superpixelization and the effective 3D representation approach are obtained to give the final semantic map. Experiments are compared with other methods on both semantic labeling and depth prediction. We also qualitatively demonstrate the map reconstructed from large-scale, difficult monocular image sequences to prove the effectiveness and superiority.', 'title': 'Monocular Outdoor Semantic Mapping with a Multi-task Network', 'embedding': []}, {'id': 15225, 'abstractText': 'The evaluation of electroencephalographic (EEG) signals is very crucial for human studies. Mostly there are animal recordings of EEG used for further research as well. One of the often used quantitative EEG methods for EEG analysis is topographical mapping. The barycentric interpolation method for 3D animal brain-mapping was implemented and tested. The 3D spline and spherical spline interpolation methods were used for comparison purpose. The surrogate data and real EEG recording were used for the interpolation methods testing. The Root Mean Square Error (RMSE) and significant probability mapping via Fisher non-parametric permutation test were used for evaluation. The RMSE of surrogate data was 0.070 for 3D spline, 0.594 for spherical spline and 0.321 for 3D barycentric interpolation method. The RMSE of the real EEG recordings nearby the electrodes was 0.1848 for 3D spline, 1.1027 for spherical spline and 0.0001 for 3D barycentric interpolation method. The 3D barycentric method differs from the 3D spline interpolation method in the large area on 0.001 alpha level based on significant probability mapping. The non-significant parts are only in the nearby electrode area. The 3D spline interpolation method gives the best results based on RMSE and significant probability testing.', 'title': '3D Barycentric Interpolation Method for Animal Brainmapping', 'embedding': []}, {'id': 15226, 'abstractText': \"In this study, we evaluated the performance of AR-assisted navigation in a real underground mine with good and limited illumination conditions as well as without the illumination considering possible search and rescue conditions. For this purpose, we utilized the Lumin SDK's embedded spatial mapping algorithm for mapping and navigating. We used the spatial mapping algorithm to create the mesh model of the escape route and to render it with the user input into the Magic Leap One. Then we compared the spatial mapping algorithm in three different scenarios for the evacuation of an underground mine in an emergency situation. The escape route has two junctions and 30 meters (100 feet). The baseline scenarios are (i) evacuation of the mine in a fully illuminated condition, (ii) evacuation with the headlamp and (iii) without any illumination. In the first scenario (fully illuminated route with the rendered meshes) the evacuation took 40 seconds. In the second scenario (illumination with the headlamp), the evacuation took 44 seconds. For the last scenario (no light source and hence in total darkness) the evacuation took 54 seconds. We found that AR-assisted navigation is effective for supporting search and rescue efforts in high attrition conditions of underground space.\", 'title': 'An Evaluation of AR-Assisted Navigation for Search and Rescue in Underground Spaces', 'embedding': []}, {'id': 15227, 'abstractText': 'An accurate global map of current cropland is needed to tackle the agricultural challenges of the next decades. Several remote sensing products have provided with global cropland maps. To create the GEOGLAM best available global crop-specific maps, we integrated the SPAM 2005 with regional maps developed by different countries and international organisms. The comparison with other available global cropland products showed a clear linear agreement between products and a tendency towards overestimation of GEOGLAM best available global crop-specific maps. In addition, a sensitivity analysis was performed using the forecast production model for winter wheat developed in previous studies. It showed how at constant values of adjusted NDVI small variations of cultivated area (5000 ha) produced a 17% rate of variation in the estimated production values.', 'title': 'Geoglam Best Available Crop-Specific Global Maps: Strengths and Limitations', 'embedding': []}, {'id': 15228, 'abstractText': 'PET imaging of small animals is often used for assessing biodistribution of a novel radioligand and pharmacology in small animal models of disease. PET acquisition and processing settings may affect reference region or image-derived input function (IDIF) kinetic modeling estimates. We examined four different factors in comparing quantitative results: 1) effect of reconstruction algorithm, 2) number of MAP iterations, 3) strength of the MAP prior, and 4) Attenuation and scatter. The effect of these parameters has not been explored for small-animal reference region and IDIF kinetic modeling approaches. Dynamic PET/CT scans were performed in 3 species with 3 different tracers: house sparrows with 11Craclopride, rats with 18FAS2471907 (11bHSD1) and mice with 11CUCB-J (SV2A). FBP yielded lower kinetic modeling estimates compared to 3D-OSEM-MAP reconstructions, in sparrow and rat studies. Target resolutions (MAP prior strength) of 1.5 and 3.0mm demonstrated reduced VT in rats but only 3.0mm reduced BPND in sparrows. Therefore, use of the highest target resolution (0.8mm) is warranted. We demonstrated using kinetic modeling that forgoing CT-based attenuation and scatter correction may be appropriate to improve animal throughput when using short-lived radioisotopes in sparrows and mice. This work provides recommendations and a framework for future optimization of kinetic modeling for preclinical PET methodology with novel radioligands.', 'title': 'Optimized Methodology for Reference Region and Image-Derived Input Function Kinetic Modeling in Preclinical PET', 'embedding': []}, {'id': 15229, 'abstractText': 'A data encryption model based on logistic map is studied and multiple chaotic dynamical systems were implemented to improve by addressing the issues related to use of logistic map. Use of different chaotic systems other than logistic map, provides us to vary control parameters to a larger extent, hence enabling to overcome the limitations of logistic map without compromising security of data. Further, a large range of control parameters were provided to make system more key dependent. The measure of degree of complexity in the cipher was examined for different chaotic dynamical systems. The proposed method eliminates the issues related to distribution of randomly generated data by using Lozi, Henon, Tent and Bernoulli maps for given values of control parameters.', 'title': 'Improved cryptographic model for better information security', 'embedding': []}, {'id': 15230, 'abstractText': 'While accuracy requirement for simulation-based efficiency map becomes high for traction motors which have high efficiency over wide drive range and where even a small improvement in efficiency is critical, their accuracy level has not been well studied. To evaluate the accuracy of Finite Element Analysis (FEA) based efficiency map, maps were generated with different loss calculation methods for an IPM machine. The simulation results were compared with a measurement-based efficiency map. The comparison indicated that FEA can reproduce an efficiency map with less than 1% error when losses were calculated taking into account high fidelity factors such as minor hysteresis loops, AC loss, manufacturing degradations, and stray losses. Furthermore, the sensitivities of those factors on the error were evaluated.', 'title': 'An Accuracy Study of Finite Element Analysis-based Efficiency Map for Traction Interior Permanent Magnet Machines', 'embedding': []}, {'id': 15231, 'abstractText': 'Electrocardiographic imaging has been shown to provide useful information for pre-procedure planning of catheter-ablation procedures. The methodology involves reconstruction of unipolar electrograms (EGMs) and isochronal maps on the epicardial surface from non-invasively acquired body-surface potentials. We have developed an algorithm for evaluating global myocardial activation times. First, the cross-correlation method determines the delay in local activation times among pairs of neighboring nodes. Next, a sparse linear system is constructed from known activation delays of neighboring nodes. To solve this system, we use a sparse Bayesian learning method to calculate the global myocardial activation times. The aim of this study was to assess the proposed method in both structurally normal and scarred ventricular myocardium. Isochronal maps of calculated activation times were compared with local activation times (LATs) derived from directly-measured epicardial EGMs obtained by electroanatomic contact mapping, for pacing delivered by an implantable cardioverter defibrillator (ICD) at the endocardial right-ventricular (RV) apex, and for catheter pacing at RV epicardial site. We found that even in the presence of infarct scar, isochronal maps calculated by the proposed method correlated closely with known LATs exported from an electroanatomic mapping system.', 'title': 'An Algorithm for Imaging Isochrones of Ventricular Activation on Patient-Specific Epicardial Surface', 'embedding': []}, {'id': 15232, 'abstractText': 'Localization with laser range finder (LRF) needs an environment map. In some environments, the localization using the environment map prepared in advance often fails. In this study, we achieve localization using both the environment map and the feature of supplemented objects as the estimated map. Moreover, to suppress the influence of the observation noises of LRF, we estimate the feature of supplemented objects using the extended Kalman filter (EKF). In this paper, we verify the influence of the estimated map on estimation accuracy through the simulation and verify the effectiveness of this localization method through the experiment conducted in a gymnasium where is larger than the scanning range of LRF.', 'title': 'Localization with LRF using both environment map and feature of supplemented objects', 'embedding': []}, {'id': 15233, 'abstractText': 'A continuous map <tex>$\\\\mathbb {C}^d\\\\longrightarrow \\\\mathbb {C}^N$</tex> is a complex <tex>$k$</tex>-regular embedding if any <tex>$k$</tex> pairwise distinct points in <tex>$\\\\mathbb {C}^d$</tex> are mapped by <tex>$f$</tex> into <tex>$k$</tex> complex linearly independent vectors in <tex>$\\\\mathbb {C}^N$</tex>. The existence of such maps is closely connected with classical problems of algebraic/differential topology, such as embedding/immersion problems. Our central result on complex <tex>$k$</tex>-regular embeddings extends results of Cohen &amp; Handel (1978), Chisholm (1979) and Blagojević, Lück &amp; Ziegler (2013) on real <tex>$k$</tex>-regular embeddings. We give the following lower bounds for the existence of complex <tex>$k$</tex>-regular embeddings. Let <tex>$p$</tex> be an odd prime, <tex>$k\\\\geq 1$</tex> and <tex>$d=p^t$</tex> for <tex>$t\\\\geq 1$</tex>. If there exists a complex <tex>$k$</tex>-regular embedding <tex>$\\\\mathbb {C}^d\\\\longrightarrow \\\\mathbb {C}^N$</tex>, then <tex>$ d(k-\\\\alpha _p(k))+\\\\alpha _p(k)\\\\leq N$</tex>. Here <tex>$\\\\alpha _p(k)$</tex> denotes the sum of coefficients in the <tex>$p$</tex>-adic expansion of <tex>$k$</tex>. These lower bounds are obtained by modifying the framework of Cohen &amp; Handel (1978) and a study of Chern classes of complex regular representations. As a main technical result we establish for this an extended Vassiliev conjecture, the following upper bound for the height of the cohomology of an unordered configuration space: If <tex>$d\\\\geq 2$</tex> and <tex>$k\\\\geq 2$</tex> are integers, and <tex>$p$</tex> is an odd prime. Then <tex>$$\\\\mathfrak{h}\\\\left(H^*\\\\left(F\\\\left(\\\\mathbb{R}^d,k\\\\right)/\\\\mathfrak{S}_k;\\\\mathbb{F}_p\\\\right)\\\\right)\\\\leq\\\\min\\\\left\\\\{p^t: 2p^t\\\\geq d \\\\right\\\\}.$$</tex> Furthermore, we give similar lower bounds for the existence of complex <tex>$\\\\ell $</tex>-skew embeddings <tex>$\\\\mathbb {C}^d\\\\longrightarrow \\\\mathbb {C}^N$</tex>, for which we require that the images of the tangent spaces at any <tex>$\\\\ell $</tex> distinct points are skew complex affine subspaces of <tex>$\\\\mathbb {C}^N$</tex>. In addition we give improved lower bounds for the Lusternik–Schnirelmann category of <tex>$F(\\\\mathbb {C}^d,k)/\\\\mathfrak {S}_k$</tex> as well as for the sectional category of the covering <tex>$F(\\\\mathbb {C}^d,k)\\\\longrightarrow F(\\\\mathbb {C}^d,k)/\\\\mathfrak {S}_k$</tex>.', 'title': 'On Complex Highly Regular Embeddings and the Extended Vassiliev Conjecture', 'embedding': []}, {'id': 15234, 'abstractText': 'In the present study, we develop a contactless optical characterization tool that quantifies and maps the trapping defects density within a thin film photovoltaic device. This is achieved by probing time-resolved photoluminescence and numerically reconstructing the experimental decays under several excitation conditions. The values of defects density in different Cu(In,Ga)Se<sub>2</sub> solar cells were extracted and linked to photovoltaic performances such as the open-circuit voltage. In the second part of the work, the authors established a micrometric map of the trapping defects density. This revealed areas within the thin film CIGS solar cell with low photovoltaic performance and high trapping defects density. The final part of the work was dedicated to finding the origin of the spatial fluctuations of the thin film transport properties. To do so, we started by establishing a micrometric map of the absolute quasi-Fermi levels splitting within the same CIGS solar cell, using the hyperspectral imager. A correlation is obtained between the map of quasi-Fermi levels splitting of and the map of the trapping defects density. The latter is found to be the origin of the frequently observed spatial fluctuations of thin film materials properties.', 'title': 'Micrometric mapping of absolute trapping defects density using quantitative luminescence imaging', 'embedding': []}, {'id': 15235, 'abstractText': \"Face recognition, although being a popular area of research and study, still has many challenges, and with the appearance of the Microsoft Kinect device, new possibilities of research were uncovered, one of which is face recognition using the Kinect. With the goal of enhancing face recognition, this paper is aiming to prove how depth maps, since not effected by illumination, can improve face recognition with a benchmark algorithm based on the Eigenface. This required some experiments to be carried out, mainly in order to check if algorithms created to recognize faces using normal images can be as effective if not more effective with depth map images. The OpenCV Eigenface algorithm implementation was used for the purpose of training and testing both normal and depth-map images. Finally, results of the experiments are presented to prove the ability of the tested algorithm to function with depth maps, also, proving the capability of depth maps face recognition's task in poor illumination.\", 'title': 'Novel approach to enhance face recognition using depth maps', 'embedding': []}, {'id': 15236, 'abstractText': 'In semiconductor foundries, wafer map defect analysis is crucial to prevent yield excursion. However, traditional manual inspection can hardly meet the high throughput demand. Deep learning based automatic defect detection shows promising efforts to achieve high accuracy and efficiency, yet the current approaches’ performance is limited by the imbalanced dataset and lack of interpretability. In this paper, we propose a Variational Autoencoder Enhanced Deep Learning Model (VAEDLM) for wafer defect imbalanced classification. It is light-weighted and effective in wafer defect pattern recognition on imbalanced dataset. It used variational autoencoder and decoder to generate similar wafer defect maps and a refined deep convolutional neural network for feature learning. We demonstrate the method using an authentic wafer map dataset, WM-811K. The performance is not only significantly improved after data augmentation, but it also beats the state-of-the art methods, reaching 99.19% accuracy, 99.10% recall, 99.23% precision, 99.96% AUC and 99.16% for F1-score. It clearly demonstrates the method’s efficacy to deal with the imbalanced defect pattern. Our study using saliency map and t-SNE further leads to enhanced interpretability.', 'title': 'A Variational Autoencoder Enhanced Deep Learning Model for Wafer Defect Imbalanced Classification', 'embedding': []}, {'id': 15237, 'abstractText': 'ASTER measures the spectral radiation from the Earth in thermal infrared (TIR; 7-14 μm) region at five bands where the major terrestrial minerals exhibit distinct spectral properties. The mineralo-lithological indices for ASTER-TIR proposed by the author have been utilized for regional mapping in detecting silica, carbonate, sulfate and mafic-ultramafic minerals as well as delineating silicate rocks. On the other hand, the global emissivity dataset (GED) are developed and recently supplied to the public. In this study, the global map generated with the indices for the GED is shown. It is compared with the regional map covering a part of Tibetan Plateau produced with the procedures developed by the author. The result indicates the global map revealing the rough trend of lithology and mineralogy on Earth, however, the quality is not sufficient. The combined use of the regional and global maps are suggested for efficient analysis of global mineralo-lithology.', 'title': 'Global Mapping of Mineralo-Lithological Indices Derived With Aster Multispectral Thermal Infrared Data', 'embedding': []}, {'id': 15238, 'abstractText': 'Land cover multiresolution mapping of remote sensing images contributes greatly to land-use management, environmental protection, and city planning. In traditional mapping of this type, the representation of different land-use types depends on the image resolution, and the geometric, topologic, and semantic characteristics are not considered. This approach can cause a loss of useful information and the redundancy of useless information. In this study, we propose a superpixel-based land cover (multiresolution representation SULR) method for remote sensing images that employs multifeature fusion. In this process, we first define three basic superpixel operations, collapse, connection, and cutting, as the basic operators of multiresolution land cover mapping. Then, the topological adjacent land parcels are combined through the amalgamation of polygons with heterogeneous properties and aggregation of polygons with homogeneous properties based on the three proposed superpixel operators. Finally, the geometric boundaries of parcels are simplified by combining the superpixel collapse operator and image thinning technologies. Compared with traditional image scale transformation methods, the proposed method can more effectively achieve multiresolution mapping of land cover from remote sensing images by considering the geometric, topologic, and semantic characteristics of land parcels.', 'title': 'Multiresolution Mapping of Land Cover From Remote Sensing Images by Geometric Generalization', 'embedding': []}, {'id': 15239, 'abstractText': 'Data usually resides on a manifold, and the minimal dimension of such a manifold is called its intrinsic dimension. This fundamental data property is not considered in the generative adversarial network (GAN) model along with its its variants; such that original data and generated data often hold different intrinsic dimensions. The different intrinsic dimensions of both generated and original data may cause generated data distribution to not match original data distribution completely, and it certainly will hurt the quality of generated data. In this study, we first show that GAN is often unable to generate simulation data, holding the same intrinsic dimension as the original data with both theoretical analysis and experimental illustration. Next, we propose a new model, called Hausdorff GAN, which removes the issue of different intrinsic dimensions and introduces the Hausdorff metric into GAN training to generate higher quality data. This provides new insights into the success of Hausdorff GAN. Specifically, we utilize a mapping function to map both original and generated data into the same manifold. We then calculate the Hausdorff distance to measure the difference between the mapped original data and the mapped generated data, toward pushing generated data to the side of original data. Finally, we conduct extensive experiments (using MNIST, CIFAR10, and CelebA datasets) to demonstrate the significant performance improvement of the Hausdorff GAN in achieving the largest Inception Score and the smallest Frechet inception distance (FID) score as well as producing diverse generated data at different resolutions.', 'title': 'Hausdorff GAN: Improving GAN Generation Quality With Hausdorff Metric', 'embedding': []}, {'id': 15240, 'abstractText': 'The Hydrogen Intensity and Real-time Analysis eXperiment (HIRAX [1], Table 1) is an intensity mapping project to be co-located with the Square Kilometer Array in South Africa. By making use of an 1024 element interferometer of low-cost 6m dishes arranged in a compact grid, HIRAX will map the low-frequency southern sky from 400-800 MHz, probing neutral hydrogen emission over the redshift range of 0.8-2.5. The principle goal of this survey is to accurately measure the observed baryon acoustic oscillation feature imprinted on large-scale structure through HI intensity mapping (See e.g. [2]). As this epoch spans the onset of the transition from matter-dominated to dark energy-dominated expansion, HIRAX will provide highly competitive constraints on cosmological parameters, particularly the equation of state of dark energy. Forecasts of these contraints using the methodology of [3] are shown in Figure 1. The final survey will map 15,000 square degrees of the southern sky, overlapping contemporary and forthcoming surveys such as those from the Large Synoptic Survey Telescope and the Dark Energy Survey, as well as ground based Cosmic Microwave Background surveys. HIRAX will therefore enable a wide range of HI cross-correlation studies with external large-scale structure probes. Additionally, HIRAX will be a powerful instrument for detecting and monitoring radio transients such as Fast Radio Bursts and pulsars.', 'title': 'The Hydrogen Intensity and Real-Time Analysis Experiment', 'embedding': []}, {'id': 15241, 'abstractText': 'High spatial resolution maps of relative water level changes in wetlands environment have been successfully generated using spaceborne interferometric synthetic aperture radar (InSAR) techniques. However, the wetland InSAR application has limited hydrological monitoring application, because it estimates water level changes not absolute water levels, which are used by hydrologists. TanDEM-X bistatic observations provide simultaneous phase measurements of water surfaces with a two-satellite constellation without temporal decorrelation. In this study, the TanDEM-X bistatic science phase observations with very large baseline (&gt; 1.3 km) geometric configuration were evaluated to extract absolute water levels of the Everglades wetland in south Florida, U.S.A. Thanks to the large perpendicular baseline, spatial variation of water level surfaces with extremely low slope were estimated. We processed two datasets of TanDEM-X bistatic observations acquired on August 26 and 31, 2015. The perpendicular baselines are 1.43 km and 1.36 km and the ambiguity heights were calculated as 3.61 m and 3.90 m in each interferometric pair. The estimated absolute water level maps with 3.6 m and 7.4 m pixel spacing in range and azimuth directions (multilook factor of 4), respectively, show vast detailed variation of the water surfaces for each acquisition date. Hourly water level measurements obtained by stage stations, which are provided by the Everglades Depth Estimation Network (EDEN), were used for verifying the estimated absolute water levels. Some of stage stations, which are located in low interferometric coherence areas, such as dense vegetated and tree areas, were considered as outliers and were excluded from the comparison. The verification results show very good agreements (code of determination &gt; 0.95) between the TanDEM-X derived absolute water levels and the stage station measurements. The root mean square error (RMSE) between the TanDEM-X results and stage records for the two datasets were 0.77 m and 0.66 m. Although, TanDEM-X bistatic observations have no temporal baseline, there are severe volume decorrelations over various tree types due to the very large perpendicular baseline. The TanDEM-L mission with longer wavelength of radar signal will enable us to generate more coherent interferometric phase observations over wetlands and, consequently, generate improved absolute water level maps.', 'title': 'Bistatic Science Phase Observations with a Large Perpendicular Baseline', 'embedding': []}, {'id': 15242, 'abstractText': \"As the scientific payload of the Chang 'e-4 lunar rover, the panoramic camera undertakes the task of exploring the lunar geology. It can be used for mapping and terrain reconstruction, directly serving the navigation planning of the rover. The lunar remote sensing mapping data, the lunar Digital elevation model (DEM), and Digital orthophoto map (DOM), the mapping of the area near the Chang 'e-4 landing site are studied, and the development direction of the future lunar fine-mapping is discussed.\", 'title': \"Mapping Research of the Area Near the Chang 'e-4 Landing Site\", 'embedding': []}, {'id': 15243, 'abstractText': 'Information security plays an important role in modern technologies. Stream encryption is one of the common tools used for secure communications. The stream encryption algorithms require sequence with pseudo-random properties. The chaotic maps as a source of pseudo-random numbers with desired statistical properties is widely studied in the last decades. One of the known problems of the implementation of the chaos-based pseudo-random number generators implemented with low-precision arithmetic is a short period length. A possible solution of the short-cycle problem is a perturbation of the orbit or the nonlinearity parameter of the chaotic map for breaking out the cycle. In the present article, we propose a new approach for increasing the cycle length by changing the symmetry coefficient of the adaptive Chirikov map. We switch two values of the symmetry coefficient according to the output of the linear feedback shift register. We calculate the estimations of the period length for the perturbed and original Chirikov maps and confirm the efficiency of the proposed approach. Properties verification for the output sequences of generators based on the adaptive Cirikov map is carried out using correlation analysis methods and the NIST statistical test suite. The obtained results can be used in cryptography applications as well as in secure communication systems design.', 'title': 'Adaptive Chirikov Map for Pseudo-random Number Generation in Chaos-based Stream Encryption', 'embedding': []}, {'id': 15244, 'abstractText': 'Tensor-based methods have been widely studied to attack inverse problems in hyperspectral imaging since a hyperspectral image (HSI) cube can be naturally represented as a third-order tensor, which can perfectly retain the spatial information in the image. In this article, we extend the linear tensor method to the nonlinear tensor method and propose a nonlinear low-rank tensor unmixing algorithm to solve the generalized bilinear model (GBM). Specifically, the linear and nonlinear parts of the GBM can both be expressed as tensors. Furthermore, the low-rank structures of abundance maps and nonlinear interaction abundance maps are exploited by minimizing their nuclear norm, thus taking full advantage of the high spatial correlation in HSIs. Synthetic and real-data experiments show that the low rank of abundance maps and nonlinear interaction abundance maps exploited in our method can improve the performance of the nonlinear unmixing. A MATLAB demo of this work will be available at https://github.com/LinaZhuang for the sake of reproducibility.', 'title': 'Using Low-Rank Representation of Abundance Maps and Nonnegative Tensor Factorization for Hyperspectral Nonlinear Unmixing', 'embedding': []}, {'id': 15245, 'abstractText': 'Biometric recognition has been proven to offer a reliable solution to authenticate the identity of the legitimate user. Because of the inherent nature of biometrics, which involves the practice of digitally scanning the physiological or behavioral characteristics of individuals as a means of identification, the biometric trait employed is associated permanently with an individual and cannot be modified. This paper proposes a multimodal biometric template protection algorithm for the security of stored biometric templates in a database. The proposed algorithm is based on watermarking, shuffling process, Hadamard matrix and chaotic map. The security of the algorithm depends on the secrecy of the keys and the chaotic map. Watermarking based on Discrete Wavelet Transform–Singular Value Decomposition has been applied to fuse the features of face and fingerprint in order to improve the security of biometric data. To satisfy the requirements of a biometric template protection algorithm, each genuine user is assigned a unique key for the shuffling process and a unique code is generated using Hadamard matrix. Hadamard matrix is used to provide orthogonality and chaotic map for randomization. The identification process is carried out using Hamming distance. The experimental results give a good performance in terms of achieving a minimum value of FRR and FAR in comparison with previous studies.', 'title': 'Hybrid Multi-Biometric Template Protection Using Watermarking', 'embedding': []}, {'id': 15246, 'abstractText': 'Body surface potential mapping (BSPM) can be an important tool in ablation therapy planning. Results obtained with high resolution (HR) computer models must be translated to realistic numbers of leads. This study aims to evaluate the impact on atrial tachycardia (AT), flutter (AFL) and fibrillation (AF) characterization by reducing the number of BSPM leads. 19 realistic computer simulations with 567 leads (HR) have been used to characterize the arrhythmias concerning the dominant frequencies (DF) and phase singularity point (SP) distributions. DF maps were generated combining Welch periodogram and activation detection with wavelet transform modulus maxima. Phase was calculated with Hilbert transform on signals filtered around the highest DF (±1Hz); dynamics of SPs were analyzed using histograms (heatmaps, HMs) and connecting SPs along time (filaments). The analyses were reproduced for 6 layouts with 252 to 16 leads and results were compared using the structural similarity index (SSIM), sensitivity and precision in SP detection and analyzing features extracted from the maps. SSIM was lower in AF than in AFL or AT for DF maps and HMs, but was in average above 0.6 for layouts with 32 leads or more. In HMs, a loss in spatial resolution with fewer leads is reflected in decreasing values for sensitivity and precision. Features from DF maps, filaments or HMs were statistically equivalent in all layouts.', 'title': 'Effect of Reducing the Number of Leads from Body Surface Potential Mapping in Computer Models of Atrial Arrhythmias', 'embedding': []}, {'id': 15247, 'abstractText': 'Dual respirator!y-cardiac gating (DG) method was shown to produce motion-freeze myocardial perfusion SPECT images. However, appropriate attenuation correction (AC) for DG SPECT is yet to be determined. This study aims to evaluate the performance of various attenuation maps for DG cardiac SPECT. We used the 4D Extended Cardiac Torso (XCAT) phantom to simulate a male patient with respiratory cycle of 5 s and respiratory -motion amplitude of 2 cm along the z-axis. The respiratory cycle was divided into 1152 frames which were grouped to 6 respiratory and 8 cardiac phases, i.e., a total of 48 phases. The corresponding attenuation maps were grouped to represent different AC maps: DG CT (DCT), respiratory gated CT (RCT), average CT (ACT), interpolated CT (ICT), HCTs at endinspiration (HCT-in), end-expiration (HCT-ex) and midrespiration (HCT-mid) respectively. The ICT was obtained by interpolation based on the motion vector generated between HCTin and HCT-ex from affine+b-spline image registration. We used an analytical projector to simulate a LEHR collimator with 120 noise-free projections over 180°, which were later reconstructed by OS-EM method using different AC maps respectively. For each cardiac phase, reconstructed images from different respiratory phases were registered to end-expiration and summed to get a registered cardiac image. Polar plots were generated for 8 registered cardiac images and relative difference (RD) was computed for each segment for the 17-segment analysis. For all cardiac phases, the average RD<sub>max</sub> for AC with ACT, HCT-in, HCT-ex, HCT-mid, RCT and ICT comparing to DCT were 4.09%, 9.24%, 5.04%, 4.02%, 3.22% and 2.85% respectively. Since DCT and RCT are clinically challenging due to the high radiation and implementation complexity, ICT is recommended for AC in DG cardiac SPECT, followed by HCT-mid or HCT-ex, with good accuracy and relative low radiation dose.', 'title': 'Attenuation Correction Methods for Dual Gated Myocardial Perfusion SPECT/CT', 'embedding': []}, {'id': 15248, 'abstractText': 'In this study, we explore a direction-of-arrival (DoA) estimation approach using the steered response power with phase transform (SRP-PHAT) and DeepSphere, a graph-based spherical convolutional neural network (CNN) suitable for spherical topology. The SRP-PHAT maps were adjusted to the Hierarchical Equal Area isoLatitude Pixelization (HEALPix) algorithm. We performed simulations for an Eigenmike spherical microphone array and different resolutions of the SRP-PHAT maps. Results show an improvement for the lower resolution maps, as the mean angular error for the Spherical CNN-derived maps was reduced by about half when compared with the original maps.', 'title': 'DOA Estimation for Spherical Microphone Array using Spherical Convolutional Neural Networks', 'embedding': []}, {'id': 15249, 'abstractText': 'Space mapping technology has been one of the first and most widely used physics-based surrogate-assisted approaches to rapid design optimization of expensive EM-simulation models in microwave engineering. When used with care and experience, it offers computational efficiency that is unmatched by conventional numerical optimization techniques. Numerous variations of space mapping have been proposed over the last two decades and a large number of design case studies have been demonstrated. Yet, limited progress has been observed so far in terms of its full automation. This includes ensuring global convergence, immunity to coarse model inaccuracy, as well as robustness with respect to the surrogate model setup. This paper discusses a few open problems pertaining to space mapping, reviews available theoretical results, provides some generic recommendations for successful usage of space mapping in microwave design, and briefly mentions various surrogate-assisted methodologies that stem from or have been inspired by space mapping.', 'title': 'Space mapping: Performance, reliability, open problems and perspectives', 'embedding': []}, {'id': 15250, 'abstractText': 'In recent years, connected and automated vehicles (CAVs) have attracted considerable attention because they can improve driver convenience and safety using vehicle to everything (V2X) communication. CAV system must satisfy safety and performance requirements, and they are required to go through rigorous evaluation processes. In this study, we design an integrated simulator to evaluate a CAV system, cooperative eco-driving system, which models vehicular ad hoc network (VANET) topology, driver models, vehicle models, and vehicle control algorithms. This is done by integrating three simulators: network simulator, traffic simulator, and driving simulator. Cooperative eco-driving system requires precise and accurate map data including real-time traffic conditions and surrounding environmental information. Therefore, we construct a local map system based on V2X communication to provide a host vehicle (HV) with surrounding traffic and environment information. A local map system generates a local map by utilizing surrounding traffic information of HV acquired from each simulator and shares it with the simulators.', 'title': 'Integrated Simulator for Evaluating Cooperative Eco-driving System', 'embedding': []}, {'id': 15251, 'abstractText': 'We present a volumetric mesh-based algorithm for parameterizing the placenta to a flattened template to enable effective visualization of local anatomy and function. MRI shows potential as a research tool as it provides signals directly related to placental function. However, due to the curved and highly variable in vivo shape of the placenta, interpreting and visualizing these images is difficult. We address interpretation challenges by mapping the placenta so that it resembles the familiar ex vivo shape. We formulate the parameterization as an optimization problem for mapping the placental shape represented by a volumetric mesh to a flattened template. We employ the symmetric Dirichlet energy to control local distortion throughout the volume. Local injectivity in the mapping is enforced by a constrained line search during the gradient descent optimization. We validate our method using a research study of 111 placental shapes extracted from BOLD MRI images. Our mapping achieves sub-voxel accuracy in matching the template while maintaining low distortion throughout the volume. We demonstrate how the resulting flattening of the placenta improves visualization of anatomy and function. Our code is freely available at https://github.com/ mabulnaga/placenta-flattening.', 'title': 'Volumetric Parameterization of the Placenta to a Flattened Template', 'embedding': []}, {'id': 15252, 'abstractText': 'Dynamic object detection, state estimation, and map-building are crucial for autonomous robot systems and intelligent transportation applications in urban scenarios. Most current LiDAR Simultaneous Localization and Mapping (SLAM) systems operate on the assumption that the observed environment is static. However, the overall accuracy and robustness of a SLAM system can be compromised by dynamic objects in the environment. Aiming at the problem of inaccurate odometry estimation and wrong mapping caused by the existing LiDAR SLAM method which cannot detect the dynamic objects, we study the SLAM problem of robots and unmanned vehicles equipped with LiDAR traveling in the dynamic urban scenes. We propose a fast LiDAR-only model-free dynamic objects detection method, which uses the spatial and temporal information of point cloud through a convolutional neural network (CNN), and the detection accuracy is improved by 35 use spatial information. We further integrate it into a state-of-the-art LiDAR SLAM framework to improve the SLAM performance. Firstly, the range image constructed by LiDAR point cloud is used for ground extraction and non-ground point clustering. Then, the motion of objects in the scene is estimated by the difference between adjacent frames, and the segmented objects are further divided into dynamic objects and static objects by their motion features. After that, the stable feature points are extracted from the static objects. Finally, the pose transformation of adjacent frames is solved by matching feature point pairs. We evaluated the accuracy and robustness of our system on datasets with different challenging dynamic environments, and the results show our system has significant improvements in accuracy and robustness of odometry and mapping, while still maintain real-time performance, which is sufficient for autonomous robot systems and intelligent transportation applications in urban scenarios.', 'title': 'DLOAM: Real-time and Robust LiDAR SLAM System Based on CNN in Dynamic Urban Environments', 'embedding': []}, {'id': 15253, 'abstractText': 'Generating and providing the next generation of geological maps for a world needing resources is an ongoing activity. New sensor technologies, improvements in processing algorithms and the means for large scale spatial data distribution are required. The extensive ASTER image archive provides one avenue for providing another geoscience data base source to supplement and help update past published geological mapping. Examples of using ASTER imagery to provide province to continental scale compositional mapping, have been demonstrated in Australia and elsewhere. This study introduces and examines the possibility of applying this approach in East Africa across difficult terrain and boundaries where traditional techniques have been applied by different mapping agencies. The preliminary results show the potential for ASTER mapping but further processing and on site field validation is needed.', 'title': 'Supplementing Geological Mapping with Aster in East Africa', 'embedding': []}, {'id': 15254, 'abstractText': 'In this work, a simple yet effective deep neural network is proposed to generate the dense depth map of the scene by exploiting both LiDAR sparse point cloud and the monocular camera image. Specifically, a feature pyramid network is firstly employed to extract feature maps from images across time. Then the relative pose is calculated by minimizing the feature distance between aligned pixels from inter-frame feature maps. Finally, the feature maps and the relative pose are further applied to compute the feature-metric loss for training the depth completion network. The key novelty of this work lies in that a self-supervised mechanism is presented to train the depth completion network by directly using visual-LiDAR odometry between consecutive frames. Comprehensive experiments and ablation studies on benchmark dataset KITTI demonstrate the superior performance over other state-of-the-art methods in terms of pose estimation and depth completion. The detailed performance of the proposed approach (referred to as SelfCompDVLO) can be found on the KITTI depth completion benchmark. The source code, models, and data have been made available at GitHub.', 'title': 'Self-Supervised Depth Completion From Direct Visual-LiDAR Odometry in Autonomous Driving', 'embedding': []}, {'id': 15255, 'abstractText': 'Onomatopoeias can simply describe sounds or state of things. Therefore they are often used in Japanese daily conversation. On the other hand, one of the problems, which Japanese learners face, is the lack of ways to learn Japanese onomatopoeias. This study proposes a thesaurus map which can describe semantic relationship among onomatopoeias. Our proposed method can transform the onomatopoeia into a 2D-vector and assign it on the map. In our experiment, we examined whether the map can describe the semantic relationship as local distance on the map. The experiment utilized onomatopoeia samples which represent \"human motion\" to evaluate the map.', 'title': 'Visualized Onomatopoeia Thesaurus Maps Based on Deep Autoencoder', 'embedding': []}, {'id': 15256, 'abstractText': 'To deploy a transmitter to correct place is so important for more qualified wireless communication systems. In order deploy the transmitter to correct place, it is required to calculate the electric field strength emanates from base station and generate coverage map. Electric field strength is calculated by Uniform Theory of Diffraction (UTD) model. mxn number coloured coverage maps (red-blue) are generated by changing of the base station place. The best coverage map is selected among all maps by signal processing techniques. In this study, optimum place having the best coverage map is determined for 3x3 scenario.', 'title': 'Determination of Optimum Base Station Location with Signal Processing Techniques', 'embedding': []}, {'id': 15257, 'abstractText': 'The automatic digitizing of paper maps is a significant and challenging task for both academia and industry. As an important procedure of map digitizing, the semantic segmentation section is mainly relied on manual visual interpretation with low efficiency. In this study, we select urban planning maps as a representative sample and investigate the feasibility of utilizing U-shape fully convolutional based architecture to perform end-to-end map semantic segmentation. The experimental results obtained from the test area in Shibuya district, Tokyo, demonstrate that our proposed method could achieve a very high Jaccard similarity coefficient of 93.63% and an overall accuracy of 99.36%. For implementation on GPGPU and cuDNN, the required processing time for the whole Shibuya district can be less than three minutes. The results indicate the proposed method can serve as a viable tool for urban planning map semantic segmentation task with high accuracy and efficiency.', 'title': 'Semantic Segmentation for Urban Planning Maps Based on U-Net', 'embedding': []}, {'id': 15258, 'abstractText': 'Vision-based intelligent systems such as driver assistance systems and transportation systems should take into account weather conditions. The presence of haze in images can be a critical threat to driving scenarios. Haze density measures the visibility and usability of hazy images captured in real-world conditions. The prediction of haze density can be valuable in various vision-based intelligent systems, especially in those systems deployed in outdoor environments. Haze density prediction is a challenging task since the haze and many scene contents have a lot in common in appearance. Existing methods generally utilize different priors and design complex handcrafted features to predict the visibility or haze density of the image. In this article, we propose a novel end-to-end convolutional neural network (CNN) based method to predict haze density, named as HazDesNet. Our HazDesNet takes a hazy image as input and predicts a pixel-level haze density map. The density map is then refined and smoothed, and the average of the refined map is calculated as the global haze density of the image. To verify the performance of HazDesNet, a subjective human study is performed to build a Human Perceptual Haze Density (HPHD) database, which includes 500 real-world hazy images and 100 synthetic hazy images, and the corresponding human-rated perceptual haze density scores. Experimental results show that our method achieves the best haze density prediction performance on our built HPHD database and existing databases. Besides the global quantitative results, our HazDesNet is capable of predicting a continuous, stable, fine, and high-resolution haze density map. We will make the database and code publicly available at https://github.com/JiaheZhang/HazDesNet.', 'title': 'HazDesNet: An End-to-End Network for Haze Density Prediction', 'embedding': []}, {'id': 15259, 'abstractText': 'Because <sup>222</sup>Rn is a progeny of <sup>238</sup>U, the relative abundance of uranium may be used to predict the areas that have the potential for high indoor radon concentration and therefore determine the best areas to conduct future surveys. Geographic Information System (GIS) mapping software was used to construct maps of South Dakota that included levels of uranium concentrations in soil and stream water and uranium deposits. Maps of existing populations and the types of land were also generated. Existing data about average indoor radon levels by county taken from a databank were included for consideration. Although the soil and stream data and existing recorded average indoor radon levels were sparse, it was determined that the most likely locations of elevated indoor radon would be in the northwest and southwest corners of the state. Indoor radon levels were only available for 9 out of 66 counties in South Dakota. This sparcity of data precluded a study of correlation of radon to geological features, but further motivates the need for more testing in the state. Only actual measurements should be used to determine levels of indoor radon because of the strong roles home construction and localized geology play in radon concentration. However, the data visualization method demonstrated here is potentially useful for directing resources relating to radon screening campaigns.', 'title': 'Use of a geographic information system (GIS) for targeting radon screening programs in South Dakota', 'embedding': []}, {'id': 15260, 'abstractText': 'Compared with continuous memristor, discrete memristor has not been deeply studied. In this work, a new discrete memristor model and its pinched hysteresis loops are explored. Based on this model, a simple 2D hyperchaotic map and its dynamics are exhibited. Reservoir computing is an extension of neural networks, which has been received adequate attention. So far, the application of discrete memristor on reservoir has not been reported. To this end, we consider the discrete memristor-based map as a reservoir and verify its performance by a nonlinear regression task. The results indicate that the memristor-based map can be used effectively as a reservoir and enable reservoir computing systems go further on the way of efficient processing temporal signals in the future.', 'title': 'A 2D Hyperchaotic Discrete Memristive Map and Application in Reservoir Computing', 'embedding': []}, {'id': 15261, 'abstractText': 'Cost-effective permafrost characterization and monitoring should be possible due to advances in the technology of earth observation satellites. In particular, the long-penetration capabilities of L-band ALOS2-PALSAR2 should permit large scale mapping of discontinuous permafrost in peatland areas. Recently, it has been shown that the long penetrating polarimetric L-band ALOS is very promising for boreal and subractic peatland mapping and monitoring [1], [2]. The unique information provided by the Touzi decomposition [3], [4], and the Touzi scattering phase in particular, on peatland subsurface water flow permits enhanced discrimination of bogs from fens; two peat- land classes that can hardly be discriminated using conventional optical remote sensing. In this study, the Touzi scattering phase is investigated for mapping discontinuous permafrost in peatland regions Northern Alberta. Polarimetric ALOS-2 (FP6-4) and field data were collected in August 2014 over discontinuous distributed within wooded palsa bogs and peat plateaus near the Namur Lake (Northern Alberta). The ALOS2 image is re-calibrated to reduce the residual error from -33 dB down to -43 dB. This permits full exploiting the excellent ALOS2 performance in term of low noise floor (NESZ about -38 dB) to increase the sensitivity of the Touzi phase to deep permafrost. It is shown that the information provided by the scattering type phase permits enhanced mapping of discontinuous permafrost. The results obtained with the long penetrating L-band polarimetric PALSAR2 are much better than the ones obtained with conventional discontinuous permafrost mapping methods based on Lidar and optical (Landsat and Spot) images.', 'title': 'Polarimetric L-band PALSAR2 for Discontinuous Permafrost Mapping In Peatland Regions', 'embedding': []}, {'id': 15262, 'abstractText': 'Despite the relevance of historical land cover maps for scientists and policy makers, an accurate high resolution record is currently lacking over the Sudano-Sahel. In this study, 30m resolution historically consistent land cover and cover fraction maps are provided over the Sudano-Sahel for the period 1986–2015. These land cover/cover fraction maps are achieved based on the Landsat archive preprocessed on Google Earth Engine and a random forest classification/regression model, while historical consistency is achieved using the hidden Markov model. Using these historical maps, a multitude of variability in the dynamic Sudano-Sahel region over the past 30 years is revealed. These include cropland expansion and the re-greening of the Sahel, forest degradation &amp; the detection of fine-scale changes, such as smallholder or subsistence farming. The historical land cover / cover fraction maps are made available via an open-access platform.', 'title': 'Thirty Years of Land Cover and Fraction Cover Changes Over the Sudano-Sahel Using Landsat Time Series', 'embedding': []}, {'id': 15263, 'abstractText': 'Hyperspectral sensors have high spectral resolution by capturing images in hundreds of bands. Despite the high spectral resolution, low spatial resolution of these sensors restricts the performance of the hyperspectral imaging applications such as target tracking and image classification. Fusing the hyper-spectral image (HSI) with higher spatial resolution RGB or multispectral image (MSI) data is a commonly used method in the resolution enhancement of the HSIs. In this paper, we propose a new fusion technique for the HSI super-resolution. The main contribution of this study is formulating the fusion problem in a quadratic manner and also regularizing the solution quadratically using smoothness prior. Moreover, another contribution of the proposed method is converting the fusion problem from spectral domain to the abundance map domain which gives more robust and spectrally consistent results. In the proposed method, first, abundance maps are obtained using linear spectral unmixing and then a quadratic energy function is obtained using these maps and high resolution (HR) RGB image. In addition, quadratic function is regularized using additional constraints. Solving the regularized quadratic function gives the HR abundance maps and these maps are used to reconstruct HR HSI. Experiments show that proposed method yields better performance as compared to state of the art methods in different performance metrics.', 'title': 'Image Fusion for Hyperspectral Image Super-Resolution', 'embedding': []}, {'id': 15264, 'abstractText': 'This paper describes the application of mobile robots in the current coronavirus epidemic. Localization is a frequently discussed topic in mobile robotics research. Before a robot can start a task, it must know its current location on a map. The system proposed in this paper scans the obstacles and terrain around the robot by LiDAR to obtain a map of the environment and then uses the image recognition algorithm proposed in this paper to achieve the robot’s location. This system can be applied to frontline medical robots, which can disinfect the environment or deliver medication, especially in the case of the COVID-19 epidemic, to help healthcare workers. The proposed localization algorithm is different from the traditional Adaptive Monte Carlo Localization (AMCL), which uses a 2D LiDAR sensor with image recognition to complete the localization. By using a modified template matching technique, the local map is compared with the known global map to deduce the robot’s position, which is more accurate than AMCL. In this study, an indoor environment is created using Gazebo 3D environment simulation software, and a robot with a 2D LiDAR sensor is used in this environment to conduct the experiment. We designed three scenarios to validate the proposed algorithm, one with simple terrain, the second scene will appear throughout the map with other scenes of similar terrain, and the third with long straight lines. The results show that this method is feasible.', 'title': 'Pose Detection of a Mobile Robot Based on LiDAR Data', 'embedding': []}, {'id': 15265, 'abstractText': 'Increasing need for fresh water resources, particularly in irrigation, makes it imperative to develop tools to monitor and improve water use efficiency at field, regional and national levels. Satellite observations are highly suitable for this task and for this reason FAO, the custodian agency of Sustainable Development Goals Indicators 6.4.1 and 6.4.2, developed the WaPOR portal through which it distributes satellite-based (Terra and Aqua, PROBA-V and Landsat) evapotranspiration (ET) maps. The aim of this study is to evaluate the suitability of using Copernicus data (Sentinel-2 and Sentinel-3 observations and ERA5 meteorological model) to produce high-resolution, national-scale ET maps during the evolution of WaPOR portal. Results indicate that Copernicus-based maps show generally similar ET patterns to WaPOR maps across climatic and land-use gradients while providing more accurate and detailed field-level estimates compared to MODIS-based WaPOR maps.', 'title': 'Assessing Utility of Copernicus-Based Evapotranspiration Maps for National Monitoring of Field-Scale Water Use', 'embedding': []}, {'id': 15266, 'abstractText': 'With the near completion of WUDAPT (World Urban Database and Access Portal Tools) Level 0 data, one of the next goals is to generate more accurate and detailed local climate zone (LCZ) maps. An important issue is how to integrate building height information into LCZ maps. We here present a multi-label classification method using very high resolution (VHR) imagery to implicitly integrate building height information. Since we humans can tell whether a place is high-rise or not based on the shading of buildings and the surrounding context, it is possible to extract such information using deep learning methods. We use Hong Kong as a case study and show the potential of LCZ mapping with VHR imagery in distinguishing small-scale landscape features like city parks. The multi-label LCZ maps also provide a solution to generate fine-grained subclass LCZ mapping, in which a place can be classified as a combination of multiple LCZs, e.g., compact low-rise with open high-rise.', 'title': 'Multi-Label Local Climate Zone Mapping as Scene Classification Using Very High Resolution Imagery: Preliminary Result of Hong Kong', 'embedding': []}, {'id': 15267, 'abstractText': 'An effective treatment for scar-related ventricular tachycardia (VT) is to interrupt the circuit by catheter ablation. If activation sequence and entrainment mapping can be performed during sustained VT, the exit and isthmus of the circuit can often be identified. However, with invasive catheter mapping, only monomorphic VT that is hemody namically stable can be mapped in this manner. A non-invaive approach to fast mapping of unstable VTs can potentially allow an improved identification of critical ablation sites. In this pilot study, noninvasive ECG-imaging were carried out on patients with unstable scar-related VT. The reconstructed reentry circuits correctly revealed both epicardial and endocardial origins of activation, consistent with locations of exit sites found during ablation procedures. The results also indicated that some reentry circuits involve both epicardial and endocardial layers, and can only be properly interpreted by mapping both layers.', 'title': 'Noninvasive epicardial and endocardial electrocardiographic imaging of scar-related ventricular tachycardia', 'embedding': []}, {'id': 15268, 'abstractText': 'This study focus on the evaluation of scientific and industry-grade hyperspectral airborne sensors for the mapping of methane (CH4) emissions in the SWIR range. An imaging dataset from areas with known CH4 emissions was processed using the classic matched filter technique, and a new CH4 index. The airborne sensors were evaluated based on sensor design (spectral sampling and band centers), effectiveness of image processing, and impact of the signal-to-noise ratio (SNR) on CH4 mapping. The gas plume was mapped only in the images acquired with scientific-grade sensors. Results demonstrated that superior performance could be achieved when the position of band centers are closely located to the center of diagnostic CH4 absorption features. The impact of SNR was examined using a noise simulation, adding white noise to simulate images with varying SNR levels. Results indicate that the noisier signal of the industry-grade sensor is probably what prevented mapping the CH4 plume in this dataset. Simulations also demonstrated that as densest the plumes lower is the impact of SNR. Combined, results indicate that an imaging spectrometer with a scientific-grade SNR and band centers properly positioned to match the main CH4 features would improve the mapping of CH4 plumes with airborne sensors operating in the SWIR range.', 'title': 'Assessing Scientific and Industry Grade SWIR Airborne Imaging Spectrometers for CH<inf>4</inf> Mapping', 'embedding': []}, {'id': 15269, 'abstractText': 'This study investigated the potential for using principal component analysis (PCA) and feature map fusion to improve real-time prostate cpasula detection algorithms. Some objective detection algorithms can realize real-time object detection, but the mean average precision (mAP) is not ideal. PCA is processed firstly to make dimensionality reduction. After that, the processed images are sent to the new network called Almost Fully Feature Fusion Single Shot Multibox Detector (AFFSSD) to train. Feature map size at 150×150, 75×75, 38×38, 19×19, 10×10, 5×5, 3×3, 1×1 are all utilized for feature collection by feature map fusion and feature pyramid therein. Results showed that PCA and AFFSSD has the highest mAP for the prostate capsula images detection, by 76.23%, compared with Faster R-CNN, R-FCN, YOLOv3, SSD, FSSD applied independently. The proposed architecture fused many low-level features, so the confidence score are very high, almost 100 percent.', 'title': 'Application of Primary Component Analysis and Feature Map fusion to Improve Real-time Prostate Capsula Detection', 'embedding': []}, {'id': 15270, 'abstractText': 'Most researchers working on electrical capacitance tomography (ECT) use a set of generic sensitivity maps, based on empty background. The aim of this study is to investigate whether or not different sensitivity maps should be used for imaging different dielectric materials. For this purpose, different sensitivity maps are generated by dot multiplication of electric fields with different permittivity backgrounds. To see the effect of different sensitivity maps on image reconstruction, stratified and annular distributions are chosen for simulation with a 12-electrode ECT sensor. The reconstructed images are evaluated by the image error. To verify the simulation results, experiments are carried out. Preliminary results show that the generic sensitivity maps based on empty background are suitable for image reconstruction in most cases.', 'title': 'Image reconstruction with different sensitivity maps generated with different background', 'embedding': []}, {'id': 15271, 'abstractText': 'This paper introduces an anti-aliasing algorithm based on saliency map for virtual reality applications. In order to do it, we first render the whole scene into a single texture image and feed it into saliency map construction. The result saliency map is then input to the second rendering step with the original texture image. The second rendering step performs the anti-aliasing algorithm selectively based on the value of saliency map. Through the user study, it turns out that participants do not distinguish between full anti-aliasing and selective anti-aliasing based on saliency map. The rendering time has a 5-10% performance increase if we use the selective anti-aliasing that we propose.', 'title': 'Selective Anti-Aliasing for Virtual Reality Based on Saliency Map', 'embedding': []}, {'id': 15272, 'abstractText': 'Model-Driven Software Engineering (MDSE) is a development method in which models are used to generate software. Despite documented advantages, projects employing MDSE may fail due to development challenges. In this paper, we study and document these challenges by conducting an up-to-date systematic mapping that goes beyond reviewing MDSE studies: we also include two derived paradigms (Model-Oriented Programming and Models at Run-time). Therefore, we present a systematic mapping with two objectives: The first objective was to identify specific domains in which MDSE is successful, while the second objective was to identify what are the challenges to apply this methodology to general purpose development processes. Following the review of 4859 studies (3727 are unique), we have identified the application and technological domains in which MDSE projects are more likely to succeed. We also discuss challenges presented by 17 primary studies. The analysis of the results indicate that MDSE application is consolidated in specific domains. A common feature identified among studies related to general purpose MDSE processes is that, initially, authors reported lack of proper methods and training. After new techniques have risen, it has been pointed that MDSE projects still face maintenance problems that can discourage their usage in other domains.', 'title': 'Understanding the Successes and Challenges of Model-Driven Software Engineering - A Comprehensive Systematic Mapping', 'embedding': []}, {'id': 15273, 'abstractText': 'Spreadsheets provide a very flexible programming environment and are used by almost every organization, company, institution or business for data processing and data storage tasks. The main objectives of this research are: (1) to have an overview of the research that is being done related to spreadsheet smells; (2) to classify the spreadsheet smells research according to ten criteria: techniques, year of publication, publication venues, publication channels, datasets, countries, contribution type, research approaches, tools used, tools/techniques proposed; and (3) analyze studies from different perspectives like study objectives, methods, method accuracy and limitations. We performed a systematic mapping on the spreadsheet smells studies published in the time span of 2010-2019, and adopted proper methods to classify, review and analyze them. In total, we were able to identify 28 studies and map the results of these studies.', 'title': 'Spreadsheet Smells: A Systematic Mapping Study', 'embedding': []}, {'id': 15274, 'abstractText': 'Publication on digital education continues to develop but is limited to one country and/or one field. From a bibliometric overview, this study aims to visually study mapping and publication trends in the field of digital education on an international scale. This study used bibliometric techniques with secondary data from Scopus. Analyze and visualize data using the VOSViewer program and the analyze search results function on Scopus. This study analyzed 696 scientific documents published from 1983 to 2020. According to the research, the Nanyang Technological University, Singapore and Josip Car had the most active individual scientists in digital education research. Social science was the most studied outlet of digital education. There were three category maps of collaborative researchers from around the world. Based on the identification of a collection of knowledge created from the past thirty-seven years of publication, this research proposes a grouping of digital education themes: Computer education, Humanities, Information technology, Knowledge, and E-learning, abbreviated as CHIKE publication themes.', 'title': 'A Bibliometric Overview and Visualization of The Digital Education Publication', 'embedding': []}, {'id': 15275, 'abstractText': \"Thinking in and understanding of three-dimensional structures is omnipresent in many sciences from chemistry to geosciences. Current visualizations, however, are still using two-dimensional media such as maps or three-dimensional representations accessible through two-dimensional interfaces (e.g., desktop computers). The emergence of immersive virtual reality environments, both accessible and of high-quality, allows for creating embodied and interactive experiences that permit for rethinking learning environments and provide access to three-dimensional information through three-dimensional interfaces. However, there is a shortage of empirical studies on immersive learning environments. In response to this shortcoming, this study examines the role of immersive VR (iVR) in improving students' learning experience and performance in terms of penetrative thinking in a critical 3D task in geosciences education: drawing cross-sections. We developed a pilot study where students were asked to draw cross-sections of the depth and geometry of earthquakes at two subduction zones after visualizing the earthquake locations either in iVR or 2D maps on a computer. The results of our study show that iVR creates a better learning experience; students reported significantly higher scores in terms of Spatial Situation Model and there is anecdotal evidence in favor of higher reflective thinking in iVR. In terms of learning performance, we did not find a significant difference in the graded exercise of drawing cross-sections. However, iVR seems to have a positive effect on understanding the geometry of earthquake locations in a complex tectonic environment such as Japan. Our results, therefore, add to the growing body of literature that draws a more nuanced picture of the benefits of immersive learning environments calling for larger scale and in-depth studies.\", 'title': 'Fostering Penetrative Thinking in Geosciences Through Immersive Experiences: A Case Study in Visualizing Earthquake Locations in 3D', 'embedding': []}, {'id': 15276, 'abstractText': 'Context: The Software Engineering (SE) research continues to gain strength and interest for researchers considering the need to apply rigor and scientific validity to research results. Objective: Establishing an overview of the topic through a classification scheme of publications and structure the field of interest. Method: We conducted a Systematic Mapping Study, including articles published until 2019, that report at least one study of empirical strategies in SE. Results: 80 initial sets of studies were selected and analyzed, identifying: i) empirical strategy type used and ii) Software Engineering hypotheses types used. Also, 20 papers of the set of studies for mapping were selected and analyzed, identifying 17 empirical strategies and 11 main characteristics to address the empirical research inception in SE. Conclusions: We corroborate that the selection of an empirical strategy in Software Engineering research depends on the nature and scope of the research and on the resources that the researcher has at that moment, in addition to the degree of scientific and methodological knowledge that he has to carry out an empirical study. It is necessary to continue studying in-depth the behavior and nature of the empirical strategies in Software Engineering research that allows strengthening the scientific taxonomy in SE, besides walking towards the automation of the experimental process.', 'title': 'Empirical Strategies in Software Engineering Research: A Literature Survey', 'embedding': []}, {'id': 15277, 'abstractText': \"The gamification in ride-sharing applications is actualized in a point system for incentives and ratings for feedback. By growth of driver's number, the current gamification is facing problems. Drivers should spend more time and serve more orders to achieve targeted points. As the impact, their performance got worse, and their customers' satisfaction was reduced. To analyze the gamification effect on driver motivation, this study synthesizes Self Determination Theory (SDT) and Motivational Affordance Perspective (MAP). This study is a case study based on empirical data using a quantitative approach. By involving 103 participants, this study examines seven variables: Identified Regulation, External Regulation, Need for Autonomy, Self-Efficacy, Playfulness, Extrinsic Motivation, and Intrinsic Motivation. After mapping them into nine hypotheses, there were five accepted ones. The results unveiled that gamification can influence extrinsic motivation, but it cannot influence intrinsic motivation. As general interpretation, gamification motivates drivers to take more orders since they are forced to reach targeted points. This study provides recommendations for ride-sharing operators to improve gamification by adding new features to cover self-efficacy, need for autonomy, and playfulness in order to influence drivers' intrinsic motivation.\", 'title': 'Does Gamification Motivate Gig Workers? A Critical Issue in Ride-Sharing Industries', 'embedding': []}, {'id': 15278, 'abstractText': \"Brain-wide and genome-wide association (BW-GWA) study is presented in this paper to identify the associations between the brain imaging phenotypes (i.e., regional volumetric measures) and the genetic variants [i.e., single nucleotide polymorphism (SNP)] in Alzheimer's disease (AD). The main challenges of this study include the data heterogeneity, complex phenotype-genotype associations, high-dimensional data (e.g., thousands of SNPs), and the existence of phenotype outliers. Previous BW-GWA studies, while addressing some of these challenges, did not consider the diagnostic label information in their formulations, thus limiting their clinical applicability. To address these issues, we present a novel joint projection and sparse regression model to discover the associations between the phenotypes and genotypes. Specifically, to alleviate the negative influence of data heterogeneity, we first map the genotypes into an intermediate imaging-phenotype-like space. Then, to better reveal the complex phenotype-genotype associations, we project both the mapped genotypes and the original imaging phenotypes into a diagnostic-label-guided joint feature space, where the intraclass projected points are constrained to be close to each other. In addition, we use ℓ<sub>2,1</sub>-norm minimization on both the regression loss function and the transformation coefficient matrices, to reduce the effect of phenotype outliers and also to encourage sparse feature selections of both the genotypes and phenotypes. We evaluate our method using AD neuroimaging initiative dataset, and the results show that our proposed method outperforms several state-of-the-art methods in term of the average root-mean-square error of genome-to-phenotype predictions. Besides, the associated SNPs and brain regions identified in this study have also been shown in the previous AD-related studies, thus verifying the effectiveness and potential of our proposed method in AD pathogenesis study.\", 'title': \"Brain-Wide Genome-Wide Association Study for Alzheimer's Disease via Joint Projection Learning and Sparse Regression Model\", 'embedding': []}, {'id': 15279, 'abstractText': 'In this study, a systematic mapping study was conducted to systematically evaluate publications on Intrusion Detection Systems with Deep Learning. 6088 papers have been examined by using systematic mapping method to evaluate the publications related to this paper, which have been used increasingly in the Intrusion Detection Systems. The goal of our study is to determine which deep learning algorithms were used mostly in the algortihms, which criteria were taken into account for selecting the preferred deep learning algorithm, and the most searched topics of intrusion detection with deep learning algorithm model. Scientific studies published in the last 10 years have been studied in the IEEE Explorer, ACM Digital Library, Science Direct, Scopus and Wiley databases.', 'title': 'Intrusion Detection Systems with Deep Learning: A Systematic Mapping Study', 'embedding': []}, {'id': 15280, 'abstractText': 'Despite the importance of software qualities, they are not well understood, especially in the context of the interrelationships between qualities. A number of systematic mapping studies have been conducted prior to 2015 to summarize the literature on the topic and to identify research gaps. To provide a better understanding of the current state of the art, we conducted a systematic mapping study on relevant studies from 2015 to 2019 through a database search and a subsequent snowballing approach. In total, 18 studies were selected as the study subjects wherein we evaluated the types of software quality interrelationships and the qualities that comprise them. Based on our findings, we report on the progress made to address previously identified research gaps.', 'title': 'Recent Trends in Software Quality Interrelationships: A Systematic Mapping Study', 'embedding': []}, {'id': 15281, 'abstractText': 'Dengue, zika and chikungunya are examples of serious diseases affecting mainly developing countries, such as Brazil. Data from the World Health Organization (WHO) about these diseases transmitted by Aedes aegypti mosquito are worrisome, especially in relation to their possible effects. Considering this scenario severity, Internet of Things based applications (IoT applications) are being used to support it. So, this paper aims to select and organize, through information collection, difficulties that can be found in the medical use of IoT applications to combat this mosquito. The objective was to find possible gaps for future research. A Systematic Literature Mapping was performed to find possible deficiencies in the methodologies addressed by the authors of the returned studies. This systematic mapping intends to list these difficulties, as well methods proposed to improve the use of IoT in Health (especially combat to Aedes aegypti). In total, 570 studies were found. When applying the inclusion and exclusion criteria, 22 studies were selected for a full reading, which led to rejection of 15, therefore 07 studies were finally accepted. By analyzing these selected studies, it was possible to observe monitoring of information is essential in dengue combat systems. It was also possible to notice the lack of software reuse initiatives that could make it easier to develop new applications in this domain.', 'title': 'IoT Applications to Combat Aedes Aegypti: A Systematic Literature Mapping', 'embedding': []}, {'id': 15282, 'abstractText': 'Research on the smart economy as a part of a smart city continues to develop but is limited to one country and/or one field. From a bibliometric review, this study aims to visually study mapping and research trends in the field of the smart economy on an international scale. This study used bibliometric techniques with secondary data from Scopus. Analyze and visualize data using the VOSViewer program and the analyze search results function on Scopus. This study analyzed 125 scientific documents published from 2011 to 2020. According to the research, the Peter the Great St. Petersburg Polytechnic University, Russia, and Mboup, G. had the most active individual scientists and in smart economy research. Computer science and Advances in 21st Century Human Settlements were the most studied and disseminated outlet of smart economy research. There was one category map of collaborative researchers from around the world. Based on the identification of a collection of knowledge generated from nine years of publication, this research proposes a grouping of smart economy research themes: Economy, Application of smart cities, Smart technology, Smart cities character, Economy of urban, Education and Smart environment, abbreviated as EASSEES research themes.', 'title': 'Mapping of Smart Economy Research Themes: A Nine-Year Review', 'embedding': []}, {'id': 15283, 'abstractText': 'The study was carried out in order to enable systems with weak processing power and motion to detect objects using cloud services. In addition, the dataset is expanded by continuous labeling to create big data. In the study, it is aimed to detect objects using cloud-based deep learning methods with an unmanned aerial vehicle (UAV). In the study, training processes were carried out with Google Colaboratory, a cloud service provider. The training processes are a YOLO-based system, and a convolutional neural network was created by revising the parameters in line with the needs. The convolutional neural network model provides communication between neurons in the convolutional layers by bringing the image data to the desired pixel ranges. Unlabeled pictures are included in the training by being tagged. In this way, it is possible to continuously enlarge the data pool. Since the microcomputers used in UAVs are insufficient for these processes, a cloud-based training model has been created. As a result of the study, cloud-based deep learning models work as desired. It is possible to show the accuracy of the model with the low losses seen in the loss functions and the mAP value. Graphic cards with high processing power are needed to provide training. It is essential to use powerful graphics cards when working on image data. Cost reduced by using cloud services. The training was accelerated and high-rate object detections were made. YOLOv5x was used in the study. It is preferred because of its fast training and high frame rate. Recall 80% Precision 93% mAP 82.6% values were taken.', 'title': 'Real-Time Puddle Detection Using Convolutional Neural Networks with Unmanned Aerial Vehicles', 'embedding': []}, {'id': 15284, 'abstractText': 'Permanent magnet synchronous machines (PMSMs) are widely used in many applications. The performance of the PMSM is highly dependent on the motor parameters. Many research studies have been done to evaluate the PMSM performances in terms of maximum torque capability, power capability, and field-weakening capability. This paper proposes a new normalized PMSM model and uses two parameters, characteristic current and saliency ratio, to uniquely define the motor characteristic. Based on this normalized model, the full map motor behaviors, including torque capability, power capability, torque/power-speed characteristics, and power factor behaviors, are studied parametrically. A new unitless metric, i.e., copper loss factor, is introduced to evaluate the copper loss variation of PMSMs. The saturation effect on the motor behavior is studied based on the 2004 Prius traction motor, which confirms that the field-weakening characteristic can be well predicted using the linearized model. A new design flow for traction PMSMs, which requires wide-speed operation, is proposed based on the full map motor behavior study, and a prototype machine is designed accordingly. The behavior study and the effectiveness of the traction motor design flow are validated experimentally by the prototype.', 'title': 'Behavior Study of Permanent Magnet Synchronous Machines Based on a New Normalized Model', 'embedding': []}, {'id': 15285, 'abstractText': \"Sustainability is a topic of increasing interest. The United Nations has released a list of 17 goals for sustainable development for the global community. Blockchain is a recent technological innovation that shows great promise in changing industries. In this paper, we look specifically at smart grids and supply chain management systems as areas where sustainable technological innovation can happen. To identify software engineering aspects of blockchain in smart grids and supply chain management, we start upon online libraries focusing on engineering and information technology, and we opted for the methodology of systematic mapping studies in software engineering. The search strategy identified 535 papers, of which 60 were identified as main studies for our mapping. To the best of the authors' knowledge, no previous similar studies exist. Results of the study show that the research connecting blockchain technology to smart grids and supply chain management systems is still young. None of the techniques or systems have yet been implemented in a real life setting. As such, more work has to be done before we can look at the actual implications of putting such technologies into use. Software engineering practices could prove to be very useful in the process of development. We propose that future studies can focus on bringing the technologies closer to real life implementations, as well as how to involve the end users in the development of the blockchain-based systems.\", 'title': 'Blockchain and Sustainability: A Systematic Mapping Study', 'embedding': []}, {'id': 15286, 'abstractText': \"The concept of Local Climate Zone (LCZ) has been developed to quantify the correlation between urban morphology and urban heat island (UHI). Each LCZ is supposed to have homogenous air temperature. However, traditional air temperature observation methods have limited spatial coverage and poor spatial resolution. Land surface temperature (LST) acquired from satellite images can be used to study the temperature characteristics of LCZ classes by providing continuous data on surface temperature. This study aims to study the relationship between LST and LCZ classes with Shanghai selected as a case study because of its high urbanization rate and serious UHI effect. This study has three major steps: Firstly, Shanghai local climate zone map was generated using the World Urban Database and Portal Tool (WUDAPT) method. Secondly, a remote sensing approach was taken to acquire Shanghai's LST from night-time Aster thermal data in different seasons. Thirdly, the LST was associated with the Shanghai LCZ map and the correlation between LCZ and LST in Shanghai was discussed. The results show that there are large variations in LST across LCZ classes in different months in Shanghai. These results will be able to offer integrated information under urban climate principles for urban planners and urban climate researchers.\", 'title': 'Investigating the relationship between Local Climate Zone and land surface temperature', 'embedding': []}, {'id': 15287, 'abstractText': 'We study the anomalous microwave emission (AME) in the Lynds Dark Nebula (LDN)\\u20091780 on two angular scales. With publicly available data at an angular resolution of 1°, we studied the spectral energy distribution of the cloud in the 0.408–2997\\u2009GHz frequency range. The cloud presents a significant (&gt;20σ) amount of AME, making it one of the clearest examples of AME on 1\\u2009° scales, and its spectrum can be well fitted with a spinning dust (SD) model. We also find at these angular scales that the location of the peak of the emission at lower frequencies (23–70\\u2009GHz) differs from the location at the higher frequencies (90–3000\\u2009GHz) maps. In addition to the analysis on 1° angular scales, we present data from the Combined Array for Research in Millimeter-wave Astronomy (CARMA) at 31\\u2009GHz with an angular resolution of 2\\u2009arcmin, in order to study the origin of the AME in LDN\\u20091780. We studied morphological correlations between the CARMA map and different infrared tracers of dust emission. We found that the best correlation is with the 70-\\u2009μm template, which traces warm dust (T ∼ 50\\u2009K). Finally, we study the difference in radio emissivity between two locations within the cloud. We measured a factor of ≈6 difference in 31-GHz emissivity. We show that this variation can be explained, using the SD model, by a variation on the dust grain size distribution across the cloud, particularly changing the fraction of polycyclic aromatic hydrocarbon for a fixed total amount of carbon.', 'title': 'Modelling the spinning dust emission from LDN\\u20091780', 'embedding': []}, {'id': 15288, 'abstractText': 'Three-Dimensional Magnetic Resonance Imaging (3D-MRI) and Computer-Aided Detection (CAD) have been widely studied in the detection of bipolar disorder (BD). In this study, the structural alterations at the grey matter (GM) and white matter (WM) of BD subjects versus healthy controls (HCs) have been compared using Voxel-Based Morphometry (VBM). In order to obtain 3D GM and WM masks, the two sample ttest method and total intracranial volumes of BD and HC as a covariate have been utilized. In addition to analyzing effects of GM and WM tissue maps separately in the detection of BD, impacts of both GM and WM ones are studied by concatenating them ın a matrix. The correlation-based feature selection (CFS) feature ranking method is applied to the obtained 3D masks to rank the features, the number of selected top-ranked features are determined using a Fisher criterion (FC) approach, and different classification algorithms are used to classify BD apart from HCs. In this study, 26 BDs and 38 HCs data are used. The experimental results indicate that the classification accuracy of Naive Bayes outperforms the other four classification algorithms used in this study. Additionally, concatenation of GM and WM tissue maps enhances the classification performances of using GM-only and WM-only ones. The classification accuracies obtained for GM, WM, and their concatenation are 72.92%, 78.33%, and 80.00% respectively.', 'title': 'Diagnosis of Bipolar Disease Using Correlation-Based Feature Selection with Different Classification Methods', 'embedding': []}, {'id': 15289, 'abstractText': 'Disruptive technology, blockchain is propelling a technological intervention in healthcare due to its unique features and advantages. The healthcare industry is migrating to Health 4.0. Therefore, peer-to-peer (P2P) transactions in a decentralized and distributed manner makes blockchain more lucrative to serve the needs of the healthcare industry of today. The revolutionary system, blockchain has been discussed in the field of healthcare over the past five years. Hence, a systematic investigation of the existing body of knowledge concerning blockchain research in healthcare is essential. The motivation of this study is to support further studies based on the current research trend analysis via graphical visualization and bibliographic material analysis. Therefore, this study maps the expansion of scientific and academic research conducted concerning blockchain that is relevant to healthcare by utilizing a bibliometric analytic method to understand the state of the art. Bibliometric statistics were utilized to analyze current scientific articles published in the Scopus database from 2016 to 2019. In addition, an overview of the publication trends over the first three months of 2020 was undertaken to understand the research trend for the current year so far. The study serves the purpose of mapping research development trends in healthcare. The outcome discovered some beneficial insights such as the yearly trend of publications, top listed authors, institutes, countries, and publishers from around the world. Moreover, this article assists scholars in developing a theoretical framework to provide a primary source of reference for further studies regarding blockchain technology in the healthcare domain.', 'title': 'Mapping Research Trends of Blockchain Technology in Healthcare', 'embedding': []}, {'id': 15290, 'abstractText': \"For decades, Software Process Improvement (SPI) programs have been implemented, inter alia, to improve quality and speed of software development. To set up, guide, and carry out SPI projects, and to measure SPI state, impact, and success, a multitude of different SPI approaches and considerable experience are available. SPI addresses many aspects ranging from individual developer skills to entire organizations. It comprises for instance the optimization of specific activities in the software lifecycle as well as the creation of organization awareness and project culture. In the course of conducting a systematic mapping study on the state-of-the-art in SPI from a general perspective, we observed Global Software Engineering (GSE) becoming a topic of interest in recent years. Therefore, in this paper, we provide a detailed investigation of those papers from the overall systematic mapping study that were classified as addressing SPI in the context of GSE. From the main study's result set, a set of 30 papers dealing with GSE was selected for an in-depth analysis using the systematic review instrument to study the contributions and to develop an initial picture of how GSE is considered from the perspective of SPI. Our findings show the analyzed papers delivering a substantial discussion of cultural models and how such models can be used to better address and align SPI programs with multi-national environments. Furthermore, experience is shared discussing how agile approaches can be implemented in companies working at the global scale. Finally, success factors and barriers are studied to help companies implementing SPI in a GSE context.\", 'title': 'How Does Software Process Improvement Address Global Software Engineering?', 'embedding': []}, {'id': 15291, 'abstractText': 'Of recent, Building Information Modelling (BIM) has become an influential paradigm for the development of better project delivery practices to improve construction and operational efficiencies. In the last 6 years, a significant number of studies have been published on the integration of BIM in Internet of Things (IoT). This paper aims to examine the general research productivity, demographics, and trends shaping the research domain. Hence, the paper will also help to identify, categorize, and synthesize important studies in the research domain. In doing so, we adopt an evidence-based systematic mapping methodology to ensure the coverage of key studies through a systematic and unbiased selection and evaluation process which results in the final selection of 55 relevant studies. The results of the mapping study show that the research on the integration of BIM in IoT is gaining more attention in last 6 years with stable and consistent publication output. Prominent application domains, validation methods, contribution facets, research types, and simulation tools in the field of study were identified and presented. Five research types were also identified, i.e. solution proposal, experience paper, evaluation research, validation research, and opinion paper, with solution proposals getting more research attention. In general, the overall demographics of the research domain were presented and discussed.', 'title': 'Building Information Modelling (BIM) and the Internet-of-Things (IoT): A Systematic Mapping Study', 'embedding': []}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [08/Dec/2021 23:32:35] \"\u001b[37mPOST /nlp HTTP/1.1\u001b[0m\" 200 -\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [08/Dec/2021 23:33:25] \"\u001b[37mPOST /nlp HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
