{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'doi': '10.1109/TrustCom50675.2020.00150',\n",
       " 'authors': 'Jehanzaib Yousuf Muhammad;Mingjun Wang;Zheng Yan;Fawad Khan',\n",
       " 'documentTitle': 'Trusted Network Slicing Among Multiple Mobile Network Operators',\n",
       " 'publicationTitle': '2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)',\n",
       " 'workAbstract': '5G mobile networks are expected to be much bigger in size, faster in speeds and better in scalability, providing varied services to different users and businesses in contrast to previous networks. 5G will also help enabling new business models and use cases. “Network Slicing” is a driving architectural concept for multi-tenancy. Network Slicing enables Mobile Network Operators (MNOs) to deploy different services over shared physical infrastructure, increasing inter-operator resource sharing. As 5G is still in its nascent, inter operator cooperation is an area that requires immediate attention of research. Traditional inter operator trust relationship models cannot fully comprehend the needs of 5G networks. In this paper, we propose an Intel SGX based multi-MNO cooperation scheme for trusted, dynamic and efficient network slice sharing in order to support inter-operator trustworthy collaboration. Furthermore, we developed a Proof of Concept of our proposed scheme using Intel SGX, flask framework and Docker containers. The obtained results indicate the applicability of the proposed scheme with little effect on performance.',\n",
       " 'date': '29 Dec.-1 Jan. 2021',\n",
       " 'authorKeywords': [{'keyword': '5g networks'},\n",
       "  {'keyword': 'network slicing'},\n",
       "  {'keyword': 'intel sgx'},\n",
       "  {'keyword': 'trust'}],\n",
       " 'extractedKeywords': [],\n",
       " 'abstractKeywords': [],\n",
       " 'issn': None,\n",
       " 'isbns': '978-1-6654-0393-1',\n",
       " 'hasFullText': False,\n",
       " 'fullTextPath': None,\n",
       " 'pdfLink': 'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9343054'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "network = open(\"./data/jsons/Network/networks.json\", 'r', encoding='utf-8')\n",
    "microservices = open(\"./data/jsons/Microservices/microservices.json\", 'r', encoding='utf-8')\n",
    "\n",
    "network_documents = json.load(network)\n",
    "microservices_documents = json.load(microservices)\n",
    "\n",
    "network.close()\n",
    "microservices.close()\n",
    "\n",
    "documents = {}\n",
    "documents[\"network\"] = network_documents\n",
    "documents[\"microservices\"] = microservices_documents\n",
    "network_documents[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class BERTComponent:\n",
    "    tokenizer = None\n",
    "    bert_model = None\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.bert_vector_size = 3072\n",
    "        self.sent_vector_size = 768\n",
    "        self.model = model\n",
    "        self.tokenizer = BERTComponent.tokenizer if BERTComponent.tokenizer else BertTokenizer.from_pretrained(model)\n",
    "        BERTComponent.tokenizer = self.tokenizer\n",
    "        self.bert_model = BERTComponent.bert_model if BERTComponent.bert_model else BertModel.from_pretrained(model)\n",
    "        BERTComponent.bert_model = self.bert_model\n",
    "        self.bert_model.eval()\n",
    "\n",
    "\n",
    "    def get_bert_spans(self, words, bert_tokens):\n",
    "        if self.model == 'bert-large-uncased':\n",
    "            words = [self._flat_word(word) for word in words]\n",
    "\n",
    "        i = 0\n",
    "        j = 1\n",
    "        idx = 0\n",
    "\n",
    "        bert_words_indexes = []\n",
    "        bert_words = []\n",
    "        while i < len(words):\n",
    "            word = words[i]\n",
    "\n",
    "            bert_word = bert_tokens[j]\n",
    "            bert_word = bert_word[2:] if bert_word.startswith(\"##\") else bert_word\n",
    "            bert_word = bert_word[idx:]\n",
    "\n",
    "            #Spacing control\n",
    "            if word in [\" \", \"  \", \"   \"]:\n",
    "                bert_words.append([word])\n",
    "                bert_words_indexes.append([-1])\n",
    "\n",
    "            #When the current word is [UNK] for bert\n",
    "            elif bert_word == \"[UNK]\":\n",
    "                bert_words.append([\"[UNK]\"])\n",
    "                bert_words_indexes.append([j])\n",
    "                j += 1\n",
    "                idx = 0\n",
    "\n",
    "            #When the current word is contained in bert token. Very weird\n",
    "            elif len(word) < len(bert_word) and bert_word.find(word) >= 0:\n",
    "                bert_words.append([bert_word])\n",
    "                bert_words_indexes.append([j])\n",
    "\n",
    "                idx = bert_word.find(word) + len(word)\n",
    "                if idx == len(bert_word):\n",
    "                    j += 1\n",
    "                    idx = 0\n",
    "\n",
    "            #Otherwise\n",
    "            else:\n",
    "                k = 0\n",
    "                span = []\n",
    "                span_indexes = []\n",
    "\n",
    "                while k < len(word):\n",
    "                    if word.find(bert_word, k) == k:\n",
    "                        span.append(bert_word)\n",
    "                        span_indexes.append(j)\n",
    "                        k += len(bert_word)\n",
    "                        j += 1\n",
    "                        idx = 0\n",
    "                        bert_word = bert_tokens[j]\n",
    "                        bert_word = bert_word[2:] if bert_word.startswith(\"##\") else bert_word\n",
    "                    else:\n",
    "                        print(\"Error\")\n",
    "                        return bert_words, bert_words_indexes\n",
    "\n",
    "                bert_words.append(span)\n",
    "                bert_words_indexes.append(span_indexes)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        assert len(bert_words_indexes) == len(words)\n",
    "\n",
    "        return bert_words, bert_words_indexes\n",
    "\n",
    "    def _flat_word(self, word):\n",
    "        word = word.lower()\n",
    "        word = word.replace(\"ñ\", \"n\")\n",
    "        word = word.replace(\"á\", \"a\")\n",
    "        word = word.replace(\"é\", \"e\")\n",
    "        word = word.replace(\"í\", \"i\")\n",
    "        word = word.replace(\"ó\", \"o\")\n",
    "        word = word.replace(\"ú\", \"u\")\n",
    "        word = word.replace(\"ä\", \"a\")\n",
    "        word = word.replace(\"ü\", \"u\")\n",
    "        word = word.replace(\"ö\", \"o\")\n",
    "        word = word.replace(\"ū\", \"u\")\n",
    "        word = word.replace(\"ā\", \"a\")\n",
    "        word = word.replace(\"ī\", \"i\")\n",
    "        word = word.replace(\"ș\", \"s\")\n",
    "        word = word.replace(\"ã\", \"a\")\n",
    "        word = word.replace(\"ô\", \"o\")\n",
    "\n",
    "        return word\n",
    "\n",
    "    def _sum_merge(self, vectors):\n",
    "        return torch.sum(torch.stack(vectors), dim=0)\n",
    "\n",
    "    def _mean_merge(self, vectors):\n",
    "        return torch.mean(torch.stack(vectors), dim=0)\n",
    "\n",
    "    def _last_merge(self, vectors):\n",
    "        return vectors[-1]\n",
    "\n",
    "    def _get_merge_tensors(self, token_vec_sums, words_indexes):\n",
    "        pad_tensor = torch.zeros(self.bert_vector_size)\n",
    "        real_vec = []\n",
    "        for word_indexes in words_indexes:\n",
    "            vectors = [(token_vec_sums[idx] if idx != -1 else pad_tensor) for idx in word_indexes]\n",
    "            real_vec.append(self._mean_merge(vectors))\n",
    "\n",
    "        return real_vec\n",
    "\n",
    "    def get_bert_embeddings(self, sentence, spans):\n",
    "        tokenized_sentence = self.tokenizer.tokenize(sentence)\n",
    "        tokenized_sentence = ['[CLS]'] + tokenized_sentence + ['[SEP]']\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "        segments_ids = [1] * len(tokenized_sentence)\n",
    "\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoded_layers = self.bert_model(tokens_tensor, segments_tensors, output_hidden_states=True)\n",
    "\n",
    "        #print(\"This is enconded layers: \", len(encoded_layers.hidden_states))\n",
    "        \n",
    "        token_embeddings = torch.stack(encoded_layers.hidden_states, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "        token_vec_sums = []\n",
    "        for token in token_embeddings:\n",
    "            cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=-1)\n",
    "            token_vec_sums.append(cat_vec)\n",
    "\n",
    "        words = [sentence[beg:end] for (beg, end) in spans]\n",
    "        bert_words, bert_words_indexes = self.get_bert_spans(words, tokenized_sentence)\n",
    "\n",
    "        bert_embeddings = self._get_merge_tensors(token_vec_sums, bert_words_indexes)\n",
    "        sentence_embedding = torch.mean(torch.stack(token_vec_sums), dim=0)\n",
    "        \n",
    "        return bert_embeddings, sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0484,  0.1071, -0.1547,  ...,  0.3583,  0.2960,  0.1679])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer as twt\n",
    "spans = twt().span_tokenize(\"Hi this is a test\")\n",
    "bert = BERTComponent('bert-large-uncased')\n",
    "q_embeddings, q_sent_embedding = bert.get_bert_embeddings(\"Hi this is a test\", spans)\n",
    "q_sent_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0943, -0.1501,  0.1642,  ..., -0.0924,  0.0940,  0.3068])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_spans = twt().span_tokenize(\"Hi this is a documment for test\")\n",
    "doc_bert = BERTComponent('bert-large-uncased')\n",
    "doc_embeddings, doc_sent_embedding = bert.get_bert_embeddings(\"Hi this is a documment for test\", spans)\n",
    "doc_sent_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(728.5737)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = torch.dot(q_sent_embedding, doc_sent_embedding)\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer as twt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DocumentRanker:\n",
    "    \n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        self.bert = BERTComponent('bert-large-uncased')\n",
    "    \n",
    "    def __get_info_rep(self, document):\n",
    "        pass\n",
    "    \n",
    "    def __get_embedding(self, text):\n",
    "        spans = twt().span_tokenize(text)\n",
    "        text_word_embeddings, text_embedding = self.bert.get_bert_embeddings(text, spans)\n",
    "        return text_embedding\n",
    "    \n",
    "    def get_related_documents(self, query, number_of_documents):\n",
    "        index = {}\n",
    "        last = 0\n",
    "        related_documents = []\n",
    "        \n",
    "        q_sent_embedding = self.__get_embedding(query)\n",
    "        \n",
    "        for document in self.documents:\n",
    "            abstract = document['workAbstract']\n",
    "            abstract_embedding = self.__get_embedding(abstract)\n",
    "            index[last] = torch.dot(q_sent_embedding, abstract_embedding)\n",
    "            last += 1\n",
    "        \n",
    "        doc_scores = list(index.items())\n",
    "        doc_scores = [(x[0], x[1].tolist()) for x in doc_scores]\n",
    "        doc_scores= sorted(doc_scores, key = lambda x: x[1], reverse=True)\n",
    "        #print(\"Scores: \", scores)\n",
    "#         probs = F.softmax(scores, dim=0)\n",
    "#         probs = [t.tolist() for t in probs]\n",
    "#         probs.sort(reverse=True)\n",
    "        #print(\"Probs: \", probs)\n",
    "        print(doc_scores)\n",
    "        \n",
    "        if number_of_documents > len(doc_scores):\n",
    "            return doc_scores\n",
    "        else:\n",
    "            return doc_scores[:number_of_documents]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = network_documents[:50]\n",
    "ranker = DocumentRanker(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 749.80615234375), (16, 739.1793823242188), (42, 733.2034301757812), (0, 733.1528930664062), (32, 731.5643310546875), (14, 729.9841918945312), (23, 729.164306640625), (19, 727.5703125), (17, 725.63818359375), (3, 723.998779296875), (4, 722.0805053710938), (12, 722.0667724609375), (8, 721.5213623046875), (38, 718.4793090820312), (7, 717.9136352539062), (36, 717.0819702148438), (44, 716.21533203125), (25, 716.06005859375), (31, 715.6558227539062), (48, 715.2150268554688), (40, 715.1771850585938), (1, 714.4179077148438), (5, 712.82421875), (2, 712.6151123046875), (49, 711.911376953125), (41, 710.5901489257812), (26, 709.453857421875), (45, 709.44384765625), (13, 709.2012939453125), (33, 708.664794921875), (21, 706.9766845703125), (47, 703.2574462890625), (34, 702.8947143554688), (28, 701.8461303710938), (27, 701.268310546875), (6, 700.61767578125), (22, 699.0571899414062), (15, 697.817138671875), (43, 695.0389404296875), (11, 692.7727661132812), (46, 691.4434204101562), (24, 691.3951416015625), (35, 689.8778686523438), (37, 689.5061645507812), (18, 688.0098876953125), (10, 686.43505859375), (30, 682.8336181640625), (29, 681.2613525390625), (20, 672.8800659179688), (39, 667.6593627929688)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(9, 749.80615234375),\n",
       " (16, 739.1793823242188),\n",
       " (42, 733.2034301757812),\n",
       " (0, 733.1528930664062),\n",
       " (32, 731.5643310546875),\n",
       " (14, 729.9841918945312),\n",
       " (23, 729.164306640625),\n",
       " (19, 727.5703125),\n",
       " (17, 725.63818359375),\n",
       " (3, 723.998779296875)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_docs = ranker.get_related_documents(\"Intel SGX based multi-MNO cooperation scheme for trusted, dynamic and efficient network slice sharing in order to support inter-operator trustworthy collaboration\", 10)\n",
    "related_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'doi': '10.1109/NOMS.2016.7502883',\n",
       " 'authors': 'Joris Claassen;Ralph Koning;Paola Grosso',\n",
       " 'documentTitle': 'Linux containers networking: Performance and scalability of kernel modules',\n",
       " 'publicationTitle': 'NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium',\n",
       " 'workAbstract': 'Linux container virtualisation is gaining momentum as lightweight technology to support cloud and distributed computing. Applications relying on container architectures might at times rely on inter-container communication, and container networking solutions are emerging to address this need. Containers can be networked together as part of an overlay network, or with actual links from the container to the network via kernel modules. Most overlay solutions are not quite production ready yet; on the other hand kernel modules that can link a container to the network are much more mature. We benchmarked three kernel modules: veth, macvlan and ipvlan, to quantify their respective raw TCP and UDP performance and scalability. Our results show that the macvlan kernel module outperforms all other solutions in raw performance. All kernel modules seem to provide sufficient scalability to be deployed effectively in multi-containers environments.',\n",
       " 'date': '25-29 April 2016',\n",
       " 'authorKeywords': [],\n",
       " 'extractedKeywords': [],\n",
       " 'abstractKeywords': [],\n",
       " 'issn': None,\n",
       " 'isbns': '978-1-5090-0223-8',\n",
       " 'hasFullText': False,\n",
       " 'fullTextPath': None,\n",
       " 'pdfLink': 'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7502883'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
